{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilienilsson/anaconda3/envs/deep/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "# import getpass\n",
    "import os\n",
    "from datetime import datetime\n",
    "from hashlib import md5\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import tiktoken\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "# from langchain_community.tools import WikipediaQueryRun\n",
    "# from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from pydantic import BaseModel, Field, conlist\n",
    "import openai\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.callbacks.openai_info import OpenAICallbackHandler\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from graphdatascience import GraphDataScience\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"NEO4J_URI\"] = os.getenv('NEO4J_URI')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('NEO4J_USERNAME')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('NEO4J_PASSWORD')\n",
    "os.environ[\"OPENAI_API_KEY\"] =  os.getenv('OPENAI_API_KEY')\n",
    "NEO4j_URI = os.getenv('NEO4J_URI')\n",
    "NEO4j_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4j_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "\n",
    "graph = Neo4jGraph(url = NEO4j_URI, username=NEO4j_USERNAME, password = NEO4j_PASSWORD, refresh_schema=False)\n",
    "\n",
    "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:AtomicFact) REQUIRE c.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:KeyElement) REQUIRE c.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the folder path\n",
    "folder_path = \"data/arxiv_stat_text\"\n",
    "documents_full = []\n",
    "document_names = []\n",
    "\n",
    "# Loop through the files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    paper_path = os.path.join(folder_path, filename)\n",
    "    # Check if it's a file\n",
    "    if os.path.isfile(paper_path):\n",
    "        document_names.append(filename)\n",
    "        documents_full.append(open(paper_path).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_system = \"\"\"\n",
    "You are an intelligent assistant tasked with extracting key elements and atomic facts from scientific articles to construct a knowledge graph. \n",
    "Your main goal is to identify the same concepts under different names.\n",
    "Your output must be structured to identify entities, relationships, and hierarchical connections. \n",
    "\n",
    "\n",
    "### JSON Output Requirements:\n",
    "- Ensure all output is valid JSON.\n",
    "- Each atomic fact must include:\n",
    "  1. `key_elements`: A **list of unique items** (equations, authors, concepts, methods, etc.). **Limit: 1 to 10 items.** Avoid duplicate or malformed items.\n",
    "  2. `atomic_fact`: A **single, clear sentence** following the subject-predicate-object structure.\n",
    "- Ensure that `key_elements` contains no invalid characters (e.g., newlines, excessive commas, or special characters). Remove redundant or irrelevant entries.\n",
    "\n",
    "\n",
    "### KEY Extraction Guidelines:\n",
    "1. **Entities**: Extract and classify essential nouns and phrases into the following categories:\n",
    "   - **Authors**: Names of paper authors.\n",
    "   - **Concepts**: Theories, definitions, or models introduced or discussed.\n",
    "   - **Methods**: Experimental techniques, algorithms, or procedures.\n",
    "   - **Findings**: Results, discoveries, or key conclusions.\n",
    "   - **References**: Other papers, datasets, or sources cited. Full titles of other papers, datasets, or sources cited. Avoid vague references like \"reference [4]\" and replace them with the actual paper title or dataset name whenever possible. If the title is unavailable, include other identifying information (e.g., authors or publication year).\n",
    "   - **Theorems**: Mathematical or logical propositions.\n",
    "   - Cross-Document Entities: Highlight entities (authors, concepts, methods, etc.) that are shared with or similar to those in other documents.\n",
    "   \n",
    "2. **Relationships**: Identify and label verbs or phrases that connect the entities. Examples include:\n",
    "   - **\"Cites\"**: Links an author or paper to a referenced work.\n",
    "   - **\"Proposes\"**: Connects an author or paper to a method or concept.\n",
    "   - **\"Builds Upon\"**: Indicates that a concept/method is an extension of prior work.\n",
    "   - **\"Validates\"**: Links findings to methods or experiments.\n",
    "   - Cross-Document Relationships: Identify relationships that connect entities across multiple documents (e.g., the same concept being expanded upon by different papers).\n",
    "\n",
    "3. **Hierarchical Relationships**: Identify nested structures or dependencies, such as:\n",
    "   - A theorem being part of a model.\n",
    "   - A method comprising multiple steps or components.\n",
    "   - Cross-document hierarchies, such as a concept from one paper being part of a larger framework in another.\n",
    "\n",
    "4. **Atomic Facts**: Extract concise, indivisible facts with clear subject-predicate-object structure. Ensure each atomic fact aligns with the entities and relationships identified.\n",
    "\n",
    "5. **Key Elements**: \n",
    "    - Ensure that there are no redundant or repeated key elements. \n",
    "    - If the same key element appears more than once, only include it once\n",
    "    - If a key element is part of an equation, include the equation.\n",
    "\n",
    "6. **Relevance and Commonality**:\n",
    "   - Prioritize facts and relationships that are repeated across multiple papers.\n",
    "   - Highlight connections between entities that are pivotal or query-worthy.\n",
    "   - Emphasize shared or query-worthy entities and relationships between documents.\n",
    "\n",
    "7. **Connecting Documents**:\n",
    "When identifying entities, relationships, or references, always check for overlaps or connections with other documents (e.g., shared concepts, methods, authors, or references).\n",
    "Explicitly note when:\n",
    "    - An entity or relationship in the current document appears in or is similar to one from another document.\n",
    "    - A finding validates or contrasts a finding from another paper.\n",
    "    - A concept or method builds upon prior work from another document.\n",
    "\n",
    "8. **Additional Guidelines**:\n",
    "   - Replace pronouns with specific nouns (e.g., \"it\" becomes the actual method or concept).\n",
    "   - Include any implicit causal or temporal relationships.\n",
    "   - Limit each `key_elements` list to **10 items maximum**. If there are more than 10 entities, select the most relevant ones.\n",
    "   - Present the key elements and atomic facts in the same language as the original text (e.g., English or Chinese).\n",
    "   - Avoid including vague or redundant entries in `key_elements` such as `,`or `\\n` or `n` or `k`.\n",
    "   - Ensure that atomic facts are distinct and not repeated.\n",
    "\n",
    "Example Output:\n",
    "---\n",
    "**Key Elements**:\n",
    "- Authors: John Doe, Jane Smith\n",
    "- Concepts: Quantum Entanglement, Bell's Theorem\n",
    "- Methods: Double-slit experiment\n",
    "- Findings: Violation of Bell's inequality\n",
    "- References: Paper A (Einstein, 1935)\n",
    "- Equations: ax=b\n",
    "- Cross-Document Entities: Bell's Theorem (also discussed in Paper B), Quantum Entanglement (validated by Paper C)\n",
    "\n",
    "**Atomic Facts**:\n",
    "1. John Doe and Jane Smith authored the paper.\n",
    "2. The paper proposes the concept of Quantum Entanglement.\n",
    "3. Bell's Theorem builds upon Einstein's 1935 work (Paper A).\n",
    "4. The Double-slit experiment validates Quantum Entanglement.\n",
    "5. The findings show a violation of Bell's inequality.\n",
    "6. Cross-Document Fact: Bell's Theorem is cited in Paper B and extended by Paper C.\n",
    "7. Cross-Document Fact: Quantum Entanglement was experimentally validated in Paper C, supporting findings in this paper.\n",
    "\n",
    "---\n",
    "Your output should maintain this structure and be as detailed and accurate as possible.\n",
    "\"\"\"\n",
    "\n",
    "construction_human = \"\"\"Use the given format to extract information from the \n",
    "following input: {input}\"\"\"\n",
    "\n",
    "construction_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            construction_system,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"Use the given format to extract information from the \"\n",
    "                \"following input: {input}\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomicFact(BaseModel):\n",
    "    key_elements: List[str]= Field(description=\"\"\"A list of essential entities (e.g., authors, theories, methods, findings) that are pivotal to the atomic fact.\n",
    "        These entities should align with the ones described in the prompt such as Authors, Concepts, Methods, Findings, References, Theorems, \n",
    "        and Cross-Document Entities. Ensure that the key elements are relevant and comprehensive. \"\"\") #Max length: 500 characters.\"\"\")\n",
    "    atomic_fact: str = Field(description=\"\"\"The smallest, indivisible facts, presented as concise sentences. These include\n",
    "        propositions, theories, existences, concepts, and implicit elements like logic, causality, event\n",
    "        sequences, interpersonal relationships, timelines, etc.\"\"\")\n",
    "\n",
    "class Extraction(BaseModel):\n",
    "    atomic_facts: List[AtomicFact] = Field(description=\"List of atomic facts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/yyxwg9ld1z95p55ths3jt4bw0000gn/T/ipykernel_15739/372851548.py:1: LangChainBetaWarning: Introduced in 0.2.24. API subject to change.\n",
      "  rate_limiter = InMemoryRateLimiter(\n"
     ]
    }
   ],
   "source": [
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.1,  \n",
    "    check_every_n_seconds=0.1,\n",
    "    max_bucket_size=10,  \n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.1, rate_limiter=rate_limiter)\n",
    "structured_llm = model.with_structured_output(Extraction)\n",
    "\n",
    "construction_chain = construction_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_query = \"\"\"\n",
    "MERGE (d:Document {id:$document_name})\n",
    "WITH d\n",
    "UNWIND $data AS row\n",
    "MERGE (c:Chunk {id: row.chunk_id})\n",
    "SET c.text = row.chunk_text,\n",
    "    c.index = row.index,\n",
    "    c.document_name = row.document_name\n",
    "MERGE (d)-[:HAS_CHUNK]->(c)\n",
    "WITH c, row\n",
    "UNWIND row.atomic_facts AS af\n",
    "MERGE (a:AtomicFact {id: af.id})\n",
    "SET a.text = af.atomic_fact\n",
    "MERGE (c)-[:HAS_ATOMIC_FACT]->(a)\n",
    "WITH c, a, af\n",
    "UNWIND af.key_elements AS ke\n",
    "MERGE (k:KeyElement {id: ke})\n",
    "MERGE (a)-[:HAS_KEY_ELEMENT]->(k)\n",
    "\"\"\"\n",
    "\n",
    "def encode_md5(text):\n",
    "    return md5(text.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper used 2k token size\n",
    "async def process_document(text, document_name, chunk_size=2000, chunk_overlap=200):\n",
    "    start = datetime.now()\n",
    "    print(f\"Started extraction at: {start}\")\n",
    "    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_text(text)\n",
    "    print(f\"Total text chunks: {len(texts)}\")\n",
    "    tasks = [\n",
    "        asyncio.create_task(construction_chain.ainvoke({\"input\":chunk_text}))\n",
    "        for index, chunk_text in enumerate(texts)\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    print(f\"Finished LLM extraction after: {datetime.now() - start}\")\n",
    "    docs = [el.dict() for el in results]\n",
    "    for index, doc in enumerate(docs):\n",
    "        doc['chunk_id'] = encode_md5(texts[index])\n",
    "        doc['chunk_text'] = texts[index]\n",
    "        doc['index'] = index\n",
    "        for af in doc[\"atomic_facts\"]:\n",
    "            af[\"id\"] = encode_md5(af[\"atomic_fact\"])\n",
    "    # Import chunks/atomic facts/key elements\n",
    "    graph.query(import_query, \n",
    "            params={\"data\": docs, \"document_name\": document_name})\n",
    "    # Create next relationships between chunks\n",
    "    graph.query(\"\"\"MATCH (c:Chunk)<-[:HAS_CHUNK]-(d:Document)\n",
    "WHERE d.id = $document_name\n",
    "WITH c ORDER BY c.index WITH collect(c) AS nodes\n",
    "UNWIND range(0, size(nodes) -2) AS index\n",
    "WITH nodes[index] AS start, nodes[index + 1] AS end\n",
    "MERGE (start)-[:NEXT]->(end)\n",
    "\"\"\",\n",
    "           params={\"document_name\":document_name})\n",
    "    print(f\"Finished import at: {datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_graph():\n",
    "    query = \"\"\"\n",
    "    MATCH (n)\n",
    "    DETACH DELETE n\n",
    "    \"\"\"\n",
    "    graph.query(query)\n",
    "# clean_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on line = [\"1812.00492v1.pdf.json\", \n",
    "        \"2201.01879v3.pdf.json\", \n",
    "        \"2209.01679v3.pdf.json\",\n",
    "        \"1502.02355v2.pdf.json\",\n",
    "        \"2204.10909v2.pdf.json\",\n",
    "        \"2301.04439v1.pdf.json\"!,\n",
    "        \"1611.04701v2.pdf.json\",\n",
    "        \"1108.1098v1.pdf.json\",\n",
    "        \"1605.04055v1.pdf.json\",\n",
    "        \"1508.02925v1.pdf.json\",\n",
    "        \"1505.04215v1.pdf.json\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "local =[\"1505.04215v1.pdf.json\",\n",
    "        \"2204.10909v2.pdf.json\",\n",
    "        \"2209.01679v3.pdf.json\",\n",
    "        \"1502.02355v2.pdf.json\",\n",
    "        \"1812.00492v1.pdf.json\",\n",
    "        \"1508.02925v1.pdf.json\",\n",
    "        \"1605.04055v1.pdf.json\",\n",
    "        \"1108.1098v1.pdf.json\",\n",
    "        \"1611.04701v2.pdf.json\",\n",
    "        \"2201.01879v3.pdf.json\",\n",
    "        \"1504.05781v2.pdf.json\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = [\"1505.04215v1.pdf.json\", \n",
    "        \"2204.10909v2.pdf.json\",\n",
    "        \"2209.01679v3.pdf.json\",\n",
    "        \"1812.00492v1.pdf.json\",\n",
    "        \"1508.02925v1.pdf.json\",\n",
    "        \"1605.04055v1.pdf.json\",\n",
    "        \"1108.1098v1.pdf.json\",\n",
    "        \"2201.01879v3.pdf.json\",\n",
    "        \"1504.05781v2.pdf.json\",\n",
    "        \"1502.02355v2.pdf.json\",\n",
    "        \"1611.04701v2.pdf.json\"\n",
    "        ]\n",
    "trouble = [\"1502.02355v2.pdf.json\", \"1611.04701v2.pdf.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1502.02355v2.pdf.json\n",
      "Started extraction at: 2024-12-08 23:13:08.109558\n",
      "Total text chunks: 194\n",
      "Finished LLM extraction after: 0:30:46.482860\n",
      "Finished import at: 0:30:46.697878\n",
      "1611.04701v2.pdf.json\n",
      "Started extraction at: 2024-12-08 23:43:54.808008\n",
      "Total text chunks: 309\n",
      "Finished LLM extraction after: 0:51:39.273226\n",
      "Finished import at: 0:51:39.919016\n"
     ]
    }
   ],
   "source": [
    "for text, name in zip(documents_full, document_names):\n",
    "    if name in local and name not in done:\n",
    "    #if name not in done and name not in trouble:\n",
    "        print(name)\n",
    "        await process_document(text, name, chunk_size=500, chunk_overlap=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
