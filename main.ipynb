{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code\n",
    "This file contains the construction of a knowledge graph, entity resolution and evaluation of knowledge graph with RAG. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from datetime import datetime\n",
    "from hashlib import md5\n",
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NEO4J_URI\"] = os.getenv('NEO4J_URI')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('NEO4J_USERNAME')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('NEO4J_PASSWORD')\n",
    "os.environ[\"OPENAI_API_KEY\"] =  os.getenv('OPENAI_API_KEY')\n",
    "NEO4j_URI = os.getenv('NEO4J_URI')\n",
    "NEO4j_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4j_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "graph = Neo4jGraph(url = NEO4j_URI, username=NEO4j_USERNAME, password = NEO4j_PASSWORD, refresh_schema=False)\n",
    "\n",
    "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:AtomicFact) REQUIRE c.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:KeyElement) REQUIRE c.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "folder_path = \"data/arxiv_stat_text\"\n",
    "documents_full = []\n",
    "document_names = []\n",
    "\n",
    "# Loop through the files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    paper_path = os.path.join(folder_path, filename)\n",
    "    # Check if it's a file\n",
    "    if os.path.isfile(paper_path):\n",
    "        document_names.append(filename)\n",
    "        documents_full.append(open(paper_path).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct system\n",
    "\n",
    "Finetuning the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_system = \"\"\"\n",
    "You are an intelligent assistant tasked with extracting key elements and atomic facts from scientific articles to construct a knowledge graph. \n",
    "Your main goal is to identify the same concepts under different names.\n",
    "Your output must be structured to identify entities, relationships, and hierarchical connections. \n",
    "\n",
    "\n",
    "### JSON Output Requirements:\n",
    "- Ensure all output is valid JSON.\n",
    "- Each atomic fact must include:\n",
    "  1. `key_elements`: A **list of unique items** (equations, authors, concepts, methods, etc.). **Limit: 1 to 10 items.** Avoid duplicate or malformed items.\n",
    "  2. `atomic_fact`: A **single, clear sentence** following the subject-predicate-object structure.\n",
    "- Ensure that `key_elements` contains no invalid characters (e.g., newlines, excessive commas, or special characters). Remove redundant or irrelevant entries.\n",
    "\n",
    "\n",
    "### KEY Extraction Guidelines:\n",
    "1. **Entities**: Extract and classify essential nouns and phrases into the following categories:\n",
    "   - **Authors**: Names of paper authors.\n",
    "   - **Concepts**: Theories, definitions, or models introduced or discussed.\n",
    "   - **Methods**: Experimental techniques, algorithms, or procedures.\n",
    "   - **Findings**: Results, discoveries, or key conclusions.\n",
    "   - **References**: Other papers, datasets, or sources cited. Full titles of other papers, datasets, or sources cited. Avoid vague references like \"reference [4]\" and replace them with the actual paper title or dataset name whenever possible. If the title is unavailable, include other identifying information (e.g., authors or publication year).\n",
    "   - **Theorems**: Mathematical or logical propositions.\n",
    "   - Cross-Document Entities: Highlight entities (authors, concepts, methods, etc.) that are shared with or similar to those in other documents.\n",
    "   \n",
    "2. **Relationships**: Identify and label verbs or phrases that connect the entities. Examples include:\n",
    "   - **\"Cites\"**: Links an author or paper to a referenced work.\n",
    "   - **\"Proposes\"**: Connects an author or paper to a method or concept.\n",
    "   - **\"Builds Upon\"**: Indicates that a concept/method is an extension of prior work.\n",
    "   - **\"Validates\"**: Links findings to methods or experiments.\n",
    "   - Cross-Document Relationships: Identify relationships that connect entities across multiple documents (e.g., the same concept being expanded upon by different papers).\n",
    "\n",
    "3. **Hierarchical Relationships**: Identify nested structures or dependencies, such as:\n",
    "   - A theorem being part of a model.\n",
    "   - A method comprising multiple steps or components.\n",
    "   - Cross-document hierarchies, such as a concept from one paper being part of a larger framework in another.\n",
    "\n",
    "4. **Atomic Facts**: Extract concise, indivisible facts with clear subject-predicate-object structure. Ensure each atomic fact aligns with the entities and relationships identified.\n",
    "\n",
    "5. **Key Elements**: \n",
    "    - Ensure that there are no redundant or repeated key elements. \n",
    "    - If the same key element appears more than once, only include it once\n",
    "    - If a key element is part of an equation, include the equation.\n",
    "\n",
    "6. **Relevance and Commonality**:\n",
    "   - Prioritize facts and relationships that are repeated across multiple papers.\n",
    "   - Highlight connections between entities that are pivotal or query-worthy.\n",
    "   - Emphasize shared or query-worthy entities and relationships between documents.\n",
    "\n",
    "7. **Connecting Documents**:\n",
    "When identifying entities, relationships, or references, always check for overlaps or connections with other documents (e.g., shared concepts, methods, authors, or references).\n",
    "Explicitly note when:\n",
    "    - An entity or relationship in the current document appears in or is similar to one from another document.\n",
    "    - A finding validates or contrasts a finding from another paper.\n",
    "    - A concept or method builds upon prior work from another document.\n",
    "\n",
    "8. **Additional Guidelines**:\n",
    "   - Replace pronouns with specific nouns (e.g., \"it\" becomes the actual method or concept).\n",
    "   - Include any implicit causal or temporal relationships.\n",
    "   - Limit each `key_elements` list to **10 items maximum**. If there are more than 10 entities, select the most relevant ones.\n",
    "   - Present the key elements and atomic facts in the same language as the original text (e.g., English or Chinese).\n",
    "   - Avoid including vague or redundant entries in `key_elements` such as `,`or `\\n` or `n` or `k`.\n",
    "   - Ensure that atomic facts are distinct and not repeated.\n",
    "\n",
    "Example Output:\n",
    "---\n",
    "**Key Elements**:\n",
    "- Authors: John Doe, Jane Smith\n",
    "- Concepts: Quantum Entanglement, Bell's Theorem\n",
    "- Methods: Double-slit experiment\n",
    "- Findings: Violation of Bell's inequality\n",
    "- References: Paper A (Einstein, 1935)\n",
    "- Equations: ax=b\n",
    "- Cross-Document Entities: Bell's Theorem (also discussed in Paper B), Quantum Entanglement (validated by Paper C)\n",
    "\n",
    "**Atomic Facts**:\n",
    "1. John Doe and Jane Smith authored the paper.\n",
    "2. The paper proposes the concept of Quantum Entanglement.\n",
    "3. Bell's Theorem builds upon Einstein's 1935 work (Paper A).\n",
    "4. The Double-slit experiment validates Quantum Entanglement.\n",
    "5. The findings show a violation of Bell's inequality.\n",
    "6. Cross-Document Fact: Bell's Theorem is cited in Paper B and extended by Paper C.\n",
    "7. Cross-Document Fact: Quantum Entanglement was experimentally validated in Paper C, supporting findings in this paper.\n",
    "\n",
    "---\n",
    "Your output should maintain this structure and be as detailed and accurate as possible.\n",
    "\"\"\"\n",
    "\n",
    "construction_human = \"\"\"Use the given format to extract information from the \n",
    "following input: {input}\"\"\"\n",
    "\n",
    "construction_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            construction_system,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"Use the given format to extract information from the \"\n",
    "                \"following input: {input}\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomicFact(BaseModel):\n",
    "    key_elements: List[str]= Field(description=\"\"\"A list of essential entities (e.g., authors, theories, methods, findings) that are pivotal to the atomic fact.\n",
    "        These entities should align with the ones described in the prompt such as Authors, Concepts, Methods, Findings, References, Theorems, \n",
    "        and Cross-Document Entities. Ensure that the key elements are relevant and comprehensive. \"\"\") #Max length: 500 characters.\"\"\")\n",
    "    atomic_fact: str = Field(description=\"\"\"The smallest, indivisible facts, presented as concise sentences. These include\n",
    "        propositions, theories, existences, concepts, and implicit elements like logic, causality, event\n",
    "        sequences, interpersonal relationships, timelines, etc.\"\"\")\n",
    "\n",
    "class Extraction(BaseModel):\n",
    "    atomic_facts: List[AtomicFact] = Field(description=\"List of atomic facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.1,  \n",
    "    check_every_n_seconds=0.1,\n",
    "    max_bucket_size=10,  \n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.1, rate_limiter=rate_limiter)\n",
    "structured_llm = model.with_structured_output(Extraction)\n",
    "\n",
    "construction_chain = construction_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_query = \"\"\"\n",
    "MERGE (d:Document {id:$document_name})\n",
    "WITH d\n",
    "UNWIND $data AS row\n",
    "MERGE (c:Chunk {id: row.chunk_id})\n",
    "SET c.text = row.chunk_text,\n",
    "    c.index = row.index,\n",
    "    c.document_name = row.document_name\n",
    "MERGE (d)-[:HAS_CHUNK]->(c)\n",
    "WITH c, row\n",
    "UNWIND row.atomic_facts AS af\n",
    "MERGE (a:AtomicFact {id: af.id})\n",
    "SET a.text = af.atomic_fact\n",
    "MERGE (c)-[:HAS_ATOMIC_FACT]->(a)\n",
    "WITH c, a, af\n",
    "UNWIND af.key_elements AS ke\n",
    "MERGE (k:KeyElement {id: ke})\n",
    "MERGE (a)-[:HAS_KEY_ELEMENT]->(k)\n",
    "\"\"\"\n",
    "\n",
    "def encode_md5(text):\n",
    "    return md5(text.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper used 2k token size\n",
    "async def process_document(text, document_name, chunk_size=2000, chunk_overlap=200):\n",
    "    start = datetime.now()\n",
    "    print(f\"Started extraction at: {start}\")\n",
    "    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_text(text)\n",
    "    print(f\"Total text chunks: {len(texts)}\")\n",
    "    tasks = [\n",
    "        asyncio.create_task(construction_chain.ainvoke({\"input\":chunk_text}))\n",
    "        for index, chunk_text in enumerate(texts)\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    print(f\"Finished LLM extraction after: {datetime.now() - start}\")\n",
    "    docs = [el.dict() for el in results]\n",
    "    for index, doc in enumerate(docs):\n",
    "        doc['chunk_id'] = encode_md5(texts[index])\n",
    "        doc['chunk_text'] = texts[index]\n",
    "        doc['index'] = index\n",
    "        for af in doc[\"atomic_facts\"]:\n",
    "            af[\"id\"] = encode_md5(af[\"atomic_fact\"])\n",
    "    # Import chunks/atomic facts/key elements\n",
    "    graph.query(import_query, \n",
    "            params={\"data\": docs, \"document_name\": document_name})\n",
    "    # Create next relationships between chunks\n",
    "    graph.query(\"\"\"MATCH (c:Chunk)<-[:HAS_CHUNK]-(d:Document)\n",
    "WHERE d.id = $document_name\n",
    "WITH c ORDER BY c.index WITH collect(c) AS nodes\n",
    "UNWIND range(0, size(nodes) -2) AS index\n",
    "WITH nodes[index] AS start, nodes[index + 1] AS end\n",
    "MERGE (start)-[:NEXT]->(end)\n",
    "\"\"\",\n",
    "           params={\"document_name\":document_name})\n",
    "    print(f\"Finished import at: {datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_graph():\n",
    "    query = \"\"\"\n",
    "    MATCH (n)\n",
    "    DETACH DELETE n\n",
    "    \"\"\"\n",
    "    graph.query(query)\n",
    "# clean_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert each paper in KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local =[\"1505.04215v1.pdf.json\",\n",
    "        \"2204.10909v2.pdf.json\",\n",
    "        \"2209.01679v3.pdf.json\",\n",
    "        \"1502.02355v2.pdf.json\",\n",
    "        \"1812.00492v1.pdf.json\",\n",
    "        \"1508.02925v1.pdf.json\",\n",
    "        \"1605.04055v1.pdf.json\",\n",
    "        \"1108.1098v1.pdf.json\",\n",
    "        \"1611.04701v2.pdf.json\",\n",
    "        \"2201.01879v3.pdf.json\",\n",
    "        \"1504.05781v2.pdf.json\"]\n",
    "\n",
    "done = []\n",
    "\n",
    "for text, name in zip(documents_full, document_names):\n",
    "    if name in local and name not in done:\n",
    "        print(name)\n",
    "        await process_document(text, name, chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Optional\n",
    "\n",
    "from retry import retry\n",
    "from tqdm import tqdm\n",
    "\n",
    "from graphdatascience import GraphDataScience\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "\n",
    "#Neo4j connection\n",
    "driver = GraphDatabase.driver(NEO4j_URI, auth=(NEO4j_USERNAME, NEO4j_PASSWORD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for key elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),\n",
    "    url=NEO4j_URI,\n",
    "    username=NEO4j_USERNAME,\n",
    "    password= NEO4j_PASSWORD,\n",
    "    node_label=\"KeyElement\", #[\"Document\",\"Chunk\", \"AtomicFact\", \"KeyElement\"],\n",
    "    text_node_properties=[\"id\"],#[\"id\", \"text\", \"index\"], #['name', 'description', 'status'],\n",
    "    embedding_node_property=\"embedding\", #'embedding'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph data science set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project graph\n",
    "gds = GraphDataScience(\n",
    "    NEO4j_URI,\n",
    "    auth=(NEO4j_USERNAME,NEO4j_PASSWORD)\n",
    ")\n",
    "\n",
    "G, result = gds.graph.project(\n",
    "    \"entities\",                   #  Graph name\n",
    "    \"KeyElement\",                 #  Node projection\n",
    "    \"*\",                          #  Relationship projection\n",
    "    nodeProperties=[\"embedding\"]  #  Configuration parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest neighbors similarity search using the embedding property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.95\n",
    "\n",
    "gds.knn.mutate(\n",
    "  G,\n",
    "  nodeProperties=['embedding'],\n",
    "  mutateRelationshipType= 'SIMILAR',\n",
    "  mutateProperty= 'score',\n",
    "  similarityCutoff=similarity_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.wcc.write(\n",
    "    G,\n",
    "    writeProperty=\"wcc\",\n",
    "    relationshipTypes=[\"SIMILAR\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Duplicate Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_edit_distance = 3\n",
    "potential_duplicate_candidates = graph.query(\n",
    "    \"\"\"MATCH (e:`KeyElement`)\n",
    "    WHERE size(e.id) > 4 // longer than 4 characters\n",
    "    WITH e.wcc AS community, collect(e) AS nodes, count(*) AS count\n",
    "    WHERE count > 1\n",
    "    UNWIND nodes AS node\n",
    "    // Add text distance\n",
    "    WITH distinct\n",
    "      [n IN nodes WHERE apoc.text.distance(toLower(node.id), toLower(n.id)) < $distance | n.id] AS intermediate_results\n",
    "    WHERE size(intermediate_results) > 1\n",
    "    WITH collect(intermediate_results) AS results\n",
    "    // combine groups together if they share elements\n",
    "    UNWIND range(0, size(results)-1, 1) as index\n",
    "    WITH results, index, results[index] as result\n",
    "    WITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n",
    "            CASE WHEN index <> index2 AND\n",
    "                size(apoc.coll.intersection(acc, results[index2])) > 0\n",
    "                THEN apoc.coll.union(acc, results[index2])\n",
    "                ELSE acc\n",
    "            END\n",
    "    )) as combinedResult\n",
    "    WITH distinct(combinedResult) as combinedResult\n",
    "    // extra filtering\n",
    "    WITH collect(combinedResult) as allCombinedResults\n",
    "    UNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\n",
    "    WITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\n",
    "    WHERE NOT any(x IN range(0,size(allCombinedResults)-1,1)\n",
    "        WHERE x <> combinedResultIndex\n",
    "        AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n",
    "    )\n",
    "    RETURN combinedResult\n",
    "    \"\"\", params={'distance': word_edit_distance})\n",
    "potential_duplicate_candidates[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Duplicate Entities with LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a data processing assistant. Your task is to identify duplicate entities in a list and decide which of them should be merged.\n",
    "The entities might be slightly different in format or content, but essentially refer to the same thing. Use your analytical skills to determine duplicates.\n",
    "\n",
    "Here are the rules for identifying duplicates:\n",
    "1. Entities with minor typographical differences should be considered duplicates.\n",
    "2. Refrain from merging entities based on equations, as they may have different meanings.\n",
    "3. Entities that refer to the same real-world object or concept, even if described differently, should be considered duplicates.\n",
    "4. If it refers to different numbers, dates, or products, do not merge results\n",
    "5. Mathematical equations must be identical to be considered duplicates.\n",
    "\"\"\"\n",
    "user_template = \"\"\"\n",
    "Here is the list of entities to process:\n",
    "{entities}\n",
    "\n",
    "Please identify duplicates, merge them, and provide the merged list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateEntities(BaseModel):\n",
    "    entities: List[str] = Field(\n",
    "        description=\"Entities that represent the same object or real-world entity and should be merged\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Disambiguate(BaseModel):\n",
    "    merge_entities: Optional[List[DuplicateEntities]] = Field(\n",
    "        description=\"Lists of entities that represent the same object or real-world entity and should be merged\"\n",
    "    )\n",
    "\n",
    "extraction_llm = ChatOpenAI(model_name=\"gpt-4o-mini\").with_structured_output(\n",
    "    Disambiguate\n",
    ")\n",
    "\n",
    "extraction_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            user_template,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = extraction_prompt | extraction_llm\n",
    "\n",
    "@retry(tries=3, delay=2)\n",
    "def entity_resolution(entities: List[str]) -> Optional[List[str]]:\n",
    "    return [\n",
    "        el.entities\n",
    "        for el in extraction_chain.invoke({\"entities\": entities}).merge_entities\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 4\n",
    "\n",
    "merged_entities = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submitting all tasks and creating a list of future objects\n",
    "    futures = [\n",
    "        executor.submit(entity_resolution, el['combinedResult'])\n",
    "        for el in potential_duplicate_candidates\n",
    "    ]\n",
    "\n",
    "    for future in tqdm(\n",
    "        as_completed(futures), total=len(futures), desc=\"Processing documents\"\n",
    "    ):\n",
    "        try:\n",
    "            to_merge = future.result()\n",
    "            # Ensure to_merge is not None and is iterable\n",
    "            if to_merge:  # this checks if it's not None or an empty value\n",
    "                merged_entities.extend(to_merge)\n",
    "        except Exception as e:\n",
    "            # Handle any exception raised during the processing of a future\n",
    "            print(f\"Error processing future: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge entities in the graph using query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break down merged entities into chunks of 100\n",
    "merged_entities_chunks = [merged_entities[i:i + 100] for i in range(0, len(merged_entities), 100)]\n",
    "\n",
    "\n",
    "for i, chunk in enumerate(merged_entities_chunks):\n",
    "  try:\n",
    "    graph.query(\"\"\"\n",
    "    UNWIND $data AS candidates\n",
    "    CALL {\n",
    "      WITH candidates\n",
    "      MATCH (e:KeyElement) WHERE e.id IN candidates\n",
    "      RETURN collect(e) AS nodes\n",
    "    }\n",
    "    CALL apoc.refactor.mergeNodes(nodes, {properties: {\n",
    "        description:'combine',\n",
    "        `.*`: 'discard'\n",
    "    }})\n",
    "    YIELD node\n",
    "    RETURN count(*)\n",
    "    \"\"\", params={\"data\": chunk})\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    print(\"failed to merge chunk\", i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\"\"\"MATCH (e) RETURN count(e)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\"\"\"MATCH (e:KeyElement) RETURN count(e)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.graph.export(G, dbName = \"thisisauniquename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.graph.get('entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertebaht/Desktop/DTU/9_semester/02456 Deep learning/02456-deeplearning-project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_core.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to query a graph database.\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use the embedding property for KeyElements\n",
    "Only look for terms in KeyElements nodes\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "Schema:\n",
    "{schema}\n",
    "Note: Respond only with the Cypher query.\n",
    "\n",
    "The question is:\n",
    "{question} \n",
    "\n",
    "Important: In the generated Cypher query, look for terms in keyelements when using WHERE. \n",
    "In the generated Cypher query, the RETURN statement must explicitly include the property values used in the query's filtering condition but never the embedding property, alongside the main information requested from the original question. \n",
    "Do not return anything related to chunks node types. When using atomic facts the text property is important.\n",
    "\n",
    "\"\"\"\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_prompt = CYPHER_GENERATION_PROMPT,\n",
    "    cypher_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\"),\n",
    "    qa_llm = ChatOpenAI(temperature=0,model_name=\"gpt-4o-mini\"), graph=graph, verbose=True, allow_dangerous_requests=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for Search agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_chain.invoke(\n",
    "    {\"query\": \"Which document has most connections to other documents?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cypher_chain.invoke({\"query\": \"List `Theorem`s in the papers\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cypher_chain.invoke({\"query\": \"Explain the matematical detail of OLS\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cypher_chain.invoke({\"query\": \"Which paper uses WIDEHIST algorithm?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
