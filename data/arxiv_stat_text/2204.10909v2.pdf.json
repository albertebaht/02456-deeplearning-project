{"text": "Proceedings of Machine Learning Research vol 145:1\u201323, 2022 3rd Annual Conference on Mathematical and Scienti\ufb01c Machine Learning\nError-in-variables modelling for operator learning\nRavi G. Patel\nRGPATEL@SANDIA.GOV\nCenter for Computing Research, Sandia National Laboratories\nIndu Manickam\nIMANICK@SANDIA.GOV\nApplied Information Sciences, Sandia National Laboratories\nMyoungkyu Lee\nMKLEE@UA.EDU\nAerospace Engineering and Mechanics, University of Alabama\nMamikon Gulian\nMGULIAN@SANDIA.GOV\nQuantitative Modeling and Analysis, Sandia National Laboratories\nEditors: Bin Dong, Qianxiao Li, Lei Wang, Zhi-Qin John Xu\nAbstract\nDeep operator learning has emerged as a promising tool for reduced-order modelling and PDE\nmodel discovery. Leveraging the expressive power of deep neural networks, especially in high\ndimensions, such methods learn the mapping between functional state variables. While proposed\nmethods have assumed noise only in the dependent variables, experimental and numerical data for\noperator learning typically exhibit noise in the independent variables as well, since both variables\nrepresent signals that are subject to measurement error. In regression on scalar data, failure to\naccount for noisy independent variables can lead to biased parameter estimates. With noisy inde-\npendent variables, linear models \ufb01tted via ordinary least squares (OLS) will show attenuation bias,\nwherein the slope will be underestimated. In this work, we derive an analogue of attenuation bias\nfor linear operator regression with white noise in both the independent and dependent variables,\nshowing that the norm upper bound of the operator learned via OLS decreases with increasing noise\nin the independent variable. In the nonlinear setting, we computationally demonstrate underpredic-\ntion of the action of the Burgers operator in the presence of noise in the independent variable. We\npropose error-in-variables (EiV) models for two operator regression methods, MOR-Physics and\nDeepONet, and demonstrate that these new models reduce bias in the presence of noisy independent\nvariables for a variety of operator learning problems. Considering the Burgers operator in 1D and\n2D, we demonstrate that EiV operator learning robustly recovers operators in high-noise regimes\nthat defeat OLS operator learning. We also introduce an EiV model for time-evolving PDE discov-\nery and show that OLS and EiV perform similarly in learning the Kuramoto-Sivashinsky evolution\noperator from corrupted data, suggesting that the effect of bias in OLS operator learning depends\non the regularity of the target operator.\nKeywords: operator learning, error-in-variables, PDE discovery, deep learning\n1. Introduction\nOperator regression, or operator learning, has emerged as an important \ufb01eld in scienti\ufb01c computing\nthat focuses on the \ufb02exible and expressive discovery of partial differential equations (PDEs) and\nrelated models. These methods \ufb01t parameterizations of operators, i.e., mappings between functions,\nusing observations of the input and output of the operators. Recently, operator regression methods\nhave begun to incorporate techniques from machine learning such as deep neural networks and\nGaussian processes, opening the way for novel applications in scienti\ufb01c computing challenges, such\nas data-driven model discovery (Trask et al., 2019), surrogate models of data-to-solution maps (You\n\u00a9 2022 R.G. Patel, I. Manickam, M. Lee & M. Gulian.\narXiv:2204.10909v2  [cs.LG]  19 Jul 2022\nPATEL MANICKAM LEE GULIAN\net al., 2022; Cai et al., 2021), and closure models in \ufb02uid \ufb02ow (Duraisamy et al., 2019). However,\ncurrent operator regression methods based on deep learning assume that the independent variable,\ni.e. the input to the operator, is free of noise. The objective of this work is to explore the bias\nin models learned from noisy independent variables and to propose a correction, applicable to a\nwide variety of operator regression architectures, based on error-in-variables methods in classical\nstatistics.\nFitting parameterized operators to observations is a classical problem and has been approached,\ne.g., with Bayesian methods (Pang et al., 2017; Stuart, 2010; Trillos and Sanz-Alonso, 2017),\nPDE-constrained optimization (D\u2019Elia and Gunzburger, 2016; Burkovska et al., 2021), and physics-\ninformed Gaussian processes (Gulian et al., 2019; Raissi et al., 2017). Earlier works leveraged sig-\nni\ufb01cant prior knowledge of physics and \ufb01tted small numbers of physically interpretable parameters.\nIn contrast, recent methods based on high-dimensional deep neural network (DNN) parameteriza-\ntion of operators have established deep operator learning as a widely applicable and domain-agnostic\narea of scienti\ufb01c computing. Deep operator learning methods vary based on the discretizations of\nfunction spaces inherent in each method as well as the architecture and training of the DNN. They\ninclude modal methods (Patel and Desjardins, 2018; Patel et al., 2021; Qin et al., 2019; Li et al.,\n2021), graph based methods (Anandkumar et al., 2020; Li et al., 2020), PCA based methods (Bhat-\ntacharya et al., 2020), meshless methods (Trask et al., 2019), trunk-branch based methods (Lu et al.,\n2019; Cai et al., 2021), and time-stepping methods (You et al., 2021; Long et al., 2018; Qin et al.,\n2019). Such methods can be purely data-driven or incorporate knowledge from physics (Wang et al.,\n2021), and can be utilized as general operator surrogates or be speci\ufb01cally for PDE model discov-\nery (Patel et al., 2021). Similar techniques in the computer science and statistics literature have\nbeen described as function-to-function regression within the \ufb01eld of functional data analysis (FDA)\n(Silverman and Ramsay, 2001) but applied outside the PDE context. These methods include repro-\nducing kernel Hilbert space approaches (Yuan and Cai, 2010; Kadri et al., 2016), additive models\n(Kim et al., 2018), wavelet-based approaches (Meyer et al., 2015), and neural network models (Rao\nand Reimherr, 2021; Kou et al., 2019). See Morris (2015) for a broad overview of the topic.\nThe effect of noisy inputs in the training and test data for deep operator regression methods is\nrelatively unexplored, despite being widely studied in the related context of adversarial examples in\ndeep learning (Szegedy et al., 2013; Yoshida and Miyato, 2017). In single and multivariate statistics,\nerror in the independent variables leads to inconsistent parameter estimates without proper error\nmodelling. In particular, linear models \ufb01t via ordinary least squares (OLS) to scalar data {(x+\u03f5, y)}\nwith error in the independent variable persistently underpredict the slope as\nm\u2217= m\nvar(x)\nvar(x) + var(\u03f5),\n(1)\nwhere m\u2217is the predicted slope, m is the true slope, x is the independent variable, and \u03f5 is the error\nin x (Hutcheon et al., 2010). This phenomenon is known as attenuation bias and methods such\nas total least squares (Markovsky and Van Huffel, 2007), and Deming regression (Linnet, 1993)\nhave been developed to counteract it. More generally, including in nonlinear contexts, an error-in-\nvariables (EiV) model is required to correct for inconsistency arising from error in the independent\nvariables. Typically, EiV models are highly specialized to speci\ufb01c problems. See Zwanzig (2000);\nChen et al. (2011) for overviews of this topic.\nFor multivariate linear regression, the method of Xu and You (2007) can recover models given\nindependent variables corrupted by additive noise. The Compensated Matrix Uncertainty selector\n2\nERROR-IN-VARIABLES MODELLING FOR OPERATOR LEARNING\nmethod (Rosenbaum and Tsybakov, 2013) has been developed to recover over-parameterized mod-\nels under the assumption that the parameter vector is sparse and the variance of the noise can be\nestimated, and was shown to compensate for missing data. The method of Loh and Wainwright\n(2011) additionally has convergence guarantees for the optimization problem used to infer the pa-\nrameters and can operate on data corrupted with multiplicative noise.\nIn contrast to single and multivariate data, where one often has more control over the error in\nthe independent variables, e.g. the placement of sensors, functional data originate from signals\nwhich are generally noisy, particularly when measured in extreme environments or at high sampling\nrates. This makes it essential that operator regression methods be robust to noise in both the input\n(independent) and output (dependent) variables. Error-in-variables models have previously been\ndeveloped for linear function-to-function regression (Chakraborty and Panaretos, 2017; Chen et al.,\n2022). However, to the best of the authors\u2019 knowledge, there is no error-in-variables model available\nfor any operator regression or nonlinear function-to-function regression methods.\nIn this work, we derive a generalization to (1) for OLS inference of linear operators and demon-\nstrate computationally that the action of OLS learned operators underpredict the action of the true\noperators. Under the assumption that the underlying functional data is smooth but corrupted by\nwhite noise, we propose EiV models for the nonlinear operator regression methods, MOR-Physics\n(Patel and Desjardins, 2018; Patel et al., 2021) and DeepONet (Lu et al., 2019). We demonstrate\nthat these models can correct for the bias introduced by noisy independent variables for several PDE\nlearning problems.\n2. Operator regression for noisy input data\nMany operator regression methods seek to infer an operator by solving an ordinary least squares\nproblem,\nL = argmin\n\u02c6L\nE\n(u,v)\n\u0014\r\r\r \u02c6Lu \u2212v\n\r\r\r\n2\nV\n\u0015\n,\n(2)\nwhere L is potentially nonlinear, (u, v) \u2208U \u00d7V represent the input and output of the target operator\nand the expectation E is over a distribution of functions that is appropriate for a given application,\nincluding any stochastic variables meant to model measurement noise. The speci\ufb01cation of the\nBanach spaces U and V and the distribution inherent to the expectation (2) of input functions is a\nmajor theoretical challenge. The distribution weights a subset within the function space U where\nthe learned operator is expected to provide an accurate surrogate of the target operator, as well as\nmeasurement noise. In practice, it assumed that a \ufb01nite sample of training functions is available\nwhich represents this theoretical distribution, and that these functions are discretized in some way\nso that they can be represented as \ufb01nite-dimensional vectors,\nu \u2208Rd1,\nv \u2208Rd2,\n(3)\nIt is also assumed that the operator L is discretized in a consistent way,\nL : Rd1 \u2192Rd2,\n(4)\nand that \u2225\u00b7 \u2225V = \u2225\u00b7 \u2225\u21132(Rd2) for ease of computation.\n3\nPATEL MANICKAM LEE GULIAN\nWe consider the case when the observations (u, v) originate from pairs (\u02c6u, \u02c6v) corrupted by\nindependent white noise. With the discretization speci\ufb01ed by (3) and (4), and the analogous dis-\ncretization of (\u02c6u, \u02c6v), we assume that we are given N observations such that\nui = \u02c6ui + \u03f5i\nu,\nvi = \u02c6vi + \u03f5i\nv,\ni = 1, 2, ..., N,\n\u03f5i\nu \u223cN(0, \u03c32\nuI),\n\u03f5i\nv \u223cN(0, \u03c32\nvI),\n(5)\nwhere N(\u00b5, \u03a3) is the multivariate normal distribution with mean \u00b5 and covariance \u03a3. We seek an\noperator L such that\nL(\u02c6u) \u2248\u02c6v\nfor pairs of functions (\u02c6u, \u02c6v) in a given sample set (the test set). The discretized ordinary least\nsquares problem from (2) for operator regression using this data is to minimize the loss\nJ = 1\nN\nN\nX\ni=1\n\u2225Lui \u2212vi\u22252\n\u21132(Rd2).\n(6)\nWe stress that the objective is to infer the mapping from noiseless input to noiseless output, having\naccess only to noisy observations of input and output data for training.\n2.1. Operator regression methods\nIn this section, we summarize the two operator regression methods examined in our experiments.\n2.1.1. MOR-PHYSICS\nThe MOR-Physics operator regression method (Patel and Desjardins, 2018; Patel et al., 2021) uses\nthe following operator parameterization,\nL(u) =\nNo\nX\ni\nF\u22121gi(\u03ba)Fhi(u),\nwhere F is the Fourier transform, gi and hi are neural networks, \u03ba is the wavenumber, and No\nis a hyperparameter. Here, hi(u) is applied pointwise, such that, hi(u(x)) = (hi \u25e6u)(x). The\noptimization over L in (2) is replaced here with optimization over the neural network parameters.\nFor all numerical studies, we select the network width = 5, depth = 5, and take the activation\nfunction to be the exponential linear unit (ELU) (Clevert et al., 2015) for both hi and gi.\n2.1.2. DEEPONET\nThe DeepONet operator regression method (Lu et al., 2019) includes a branch network, b, that takes\nas input u and a trunk network, t, that takes as input x, a single grid point on the output function v. In\nthis work, we use the unstacked variant of DeepONet with the following operator parameterization\nto approximate v at a single grid point x,\nL(u, x) =\np\nX\nk=1\nbk(u)tk(x) + b0,\n4\nERROR-IN-VARIABLES MODELLING FOR OPERATOR LEARNING\nwhere p is the network width and b0 is an additional bias term.\nFor all computational studies involving DeepONet, we use fully connected networks with tanh\nactivation functions for both the trunk and branch. We chose depths of 3 and 2 for the trunk and\nbranch, respectively and 70 for the network width, p.\n2.2. Bias in operators inferred from Ordinary Least-Squares (OLS) operator regression\nThe presence of noise given by (5) when minimizing a loss function as in (2) leads to systematic\nbias in the prediction of L, in a way that is analogous to the attenuation bias described previously.\nWe generate pairs of noisy functions,\n(ui, vi) = (\u02c6ui + \u03f5i\nu, \u2202x\u02c6ui2 + \u03f5i\nv),\nwhere ui and vi are generated as described in Section 5, and attempt to infer the Burgers operator\nLu = \u2202xu2 using the MOR-Physics operator regression method, described in Section 2.1.1. Fig-\nure 1 demonstrates the bias in the learned operator by plotting the action on a test function; the\npredicted output function exhibits \u21132-norm smaller than the true output function. This is typical of\nthe output on test functions, suggesting that the operator norm of L itself is attenuated, i.e., that the\nOLS learned operator \u201cunderpredicts\u201d the action of the true Burgers operator. In Table 1, we com-\npute the maximum and average norms for the action of the OLS learned and true Burgers operator\nover a set of 1000 test functions in the unit ball of \u21132; note that the former approximates the operator\nnorm of the learned L over the sample space. These statistics strongly suggest that the operator\nnorm is biased to zero.\nOLS operator\nTrue operator\nmax \u2225Lu\u2225\u21132(Rd2)\n4.42\n12.45\naverage \u2225Lu\u2225\u21132(Rd2)\n2.40\n7.75\nTable 1: Operator norm statistics for true Burgers operator and OLS inferred operator, computed\nover 1000 samples with ||u||\u21132(Rd1) = 1.\nIn addition to the bias induced by noise in the independent variable, a natural question is whether\nover\ufb01tting by the DNN is a cause of attenuation bias. In Figure 2, we again attempt to recover the\nBurgers operator but include regularization techniques for neural networks to prevent over\ufb01tting,\ni.e., weight decay (Krogh and Hertz, 1991) and dropout (Srivastava et al., 2014). For various penal-\nties and dropout rates, the OLS learned operators still underpredict the action of the true Burgers\noperator. This suggests that the observed bias is not a consequence of over\ufb01tting, but a result of in-\nadequate error modelling. The hyperparameters used in these studies are available in Appendix B.\nThe following theorem shows that the bias observed in the examples above can be expected for\nlinear operators. It extends the classical result on attenuation bias to the case of a linear operator\nlearned by OLS. We obtain the theorem for an idealized case of in\ufb01nitely many \ufb01nite-dimensional\nsamples. That is, we assume a discretization of the form (3), but for in\ufb01nitely many pairs, consistent\nwith the implicit distribution of samples. Then, rather than the fully discrete loss (6), the theorem is\n5\nPATEL MANICKAM LEE GULIAN\n\u22120.25\n0.00\n0.25\n0.50\nudata\nutest\n0\n2\n4\n6\nx\n\u22122\n\u22121\n0\n1\n2\nvdata\n0\n2\n4\n6\nx\nvtest\nFigure 1: Operator learned from noisy independent variables with OLS underpredicts the action of\nthe true operator. (Top left) sample of training u (\n) and underlying smooth function \u02c6u\n(\n). (Bottom left) sample of training v (\n) and underlying smooth function \u02c6v = \u2202x\u02c6u2\n(\n). (Top right) smooth test function. (Bottom right) action of true Burgers operator on\nsmooth test function (\n) and action of OLS learned operator (\n).\n6\nERROR-IN-VARIABLES MODELLING FOR OPERATOR LEARNING\n0\n2\n4\n6\nx\n\u22121\n0\n1\nvtest\nWeight decay\n0\n2\n4\n6\nx\nDropout\nFigure 2: Standard neural network regularization methods do not reduce bias in operators learned\nfrom noisy independent variables. Action of true Burgers operator (\n). (Left) Weight\ndecay with penalties, \u03bb = 0.01 (\n), \u03bb = 0.1 (\n), and \u03bb = 1 (\n). (Right) Dropout\nwith dropout rates, r = 0.01 (\n), r = 0.05 (\n), and r = 0.1 (\n).\nstated for a loss of the form\nJ = E\n\u0014\r\r\r \u02c6Lu \u2212v\n\r\r\r\n2\n\u21132(Rd2)\n\u0015\n,\nu \u2208Rd1, v \u2208Rd2.\n(7)\nThe fully discrete loss given in (6) converges to J as the sample size increases. The following\ntheorem is proven in Appendix A.\nTheorem 1 The minimizer to (7) among linear \u02c6L is\nL = E\nh\n\u02c6v\u02c6u\u22a4i \u0010\nE\nh\n\u02c6u\u02c6u\u22a4i\n+ \u03c32\nuI\n\u0011\u22121\n.\nThe norm upperbound for this operator is\n\u2225L\u2225\u2264\n\u2225E\n\u0002\u02c6v\u02c6u\u22a4\u0003\n\u2225\n\u2225E [\u02c6u\u02c6u\u22a4] + \u03c32uI\u2225.\nThis norm upperbound decreases with increasing levels of noise, \u03c3u, suggesting attenuation bias for\nthe OLS inferred operator with large noise in u. Theorem 1 should be compared to the attenuation\nbias result for classical regression (1). The signi\ufb01cance of this result is that while zero-bias noise in\nthe dependent variable v can be compensated for by a large sample size, the presence of seemingly\ninnocuous zero-bias noise in the independent variable u persists in biasing the optimal solution of\nthe ordinary least squares problem (7) itself. The numerical results shown in Figure 1 and Table\n1 above suggests similar behavior for nonlinear operator regression as well. This necessitates the\nerror-in-variables model we propose below, which corrects the bias in the predicted operator.\n7\nPATEL MANICKAM LEE GULIAN\n3. Error-in-variables (EiV) model\nInstead of applying OLS to infer the operator, we model the error for both u and v with the joint\nprobability density function (PDF),\n\u0014 \u02dcui \u2212ui\nvi \u2212L\u02dcui\n\u0015\n\u223cN\n\u0012\n0,\n\u0014\u03c32\nuI\n\u03c32\nvI\n\u0015\u0013\nwhere \u02dcu is a \u201cdenoised\u201d version of u that ideally approximates the true underlying, noiseless func-\ntion, \u02c6u. L, along with \u02dcu, \u03c3u, \u03c3v, can be recovered via maximum likelihood estimation (MLE),\nL, \u02dcu, \u03c3u, \u03c3v = argmax\nL,\u02dcu,\u03c3u,\u03c3v\nY\ni\np\n\u0012\u0014 \u02dcui \u2212ui\nvi \u2212L\u02dcui\n\u0015\u0013\n,\n(8)\nwhere p(\u00b7) refers to the probability density of the indicated random variable. Real-world functional\ndata is often smooth. Therefore, we simplify recovery of \u02dcu via a low-pass \ufb01lter, \u02dcu = Gu, and\noptimize over G instead of \u02dcu. We select for G, the smooth spectral \ufb01lter (Boyd, 1996),\nG(u) = F\u22121erfc(a(\u03ba \u2212\u03bac))Fu\nwhere F, a, \u03ba, and \u03bac are the Fourier transform, the \ufb01lter bandwidth, the wavenumber, and the\ncutoff wavenumber, respectively. Since u is assumed to be smooth and all examples in Section 5 are\nperformed on the periodic domain, this spectral \ufb01lter as a simple and ef\ufb01cient denoising method.\nThe optimization over \u02dcu = Gu in (8) is replaced with optimization over the two \ufb01lter parameters.\nIn our numerical studies, we have observed that priors on the \ufb01lter parameters can improve\nrobustness. Ideally, we would like as weak a prior as possible on \u03bac, since we do not a priori know\nthe smoothness of u. A uniform prior would be the weakest, but its PDF is discontinuous near the\nboundaries of the support. Instead, we use the Beta distribution (Murphy, 2013) as a prior on \u03bac,\n\u03bac/\u03b2\u03bac \u223cBeta(1 + \u03b5, 1 + \u03b5).\nwhere \u03b2\u03bac is a hyperparameter that speci\ufb01es the maximum allowable \u03bac. With this prior, we perform\nmaximum a posteriori (MAP) estimation to obtain L,\nL, \u03bac, a, \u03c3u, \u03c3v =\nargmax\nL,\u03bac,a,\u03c3u,\u03c3v\nY\ni\np\n\u0012\u0014 Gui \u2212ui\nvi \u2212LGui\n\u0015\u0013\np(\u03bac/\u03b2\u03bac).\n(9)\nFor small \u03b5, Beta is a smooth approximation to the uniform distribution, and therefore MAP is\namenable to gradient descent based algorithms. This prior forces our EiV model to select a smaller\n\u03bac so that Gui is smooth. For our numerical studies, we use \u03b5 = 0.01.\n4. Operator regression with error-in-variables model for time-evolving systems\nIn addition to the inference problem discussed in the previous sections, we are also interested in\ninferring time-evolving PDE\u2019s from noise corrupted solutions. Given a solution, u(x, t) = \u02c6u(x, t)+\n\u03f5(x, t), we seek to recover a PDE in the following form,\n\u2202tu = L(u),\nx \u2208\u2126\nu(x, 0) = u0(x)\nBu = 0,\nx \u2208\u2202\u2126\n8\nERROR-IN-VARIABLES MODELLING FOR OPERATOR LEARNING\nWe discretize this PDE in time with forward Euler, obtaining,\nun+1 = un + \u2206tL(un) = P(un)\nn = 1, 2, . . . , \u02c6Nt,\nThis temporal discretization results in a ResNet-type architecture which has been effectively used\nto parameterize and discover governing evolution equations (Haber and Ruthotto, 2017; Long et al.,\n2018; Patel and Desjardins, 2018; Qin et al., 2019). We still parameterize L with either MOR-\nPhysics or DeepONet. However, we can now ideally model the error in u over all time steps with\nthe following joint PDF,\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nGu0 \u2212u0\nPGu0 \u2212u1\n...\nP \u02c6\nNtGu0 \u2212un\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\u223cN\n\u00000, \u03c32\nuI\n\u0001\n.\nNote that we have assumed the error is spatially and temporally independent and identical dis-\ntributed (i.i.d.). The validity of this assumption depends on the data and the physical system under\ninvestigation. Due to the computational complexity of evaluating probability densities from the\nabove distribution, we instead consider the following marginalized distribution as a simpli\ufb01cation,\n\u0014\nGu0 \u2212u0\nPNtGu0 \u2212uNt\n\u0015\n\u223cN\n\u00000, \u03c32\nuI\n\u0001\n,\nwhere Nt is a hyperparameter indicating a maximum timestep.\n5. Results\nIn the following sections, we compare OLS and EiV operator regression using both MOR-Physics\nand DeepONet. We perform all test cases on the periodic domain, \u2126= [0, L]d, where d is the spatial\ndimension of the problem. For each test, we generate samples of smooth functions, \u02c6ui by applying\na low pass \ufb01lter to samples of white noise. For time independent problems, we then apply the a\npriori known operator to \u02c6ui and obtain \u02c6vi.\nWe next corrupt the data by adding white noise and obtain ui and vi. To standardize the amount\nof noise added to \u02c6u and \u02c6v, we tune the standard deviation of the white noise to target speci\ufb01c\nsignal-to-noise ratios (SNR\u2019s), expressed in decibels (dB) as,\nSNR = 10 log10\n\u0012RMS(u)\n\u03c3u\n\u00132\n,\nand likewise for v. For all test cases, we target the same SNR for both u and v. With this noisy data,\nwe attempt to infer the operator or PDE. Finally we evaluate our inference by generating noiseless\ntest data, utest using the same method discussed above, and comparing the action of the inferred\noperator to the action of the true operator on utest.\nFor time evolving problems, we numerically integrate the a priori known PDE, using \u02c6ui\n0 = \u02c6ui\nas the initial condition, obtaining \u02c6ui\nn for all n timesteps. We next corrupt this data with white noise\nas discussed above, obtaining un, from which we attempt to infer an evolution PDE. We evaluate\nour inference by generating a noiseless initial condition u0 = utest, evolving the inferred PDE, and\ncomparing to the evolution of the true PDE using the same utest as an initial condition.\n9\nPATEL MANICKAM LEE GULIAN\n0\n1\nSNR = 8\nSamples from ui with decreasing SNR\n\u22121\n0\nSNR = 4\n\u22121\n0\n1\nSNR = 0\n0.0\n2.5\n5.0\nx\n\u22121\n0\n1\nSNR = \u22124\n\u22122\n\u22121\n0\n1\n2\nOLS\nAction from operators on smooth test v\n0\n1\n2\n3\n4\n5\n6\nx\n\u22122\n\u22121\n0\n1\n2\nEiV\nFigure 3: EiV model improves recovery of true Burgers operator in the presence of noisy indepen-\ndent variables. (Left) Underlying smooth function \u02c6u (\n) and training u for SNR = 8\n(\n), SNR = 4 (\n), SNR = 0 (\n), and SNR = \u22124 (\n). (Right) Action of true\nBurgers operator (\n) on noiseless test utest and action of learned operators from data\nwith decreasing SNR for OLS (Top right) and EiV (Bottom right).\nTo perform the optimization in (8) and (9), we use the stochastic gradient based optimizer,\nADAM (Kingma and Ba, 2014). Batch sizes, learning rates, sample sizes, and other hyperparame-\nters for all numerical studies are listed in Appendix B.\n5.1. Learning operators with the MOR-Physics EiV model\nIn these sections, we compare the OLS and EiV models using the MOR-Physics operator parame-\nterization discussed in Section 2.1.1.\n5.1.1. RECOVERING THE BURGERS OPERATOR\nWe consider the learning problem discussed in Figure 1 where we attempt to recover the Burgers\noperator, Lu = \u2202xu2. In Figure 3, we apply OLS and EiV inferred error models to this dataset,\nrecover operators, compute the action of these operators on a noiseless test function, utest, and\ncompare to the action of the true operator, vtest. We observe that the OLS model recovered oper-\nators only reproduce the action of the true Burgers operator for the highest SNR examined, while\nunderpredicting the action for lower SNR\u2019s. In contrast, the EiV model is able to recover operators\n10\nERROR-IN-VARIABLES MODELLING FOR OPERATOR LEARNING\ndown to SNR = 4 with high accuracy. However, for lower SNR\u2019s the EiV model also fails to\nrecover a suitable operator.\n\u22121\n0\n1\nGu\nMLE, SNR =8\n0\n5\nx\n\u22121\n0\n1\nGu\nMLE, SNR =0\nMLE, SNR =4\n0\n5\nx\nMLE, SNR =-4\nMAP, SNR =8\n0\n5\nx\nMAP, SNR =0\nMAP, SNR =4\n0\n5\nx\nMAP, SNR =-4\nFigure 4: Effect of cutoff wavenumber prior on \ufb01lter for EiV model. (Left) Action of MLE estimate\nof \ufb01lters on noisy ui (\n) for decreasing SNR and corresponding noiseless \u02c6ui (\n).\n(Right) Action of MAP estimate of \ufb01lters (\u03bac prior) on ui with hyperparameters, \u03b2\u03bac = 10\n(\n), \u03b2\u03bac = 20(\n), \u03b2\u03bac = 40(\n), and \u03b2\u03bac = 80(\n).\nOn the left four subplots of Figure 4, we examine the action of the \ufb01lter G on samples of\nnoisy input functions, ui, with decreasing SNR. For the high SNR data, the EiV model successfully\nidenti\ufb01es \ufb01lter parameters that smooth the input functions. However, for low SNR data, the EiV\nmodel selects \ufb01lters that fail to smooth the input functions, suggesting the method \ufb01nds high \u03bac.\n5.1.2. IMPROVED OPERATOR LEARNING WITH SMOOTHNESS PRIOR\nThe results in Figure 3 can be improved by adding priors on the \ufb01lter parameters as discussed in\nSection 3 to constrain \u03bac to smaller values and enforce smoothness on Gui. In Figure 5, we plot\nthe action of the learned operators with priors on \u03bac and four different hyperparameters, \u03b2\u03bac. We\n\ufb01nd, for a wide range of \u03b2\u03bac, that the prior enables successful recovery of the Burgers operator even\nfor the lowest SNR. However for the largest \u03b2\u03bac = 80, our EiV model can no longer consistently\nrecover the Burgers operator. Notably, these results are relatively insensitive to \u03b2\u03bac, so careful\nhyperparameter selection is not necessary.\nOn the right four subplots of Figure 4, we examine the action of the \ufb01lter G on sample noisy\ninput functions, ui, with decreasing SNR and the four \u03b2\u03bac hyperparameters. We \ufb01nd that the MAP\nestimate produces an operative \ufb01lter that smooths ui, even for the lowest SNR.\n5.1.3. LEARNING THE 2D BURGERS OPERATOR\nIn this section, we attempt to recover a 2D generalization of the Burgers operator (Mohamed, 2019),\nL = \u2202xu2 + \u2202yu2. Figure 6 compares the OLS and EiV learned operators for decreasing SNR. For\n11\nPATEL MANICKAM LEE GULIAN\n\u22122\n\u22121\n0\n1\n2\nvtest\n\u03b2\u03bac = 10\n\u03b2\u03bac = 20\n0\n2\n4\n6\nx\n\u22122\n\u22121\n0\n1\n2\nvtest\n\u03b2\u03bac = 40\n0\n2\n4\n6\nx\n\u03b2\u03bac = 80\nFigure 5: Cutoff wavenumber prior improves EiV model. Action of EiV operator on utest learned\nfrom SNR = 8 (\n), SNR = 4 (\n), SNR = 0 (\n), and SNR = \u22124 (\n) for\nvarious \u03b2\u03bac. Action of true operator (\n).\nthe EiV model, we included the \u03bac prior with \u03b2\u03bac = 10. As in the 1D Burgers operator test, we\n\ufb01nd that action of the OLS learned operator severely underpredicts the action of the true operator,\nespecially at very low SNR. The EiV learned operator, however, captured the action of the true\noperator with SNR down to \u22124. Even with lower SNR\u2019s, the EiV operator did not underpredict the\ntrue operator as severely as the OLS operator.\n5.1.4. LEARNING THE KURAMOTO\u2013SIVASHINSKY EQUATION\nIn this section we attempt to recover the Kuramoto\u2013Sivashinsky (KS) equation,\n\u2202tu + 0.5\u2202xu2 + \u22022\nxu + \u22024\nxu = 0,\nusing the method outlined in Section 4. To generate the data, we solve KS equation using Pseu-\ndospectral methods. We use Orszag\u2019s 3/2 zero-padding technique to eliminate aliases from the\nFourier transform of u2, which is computed in the (x,t) domain (Orszag, 1971). We use the low-\nstorage Runge-Kutta method for temporal discretization (Spalart et al., 1991) which has been widely\nused in the direct numerical simulation of turbulent \ufb02ows (Hoyas and Jim\u00b4enez, 2006; Lee and\nMoser, 2015). It treats the nonlinear term explicitly with third-order accuracy. Linear terms are\nimplicitly treated, similarly to the Crank-Nicolson method, with second-order accuracy.\nIn Figure 7, we infer the equation using the OLS and EiV models for increasing \ufb01nal timestep,\nNt, and found that both performed similarly. For this study, we used SNR = 4. Qualitatively,\nneither model was able to capture the dynamics of the KS equation with Nt = 2 and Nt = 4, but\nboth models were able to capture the dynamics with Nt = 8.\n12\nERROR-IN-VARIABLES MODELLING FOR OPERATOR LEARNING\nx\ny\nutest\nx\ny\nvtest\n\u22121\n0\n1\n\u22120.2\n0.0\n0.2\n0.4\nOLS, SNR = 0\nOLS, SNR = \u22128\nOLS, SNR = \u22124\nOLS, SNR = \u221212\nEiV, SNR = 0\nEiV, SNR = \u22128\nEiV, SNR = \u22124\nEiV, SNR = \u221212\nFigure 6: EiV model improves recovery of 2D Burgers operator from noisy data. (Top) Noiseless\ntest data, utest, and action of true operator, vtest. Action of OLS (Bottom left) and EiV\n(Bottom right) learned operators on utest for decreasing SNR.\nAs a chaotic system, the KS equation can be characterized by computing Lyapunov exponents as\noutlined in Edson et al. (2019). In Figure 7, we also compute the \ufb01rst four Lyapunov exponents for\nthe true and learned equations. As observed in the qualitative comparison, OLS and EiV perform\nsimilarly, only successfully recovering the true KS equation for Nt = 8. This is in contrast to\nregressing the Burgers operator, in which EiV outperformed OLS. We hypothesize that is due to\nthe difference in regularity of the target operators; the Burgers operator, a differential operator,\nreduces the regularity of the input function, while the KS evolution operator possesses smoothing\nproperties (Collet et al., 1993). As a result, one can expect the operator norm over most subspaces\nof sample functions to be much higher for the Burgers operator than for the KS evolution operator,\nrendering the OLS attenuation bias more signi\ufb01cant in regressing the former. We plan to explore in\nmore detail the advantages of EiV over OLS in time dependent PDE learning and time independent\noperator learning in future work.\n5.2. Learning the Burgers operator with DeepONet EiV model\nIn this section, we examine EiV operator learning for the 1D Burgers operator using the DeepONet\nmethod outlined in Section 2.1.2. In Figure 8, we compute the \u21132 error between the actions of\nOLS and EiV learned operators on test functions and the true actions. As in MOR-Physics, the\n13\nPATEL MANICKAM LEE GULIAN\nt\nx\nutest\n1\n2\n3\n4\ni\n0.00\n0.25\n\u03bbi\nLyapunov exponents\n\u22122.5\n0.0\n2.5\nOLS, Nt = 2\nEiV, Nt = 2\nOLS, Nt = 4\nEiV, Nt = 4\nOLS, Nt = 8\nEiV, Nt = 8\nFigure 7: OLS and EiV models perform similarly for KS equation inference. (Top left) Noiseless\ntest data, utest. (Bottom left) OLS and (Bottom right) EiV inferred operators for increas-\ning hyperparameter, Nt. (Top right) Lyapunov exponents for true equation (\n); OLS\nequation with Nt = 2 (\n), Nt = 4 (\n), Nt = 8 (\n); and EiV equation with\nNt = 2 (\n), Nt = 4 (\n), Nt = 8 (\n).\nEiV learned operator robustly learns from noisy data, even at very low SNR. Finally, we examine\nthe effect of training set size on EiV learning in Figure 9. We \ufb01nd that for very low samples, EiV\nfails to recover the Burgers operator to high accuracy. With larger numbers of samples, EiV can\nsuccessfully recover the operator.\nTo test the robustness of our EiV model, we next consider alternative distributions from which to\nsample the smooth functions u. To create the dataset we repeat the procedure discussed in Section 5,\nexcept instead of low pass \ufb01ltering white noise to generate u, we apply one of the following Fourier\nkernels to white noise,\nK(\u03ba) = erfc(\u03ba \u22126)\nK(\u03ba) = 1\n2\u03ba2e\u2212\u03ba/2\nK(\u03ba) = \u03ba \u22121\n25 e\u2212(\u03ba\u22121)2/50\nwhere the second and third functions are PDFs of the \u03c72 distribution with 6 degrees of freedom and\nthe Rayleigh distribution with location parameter = \u22121 and scale parameter = 5. With the transfer\n14\nERROR-IN-VARIABLES MODELLING FOR OPERATOR LEARNING\n0\n20\n40\nSNR (dB)\n10\n1\n100\nRelative 2 Error\n0\n200\n400\nx\n0.25\n0.00\n0.25\nSNR = 47 dB\n0\n200\n400\n0.25\n0.00\n0.25\nSNR = 27 dB\n0\n200\n400\n0.25\n0.00\n0.25\nSNR = 7 dB\nFigure 8: EiV inference performs better than OLS inference for low SNR in DeepONet. (Left)\nRelative \u21132 error vs. SNR for OLS ( ) and EiV ( ). (Right) Action of true (\n), OLS\n(\n), and EiV (\n) operators on utest for increasing SNR. Error bars indicate 25th and\n75th percentile over 100 test samples.\n81\n82\n83\n84\nNumber of Training Samples\n10\n1\n100\nRelative 2 Error\n0\n200\n400\nx\n0.25\n0.00\n0.25\n4096 Samples\n0\n200\n400\n0.25\n0.00\n0.25\n512 Samples\n0\n200\n400\n0.25\n0.00\n0.25\n64 Samples\nFigure 9: EiV inference improves with training set size in DeepONet. (Left) Relative \u21132 error vs.\ntraining set size for EiV ( ). (Right) Action of true (\n) and EiV (\n) operators on utest\nfor increasing training set size. Results computed for SNR = 17. Error bars indicate 25th\nand 75th percentile over 100 test samples.\nfunction the smooth input functions are computed as\nu(x) = FK(\u03ba)e\u2212jR(\u03ba)\nR(\u03ba) \u223cU[0, 2\u03c0]\n15\nPATEL MANICKAM LEE GULIAN\nwhere U is the uniform distribution and j is the imaginary number. These transfer functions are\nchosen such that u is smooth and has no high frequency content. We \ufb01nd in Figure 10 that DeepONet\nwith our EiV model is able to successfully recover the Burgers operator regardless of the distribution\nused to generate u.\n0\n100\n200\nk\n0.0\n0.1\nRayleigh\n0\n100\n200\n0.0\n0.1\nChi-squared\n0\n100\n200\n0\n2\nErfc\n0\n200\n400\nx\n0.1\n0.0\n0.1\n0\n200\n400\n0.1\n0.0\n0.1\n0\n200\n400\n0.25\n0.00\n0.25\nFigure 10: Effect of spectral \ufb01lter used to generate input signals. (Left) Frequency content of utest.\n(Right) Action of true (\n) and DeepONet EiV (\n) operators on utest.\n6. Conclusion\nWe have demonstrated that operator regression performed by minimizing a least-squares loss is\nprone to attenuation bias when the input, or independent variable, is corrupted by white noise. This\nis supported both by an upper bound on the operator norm for linear operator regression using ordi-\nnary least squares (OLS) as well as numerical results for nonlinear operator regression for problems\nsuch as recovering the Burgers operator from noisy observations. We proposed an Error-in-Variables\n(EiV) model for operator regression that replaces the least-squares loss with maximum likelihood\nestimation or maximum a posteriori estimation with an appropriate smoothness prior. The EiV\nmethod is applicable to a wide variety of existing operator regression methods; we demonstrate this\nby combining it with the MOR-physics method of Patel and Desjardins (2018) and the DeepONet\nmethod of Lu et al. (2019). For these methods, the EiV model signi\ufb01cantly improves the opera-\ntor prediction \ufb01delity for a variety of problems given noise in the input variable, as demonstrated\nfor the Burgers equation in one and two dimensions. Finally, we introduce an EiV model for time\nevolving PDEs and show OLS and EiV perform similarly in recovering the Kuramoto-Sivashinky\nequation. This suggests that the regularity properties of the target operator effect the severity of OLS\nattenuation bias in practice. For many physical systems, the white noise assumption in this work\nmay be invalid. Future work may consider other types of error, such as multiplicative noise and\nspatially correlated noise. We may also examine alternative means of denoising, e.g., total varia-\ntion denoising (Vogel and Oman, 1996), and alternative operator learning frameworks, e.g., wavelet\nneural operators (Gupta et al., 2021). Additionally, while we have solely focused on obtaining MAP\n16\nERROR-IN-VARIABLES MODELLING FOR OPERATOR LEARNING\nestimates for operators, particularly in small data regimes, the posterior distributions may hold other\nlikely operators. Future work may focus on examining this posterior distribution of operators.\nAcknowledgments\nSandia National Laboratories is a multimission laboratory managed and operated by National Tech-\nnology and Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell Inter-\nnational, Inc., for the U.S. Department of Energy\u2019s National Nuclear Security Administration under\ncontract DE-NA0003525. This paper describes objective technical results and analysis. Any sub-\njective views or opinions that might be expressed in the paper do not necessarily represent the views\nof the U.S. Department of Energy or the United States Government.\nThe work of R. Patel, I. Manickam, and M. Gulian has also been supported by the U.S. De-\npartment of Energy, Of\ufb01ce of Advanced Scienti\ufb01c Computing Research under the Collaboratory on\nMathematics and Physics-Informed Learning Machines for Multiscale and Multiphysics Problems\n(PhILMs) project. SAND Number: SAND2022-5179 C.\nReferences\nAnima Anandkumar, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Nikola Kovachki, Zongyi\nLi, Burigede Liu, and Andrew Stuart. Neural operator: Graph kernel network for partial differ-\nential equations. In ICLR 2020 Workshop on Integration of Deep Neural Models and Differential\nEquations, 2020.\nKaushik Bhattacharya, Bamdad Hosseini, Nikola B Kovachki, and Andrew M Stuart. Model reduc-\ntion and neural networks for parametric PDEs. arXiv preprint arXiv:2005.03180, 2020.\nJP Boyd. The erfc-log \ufb01lter and the asymptotics of the euler and vandeven sequence accelerations.\nIn Proceedings of the Third International Conference on Spectral and High Order Methods, pages\n267\u2013276, 1996.\nOlena Burkovska, Christian Glusa, and Marta D\u2019Elia. An optimization-based approach to parameter\nlearning for fractional type nonlocal models. Computers & Mathematics with Applications, 2021.\nShengze Cai, Zhicheng Wang, Lu Lu, Tamer A Zaki, and George Em Karniadakis. DeepM&Mnet:\nInferring the electroconvection multiphysics \ufb01elds based on operator approximation by neural\nnetworks. Journal of Computational Physics, 436:110296, 2021.\nAnirvan Chakraborty and Victor M Panaretos.\nRegression with genuinely functional errors-in-\ncovariates. arXiv preprint arXiv:1712.04290, 2017.\nCheng Chen, Shaojun Guo, and Xinghao Qiao. Functional linear regression: dependence and error\ncontamination. Journal of Business & Economic Statistics, 40(1):444\u2013457, 2022.\nXiaohong Chen, Han Hong, and Denis Nekipelov. Nonlinear models of measurement errors. Jour-\nnal of Economic Literature, 49(4):901\u2013937, 2011.\nDjork-Arn\u00b4e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network\nlearning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.\n17\nPATEL MANICKAM LEE GULIAN\nPierre Collet, J-P Eckmann, Henri Epstein, and Joachim Stubbe. Analyticity for the Kuramoto-\nSivashinsky equation. Physica D: Nonlinear Phenomena, 67(4):321\u2013326, 1993.\nKarthik Duraisamy, Gianluca Iaccarino, and Heng Xiao. Turbulence modeling in the age of data.\nAnnual Review of Fluid Mechanics, 51:357\u2013377, 2019.\nMarta D\u2019Elia and Max Gunzburger. Identi\ufb01cation of the diffusion parameter in nonlocal steady\ndiffusion problems. Applied Mathematics & Optimization, 73(2):227\u2013249, 2016.\nRussell A. Edson, J. E. Bunder, Trent W. Mattner, and A. J. Roberts. Lyapunov exponents of the\nKuramoto\u2013Sivashinsky PDE. The ANZIAM Journal, 61(3):270\u2013285, 2019.\nMamikon Gulian, Maziar Raissi, Paris Perdikaris, and George Karniadakis. Machine learning of\nspace-fractional differential equations. SIAM Journal on Scienti\ufb01c Computing, 41(4):A2485\u2013\nA2509, 2019.\nGaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differ-\nential equations. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors,\nAdvances in Neural Information Processing Systems, 2021.\nEldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse Problems,\n34(1):014004, dec 2017.\nSergio Hoyas and Javier Jim\u00b4enez. Scaling of the velocity \ufb02uctuations in turbulent channels up to Re\n= 2003. Physics of Fluids, 18:011702, 2006.\nJennifer A. Hutcheon, Arnaud Chiolero, and James A. Hanley. Random measurement error and\nregression dilution bias. BMJ (Online), 340, 2010.\nHachem Kadri, Emmanuel Du\ufb02os, Philippe Preux, St\u00b4ephane Canu, Alain Rakotomamonjy, and\nJulien Audiffren. Operator-valued kernels for learning from functional response data. Journal of\nMachine Learning Research, 17(20):1\u201354, 2016.\nJanet S. Kim, Ana-Maria Staicu, Arnab Maity, Raymond J. Carroll, and David Ruppert. Addi-\ntive function-on-function regression. Journal of Computational and Graphical Statistics, 27(1):\n234\u2013244, 2018.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nConnie Khor Li Kou, Hwee Kuan Lee, and Teck Khim Ng. A compact network learning model for\ndistribution regression. Neural Networks, 110:199\u2013212, 2019.\nAnders Krogh and John Hertz. A simple weight decay can improve generalization. Advances in\nneural information processing systems, 4, 1991.\nMyoungkyu Lee and Robert D. Moser. Direct numerical simulation of turbulent channel \ufb02ow up to\nRe = 5200. Journal of Fluid Mechanics, 774:395\u2013415, 7 2015.\n18\nERROR-IN-VARIABLES MODELLING FOR OPERATOR LEARNING\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, An-\ndrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differ-\nential equations. arXiv preprint arXiv:2003.03485, 2020.\nZongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhat-\ntacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial\ndifferential equations. In International Conference on Learning Representations, 2021.\nKristian Linnet. Evaluation of regression procedures for methods comparison studies. Clinical\nchemistry, 39(3):424\u2013432, 1993.\nPo-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing data:\nProvable guarantees with non-convexity. Advances in Neural Information Processing Systems,\n24, 2011.\nZichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. PDE-net: Learning PDEs from data. In\nJennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on\nMachine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3208\u20133216.\nPMLR, 10\u201315 Jul 2018.\nLu Lu, Pengzhan Jin, and George Em Karniadakis. DeepONet: Learning nonlinear operators for\nidentifying differential equations based on the universal approximation theorem of operators.\narXiv preprint arXiv:1910.03193, 2019.\nIvan Markovsky and Sabine Van Huffel. Overview of total least-squares methods. Signal Pro-\ncessing, 87(10):2283\u20132302, 2007. Special Section: Total Least Squares and Errors-in-Variables\nModeling.\nMark J Meyer, Brent A Coull, Francesco Versace, Paul Cinciripini, and Jeffrey S Morris. Bayesian\nfunction-on-function regression for multilevel functional data. Biometrics, 71(3):563\u2013574, 2015.\nNorhan A. Mohamed. Solving one- and two-dimensional unsteady Burgers\u2019 equation using fully\nimplicit \ufb01nite difference schemes. Arab Journal of Basic and Applied Sciences, 26(1):254\u2013268,\n2019.\nJeffrey S. Morris. Functional regression. Annual Review of Statistics and Its Application, 2(1):\n321\u2013359, 2015.\nKevin P. Murphy. Machine Learning : A Probabilistic Perspective. MIT Press, 2013.\nSteven A. Orszag. On the elimination of aliasing in \ufb01nite-difference schemes by \ufb01ltering high-\nwavenumber components. Journal of the Atmospheric sciences, 28:1074, 1971.\nGuofei Pang, Paris Perdikaris, Wei Cai, and George Em Karniadakis. Discovering variable frac-\ntional orders of advection\u2013dispersion equations from \ufb01eld data using multi-\ufb01delity Bayesian op-\ntimization. Journal of Computational Physics, 348:694\u2013714, 2017.\nRavi G. Patel and Olivier Desjardins. Nonlinear integro-differential operator regression with neural\nnetworks. arXiv preprint arXiv:1810.08552, 2018.\n19\nPATEL MANICKAM LEE GULIAN\nRavi G Patel, Nathaniel A Trask, Mitchell A Wood, and Eric C Cyr. A physics-informed opera-\ntor regression framework for extracting data-driven continuum models. Computer Methods in\nApplied Mechanics and Engineering, 373:113500, 2021.\nTong Qin, Kailiang Wu, and Dongbin Xiu. Data driven governing equations approximation using\ndeep neural networks. Journal of Computational Physics, 395:620\u2013635, 2019.\nMaziar Raissi, Paris Perdikaris, and George Em Karniadakis. Machine learning of linear differential\nequations using Gaussian processes. Journal of Computational Physics, 348:683\u2013693, 2017.\nAniruddha Rajendra Rao and Matthew Reimherr. Modern non-linear function-on-function regres-\nsion. arXiv preprint arXiv:2107.14151, 2021.\nMathieu Rosenbaum and Alexandre B Tsybakov. Improved matrix uncertainty selector. In From\nProbability to Statistics and Back: High-Dimensional Models and Processes\u2013A Festschrift in\nHonor of Jon A. Wellner, pages 276\u2013290. Institute of Mathematical Statistics, 2013.\nBernard Walter Silverman and James O Ramsay. Functional data analysis. In International Ency-\nclopedia of the Social and Behavioral Sciences. Amsterdam: Elsevier, 2001.\nPhilippe R. Spalart, Robert D. Moser, and Michael M. Rogers. Spectral methods for the navier-\nstokes equations with one in\ufb01nite and two periodic directions. Journal of Computational Physics,\n96:297\u2013324, 10 1991.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from over\ufb01tting. The Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.\nAndrew M Stuart. Inverse problems: a Bayesian perspective. Acta Numerica, 19:451\u2013559, 2010.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,\nand Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\nNathaniel Trask, Ravi G. Patel, Ben J. Gross, and Paul J. Atzberger. GMLS-Nets: A framework for\nlearning from unstructured data. arXiv preprint arXiv:1909.05371, 2019.\nNicolas Garcia Trillos and Daniel Sanz-Alonso. The Bayesian formulation and well-posedness of\nfractional elliptic inverse problems. Inverse Problems, 33(6):065006, 2017.\nCurtis R Vogel and Mary E Oman. Iterative methods for total variation denoising. SIAM Journal\non Scienti\ufb01c Computing, 17(1):227\u2013238, 1996.\nSifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric par-\ntial differential equations with physics-informed DeepONets. Science advances, 7(40):eabi8605,\n2021.\nQinfeng Xu and Jinhong You. Covariate selection for linear errors-in-variables regression models.\nCommunications in Statistics\u2014Theory and Methods, 36(2):375\u2013386, 2007.\nYuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability\nof deep learning. arXiv preprint arXiv:1705.10941, 2017.\n20\nERROR-IN-VARIABLES MODELLING FOR OPERATOR LEARNING\nHuaiqian You, Yue Yu, Nathaniel Trask, Mamikon Gulian, and Marta D\u2019Elia. Data-driven learning\nof nonlocal physics from high-\ufb01delity synthetic data. Computer Methods in Applied Mechanics\nand Engineering, 374:113553, Feb 2021.\nHuaiqian You, Yue Yu, Marta D\u2019Elia, Tian Gao, and Stewart Silling.\nNonlocal Kernel Net-\nwork (NKN): a stable and resolution-independent deep neural network.\narXiv preprint\narXiv:2201.02217, 2022.\nMing Yuan and T. Tony Cai. A reproducing kernel Hilbert space approach to functional linear\nregression. The Annals of Statistics, 38(6):3412 \u2013 3444, 2010.\nSilvelyn Zwanzig. Estimation in nonlinear functional error-in-variables models. PhD thesis, Uni-\nversit\u00a8at Hamburg, 2000.\nAppendix A. Proof of Theorem 1\nWe write the objective function in (7) as\nJ = E\nh\n(Lu \u2212v)\u22a4(Lu \u2212v)\ni\n= E\nh\n(u\u22a4L\u22a4\u2212v\u22a4)(Lu \u2212v)\ni\n= E\nh\nu\u22a4L\u22a4Lu \u2212v\u22a4Lu \u2212u\u22a4L\u22a4v + v\u22a4v\ni\n= E\nh\nu\u22a4L\u22a4Lu \u22122v\u22a4Lu + v\u22a4v\ni\n.\nUsing linearity of expectation, this can be written as a sum of three terms, namely\nI = E\nh\nu\u22a4L\u22a4Lu\ni\n= E\nh\n(\u02c6u + \u03f5u)\u22a4L\u22a4L(\u02c6u + \u03f5u)\ni\n= E\nh\n(\u02c6u\u22a4+ \u03f5\u22a4\nu )L\u22a4L(\u02c6u + \u03f5u)\ni\n= E\nh\n\u02c6u\u22a4L\u22a4L\u02c6u + \u02c6u\u22a4L\u22a4L\u03f5u + \u03f5\u22a4\nu L\u22a4L\u02c6u + \u03f5\u22a4\nu L\u22a4L\u03f5u\ni\n= E\nh\n\u02c6u\u22a4L\u22a4L\u02c6u + 2\u03f5\u22a4\nu L\u22a4L\u02c6u + \u03f5\u22a4\nu L\u22a4L\u03f5u\ni\n= E\nh\n\u02c6u\u22a4L\u22a4L\u02c6u\ni\n+ E\nh\n\u03f5\u22a4\nu L\u22a4L\u03f5u\ni\n;\nII = \u22122E\nh\nv\u22a4Lu\ni\n= \u22122E\nh\n(\u02c6v + \u03f5v)\u22a4L(\u02c6u + \u03f5u)\ni\n= \u22122E\nh\n\u02c6v\u22a4L\u02c6u + \u02c6v\u22a4L\u03f5u + \u03f5\u22a4\nv L\u02c6u + \u03f5\u22a4\nv L\u03f5u\ni\n= \u22122\n\u0010\nE\nh\n\u02c6v\u22a4L\u02c6u\ni\n+ \u02c6v\u22a4L [E\u03f5u] +\nh\nE\u03f5\u22a4\nv\ni\nL\u02c6u +\nh\nE\u03f5\u22a4\nv\ni\nL [E\u03f5u]\n\u0011\n= \u22122E\nh\n\u02c6v\u22a4L\u02c6u\ni\n;\n21\nPATEL MANICKAM LEE GULIAN\nand III = E\n\u0002\nv\u22a4v\n\u0003\n. While \u2202III/\u2202L = 0, we have\n\u2202I\n\u2202L = E\n\u0014 \u2202\n\u2202L \u02c6u\u22a4L\u22a4L\u02c6u\n\u0015\n+ E\n\u0014 \u2202\n\u2202L\u03f5\u22a4\nu L\u22a4L\u03f5u\n\u0015\n= E\nh\n2L\u02c6u\u02c6u\u22a4i\n+ E\nh\n2L\u03f5u\u03f5\u22a4\nu\ni\n= 2LE\nh\n\u02c6u\u02c6u\u22a4i\n+ 2LE\nh\n\u03f5u\u03f5\u22a4\nu\ni\n= 2L\n\u0010\nE\nh\n\u02c6u\u02c6u\u22a4i\n+ \u03c32\nuI\n\u0011\n,\nand\n\u2202II\n\u2202L = \u22122E\n\u0014 \u2202\n\u2202L \u02c6v\u22a4L\u02c6u\n\u0015\n= \u22122E\nh\n\u02c6v\u02c6u\u22a4i\n.\nSetting the derivative of the objective function equal to zero, we obtain\nL\n\u0010\nE\nh\n\u02c6u\u02c6u\u22a4i\n+ \u03c32\nuI\n\u0011\n= E\nh\n\u02c6v\u02c6u\u22a4i\nwhich has solution\nL = E\nh\n\u02c6v\u02c6u\u22a4i \u0010\nE\nh\n\u02c6u\u02c6u\u22a4i\n+ \u03c32\nuI\n\u0011\u22121\n.\n22\nERROR-IN-VARIABLES MODELLING FOR OPERATOR LEARNING\nAppendix B. Hyperparameters\nThe following tables lists the hyperparameters used in our numerical examples. The multiple learn-\ning rates indicate that we reinitialized the Adam optimizer during training and lowered the learning\nrate.\nFigures 1, 2, 3, 4, and 5\nFigure 6\nFigure 7\nFigures 8 and 9\nFigure 10\nDomain size, L\n2\u03c0\n2\u03c0 \u00d7 2\u03c0\n30\u03c0\n512\n512\nGrid size\n128\n128 \u00d7 128\n256\n512\n512\nTraining set size\n64\n64\n128\n1500\n3000\nBatch size\n4\n4\n32\n1500\n3000\nLearning rate 1\n1E-3\n1E-3\n1E-3\n1e-4\n1e-3\nLearning rate 2\n1E-4\n1E-4\n1E-3\n5e-5\n3e-4\nLearning rate 3\n1E-5\nN/A\n1E-3\n1e-5\n5e-4\nEpochs\n400\n200\n1\n1.5E6\n1.5E6\nNumber of operators, No\n1\n1\n2\nN/A\nN/A\nTable 2: Hyperparameters used for all computational studies.\n23\n"}