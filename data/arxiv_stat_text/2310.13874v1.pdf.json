{"text": "arXiv:2310.13874v1  [stat.ME]  21 Oct 2023\nA Linear Errors-in-Variables Model with Unknown\nHeteroscedastic Measurement Errors\nLinh H. Nghiem\u22171 and Cornelis J. Potgieter2\n1University of Sydney\n2Texas Christian University & University of Johannesburg\nAbstract: In the classic measurement error framework, covariates are contami-\nnated by independent additive noise. This paper considers parameter estimation\nin such a linear errors-in-variables model where the unknown measurement error\ndistribution is heteroscedastic across observations. We propose a new general-\nized method of moment (GMM) estimator that combines a moment correction\napproach and a phase function-based approach. The former requires distributions\nto have four \ufb01nite moments, while the latter relies on covariates having asymmet-\nric distributions. The new estimator is shown to be consistent and asymptotically\nnormal under appropriate regularity conditions. The asymptotic covariance of\nthe estimator is derived, and the estimated standard error is computed using a\nfast bootstrap procedure. The GMM estimator is demonstrated to have strong\n\ufb01nite sample performance in numerical studies, especially when the measurement\nerrors follow non-Gaussian distributions.\nKey words: Asymmetric Distributions; Bootstrap; Generalized Method of Mo-\nments; Nutrition; Phase Function; Variance Heterogeneity.\n\u2217Corresponding author: linh.nghiem@sydney.edu.au\n1\n1.\nIntroduction\nThe errors-in-variables linear model arises when certain covariates su\ufb00er from measurement\nerror contamination. This can stem from sources like instrumentation and self-reporting\nerrors, as well as the inadequate use of short-term measurements as proxies for long-term\nvariables.\nIgnoring measurement error can result in biased estimators, see Carroll et al.\n(2006) regarding the importance of measurement error correction in understanding the ef-\nfects of the covariates on the outcome. This paper considers a heteroscedastic measurement\nerror setting, allowing the measurement error covariance to vary across observations. This\nobservation-speci\ufb01c measurement error variance structure, treated as unknown, requires esti-\nmation from replicate data. We adopt the classic additive measurement error model wherein\nthe contaminated covariates, i.e the surrogates, are treated as the sum of the true covariates\nand independent measurement errors, so surrogate variances exceed true covariate variances.\nOne of the \ufb01rst papers to address the problem of a predictor variable contaminated by\nmeasurement error is Wald (1940). Since then, many parametric methods have been pro-\nposed, such as the maximum likelihood approach of Higdon and Schafer (2001). The con-\nditional scores approach (Stefanski and Carroll, 1987) and the conditional quasi-likelihood\napproach (Hanfelt and Liang, 1997) require the conditional distributions of the outcomes and\nthe contaminated covariates to be speci\ufb01ed, both in terms of the true covariates. Regression\ncalibration (Carroll and Stefanski, 1990) estimates the true covariates from the contaminated\ncovariates in a validation sample. Simulation-extrapolation (SIMEX) (Stefanski and Cook,\n1995) is a computationally-intensive method that adds additional measurement error to esti-\nmate model parameters and then extrapolates to the error-free case. All the methods listed\nin this paragraph require parametric speci\ufb01cations for some distributional components of the\nmodel.\nOur paper proposes an e\ufb03cient distribution-free estimator for a linear errors-in-variables\nmodel with heteroscedastic measurement errors. Our estimator combines two existing meth-\nods: a moment correction approach and a phase-function based estimator. The moment\ncorrection approach dates back to Reiers\u00f8l (1941) and has also been considered by Gillard\n(2014) and Erickson et al. (2014). This method requires the existence of model moments up\nto order 2M, where M \u22652 is the number of moments that are used to derive moment equa-\ntions. In contrast, the phase function-based estimator, proposed by Nghiem et al. (2020),\ndoes not impose moment conditions on the underlying random variables, but requires the\ntrue covariates to have asymmetric distributions. These authors de\ufb01ne minimum distance es-\ntimators based on the di\ufb00erence of two empirical phase functions. Our paper proposes a new\nmethod combining the moment-based and phase function-based approaches within a gen-\neralized method of moments (GMM) framework. We assume that the measurement errors\nhave a joint symmetric distribution, but both the distribution type and the observation-\n2\nspeci\ufb01c covariance matrix are unknown. Our estimator relaxes the asymmetry condition of\nNghiem et al. (2020) for a common scenario in practice where all observations have at least\ntwo replicates (Carroll et al., 2006). We propose two di\ufb00erent weighting schemes to construct\nweighted empirical phase functions that account for measurement error heteroscedasticity.\nFurthermore, we present a computationally e\ufb03cient bootstrap technique to estimate the co-\nvariance matrix, serving both GMM estimator computation and standard error estimation.\nSimulation studies and a data application demonstrate that our combined GMM estimator\nhave strong \ufb01nite sample performance.\nThe remaining sections of the paper are organized as follows. Section 2 reviews moment-\ncorrected and phase function estimators. Section 3 introduces the GMM estimator along\nwith a bootstrap approach for covariance matrix estimation. Section 4 conducts simulation\nstudies to compare estimation methods and assess standard error recovery. Section 5 presents\nan illustrative NHANES dataset analysis, and Section 6 concludes the study.\n2.\nMoment-corrected and phase function estimators\n2.1\nHeteroscedastic errors-in-variables model formulation\nLet X = {D1, . . . , Dn} denote an observed sample following a linear errors-in-variables (EIV)\nmodel. Here, Dj = (W\n(nj)\nj\n, \u02dcZj, yj) is a random sample following EIV model structure that\nyj = X\u22a4\nj \u03b20 + Z\u22a4\nj \u03b30 +\u03b5j and Wjk = Xj + Ujk,\n(2.1)\nfor k = 1, . . . , nj and j = 1, . . . , n.\nIn this model, we let yj \u2208R denote the outcome\nof interest, Xj = (Xj1, . . . , Xjp)\u22a4\u2208Rp denote the true values of the error-prone covari-\nates, Zj =\n\u00001, \u02dcZ\n\u22a4\nj\n\u0001\u22a4with \u02dcZj = (Zj1, . . . , Zjq)\u22a4\u2208Rq denote the error-free covariates, and\nWjk = (Wjk,1, . . . , Wjk,p)\u22a4denote a contaminated version of Xj subject to independent\nmeasurement error Ujk = (Ujk,1, . . . , Ujk,p)\u22a4. Furthermore, we let W\n(nj)\nj\n= (Wj1, . . . , Wjnj)\nand U\n(nj)\nj\n= (Uj1, . . . , Ujnj) denote, respectively, the collections of contaminated repli-\ncates and measurement errors associated with the nj \u22652 replicates of the jth obser-\nvation, and \u03b5j denote the usual regression error.\nAlso, \u03b20 = (\u03b201, . . . , \u03b20p)\u22a4\u2208Rp and\n\u03b30 = (\u03b300, \u03b301, . . . , \u03b30q)\u22a4\u2208Rq+1 denote, respectively, the coe\ufb03cients vectors associated with\nXj and Zj. We assume the measurement errors Ujk have mean zero and covariance \u03a3j that\nis potentially distinct for all observations. Consequently, we have Var[Wjk] = \u03a3x + \u03a3j where\n\u03a3x = Var[Xj] for k = 1, . . . , nj and j = 1, . . . , n. The regression errors \u03b5j\u2019s are assumed to\nbe independent and identically distributed (iid) with mean zero and variance \u03c32\n\u03b5.\nIn subsequent sections of this paper, several conditions will be important when consid-\nering the estimation methods for the linear EIV model (2.1). These conditions are now\n3\npresented and discussed. To this end, let operator \u22a5denote the independence of random\nquantities, let \u2225A\u2225max = max{|ajk|} denote the element-wise in\ufb01nity norm of an arbitrary\nmatrix A, and let i = \u221a\u22121 denote the imaginary unit.\nCondition C1. For the jth observation, Ujk \u22a5Ujk\u2032 for k \u0338= k\u2032, k, k\u2032 \u2208{1, . . . , nj}. Fur-\nthermore, U\n(nj)\nj\n\u22a5(Xj, Zj, \u03b5j). Finally, the random quantities (Xj, Zj, \u03b5j), j = 1, . . . , n are\niid copies of random variables (X, Z, \u03b5).\nIn addition to requiring independent observations, Condition C1 requires that the mea-\nsurement error components across the replicates associated within a given observation are\nmutually independent. Furthermore, the measurement errors, true covariates, and regres-\nsion errors are required to be mutually independent as well. An example of a well-studied\nscenario that would violate this assumption is the Berkson error model wherein the observed\npredictor Wj has a smaller variance than the true predictor Xj, see Song (2021) for an\noverview. Many time-series error models would also violate Condition C1. Nevertheless,\nthese settings are outside the scope of the current paper.\nCondition C2. For the jth observation, random quantities \u02dcXj = [X\u22a4\nj , \u02dcZ\n\u22a4\nj ]\u22a4and \u03b5j satisfy\nE\n\u0010\r\r \u02dcXj \u02dcX\n\u22a4\nj\n\r\r4\nmax\n\u0011\n< \u221eand E(\u03b54\nj) < \u221e. Also, for the replicates associated with observation\nj, E\n\u0010\r\rUjk U\u22a4\njk\n\r\r4\nmax\n\u0011\n< \u221e, j = 1, . . . , n.\nWhile this paper imposes no parametric distributional assumption on the underlying\nvariables in the model, we do require that the covariates, regression errors, and measurement\nerrors have distributions with at least four \ufb01nite moments. Examples of situations where\nCondition C2 would be violated would be if the covariates and/or measurement error terms\nfollowed a multivariate t distribution with fewer than 4 degrees of freedom, or a multivariate\nstable law with index parameter \u03b1 < 2. Condition C2 is central to the moment-corrected\napproach of Section 2.2 having an asymptotic normal distribution. While the phase function-\nbased approach of Section 2.3 is less concerned with the higher-order moments, the following\ntwo conditions are central to it.\nCondition C3. The replicate measurement error vectors Ujk have a distribution symmetric\nabout zero with strictly positive characteristic function, \u03c6Uj(t) = E[exp(it\u22a4Ujk)] > 0, k =\n1, . . . , nj, for all t = (t1, . . . , tp)\u22a4\u2208Rp and possibly distinct for each j = 1, . . . , n. Similarly,\nthe regression errors \u03b5j have a distribution that is symmetric around zero with strictly positive\ncharacteristic function \u03c6\u03b5(t) = E[exp(it\u03b5j)] > 0 for all t \u2208R and common to all j = 1, . . . , n.\nMany distributions commonly encountered in the measurement error literature satisfy\nCondition C3, including the Gaussian, the Laplace, and Student\u2019s t distributions. Excluded\n4\nby this condition are distributions only taking values on a bounded interval, for example,\nthe (multivariate) uniform distribution, because the corresponding characteristic functions\nare negative for some t.\nIn order to state the next condition, the phase function of a random variable has to be\nde\ufb01ned. For an arbitrary random variable V , let \u03c6V (t) denote the characteristic function\nof V . The phase function of V is then de\ufb01ned as \u03c1V (t) = \u03c6V (t)/|\u03c6V (t)| with |\u03c6V (t)|2 =\n\u03c6V (t)\u00af\u03c6V (t) being the squared complex norm and \u00af\u03c6V (t) the complex conjugate of \u03c6V (t). For\na more in-depth discussion of the phase function and its properties, consult Delaigle and Hall\n(2016) and Nghiem et al. (2020).\nCondition C4. Let V (\u03b2, \u03b3) = X\u22a4\u03b2 + Z\u22a4\u03b3 and let \u03c1V (t| \u03b2, \u03b3) denote the corresponding\nphase function of V . Note that this phase function depends on parameters (\u03b2, \u03b3). Then,\n\u03c1V (t| \u03b2, \u03b3) is continuously di\ufb00erentiable with respect to all elements of \u03b2 and \u03b3. Furthermore,\n\u2202\u03c1V (t| \u03b2, \u03b3)/\u2202\u03b2k \u0338= 0 for k = 1, . . . , p and \u2202\u03c1V (t| \u03b2, \u03b3)/\u2202\u03b3k \u0338= 0 for k = 0, . . . , q.\nCondition C4 may appear esoteric. In essence, this condition imposes a joint skewness\nrequirement on the true covariates, as any symmetric variable independent of the other\nvariables will not contribute to the phase function in any way. A related su\ufb03cient condition\nused by Nghiem et al. (2020) is that for covariates \u02dcX = (X\u22a4, \u02dcZ\n\u22a4)\u22a4= ( \u02dcX1, . . . , \u02dcXp+q)\u22a4and\ntrue parameter values \u02dc\u03b20 = (\u02dc\u03b201, . . . , \u02dc\u03b20,p+q) = (\u03b201, . . . , \u03b20p, \u03b301, . . . , \u03b30q)\u22a4, there exists no\nsubset of variables P \u2286{1, . . . , p + q} such that P\nk\u2208P \u02dc\u03b20k \u02dcXk has a symmetric distribution.\nCondition C5. The true parameter \u03b80 = (\u03b2\u22a4\n0 , \u03b3\u22a4\n0 )\u22a4is an interior point of a compact and\nconvex parameter space \u0398 \u2286Rp+q+1.\nCondition C5 is a regularity condition imposed on the parameter space; here it is satis\ufb01ed\nwhen all parameter values are \ufb01nite.\n2.2\nMoment correction\nMoment correction is a well-established approach to estimate the parameters of the het-\neroscedastic EIV model as per equation (2.1). As moment correction is also central to the\nnew estimation method proposed in Section 3, a brief overview is provided here. The inter-\nested reader can consult Buonaccorsi (2010, Section 5.4) for more details.\nLet Wj = n\u22121\nj\nPnj\nk=1 Wjk = Xj + Uj denote the averaged contaminated replicates of\nthe jth observation. Here, Uj = n\u22121\nj\nPnj\nk=1 Ujk, whose variance is Var (Uj) = n\u22121\nj \u03a3j. Mo-\nment correction relies on the corrected L2 norm L(\u03b2, \u03b3) = n\u22121 Pn\nj=1\n\u0000yj \u2212W\u22a4\nj \u03b2 \u2212Z\u22a4\nj \u03b3\n\u00012\u2212\nn\u22121 Pn\nj=1 n\u22121\nj\n\u03b2\u22a4\u03a3j \u03b2, which satis\ufb01es E[L(\u03b20, \u03b30)] = \u03c32\n\u03b5 when Condition C1 holds. The gra-\n5\ndients corresponding to this function are\nSL,\u03b2 = \u2202L\n\u2202\u03b2 = \u22122\nn\nn\nX\nj=1\nWj\n\u0000yj \u2212W\u22a4\nj \u03b2 \u2212Z\u22a4\nj \u03b3\n\u0001\n\u22122\nn\nn\nX\nj=1\n1\nnj\n\u03a3j \u03b2,\nSL,\u03b3 = \u2202L\n\u2202\u03b3 = \u22122\nn\nn\nX\nj=1\nZj\n\u0000yj \u2212W\u22a4\nj \u03b2 \u2212Z\u22a4\nj \u03b3\n\u0001\n.\nThe moment corrected estimator is subsequently de\ufb01ned to be the solution of the p + q + 1\nestimating equations SL = [S\u22a4\nL,\u03b2, S\u22a4\nL,\u03b3]\u22a4= 0. These estimating equations are well-de\ufb01ned in\na statistical sense whenever the underlying variables have at least two \ufb01nite moments. How-\never, to establish that the estimators are asymptotically normally distributed, Conditions\nC2 and C5 are required to ensure the variances of the gradients SL,\u03b2 and SL,\u03b3 are \ufb01nite.\nTo implement the above moment correction, knowledge of the variance-covariance ma-\ntrices \u03a3j is required. When these are unknown, they can be consistently estimated using\n\u02c6\u03a3j =\n1\nnj(nj \u22121)\nnj\u22121\nX\nk=1\nnj\nX\nk\u2032>k\n(Wjk \u2212Wjk\u2032)(Wjk \u2212Wjk\u2032)\u22a4.\n(2.2)\nThe covariance estimator in (2.2) follows upon noting that\nE\n\u0002\n(Wjk \u2212Wjk\u2032)(Wjk \u2212Wjk\u2032)\u22a4\u0003\n= E\n\u0002\n(Ujk \u2212Ujk\u2032)(Ujk \u2212Ujk\u2032)\u22a4\u0003\n= 2 \u03a3j\nfor any pair of replicates (Wjk, Wjk\u2032) with k \u0338= k\u2032. The second equality follows from the\nindependence of measurement error terms (Ujk, Ujk\u2032), and the assumed mean and covariance\nstructure of the Ujk. Subsequently, the estimator \u02c6\u03a3j is de\ufb01ned by averaging over the squared\ndi\ufb00erences of the nj(nj \u22121)/2 such pairs associated with the jth observation. Therefore, in\npractice, the gradient SL,\u03b2 is replaced by an approximation \u02c6SL,\u03b2 which substitutes unknown\n\u03a3j by \u02c6\u03a3j for j = 1, . . . , n. The moment-corrected estimators are then calculated as the\nsolution of the estimating equation\n\u02c6SL = [\u02c6S\n\u22a4\nL,\u03b2, S\u22a4\nL,\u03b3]\u22a4= 0.\n(2.3)\nThese estimators \u02c6\u03a3j and the gradient \u02c6SL will be further used in Section 3.\n2.3\nPhase function-based estimation\nRecently, in the context of a homoscedastic EIV model without replicate data, Nghiem et al.\n(2020) proposed a phase function-based estimator.\nTheir approach, unlike the moment-\ncorrected estimator, does not require estimation of the measurement error covariances, but\n6\nstill leads to a consistent estimator even when the underlying random variables do not have\n\ufb01nite variances. We propose here a new variation of the phase function method that adjusts\nfor heteroscedasticity of the measurement error, which is made possible by the availability\nof replicates. Furthermore, the asymmetric linear combination assumption of Nghiem et al.\n(2020) is replaced by Condition C4.\nFor the model de\ufb01ned in (2.1), de\ufb01ne V0j = X\u22a4\nj \u03b20 + Z\u22a4\nj \u03b30 so that yj = V0j + \u03b5j, j =\n1, . . . , n. Since (Xj, Zj, \u03b5j) are independent copies of (X, Z) by Condition C1, the random\nvariable V0j is an independent copy of V0 := V (\u03b20, \u03b30) = X\u22a4\u03b20 + Z\u22a4\u03b30 and yj is an\nindependent copy of Y = V0 + \u03b5.\nRecall that \u03c6Y (t) and \u03c1Y (t) denote, respectively, the\ncharacteristic and phase functions of Y , and the same holds for \u03c6V0(t) and \u03c1V0(t) in terms of\nV0. From Condition C3, the phase function \u03c1Y (t) is given by\n\u03c1Y (t) = \u03c6Y (t)\n|\u03c6Y (t)|\n(i)= \u03c6V0(t)\u03c6\u03b5(t)\n|\u03c6V0(t)\u03c6\u03b5(t)|\n(ii)\n= \u03c6V0(t)\n|\u03c6V0(t)| = \u03c1V0(t),\nwhere (i) follows from the independence of the regression error as per Condition C1, and (ii)\nfollows from Condition C3. Moreover, recalling that Wj and Uj are the averaged replicates\nand measurement error terms, respectively.\nSince the term U\u22a4\nj \u03b2 also has a symmetric\ndistribution around zero, a similar argument shows that the phase function of arbitrary\nlinear combination \u02dcVj(\u03b2, \u03b3) = W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3 = X\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3 + U\u22a4\nj \u03b2 is the same as the\nphase function of V (\u03b2, \u03b3), i.e \u03c1 \u02dcVj(t| \u03b2, \u03b3) = \u03c1V (t| \u03b2, \u03b3) for all j = 1, . . . , n.\nOur method now proceed by equating two di\ufb00erent empirical phase functions. Firstly,\nbased on the outcomes yj, de\ufb01ne empirical phase function\n\u02c6\u03c1Y (t) =\nPn\nj=1 exp(ityj)\nhPn\nj=1\nPn\nk=1 exp {it (yj \u2212yk)}\ni1/2,\nand based on covariates (Wj, Zj), we de\ufb01ne the weighted empirical phase function (WEPF),\n\u02c6\u03c1V (t| \u03b2, \u03b3) =\nPn\nj=1 qj exp\n\b\nit(W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3)\n\t\n\u0010Pn\nj=1\nPn\nk=1 qjqk exp [it {(Wj \u2212Wk)\u22a4\u03b2 +(Zj \u2212Zk)\u22a4\u03b3}]\n\u00111/2.\n(2.4)\nwhere the weights {qj}n\nj=1 satisfy qj \u22650 and Pn\nj=1 qj = 1.\nThe phase function-based\nestimator is motivated by noting that the population level equivalents \u03c1Y (t) and \u03c1V (t| \u03b2, \u03b3),\nunder the asymmetry imposed on the covariates by Condition C4, are equal if and only if\n(\u03b2, \u03b3) = (\u03b20, \u03b30). Thus, the estimator for (\u03b20, \u03b30) is de\ufb01ned to be the minimizer of the\ndiscrepancy\nD(\u03b2, \u03b3) =\nZ \u221e\n\u2212\u221e\n|\u02c6\u03c1Y (t) \u2212\u02c6\u03c1V (t| \u03b2, \u03b3)|2\u03c9(t)dt,\n(2.5)\n7\nwhere \u03c9(t) is a weighting function to ensure the integral is \ufb01nite. Direct minimization of (2.5)\nis computationally expensive. Following the example of Nghiem et al. (2020), we consider\nthe alternative minimization problem\n\u02dcD(\u03b2, \u03b3)\n=\nZ t\u2217\n0\n\u0014\nCy(t)\nn\nX\nj=1\nqj sin\n\b\nt\n\u0000W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3\n\u0001\t\n\u2212Sy(t)\nn\nX\nj=1\nqj cos\n\b\nt\n\u0000W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3\n\u0001\t \u00152\nKt\u2217(t)dt\n(2.6)\nwhere Cy(t) = n\u22121 Pn\nj=1 cos(t yj), Sy(t) = n\u22121 Pn\nj=1 sin(t yj), and Kt\u2217(t) is a kernel function\nthat is only non-zero on the interval [0, t\u2217]. Note that the two forms (2.5) and (2.6) are\nequivalent for appropriate choices of \u03c9(t) and Kt\u2217(t). Following Delaigle and Hall (2016),\nwe use Kt\u2217(t) = (1 \u2212t/t\u2217)2 for numerical implementation with t\u2217being the largest t such\nthat |\u02c6\u03c6y(t)| \u2264n\u22121/2. For any \ufb01xed (\u03b2, \u03b3), by a similar argument to Nghiem and Potgieter\n(2018), the WEPF in (2.4) is a consistent estimator of \u03c1V (\u03b2, \u03b3) for any set of weights\nhaving maxj qj = O(n\u22121). Therefore, our intended goal is to adjust for measurement error\nheteroscedasticity through an appropriate choice of weights; we will elaborate on this in\nSection 3.2.\n3.\nA GMM estimator\n3.1\nGeneralized method of moments\nThe moment-corrected and phase function-based estimators from Section 2 rely on di\ufb00erent\nyet complementary sets of assumptions. On the one hand, moment correction is a least\nsquares approach with a variance correction term, thus using information from the \ufb01rst two\nunderlying model moments. On the other hand, as discussed in Nghiem et al. (2020), the\nphase function method essentially uses all odd moments of the underlying model, provided\nthese moments exist. Hence, a method combining these two approaches will generally make\nuse of more model information, allowing for the possibility of a more e\ufb03cient estimator.\nIn this paper, we describe how to combine the two methods using a generalized method of\nmoments (GMM) approach, which is widely used to estimate parameters in over-identi\ufb01ed\nsystems (Hansen, 1982).\nTo de\ufb01ne the GMM estimator, recall that the phase function-based estimator is found\nby minimizing function \u02dcD(\u03b2, \u03b3) in (2.6). The minimizer can be expressed as the solution of\nthe p + q + 1 estimating equations S \u02dcD = [S\u22a4\n\u02dcD,\u03b2, S\u22a4\n\u02dcD,\u03b3]\u22a4= 0 where\nS \u02dcD,\u03b2 = \u2202\u02dcD\n\u2202\u03b2 and S \u02dcD,\u03b3 = \u2202\u02dcD\n\u2202\u03b3 .\n(3.7)\n8\nThus, if we simultaneously consider the estimating equations from moment correction, \u02c6SL =\n0 in (2.3), and the phase-function based estimators, S \u02dcD = 0 de\ufb01ned above in (3.7), we have\na system of 2(p + q + 1) estimating equations in terms of the p + q + 1 model parameters.\nMore formally, let\nS := S(\u03b2, \u03b3) = [\u02c6S\n\u22a4\nL, S\u22a4\nD]\u22a4= [\u02c6S\n\u22a4\nL,\u03b2, S\u22a4\nL,\u03b3, S\u22a4\n\u02dcD,\u03b2, S\u22a4\n\u02dcD,\u03b3]\u22a4\n(3.8)\ndenote the vector of 2(p + q + 1) gradient equations. The system S(\u03b2, \u03b3) = 0 is generally\nover-identi\ufb01ed and does not have an exact solution. Suppressing the dependence of S on\n(\u03b2, \u03b3), we de\ufb01ne a quadratic form in S,\nQ(\u03b2, \u03b3) = S\u22a4\u2126\u22121\nS S,\n(3.9)\nwhere \u2126S =\n\u0002\nVar(S)\n\u0003\n(\u03b2,\u03b3)=(\u03b20,\u03b30) is the 2(p+q+1)\u00d72(p+q+1) covariance matrix correspond-\ning of the gradient equations S evaluated at the true parameter values (\u03b20, \u03b30). The GMM\nestimator is then de\ufb01ned to be the minimizer of Q(\u03b2, \u03b3). Minimizing the GMM discrepancy\nfunction Q(\u03b2, \u03b3) is equivalent to projecting S onto a (p + q + 1)-dimensional subspace and\nsolving the resulting equations, so the GMM estimator can be thought of as the solution to\nan optimal linear combination of the estimating equations in S.\nWe next study the asymptotic properties of the proposed GMM estimator. Both the\nconsistency and asymptotic normality of our proposed estimator follow from the properties\nof the GMM approach under suitable regularity conditions. First, we establish the uniform\nconvergence of Q(\u03b2, \u03b3).\nLemma 1. Assume that all random variables in the model (2.1) have at least two \ufb01nite\nmoments. Then, for (\u03b2, \u03b3) \u2208\u0398 as per Condition C5, the function Q(\u03b2, \u03b3)\np\u2192Q0(\u03b2, \u03b3)\nuniformly, where Q0(\u03b2, \u03b3) = S\u22a4\n0 \u2126\u22121\nS S0 and S0 := S0(\u03b2, \u03b3) = limn\u2192\u221eE[S(\u03b2, \u03b3)] is the\nlimiting expectation of the gradient equations.\nThe proof of Lemma 1 is presented in Section S1 of the Supplementary Materials. The\nproof relies of verifying su\ufb03cient conditions for uniform convergence as per Lemma 2.9 of\nNewey and McFadden (1994). The uniform convergence of Q(\u03b2, \u03b3) is an essential step in\nestablishing the consistency of the GMM estimator in our next theorem.\nTheorem 1. Consider the heteroscedastic linear EIV model de\ufb01ned in (2.1). Assume Con-\nditions C1, C3 and C5 hold. Also assume that all variables in the model have at least two\n\ufb01nite moments. Finally, assume the weights qj used for constructing the empirical phase\nfunction in (2.4) satisfy maxj qj = O(n\u22121). Then, the estimator obtained by minimizing\nQ(\u03b2, \u03b3) = S\u22a4\u2126\u22121\nS S is consistent for true value (\u03b20, \u03b30).\n9\nTheorem 1 follows from Theorem 2.1 of Newey and McFadden (1994) combining the uni-\nform convergence in Lemma 1 along with establishing that Q0(\u03b2, \u03b3) has a global minimum\nat the true parameters (\u03b20, \u03b30). The proof is presented in Section S2 of the Supplementary\nMaterials. We further note that for the GMM estimator to be consistent, Condition C4 (co-\nvariate asymmetry) is not a requirement. When it does not hold, some of the components of\nS converge to 0 for all values of the underlying parameters. However, the limiting quadratic\nform still has a unique minimum at the true parameter values, because the estimating equa-\ntions originating with the moment-corrected approach do not rely on asymmetry. With this\nin mind, we focus the GMM method on a situation where \u2202\u03c1V (t| \u03b2, \u03b3)/\u2202\u03b2k \u0338= 0 for some\nk = 1, . . . , p and/or \u2202\u03c1(t| \u03b2, \u03b3)/\u2202\u03b3k \u0338= 0 for some k = 1, . . . , q. When some of these partial\nderivatives are non-zero, then evaluation of the empirical phase function contributes infor-\nmation to the estimation procedure and it is possible that the e\ufb03ciency of the estimator is\nimproved. Finally, we establish the asymptotic normality of the proposed estimator.\nTheorem 2. Consider the heteroscedastic linear EIV model de\ufb01ned in (2.1). Assume Con-\nditions C1, C2, C3, and C5 hold. Furthermore, assume the weights qj used for construct-\ning the empirical phase function in (2.4) satisfy maxj qj = O(n\u22121). Then, the estimator\n(\u02c6\u03b2gmm, \u02c6\u03b3gmm) obtained by minimizing Q(\u03b2, \u03b3) = S\u22a4\u2126\u22121\nS S satis\ufb01es\nn1/2 n\u0000 \u02c6\u03b2\u22a4\ngmm \u02c6\u03b3\u22a4\ngmm\n\u0001\u22a4\u2212\n\u0000\u03b2\u22a4\n0 \u03b3\u22a4\n0\n\u0001\u22a4o\n\u223cN\n\u0010\n0,\n\u0000P1\u2126\u22121\nS P \u22a4\n1\n\u0001\u22121\u0011\nwhere\nP1 = E\n\" \u0012\u2202S\n\u2202\u03b2\n\u22a4\n, \u2202S\n\u2202\u03b3\n\u22a4\u0013 #\nwith the expectations in P1 evaluated at the true parameter values (\u03b20, \u03b30).\nTheorem 2 follows from Theorem 3.4 of Newey and McFadden (1994).\nCompared to\nthe required conditions for the consistency established in Theorem 1, Theorem 2 requires\nstronger moment conditions that all the variables in the model having four \ufb01nite moments\nas per Condition C2. We furthermore note that versions of Theorems 1 and 2 still hold if\nwe replace the covariance matrix \u2126S by any positive de\ufb01nite matrix \u2126\u2217. Nevertheless, the\nchoice of \u2126S leads to the most asymptotically e\ufb03cient estimator, as discussed in Section 5.2\nof Newey and McFadden (1994).\nIn practice, the covariance matrix \u2126S is unknown and needs to be replaced by a suitable\nestimator \u02c6\u2126S. As per Section 4 of Newey and McFadden (1994), using a consistent estimator\nof \u2126S leaves the asymptotic distribution of the estimators unchanged. We therefore propose\na bootstrap resampling algorithm in Section 3.3 to obtain a suitable estimator \u02c6\u2126S. First,\nhowever, we explore the calculation of weights qj, j = 1, . . . , n in the WEPF to adjust for\nmeasurement error heteroscedasticity.\n10\n3.2\nChoice of phase function weights\nThe weighted phase function in (2.4) requires the speci\ufb01cation of weights {qj}n\nj=1 with qj \u22650\nand Pn\nj=1 qj = 1. If the underlying distributions were known, it would be possible to directly\nminimize an asymptotic variance metric directly related to this estimated phase function \u2013\nsee Nghiem and Potgieter (2018) for a derivation of the pointwise asymptotic variance of\nsuch a WEPF. However, since the underlying distributions are assumed unknown, the direct\napproach is unavailable to us. We therefore present here two weighting schemes that are\neasily implemented in practice.\nFor the \ufb01rst approach, note that the variable Pn\nj=1 qj W\u22a4\nj \u03b20 is an unbiased estimator of\nE(X\u22a4\u03b20). Therefore, one can consider \ufb01nding a set of weights that minimize the variance of\nthe above mean estimator. Speci\ufb01cally, we have Var\n\u0010P\nj qj W\u22a4\nj \u03b20\n\u0011\n= P\nj q2\nj \u03b2\u22a4\n0 (\u03a3x +n\u22121\nj\n\u03a3j) \u03b20,\nwhich is minimized by weights qj = a\u22121\nj /Pn\nk=1 a\u22121\nk , where aj = \u03b2\u22a4\n0 (\u03a3x +n\u22121\nj\n\u03a3j) \u03b20. Un-\nfortunately, these weights depend on the unknown true \u03b20 and are therefore impossible\nto calculate. We propose the following proxy estimator based on a \u201cminimax\u201d argument.\nLet \u03bbj denote the largest eigenvalue of \u03a3x +n\u22121\nj\n\u03a3j.\nWe then have aj \u2264\u03bbj\u2225\u03b20 \u22252 with\n\u2225\u03b20 \u22252 = Pp\nk=1 \u03b22\n0k denoting the squared L2 norm. Therefore, we calculate weights based\non replacing the aj by the corresponding upper bounds. Note that using the upper bounds\nhas the potential to underweight observations with the large measurement error i.e. their\nin\ufb02uence is even further mitigated. The proposed minimax weights are given by\n\u02c6qmm\nj\n=\n\u02c6\u03bb\u22121\nj\nPn\nk=1 \u02c6\u03bb\u22121\nk\n, j = 1, . . . , n,\n(3.10)\nwhere \u02c6\u03bbj is the largest eigenvalue of \u02c6\u03a3x + n\u22121\nj\n\u02c6\u03a3j with \u02c6\u03a3j given in (2.2) and with \u02c6\u03a3x =\n(n \u22121)\u22121 Pn\ni=1(Wj \u2212W)(Wj \u2212W)\u22a4\u2212n\u22121 Pn\nj=1 n\u22121\nj\n\u02c6\u03a3j, with W = n\u22121 Pn\nj=1 Wj.\nFor the second approach, note that for all j = 1, . . . , n, we have E(Wj) = E(X). There-\nfore, the quantity \u02c6\u00b5q = Pn\nj=1 qjWj is an unbiased estimator of E(X).\nRecalling that\nVar(Wj) = \u03a3x + n\u22121\nj \u03a3j, we de\ufb01ne the L2 discrepancy\nL(q) =\nn\nX\nj=1\n(Wj \u2212\u02c6\u00b5q)\u22a4\u0000\u03a3x + n\u22121\nj \u03a3j\n\u0001\u22121 (Wj \u2212\u02c6\u00b5q)\u22a4,\n(3.11)\nwith q = (q1, . . . , qn). Our second weighting scheme proposes minimizing L(q) in terms\nof the weights q subject to qj \u22650 and Pn\nj=1 qj = 1. We note that minimizing L(q) is\nequivalent to maximizing the log-likelihood of the {Wj}n\nj=1 in terms of q assuming each Wj\nfollows a multivariate normal distribution. We therefore refer to this approach as the quasi-\nlikelihood weighting scheme. In Section S3 of the Supplementary Materials, we show that\n11\nthe quasi-likelihood weights are found by solving a system of linear equations. Replacing\n\u03a3x and the \u03a3j in (3.11) with their corresponding estimator \u02c6\u03a3x and \u02c6\u03a3j, we denote the\nresulting minimizing weights by \u02c6qql\nj for j = 1, . . . , n. The performance of the minimax and\nquasi-likelihood weights are further considered in the simulation studies of Section 4.\n3.3\nGMM covariance matrix and standard error estimation\nImplementation of the proposed GMM estimator requires a suitable estimator for \u2126S in\n(3.9), where \u2126S is the covariance matrix of the 2(p + q + 1) gradient equations S at the\ntrue parameter values (\u03b20, \u03b30). We propose a computationally\u2013e\ufb03cient strategy based on\nthe estimating function bootstrap approach of Hu and Kalb\ufb02eisch (1997); this strategy is\nalso used in Nghiem et al. (2020) for estimating the variances of the phase function-based\nestimators.\nRecall that X = {D1, . . . , Dn} denotes the random sample from the linear EIV model\nwith Dj = (W\n(nj)\nj\n, \u02dcZj, yj), j = 1, . . . , n. The bth bootstrap sample, X \u2217\nb = {D\u2217\nb1, . . . , D\u2217\nbn},\nb = 1, . . . , B, is obtained by sampling n times with replacement from X . No re-sampling\nis done at the replicate level. For the bth bootstrap sample, we compute \u02c6\u03a3\n\u2217\nb,x, \u02c6\u03a3\n\u2217\nb,j, and\n\u02c6q\u2217\nb,j which correspond to the observation-level measurement error covariance matrices, and\nthe weights for the phase function estimator calculated using Xb for j = 1, . . . , n.\nWe\nsubsequently use these quantities and our bootstrap sample to evaluate S\u2217\nb(\u02c6\u03b2in, \u02c6\u03b3in) =\n[\u02c6S\n\u2217\nb,L(\u02c6\u03b2in, \u02c6\u03b3in)\u22a4S\u2217\nb, \u02dcD(\u02c6\u03b2in, \u02c6\u03b3in)\u22a4]\u22a4as de\ufb01ned in equation (3.8). Here, (\u02c6\u03b2in, \u02c6\u03b3in) denote consis-\ntent initial estimators of (\u03b20, \u03b30); in our implementation, the moment-corrected estimators\n(\u02c6\u03b2mc, \u02c6\u03b3mc) are used as initial estimators. Finally, using the B bootstrap samples, we estimate\n\u2126S by \u02c6\u2126\u2217\nS = B\u22121 PB\nb=1 S\u2217\nb(\u02c6\u03b2in, \u02c6\u03b3in) S\u2217\nb(\u02c6\u03b2in, \u02c6\u03b3in)\u22a4. The method is fast and can be implemented\nwithout the need to minimize bootstrap versions of the discrepancy function Q(\u03b2, \u03b3); imple-\nmentation requires only evaluation of the gradient vector for each bootstrap sample prior to\ncalculating \u02c6\u2126\u2217\nS. We have found that B = 100 bootstrap samples lead to a good performance.\nThe GMM estimator is \ufb01nally computed by minimizing \u02c6Q(\u03b2, \u03b3) = S\u22a4\u0000 \u02c6\u2126\u2217\nS\n\u0001\u22121 S.\nAs pointed out by a referee, our approach is similar to the traditional two\u2013step feasible\nGMM approach, in which we \ufb01rst obtain a consistent estimator (\u02c6\u03b2mc, \u02c6\u03b3mc) and then use\nit to estimate the covariance matrix \u2126S in the GMM discrepancy function.\nNote that\nan iterated GMM approach can be pursued, whereby this estimator and a new round of\nbootstrap samples are used to update the GMM covariance matrix and then minimize the\nresulting statistic. Asymptotically, this iterated GMM estimator is equivalent to the two-\nstep estimator, although in \ufb01nite samples, the relative performance between two\u2013step and\niterative GMM estimators have been reported to be mixed, see Hansen et al. (1996).\nThis bootstrap quantity \u02c6\u2126\u2217\nS can also subsequently be used to estimate standard errors of\nthe GMM estimators. Based on the GMM covariance matrix from Theorem 2, the asymptotic\n12\ncovariance matrix of ( \u02c6\u03b2gmm, \u02c6\u03b3gmm) is estimated by ( \u02c6P1 \u02c6\u2126\u2217\nS \u02c6P \u22a4\n1 )\u22121. Here, \u02c6P1 an empirical coun-\nterpart of the expected gradient P1 and is evaluated at the GMM estimator ( \u02c6\u03b2gmm, \u02c6\u03b3gmm).\nSpeci\ufb01cally, the matrix \u02c6P1 is given by\n\u02c6P1 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2n\u22121 Pn\nj=1 Wj W\u22a4\nj \u22122n\u22121 Pn\nj=1 \u02c6\u03a3j\n2n\u22121 Pn\nj=1 Wj Z\u22a4\nj\n2n\u22121 Pn\nj=1 Zj W\u22a4\nj\n2n\u22121 Pn\nj=1 Zj Z\u22a4\nj\n\u2202\u02c6S \u02dcD,\u03b2\n\u2202\u03b2\n\f\f\f\f\n( \u02c6\u03b2gmm,\u02c6\u03b3gmm)\n\u2202\u02c6S \u02dcD,\u03b2\n\u2202\u03b3\n\f\f\f\f\n( \u02c6\u03b2gmm,\u02c6\u03b3gmm)\n\u2202\u02c6S \u02dcD,\u03b3\n\u2202\u03b2\n\f\f\f\f\n( \u02c6\u03b2gmm,\u02c6\u03b3gmm)\n\u2202\u02c6S \u02dcD,\u03b3\n\u2202\u03b3\n\f\f\f\f\n( \u02c6\u03b2gmm,\u02c6\u03b3gmm)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\nFor implementation, the partial derivatives corresponding to the phase function criterion are\ncalculated numerically. The estimated standard errors are given by the square root of the\ndiagonal elements of ( \u02c6P1 \u02c6\u2126\u2217\nS \u02c6P \u22a4\n1 )\u22121. In Section 4.3, we consider the performance of these\nestimated standard errors.\n4.\nSimulation studies\n4.1\nSimulation description\nThe \ufb01nite-sample performance of the GMM estimators was evaluated using a simulation\nstudy.\nWe considered three di\ufb00erent settings for mutiple linear EIV models (2.1) where\neach observation has the same number of replicates, nj = nrep for j = 1, . . . , n, with nrep \u2208\n{2, 3}. We chose sample sizes n \u2208{250, 500, 1000} and generate M = 500 samples for each\nsimulation con\ufb01guration. A univariate linear EIV model was also considered in Section S4.1\nof the Supplementary Materials. Details of the simulation settings are given as follows:\n(I) A model with p = 2 error-prone and q = 0 error-free covariates. The true covariates\nwere generated from a Gaussian copula (Xue-Kun Song, 2000), where the two covariates\nhave scaled half-normal marginals with variance 1, i.e. Xjk\niid\n\u223c(1 \u22122/\u03c0)\u22121/2 \f\fN(0, 1)\n\f\f for\nk = 1, . . . , nrep, and correlation 0.5. Three distributions were considered for the measurement\nerror vectors Ujk, namely a bivariate normal, a bivariate t with 2.5 degrees of freedom, and\na contaminated bivariate normal Ujk \u223c0.9 N(0, \u03a3j) + 0.1 N(0, 102\u03a3j). These distributions\nwere all scaled to have covariance matrices \u03a3j for the replicates associated with the jth\nobservation, where \u03a3j = DjRDj with Dj the marginal standard deviations and R the\ncorrelation matrix. We considered two choices of R, namely the identity matrix and a model\nwith common correlation \u03c1 = 0.5 between all measurement error components. The diagonal\nelements of the matrix Dj were independently generated from the uniform distribution\n13\n\u221anj \u00d7 U(\n\u221a\n0.2,\n\u221a\n1.5) by which the marginal signal-to-noise ratios Var(Xjk)/Var(Ujk) range\nfrom 2/3 (fairly weak signal) to 5 (fairly strong signal). The true model coe\ufb03cients were set\nto \u03b20 = (1, 0.5)\u22a4and intercept \u03b30 = 2. The regression error \u03b5j was generated to match the\ndistribution of the measurement error in each scenario and with constant variance \u03c32\n\u03b5 = 0.25\nfor j = 1, . . . , n.\n(II) A model with p = 2 error-prone and q = 2 error-free covariates, di\ufb00ering from\nsetting I due to the inclusion of two error-free covariates.\nThe true covariates (Xj, Zj)\nwere generated to have scaled half-normal marginals with the joint structure speci\ufb01ed by\na Gaussian copula with correlation 0.5 between each pair of predictors. The true model\ncoe\ufb03cients were \u03b20 = (1, 0.5)\u22a4and \u03b30 = (2, 1, 0.5)\u22a4. All other con\ufb01gurations are equivalent\nto the speci\ufb01cations in setting I.\n(III) The model as in setting II, but the two error-free covariates were generated to be\nsymmetric having standard normal marginals. The predictor correlation structure was still\nspeci\ufb01ed by a Gaussian copula with correlation parameter 0.5 between each pair of predictor\nvariables.\nAmong the three error distributions considering, the t2.5 does not have four \ufb01nite mo-\nments. Consequently, asymptotic normality as per Theorem 2 does not hold in this case.\nHowever, the consistency requirements are still satis\ufb01ed. We also note that setting III de-\nparts from Condition C4, allowing us to explore the e\ufb00ect of symmetric predictors when\nusing the GMM method.\nFor each generated sample, we computed several di\ufb00erent estimators. Firstly, we com-\npute the true ordinary least squares (OLS) estimator which regresses yj on the true uncon-\ntaminated data (Xj, Zj), as well as a naive OLS estimator which regresses yj on (Wj, Zj)\nwith Wj denoting the averaged replicates for the jth observations. Next, we computed the\nmoment-corrected estimator with plug-in covariance matrices \u02c6\u03a3j as per Section 2.2. Finally,\nwe computed three versions of the proposed GMM estimator, corresponding to an equal\nweighting scheme, the minimax weights, and the quasi-likelihood weights as per Section 3.2.\nFor the GMM estimators, covariance matrix \u02c6\u2126\u2217\nS was estimated using B = 100 bootstrap\nsamples.\nA robust performance metric was adopted to evaluate estimator performance. Let \u03b80 =\n(\u03b2\u22a4\n0 , \u03b3\u22a4\n0 )\u22a4be the true coe\ufb03cients, and \u02c6\u03b8\n(m) = (\u02c6\u03b2\n(m)\u22a4, \u02c6\u03b3(m)\u22a4)\u22a4denote the estimators ob-\ntained using one of the outlined approaches in the mth generated sample, m = 1, . . . , M.\nTo remove outliers, we constructed the M \u00d7 (p + q + 1) error matrix A with rows Am =\n(\u02c6\u03b8\n(m) \u2212\u03b8)\u22a4. Next, we formed a vector of medians Amed by taking the median of each column\nof A. Then, we computed the Mahalanobis distance between each row of A and Amed using\nthe robust minimum covariance determinant estimator of Rousseeuw and Driessen (1999).\nFinally, we removed the rows with Mahalanobis distances larger than the 90th percentile\n14\nand calculate the robust mean square error matrix MSErob = \u02dc\nA\u22a4\u02dc\nA/ \u02dc\nM where \u02dc\nA denotes the\nmatrix A with the outlier rows removed and \u02dc\nM is the number of rows in \u02dc\nA. The quantity\nMSErob is a robust estimator of E[(\u02c6\u03b8 \u2212\u03b8)\u22a4(\u02c6\u03b8 \u2212\u03b8)], and det(1000 \u00d7 MSErob) is the reported\nperformance metric. We reported the determinant rather than the trace as it better accounts\nfor unequal variances for and correlations between estimated parameters. Smaller (larger)\nvalues of det(MSErob) indicate better (worse) performance of estimators in a square error\nloss sense.\n4.2\nSimulation results: Parameter recovery\nTables 1 and 2 present results for simulation settings I and II with nrep = 2; results with\nnrep = 3 and are presented in Section S4.2 of the Supplementary Materials and show similar\nconclusions. The true OLS estimator stands out with much smaller performance metrics\nthan those of the estimators computed using data with measurement error; note that this\ntrue estimator is not available in practice since the true covariates Xi\u2019s are not observed.\nOn the other hand, the naive OLS estimator has the worst performance in all the settings,\nhighlighting the deleterious e\ufb00ect of ignoring measurement error as well as the importance\nof measurement error correction. Next, we compare all the correction estimators. For the\nt2.5 and contaminated normal errors, all versions of the GMM estimator outperform the\nmoment-corrected estimator, often with signi\ufb01cant e\ufb03ciency gains. For normally distributed\nerrors, the GMM estimators perform competitively compared to moment-correction. That\nbeing said, in the normal error scenarios, GMM occasionally performs worse than moment-\ncorrection; see Setting II. When comparing GMM weighting schemes, there is no clear supe-\nrior weighting scheme, but minimax and quasi-likelihood weights perform better than equal\nweighting for non-normal errors.\nTable 3 presents the results for setting III where the asymmetry condition C4 is violated,\nagain for the case with nrep = 2 replicates. The case with nrep = 3 replicates is summarized\nin Section S4.2 in the Supplementary Materials. Similar to other settings, there is little\ndi\ufb00erence in the performance between the moment-corrected estimator and the GMM esti-\nmator when the measurement error is Gaussian. However, we continue to note that GMM\nrepresents a substantive improvement over moment correction for contaminated normal and\nt2.5 error distributions. Also, in the latter two error settings, minimax and quasi-likelihood\nweights outperform equal weighting; neither of the latter two weighting scheme is clearly\npreferred.\nWe also note that across settings I through III, it is observed that the MSE metric for\nthe naive estimators also decreases as sample size increases. This is an artifact of the scaling\nused. The interested reader can easily verify that the MSE metric for the naive estimator\ndecreases at a much slower rate than the same metric for the MC and GMM estimators.\n15\nTable 1: Setting I performance of uncontaminated OLS (True), naive OLS (Naive), moment-\ncorrected (MC), and GMM estimators with equal (Equal), minimax (MM) and quasi-\nlikelihood (QL) weights with nrep = 2 as measured by det(1000 \u00d7 MSErob).\nR\nU\nn\nTrue\nNaive\nMC\nGMM\nEqual\nMM\nQL\n\u03c1 = 0\nNormal\n250\n0.012\n9.799\n1.394\n1.040\n1.020\n1.102\n500\n0.001\n2.889\n0.240\n0.224\n0.222\n0.234\n1000\n0.000\n0.668\n0.026\n0.025\n0.027\n0.030\nt2.5\n250\n0.018\n11.413\n0.816\n0.576\n0.525\n0.593\n500\n0.001\n5.128\n0.134\n0.100\n0.075\n0.077\n1000\n0.000\n1.502\n0.021\n0.013\n0.011\n0.010\nCont.Normal\n250\n0.010\n17.965\n1.290\n0.693\n0.605\n0.556\n500\n0.001\n4.305\n0.149\n0.105\n0.077\n0.090\n1000\n0.000\n1.143\n0.022\n0.014\n0.009\n0.011\n\u03c1 = 0.5\nNormal\n250\n0.008\n27.999\n3.155\n2.698\n3.399\n2.953\n500\n0.001\n7.365\n0.355\n0.362\n0.378\n0.373\n1000\n0.000\n1.339\n0.037\n0.036\n0.038\n0.039\nt2.5\n250\n0.015\n34.374\n1.351\n0.643\n0.661\n0.730\n500\n0.001\n8.390\n0.211\n0.128\n0.112\n0.111\n1000\n0.000\n2.583\n0.026\n0.017\n0.016\n0.016\nCont.Normal\n250\n0.008\n45.002\n2.976\n1.405\n1.048\n1.228\n500\n0.001\n10.191\n0.287\n0.193\n0.120\n0.157\n1000\n0.000\n2.331\n0.034\n0.024\n0.015\n0.021\nIn conclusion, the GMM estimator has a competitive performance compared to the\nmoment-corrected estimator when the measurement errors are Gaussian. However, the rel-\native decrease in det(MSErob) is often so large for the heavier-tailed error distributions that\nsome would be willing to risk a small loss in e\ufb03ciency were the error distribution closer to\na true normal. Furthermore, the two weighting schemes that account for heteroscedasticity\ntend to result in better estimators than the equal weighting.\n4.3\nSimulation results: Standard error estimation\nWe also performed a simulation study to examine the performance of the asymptotic covari-\nance estimator in estimating the standard errors of the GMM estimators. The data were\nsimulated from Settings I and III with the sample size n \u2208{500, 1000}, the measurement\nerror Ujk following either a bivariate normal or bivariate t distribution with 2.5 degrees of\nfreedom, and the correlation matrix R having \u03c1 = 0.5. We reported the Monte Carlo esti-\nmates of the true standard errors obtained from 500 simulated pairs ( \u02c6\u03b2gmm, \u02c6\u03b3gmm) obtained\nfrom independently generated datasets, as well as the average bootstrap plug-in standard\n16\nTable 2: Setting II performance of uncontaminated OLS (True), naive OLS (Naive), moment-\ncorrected (MC), and GMM estimators with equal (Equal), minimax (MM) and quasi-\nlikelihood (QL) weights with nrep = 2 as measured by det(1000 \u00d7 MSErob).\nR\nU\nn\nTrue\nNaive\nMC\nGMM\nEqual\nMM\nQL\n\u03c1 = 0\nNormal\n250\n0.001\n19.898\n3.427\n4.418\n4.472\n3.763\n500\n0.000\n2.053\n0.145\n0.161\n0.203\n0.157\n1000\n0.000\n0.121\n0.003\n0.004\n0.004\n0.004\nt2.5\n250\n0.001\n24.749\n0.985\n0.751\n0.594\n0.574\n500\n0.000\n2.359\n0.087\n0.055\n0.052\n0.045\n1000\n0.000\n0.139\n0.001\n0.001\n0.001\n0.001\nCont.Normal\n250\n0.001\n13.924\n1.564\n1.200\n1.048\n0.793\n500\n0.000\n0.985\n0.043\n0.042\n0.038\n0.033\n1000\n0.000\n0.063\n0.002\n0.002\n0.001\n0.001\n\u03c1 = 0.5\nNormal\n250\n0.001\n83.078\n11.799\n11.058\n11.721\n12.045\n500\n0.000\n5.037\n0.266\n0.288\n0.332\n0.292\n1000\n0.000\n0.286\n0.007\n0.008\n0.009\n0.009\nt2.5\n250\n0.001\n81.873\n5.663\n3.799\n3.669\n4.338\n500\n0.000\n4.953\n0.130\n0.104\n0.097\n0.086\n1000\n0.000\n0.329\n0.003\n0.003\n0.003\n0.002\nCont.Normal\n250\n0.001\n71.486\n5.833\n3.522\n2.922\n2.260\n500\n0.000\n3.010\n0.150\n0.126\n0.105\n0.088\n1000\n0.000\n0.230\n0.004\n0.004\n0.003\n0.002\nerrors as de\ufb01ned in Section 3.3 for the GMM estimators with the minimax weighting scheme\n\u02c6qmm\nj\nde\ufb01ned in (3.10), while the results for other weighting schemes are similar and hence are\nomitted. Table 4 presents the results for nrep = 2, while the results for nrep = 3 are presented\nin Section S4.2 the Supplementary Materials. The average bootstrap plug-in standard errors\nare similar to the Monte Carlo standard error in all the considered settings, suggesting that\nthe bootstrap procedure in Section 3.3 provides a reliable estimator for the standard errors\nof the proposed GMM estimators.\n5.\nAnalysis of NHANES data\nThe National Health and Nutrition Examination Survey (NHANES) is a long-running re-\nsearch survey conducted by the National Center for Health Statistics (NCHS). The goal of\nthis longitudinal survey study is to assess the health and nutritional status of both adults\nand children in the United States, tracking the evolution of this status over time. Dur-\ning the 2009-2010 survey period, participants were interviewed and asked to provide their\n17\nTable 3:\nSetting III performance of uncontaminated OLS (True), naive OLS (Naive),\nmoment-corrected (MC), and GMM estimators with equal (Equal), minimax (MM) and\nquasi-likelihood (QL) weights with nrep = 2 as measured by det(1000 \u00d7 MSErob).\nR\nU\nn\nTrue\nNaive\nMC\nGMM\nEqual\nMM\nQL\n\u03c1 = 0\nNormal\n250\n0.001\n10.963\n1.625\n1.623\n1.615\n1.623\n500\n0.000\n0.926\n0.069\n0.069\n0.068\n0.068\n1000\n0.000\n0.064\n0.002\n0.002\n0.002\n0.002\nt2.5\n250\n0.001\n11.607\n0.944\n0.936\n0.855\n0.890\n500\n0.000\n0.996\n0.041\n0.040\n0.032\n0.036\n1000\n0.000\n0.062\n0.001\n0.001\n0.001\n0.001\nCont.Normal\n250\n0.001\n12.622\n1.665\n1.663\n1.467\n1.572\n500\n0.000\n1.477\n0.068\n0.070\n0.066\n0.067\n1000\n0.000\n0.072\n0.002\n0.002\n0.001\n0.001\n\u03c1 = 0.5\nNormal\n250\n0.001\n54.053\n7.288\n7.162\n7.214\n7.338\n500\n0.000\n2.879\n0.147\n0.145\n0.147\n0.148\n1000\n0.000\n0.169\n0.004\n0.004\n0.004\n0.004\nt2.5\n250\n0.001\n41.193\n2.852\n2.782\n2.374\n2.524\n500\n0.000\n2.484\n0.078\n0.074\n0.067\n0.070\n1000\n0.000\n0.162\n0.002\n0.002\n0.002\n0.002\nCont.Normal\n250\n0.001\n65.616\n5.556\n5.552\n4.945\n5.375\n500\n0.000\n3.563\n0.144\n0.143\n0.130\n0.140\n1000\n0.000\n0.205\n0.004\n0.004\n0.004\n0.004\ndemographic background as well as information about nutrition habits. Participants also\nundertook a series of health examinations. To assess the nutritional habits of participants,\ndietary data were collected using two 24-hour recall interviews wherein the participants\nself-reported the consumed amount for a series of food items during the 24 hours prior to\neach interview. Based on these recalls, daily aggregated consumption of water, food energy,\nand other nutrition components such as total fat and total sugar consumption were com-\nputed. We used the 2009-2010 NHANES dietary data to illustrate our new GMM estimation\nprocedure.\nIn this illustrative analysis, we considered the relationship between participants\u2019 BMI\n(outcome of interest) and their age as well as daily aggregates of energy, protein and fat\nconsumption. As these nutritional variables were calculated based on self-reported data,\nthey are well-known to be subject to measurement error.\nWe restricted our analysis to\nn = 1595 white women and treat the nutritional data from each of the interviews as nrep = 2\nindependently observed replicates. We \ufb01tted the multiple linear EIV model to this data,\ntreating the nutritional quantities energy, protein, and fat consumption as error-prone co-\n18\nTable 4: Monte Carlo standard errors (MC-SE) and average of the bootstrap plug-in standard\nerrors (Avg-SE) for the GMM estimators with the minimax weighting scheme in simulation\nsettings I and III with nrep = 2 replicates and \u03c1 = 0.5.\nSetting\nU\nCoe\ufb00\nn = 500\nn = 1000\nMC-SE\nAvg-SE\nMC-SE\nAvg-SE\nI\nNormal\n\u02c6\u03b21\n0.031\n0.030\n0.025\n0.022\n\u02c6\u03b22\n0.031\n0.029\n0.023\n0.021\n\u02c6\u03b300\n0.043\n0.044\n0.033\n0.032\nt2.5\n\u02c6\u03b21\n0.029\n0.026\n0.021\n0.018\n\u02c6\u03b22\n0.029\n0.026\n0.019\n0.019\n\u02c6\u03b300\n0.038\n0.036\n0.028\n0.025\nIII\nNormal\n\u02c6\u03b21\n0.037\n0.033\n0.021\n0.024\n\u02c6\u03b22\n0.031\n0.031\n0.024\n0.022\n\u02c6\u03b300\n0.055\n0.050\n0.038\n0.036\n\u02c6\u03b31\n0.028\n0.027\n0.019\n0.020\n\u02c6\u03b32\n0.029\n0.028\n0.019\n0.019\nt2.5\n\u02c6\u03b21\n0.030\n0.028\n0.018\n0.021\n\u02c6\u03b22\n0.029\n0.028\n0.019\n0.021\n\u02c6\u03b300\n0.042\n0.043\n0.029\n0.032\n\u02c6\u03b31\n0.024\n0.026\n0.019\n0.018\n\u02c6\u03b32\n0.025\n0.024\n0.016\n0.018\nvariates with age considered an error-free covariate. Furthermore, all of these covariates\nwere standardized in the analysis. We computed the naive, moment-corrected, and GMM\nestimators, the latter with the three di\ufb00erent weighting schemes. We further reported the\nestimated standard errors for these estimators: The estimated standard errors for the naive\nestimators were obtained from the linear regression model of the outcome on all the observed\ncovariates, while the standard errors for the moment corrected estimators were calculated\nfrom the robust estimator given in Buonaccorsi (2010, Section 5.4). The estimated standard\nerrors for the GMM estimators were calculated in the same manner as in our simulation\nusing the asymptotic covariance and the bootstrap procedure.\nTable 5 demonstrates that the consequences of ignoring measurement error are apparent\n\u2013 the naive estimator exhibits dramatic attenuation for all the nutritional variables. On the\nother hand, the moment-corrected and GMM approaches all appear to correct for the bias as\nthe estimated coe\ufb03cients for energy, protein and fat intake are much larger in absolute terms.\nComparing the di\ufb00erent corrected estimators, we note the magnitude of the coe\ufb03cients\n19\nTable 5: NHANES study estimated coe\ufb03cients for naive, moment-corrected (MC) and GMM\nestimators with equal (Equal), minimax (MM), and quasi-likelihood weights (QL) weights.\nNaive\nMC\nGMM\nEqual\nMM\nQL\nIntercept\n26.39 (0.18)\n26.39 (0.18)\n26.63 (0.18)\n26.59 (0.18)\n26.56 (0.18)\nEnergy\n-0.86 (0.47)\n-2.3 (1.05)\n-2.7 (0.78)\n-2.59 (0.85)\n-2.49 (0.83)\nProtein\n1.29 (0.34)\n3.35 (1.03)\n3.56 (0.43)\n3.68 (0.49)\n3.54 (0.47)\nFat\n0.54 (0.43)\n1.03 (0.99)\n0.88 (0.57)\n0.89 (0.64)\n0.89 (0.59)\nAge\n3.92 (0.18)\n3.71 (0.19)\n3.82 (0.16)\n3.78 (0.16)\n3.79 (0.16)\ncorresponding to energy and protein are smaller for moment correction than for GMM. The\nreverse holds for the coe\ufb03cient corresponding to fat consumption. The e\ufb00ect of age on BMI\nis similar across all the estimators, which can be explained by the low correlation between age\nand each of the nutritional variables (the correlations between age and the averaged energy,\nprotein, and fat intakes are -0.02, 0.10, and 0.03, respectively). Finally, the GMM estimators\ntend to have lower estimated standard errors than the moment-corrected estimators. This\npossibly re\ufb02ects information contributions from the phase function beyond the \ufb01rst two\nmoments.\n6.\nConclusion\nIn this paper, we have explored distribution-free solutions for parameter estimation in the\nlinear errors-in-variables model with heteroscedastic measurement errors across cases. The\nnewly proposed solution combines the popular moment-corrected estimator with a phase\nfunction-based estimator using a GMM framework. On the one hand, the proposed GMM\nestimator inherits estimating equations from the moment-corrected estimator and in this\nway is able to relax a strict asymmetry condition imposed on the covariates as in the phase\nfunction-based estimator proposed by Nghiem et al. (2020). On the other hand, the proposed\nGMM estimator inherits estimating equations from the phase function-based estimator that\nleverages the skewness of the true covariates if present, and introduces observation-speci\ufb01c\nweighting to account for the measurement error heteroscedasticity. Our simulation studies\nshow that when the measurement errors are normal, the GMM estimator is competitive with\nthe moment-corrected estimator. Nevertheless, when measurement errors are non-normal,\nthe GMM estimator has superior performance across all simulation settings considered, in-\ncluding ones where some covariates are symmetrically distributed.\nWe close by noting some future research that could be explored relating to this problem.\nFirstly, in the multivariate setting, the estimation of observation-level measurement error\n20\ncovariance matrices is a challenge. Some form of covariance regularization could be applied\nto further improve the estimation e\ufb03ciency of the regression parameters. Secondly, the idea\nof a continuously-updating GMM treating the matrix \u2126S as an explicit function of (\u03b2, \u03b3)\ncan be explored. This could further improve estimator performance, but does increase the\ncomputational complexity of the problem. Finally, the development of tools for exploring\nand quantifying the skewness of the true covariates subject to symmetric measurement error\nwill help practitioners understand when the GMM approach is bene\ufb01cial.\n21\nSupplementary Materials\nS1.\nProof of Lemma 1\nIn this section, we provide proof of Lemma 1 from Section 3.1 in the main paper. For clarity,\nthe lemma is restated here. To this end, let \u03b8 = (\u03b2, \u03b3) and recall that S(\u03b8) = [S\u22a4\nL(\u03b8), S\u22a4\n\u02dcD(\u03b8)]\u22a4\ndenotes the vector of gradient equations with SL(\u03b8) the gradient vector of the corrected L2\nnorm L(\u03b8) as de\ufb01ned in Section 2.2, and with S \u02dcD(\u03b8) the gradient of the phase function-based\nstatistic \u02dcD(\u03b8) as de\ufb01ned in Section 2.3 of the main paper. Let S0(\u03b8) = limn\u2192\u221eE[S(\u03b8)]\ndenote the limiting expectation of the gradient equations. Subsequently, de\ufb01ne\nQ0(\u03b8) = S\u22a4\n0 (\u03b8)\u2126\u22121\nS S0(\u03b8).\nLemma 1 now follows.\nLemma 2. Assume that all variables in the model have at least two \ufb01nite moments. For\n\u03b8 \u2208\u0398 \u2286Rp+q+1, the function Q(\u03b8)\np\u2192Q0(\u03b8) uniformly.\nThe proof of Lemma 1 relies on establishing the conditions established in Lemma 2.9 of\nNewey and McFadden (1994). Speci\ufb01cally, the proof \ufb01rst shows that function Q converges in\nprobability to Q0 for a \ufb01xed value of \u03b8 and is continuously di\ufb00erentiable for all \u03b8 \u2208\u0398. Then,\nit establishes a Lipschitz condition that bounds the di\ufb00erence between Q at two arbitrary\nparameter values. This Lipschitz condition is crucial to show that Q converges uniformly\nin probability to Q0. Finally, the proof shows that S(\u03b8) and \u2207S(\u03b8) = \u2202S(\u03b8)/\u2202\u03b8 converge\nuniformly to their limiting expected values.\nThus, the result from Newey & McFadden\napplies and the required uniform convergence of Q to Q0 follows.\nProof of Lemma 1. Consider the function Q(\u03b8) = S\u22a4(\u03b8)\u2126\u22121\nS S(\u03b8) and a \ufb01xed value \u03b8 \u2208\u0398.\nThen, Q(\u03b8)\nP\u2192Q0(\u03b8) by Slutsky\u2019s theorem and the continuous mapping theorem. Moreover,\nQ(\u03b8) is continuously di\ufb00erentiable for all \u03b8 \u2208\u0398. Thus, by the mean value theorem, for\n\u03b81, \u03b82 \u2208\u0398, we have\nQ(\u03b81) \u2212Q(\u03b82) = \u2207Q(\u03b8z)(\u03b81 \u2212\u03b82),\nwhere \u2207Q(\u03b8) = \u2202Q/\u2202\u03b8 and \u03b8z is a linear interpolant of \u03b81 and \u03b82. Applying the Cauchy-\nSchwarz inequality, we obtain\n|Q(\u03b81) \u2212Q(\u03b82)| \u2264\u2225\u2207Q(\u03b8z)\u2225\u2225\u03b81 \u2212\u03b82\u2225.\nSubsequently, uniform convergence in probability will follow by by Lemma 2.9 of Newey and McFadden\n(1994) if we can establish the the Lipschitz condition that for some constants \u03b1 > 0 and a\n22\nsequence Bn = Op(1) such that for \u03b81 and \u03b82, we have\n|Q(\u03b81) \u2212Q(\u03b82)| \u2264Bn\u2225\u03b81 \u2212\u03b82\u2225\u03b1.\n(S1.12)\nObserve that equation (S1.12) will hold for \u03b1 = 1 if \u2225\u2207Q(\u03b8z)\u2225= Op(1).\nNow, by\nde\ufb01nition \u2207Q(\u03b8) = 2[\u2207S(\u03b8)]\u22a4\u2126\u22121\nS S(\u03b8) with \u2207S(\u03b8) = \u2202S(\u03b8)/\u2202\u03b8 a (p+q +1)\u00d7(p+q +1)\nmatrix of partial derivatives. By another application of the Cauchy-Schwarz inequality, we\nobtain\n\u2225\u2207Q(\u03b8z)\u2225\u2264cS\u2225\u2207S(\u03b8z)\u2225\u2225S(\u03b8z)\u2225\u2264cS sup\n\u03b8\u2208\u0398\n\u2225\u2207S(\u03b8)\u2225\u00b7 sup\n\u03b8\u2208\u0398\n\u2225S(\u03b8)\u2225\nwhere cS is a constant depending only on \u2126S and not on the arguments \u03b8. Thus, it remains\nonly to be shown that S(\u03b8) and \u2207S(\u03b8) converge uniformly to E[S(\u03b8)] and E[\u2207S(\u03b8)].\nTo this end, consider the components SL(\u03b8) and \u2207SL(\u03b8). These functions are continuous\nfor all \u03b8 \u2208\u0398. Furthermore, provided the random variables (Xj, Zj), Uj, and \u03b5j used to\nde\ufb01ne Q have \ufb01nite variances, there exists dominating functions dL,1(\u03b8) and dL,2(\u03b8) such\nthat \u2225SL(\u03b8)\u2225\u2264dL,1(\u03b8) and \u2225\u2207SL(\u03b8)\u2225\u2264dL,2(\u03b8) for all \u03b8 \u2208\u0398 by a uniform law of large\nnumbers as in Hoadley (1971) or P\u00a8otscher and Prucha (1989).\nNext, consider the component S \u02dcD(\u03b8) and \u2207S \u02dcD(\u03b8). Again, these functions are continuous\nfor all \u03b8 \u2208\u0398. While slightly tedious, one can also verify that a dominating function d \u02dcD,1(\u03b8)\nexists for S \u02dcD(\u03b8) provided (Xj, Zj) and Uj have \ufb01nite \ufb01rst moments. Similarly, a dominating\nfunction d \u02dcD,2(\u03b8) exists for \u2207S \u02dcD(\u03b8) provided (Xj, Zj) and Uj have \ufb01nite second moments.\nAgain, by the same uniform law of large numbers, uniform convergence is achieved. Conse-\nquently, we have sup\u03b8\u2208\u0398 \u2225S(\u03b8)\u2225= Op(1) and sup\u03b8\u2208\u0398 \u2225\u2207S(\u03b8)\u2225= Op(1), the conditions of\nNewey and McFadden (1994) are satis\ufb01ed, and the required uniform convergence of Q(\u03b8) to\nQ0(\u03b8) follows.\n23\nS2.\nProof of Theorem 1\nAs Q0(\u03b8) is a positive de\ufb01nite quadratic form in terms of S0(\u03b8), the global minimum occurs\nat a point \u03b8\u2217if and only if S0(\u03b8\u2217) = 0 for a unique value of \u03b8\u2217. The proof of Theorem 1 thus\nrelies on showing that Q0(\u03b8), the uniform-in-probability limit of Q(\u03b8), has a unique global\nminimum at the true parameter values \u03b80 due to S0(\u03b8) = 0 only when \u03b8 = \u03b80. Throughout\nthe proof, for any random variable V , we let \u03c6V (t) denote its characteristic function. For\nany complex number z, let Re(z) and Im(z) be its real and imaginary parts, respectively.\nProof of Theorem 1. By Lemma 1, the GMM objective function Q(\u03b8) converges uniformly\nin probability to Q0(\u03b8). For consistency of the estimators obtained by minimizing Q(\u03b8), it\nsu\ufb03ces to establish that limiting function Q0(\u03b8) has a unique global minimum at the true\nparameter \u03b80 = (\u03b20, \u03b30). This proof will separately consider the L2 norm and phase function\ncomponents contributing to Q(\u03b8). Particularly, we will prove the two following statements:\nStatement 1: The \ufb01rst p + q + 1 elements of S0(\u03b8), which correspond to the estimating\nequations from the corrected L2 norm, has a unique solution at \u03b80.\nStatement 2: \u03b80 is always a solution to the last p + q + 1 elements of S0(\u03b8), which\ncorrespond to the estimating equations from the phase function distance \u02dcD.\nGiven the two previous statements, Theorem 1 will follow immediately. Indeed, these\ntwo statements imply that \u03b80 is the unique solution for S0(\u03b8) = 0 as a whole. This also\nestablishes that Q0(\u03b80) = 0, meaning Q0(\u03b8) has a unique global minimum of zero at \u03b80. As\na result, the estimator \u02c6\u03b8 that minimizes Q(\u03b8) is consistent for \u03b80.\nHence it remains to prove the two statements, and we will do it in two separate subsec-\ntions.\nS2.1\nProof of Statement 1\nConsider \ufb01rst the corrected L2 norm function with estimating equations SL(\u03b8) = [SL,\u03b2(\u03b8)\u22a4, SL,\u03b3(\u03b8)\u22a4]\u22a4\nas de\ufb01ned in equation (2.3) of the main paper with\nSL,\u03b2(\u03b8) = \u22122\nn\nn\nX\nj=1\nWj\n\u0000yj \u2212W\u22a4\nj \u03b2 \u2212Z\u22a4\nj \u03b3\n\u0001\n\u22122\nn\nn\nX\nj=1\n1\nnj\n\u03a3j \u03b2,\nSL,\u03b3(\u03b8) = \u22122\nn\nn\nX\nj=1\nZj\n\u0000yj \u2212W\u22a4\nj \u03b2 \u2212Z\u22a4\nj \u03b3\n\u0001\n.\n24\nThe expected values E[SL,\u03b2(\u03b8)] and E[SL,\u03b3(\u03b8)] are found by evaluating the conditional ex-\npectations of the component summands. For SL,\u03b2(\u03b8), we have\nE\nh\nWj\n\u0000yj \u2212W\u22a4\nj \u03b2 \u2212Z\u22a4\nj \u03b3\n\u0001\f\f\f Xj, Zj\ni\n= E\nh\u0000Xj + Uj\n\u0001\u0000X\u22a4\nj \u03b20 + Z\u22a4\nj \u03b30 +\u03b5j \u2212X\u22a4\nj \u03b2 \u2212Z\u22a4\nj \u03b3 \u2212U\u22a4\nj \u03b2\n\u0001\f\f\f Xj, Zj\ni\n(i)= Xj X\u22a4\nj\n\u0000\u03b20 \u2212\u03b2\n\u0001\n+ Xj Z\u22a4\nj\n\u0000\u03b30 \u2212\u03b3\n\u0001\n\u2212n\u22121\nj\n\u03a3j \u03b2,\nwhere step (i) follows from the independence of Uj and (Xj, Zj), as well as noting that\nE\n\u0002\nUj U\u22a4\nj\n\u0003\n= n\u22121\nj\n\u03a3j. Similarly, for SL,\u03b3(\u03b8), we have\nE\nh\nZj\n\u0000yj \u2212W\u22a4\nj \u03b2 \u2212Z\u22a4\nj \u03b3\n\u0001\f\f\f Xj, Zj\ni\n= Zj X\u22a4\nj\n\u0000\u03b20 \u2212\u03b2\n\u0001\n+ Zj Z\u22a4\nj\n\u0000\u03b30 \u2212\u03b3\n\u0001\n.\nLetting (X, Z) denote an independent copy of (Xj, Zj), we have\nE[SL,\u03b2(\u03b8)] = \u22122 E\n\u0002\nX X\u22a4\u0003\u0000\u03b20 \u2212\u03b2\n\u0001\n\u22122 E\n\u0002\nX Z\u22a4\u0003\u0000\u03b30 \u2212\u03b3\n\u0001\nE[SL,\u03b3(\u03b8)] = \u22122 E\n\u0002\nZ X\u22a4\u0003\u0000\u03b20 \u2212\u03b2\n\u0001\n\u22122 E\n\u0002\nZ Z\u22a4\u0003\u0000\u03b30 \u2212\u03b3\n\u0001\n.\nAs a consequence of Lemma 1, SL,\u03b2(\u03b8) and SL,\u03b3(\u03b8) converge uniformly to E[SL,\u03b2(\u03b8)] and\nE[SL,\u03b3(\u03b8)], the \ufb01rst p + q + 1 components of S0(\u03b8). It is straightforward to see that the\ncorresponding system of equations\nE[SL,\u03b2(\u03b8)] = 0\nand\nE[SL,\u03b3(\u03b8)] = 0\nhas a unique solution at \u03b8 = \u03b80 = (\u03b20, \u03b30).\nS2.2\nProof of Statement 2\nConsider next the phase function-based criterion \u02dcD(\u03b8) directly. From Lemma 1, we have\nthe uniform convergence of \u02dcD(\u03b8) to a limiting function D0(\u03b8). We will now evaluate this\nlimiting function. Recall that\n\u02dcD(\u03b8) =\nZ t\u2217\n0\n \nCy(t)\n\" n\nX\nj=1\nqj sin\n\b\nt\n\u0000W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3\n\u0001\t\n#\n\u2212Sy(t)\n\" n\nX\nj=1\nqj cos\n\b\nt\n\u0000W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3\n\u0001\t\n# !2\nKt\u2217(t)dt.\n(S2.13)\n25\nLet V0 = X\u22a4\u03b20 + Z\u22a4\u03b30 and Y = V0 + \u03b5. For arbitrary t, by the weak law of large numbers,\nCy(t) = 1\nn\nn\nX\nj=1\ncos(yjt)\np\u2192E [cos(Y t)] = Re[\u03c6Y (t)] = Re[\u03c6V0(t)]\u03c6\u03b5(t),\nwhere the last equality follows upon noting that \u03b5 has a real-valued characteristic function.\nSimilarly, Sy(t)\np\u2192Im[\u03c6V0(t)]\u03c6\u03b5(t). Furthermore, noting that for any \u03b8, the random variables\nsin\n\u0002\nt(W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3)\n\u0003\nand cos\n\u0002\nt(W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3)\n\u0003\nare bounded, and subsequently have \ufb01nite\nvariances. By a generalized weak law of large numbers,\nn\nX\nj=1\nqj sin\n\b\nt\n\u0000W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3\n\u0001 \t\np\u2192E\n\" n\nX\nj=1\nqj sin\n\b\nt\n\u0000W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3\n\u0001\t\n#\n.\nLetting Vj(\u03b2, \u03b3) = X\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3, we have\nE\n\u0002\nsin\n\b\nt\n\u0000W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3\n\u0001\t\u0003\n= Im\n\b\n\u03c6Vj(\u03b2,\u03b3)(t)\n\t\n\u03c6U\u22a4\nj \u03b2(t),\nwhere we make use of the fact that U\u22a4\nj \u03b2 has a symmetric distribution about zero and hence\na real-valued characteristic function. Since (Xj, Zj) are iid by Condition C1, the random\nvariables Vj(\u03b2, \u03b3), j = 1, . . . , n are also iid and have a common characteristic function\n\u03c6V (\u03b2,\u03b3)(t) = \u03c6Vj(\u03b2,\u03b3)(t) for j = 1, . . . , n. As a consequence, we have\nE\n\" n\nX\nj=1\nqj sin\n\b\nt\n\u0000W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3\n\u0001\t\n#\n= Im\n\b\n\u03c6V (\u03b2,\u03b3)(t)\n\t\n n\nX\nj=1\nqj\u03c6U\u22a4\nj \u03b2(t)\n!\n.\nSimilarly,\nE\n\" n\nX\nj=1\nqj cos\n\b\nt\n\u0000W\u22a4\nj \u03b2 + Z\u22a4\nj \u03b3\n\u0001\t\n#\n= Re\n\b\n\u03c6V (\u03b2,\u03b3)(t)\n\t\n n\nX\nj=1\nqj\u03c6U\u22a4\nj \u03b2(t)\n!\n.\nLetting h(t, \u03b2) = \u03c6\u03b5(t)2 hPn\nj=1 qj\u03c6U\u22a4\nj \u03b2(t)\ni2\n, and recalling the established uniform conver-\ngence for (\u03b2, \u03b3) \u2208\u0398, the statistic \u02dcD(\u03b8) converges uniformly to\nD0(\u03b8) =\nZ t\u2217\n0\nh\nRe {\u03c6V0 (t)} Im\n\b\n\u03c6V (\u03b2,\u03b3)(t)\n\t\n\u2212Im {\u03c6V0(t)} Re\n\b\n\u03c6V (\u03b2,\u03b3)(t)\n\t i2\nh(t, \u03b2)Kt\u2217(t)dt\n=\nZ t\u2217\n0\nh\nIm{\u03c6V0\u2212V (\u03b2,\u03b3)(t)}\ni2\nh(t, \u03b2)Kt\u2217dt,\n26\nwhere random variables V0 and V (\u03b2, \u03b3) are independent. Note that\nIm\n\b\n\u03c6V0\u2212V (\u03b2,\u03b3)(t)\n\t\n= 0 for all t \u2208R\nif and only if the distribution of V0 \u2212V (\u03b2, \u03b3) is symmetric about 0. When Condition C4\nholds, this is only true for \u03b8 = \u03b80 = (\u03b20, \u03b30). On the other hand, when Condition C4\ndoes not hold, D0(\u03b8) may have in\ufb01nitely many global minima, but one of those minima still\noccurs at \u03b8 = \u03b80.\nWe conclude by noting that \u02dcD(\u03b8) is continuous, as is the gradient vector \u2207\u02dcD(\u03b8) =\n\u2202\u02dcD(\u03b8)/\u2202\u03b8. From Lemma 1, it subsequently follows that \u2207\u02dcD(\u03b8) also converges uniformly\nto \u2207D0(\u03b8). Thus, \u2207D0(\u03b80) = 0, even though this is not a unique solution to this system\nof equations. Note also that \u2207D0(\u03b80) represents the last p + q + 1 elements of S0(\u03b8). The\nproof is now complete.\nS3.\nCalculating the Quasi-Likelihood Weights\nThe quasi-likelihood weights are de\ufb01ned in Section 3.2 of the main paper to be the minimizer\nof the L2 discrepancy\nL(q) =\nn\nX\nj=1\n(Wj \u2212\u02c6\u00b5q)\u22a4\u0000\u03a3x + n\u22121\nj \u03a3j\n\u0001\u22121 (Wj \u2212\u02c6\u00b5q)\u22a4,\nwhere \u02c6\u00b5q = Pn\nj=1 qj Wj, subject to qj \u22650, j = 1, . . . , n and Pn\nj=1 qj = 1. To \ufb01nd the\nminimizer, let \u2126j = \u03a3x + n\u22121\nj \u03a3j for j = 1, . . . , n. Some algebraic manipulation gives\nL(q) =\nn\nX\nj=1\nW\u22a4\nj \u2126\u22121\nj\nWj \u2212\u02c6\u00b5\u22a4\nq\n n\nX\nj=1\n\u2126\u22121\nj\nWj\n!\n\u2212\n n\nX\nj=1\nW\u22a4\nj \u2126\u22121\nj\n!\n\u02c6\u00b5q + \u02c6\u00b5\u22a4\nq\n n\nX\nj=1\n\u2126\u22121\nj\n!\n\u02c6\u00b5q.\nNote that L(q) is a function of q only through \u02c6\u00b5q. To calculate the weights, we de\ufb01ne the\nfunction D(q) to be only the terms in L(q) involving \u02c6\u00b5q, and also introducing two Lagrange-\nmultiplier type terms. The \ufb01rst of these ensures the weights qj sum to 1, while the second\nensures a numerically stable solution by constraining the squared di\ufb00erences between the qj.\n27\nThe resulting function to be minimized is\nD(q, \u03bb)\n=\n\u2212\u02c6\u00b5\u22a4\nq\n n\nX\nj=1\n\u2126\u22121\nj\nWj\n!\n\u2212\n n\nX\nj=1\nW\u22a4\nj \u2126\u22121\nj\n!\n\u02c6\u00b5q + \u02c6\u00b5\u22a4\nq\n n\nX\nj=1\n\u2126\u22121\nj\n!\n\u02c6\u00b5q\n+2\u03bb\n n\nX\nj=1\nqj \u22121\n!\n+ \u03b3\nX\nj,k\n(qj \u2212qk)2.\nThis function is minimized over (q, \u03bb), while \u03b3 is a user-speci\ufb01ed constant ensuring a nu-\nmerically stable solution. Now, de\ufb01ning\nA1 =\nn\nX\nj=1\n\u2126\u22121\nj\nWj\nand\nA2 =\nn\nX\nj=1\n\u2126\u22121\nj\nthe target function can be written in the convenient form\nD(q, \u03bb) = \u22122\u02c6\u00b5\u22a4\nq A1 + \u02c6\u00b5\u22a4\nq A2 \u02c6\u00b5q + 2\u03bb\n n\nX\nj=1\nqj \u22121\n!\n+ \u03b3\nX\ni,j\n(qi \u2212qj)2.\nThis function is quadratic in (q, \u03bb) with a global minimum that can be found by solving\na linear system of equations. Taking partial derivatives of with respect to the qk, for k =\n1, . . . , n, we have estimating equations\n\u2202D\n\u2202qk\n= \u22122V\u22a4\nk A1 + 2V\u22a4\nk A2 \u02c6\u00b5q + 2\u03bb + 2\u03b3\nX\nj\u0338=k\n(qk \u2212qj) = 0.\nHere, Vk denotes a n \u00d7 1 column vector of zeros, with the kth entry equal to 1. Writing\nthese estimating equations explicitly in terms of the weights q gives\n\u2212V\u22a4\nk A1 +\nn\nX\nj=1\nqj\n\u0000V\u22a4\nk A2Vj\n\u0001\n+ \u03bb + \u03b3\nX\nj\u0338=k\n(qk \u2212qj) = 0, k = 1, . . . , n.\nThe minimizer of D(q, \u03bb) is found by solving the linear system\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nV\u22a4\n1 A2V1 + (n \u22121)\u03b3\nV\u22a4\n1 A2V2 \u2212\u03b3\n\u00b7 \u00b7 \u00b7\nV\u22a4\n1 A2Vn \u2212\u03b3\n1\nV\u22a4\n2 A2V1 \u2212\u03b3\nV\u22a4\n2 A2V2 + (n \u22121)\u03b3\n\u00b7 \u00b7 \u00b7\nV\u22a4\n2 A2Vn \u2212\u03b3\n1\n...\n...\n...\n...\n...\nV\u22a4\nn A2V1 \u2212\u03b3\nV\u22a4\nn A2X2 \u2212\u03b3\n\u00b7 \u00b7 \u00b7\nV\u22a4\nn A2Vn + (n \u22121)\u03b3\n1\n1\n1\n\u00b7 \u00b7 \u00b7\n1\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nq1\nq2\n...\nqn\n\u03bb\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nV\u22a4\n1 A1\nV\u22a4\n2 A1\n...\nV\u22a4\nn A1\n1\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n.\nNumerical exploration suggests the solution \u02c6q of this system, which is easily obtained\n28\nnumerically, is fairly robust with regards to the speci\ufb01c choice of \u03b3. In a simulation study not\nreported here, the choices \u03b3 = 1/n and \u03b3 = log(n) resulted in nearly identical estimators of\nthe observation-speci\ufb01c weights. Furthermore, any value of \u03b3 > 0 resulted in a numerically\nstable system to solve. We therefore use \u03b3 = 1/n in the remainder of the paper.\nS4.\nAdditional Simulation Results\nS4.1\nSimulation for simple linear EIV models\nIn this section, we report a set of simulation results for the simple errors-in-variables model\nyj = \u03b3 + \u03b2Xj + \u01ebj, Wjk = Xj + Ujk for j = 1, . . . , n and k = 1, . . . , nrep. In this setting, the\ntrue covariates Xj are generated from the half-normal distribution scaled to have variance\n1, i.e. Xj\niid\n\u223c(1 \u22122/\u03c0)\u22121/2 \f\fN(0, 1)\n\f\f. Three scenarios are considered for the distribution of\nthe measurement error terms, namely normal, Ujk \u223cN(0, \u03c32\nj), Student\u2019s t with 2.5 degrees\nof freedom, Ujk \u223c\u03c3j/\n\u221a\n5 t2.5, and a contaminated normal, Ujk \u223c\u03c3j/\n\u221a\n10.9{0.9N(0, 1) +\n0.1N(0, 102)}. In each scenario, the Ujk are generated independently, have mean 0, and are\nscaled to have Var(Ujk) = \u03c32\nj for k = 1, . . . , nrep. In all the three scenarios, the measurement\nerror variances \u03c32\nj are generated from the uniform distribution nrep\u00d7U(0.2, 1.5), so the signal-\nto-noise ratio Var(Xj)/Var(Uj) for the averaged replicate measurement error Uj ranges from\n2/3 (fairly weak signal) to 5 (fairly strong signal). The true intercept and slope are set to\nbe \u03b300 = 2 and \u03b2 = 1, respectively. The regression error \u01ebj\u2019s are generated to match the\ndistribution of the measurement error in each scenario and with constant variance \u03c32\n\u03b5 = 0.25.\nFor each generated sample, we compute the same set of estimators and used the same\ncriterion det(MSErob) as described in Section 4.2 of the main paper. Note that in the simple\nlinear EIV model, the two heteroscedastic weighting schemes (minimax and quasi-likelihood)\nresult in the same estimates. Table 6 summarizes the results.\nIt comes as no surprise that the naive estimator has the worst performance among the\nestimators considered. The large values observed demonstrate the consequences of ignoring\nmeasurement errors when they are present. When considering the various corrected esti-\nmators, the GEE estimators outperform the moment-corrected approach regardless of the\nweighting scheme used. The improvement of GMM estimators over moment correction is\nespecially pronounced in contaminated normal error scenario, with the t2.5 error scenario\nalso showing some large decreases in estimation error. These cases illustrate the value of\ncombining moment correction with the phase function-based approach. When comparing\nthe two GMM weighting schemes, the estimator with quasi-likelihood weights also outper-\nforms the equal weights estimator in almost all cases. The only exceptions occur under the\nnormal measurement error model, where equal weighting in two instances performs better\nthan quasi-likelihood weighting.\n29\nTable 6: Simple Linear EIV model performance naive OLS (Naive), moment-corrected (MC),\nand GMM estimators with equal (Equal), minimax (MM) and quasi-likelihood (QL) weights\nwith nrep \u2208{2, 3} as measured by det(1000 \u00d7 MSErob).\nU\nn\nnrep = 2\nnrep = 3\nNaive\nMC\nGMM\nNaive\nMC\nGMM\nEqual\nQL\nEqual\nQL\nNormal\n250\n381.98\n29.58\n26.80\n25.80\n397.80\n28.47\n25.08\n25.24\n500\n181.67\n9.44\n6.21\n6.65\n160.34\n8.13\n6.52\n6.10\n1000\n88.78\n2.13\n1.63\n1.56\n90.83\n1.80\n1.40\n1.40\nt2.5\n250\n263.14\n20.89\n10.56\n8.74\n322.03\n17.25\n9.32\n7.99\n500\n128.75\n5.66\n2.29\n1.90\n143.20\n3.75\n2.05\n1.60\n1000\n87.26\n1.44\n0.52\n0.42\n72.70\n1.27\n0.49\n0.36\nCont.Normal\n250\n19.96\n22.19\n2.18\n1.97\n20.05\n12.25\n1.94\n1.26\n500\n44.30\n15.28\n0.98\n0.66\n46.25\n11.49\n0.90\n0.48\n1000\n98.12\n14.27\n0.75\n0.48\n90.87\n12.58\n0.66\n0.37\nS4.2\nAdditional results for multiple linear EIV models\nIn this section, we summarize simulation results equivalent to those in Section 4.2 of the\nmain paper. Tables S2 through S4 are analogous to Tables 1 through 3, but with nrep = 3\nwhereas the main paper presents results for nrep = 2. For greater speci\ufb01city regarding the\nsimulation con\ufb01gurations, see the descriptions in Section 4.2 of the main paper. We note here\nhere that similar conclusions can be drawn for the case with nrep = 3 replicates. Speci\ufb01cally,\nwhen the measurement error follows a t2.5 or contaminated normal distribution, the GMM\nestimators outperform moment correction. On the other hand, when the measurement error\nfollows a normal distribution, there isn\u2019t a clear preference for either the moment corrected\nor GMM estimators, each in turn outperforming the other. Finally, Table S5 is analogous to\nTable 4 of the main paper, showing the accuracy with which the standard error is estimated\nfor the model parameters.\n30\nTable 7: Setting I performance of uncontaminated OLS (True), naive OLS (Naive), moment-\ncorrected (MC), and GMM estimators with equal (Equal), minimax (MM) and quasi-\nlikelihood (QL) weights with nrep = 3 as measured by det(1000 \u00d7 MSErob).\nR\nU\nn\nTrue\nNaive\nMC\nGMM\nEqual\nMM\nQL\n\u03c1 = 0\nNormal\n250\n0.010\n14.237\n1.565\n2.082\n2.023\n2.087\n500\n0.001\n3.385\n0.188\n0.182\n0.180\n0.180\n1000\n0.000\n0.944\n0.028\n0.029\n0.031\n0.030\nt2.5\n250\n0.012\n20.132\n1.827\n0.967\n0.805\n0.802\n500\n0.001\n4.778\n0.108\n0.075\n0.072\n0.071\n1000\n0.000\n1.364\n0.018\n0.014\n0.012\n0.012\nCont.Normal\n250\n0.009\n13.100\n1.009\n0.853\n0.650\n0.676\n500\n0.001\n3.744\n0.128\n0.100\n0.073\n0.080\n1000\n0.000\n1.036\n0.017\n0.015\n0.012\n0.013\n\u03c1 = 0.5\nNormal\n250\n0.009\n34.094\n2.511\n3.097\n2.892\n2.771\n500\n0.001\n6.323\n0.286\n0.252\n0.274\n0.253\n1000\n0.000\n1.715\n0.042\n0.042\n0.044\n0.042\nt2.5\n250\n0.008\n39.974\n2.152\n1.134\n1.004\n1.129\n500\n0.001\n8.377\n0.166\n0.105\n0.108\n0.111\n1000\n0.000\n3.272\n0.027\n0.020\n0.017\n0.019\nCont.Normal\n250\n0.009\n41.149\n2.271\n1.761\n1.400\n1.475\n500\n0.001\n10.698\n0.311\n0.241\n0.157\n0.191\n1000\n0.000\n2.413\n0.035\n0.027\n0.019\n0.023\n31\nTable 8: Setting II performance of uncontaminated OLS (True), naive OLS (Naive), moment-\ncorrected (MC), and GMM estimators with equal (Equal), minimax (MM) and quasi-\nlikelihood (QL) weights with nrep = 3 as measured by det(1000 \u00d7 MSErob).\nR\nU\nn\nTrue\nNaive\nMC\nGMM\nEqual\nMM\nQL\n\u03c1 = 0\nNormal\n250\n0.001\n34.108\n2.509\n2.777\n2.673\n2.403\n500\n0.000\n1.716\n0.096\n0.108\n0.122\n0.108\n1000\n0.000\n0.089\n0.002\n0.003\n0.003\n0.003\nt2.5\n250\n0.002\n13.080\n1.388\n1.133\n0.814\n0.746\n500\n0.000\n2.394\n0.051\n0.042\n0.036\n0.034\n1000\n0.000\n0.255\n0.002\n0.001\n0.002\n0.001\nCont.Normal\n250\n0.001\n18.461\n1.571\n1.483\n1.460\n1.041\n500\n0.000\n1.495\n0.055\n0.055\n0.055\n0.036\n1000\n0.000\n0.057\n0.001\n0.001\n0.001\n0.001\n\u03c1 = 0.5\nNormal\n250\n0.001\n51.039\n9.331\n14.770\n18.866\n14.313\n500\n0.000\n4.915\n0.234\n0.266\n0.291\n0.257\n1000\n0.000\n0.336\n0.009\n0.010\n0.011\n0.011\nt2.5\n250\n0.002\n92.843\n6.001\n3.647\n2.989\n2.726\n500\n0.000\n5.910\n0.143\n0.105\n0.104\n0.090\n1000\n0.000\n0.648\n0.006\n0.004\n0.003\n0.003\nCont.Normal\n250\n0.001\n60.581\n4.318\n3.805\n3.496\n2.731\n500\n0.000\n4.936\n0.150\n0.149\n0.128\n0.094\n1000\n0.000\n0.296\n0.005\n0.004\n0.004\n0.003\n32\nTable 9:\nSetting III performance of uncontaminated OLS (True), naive OLS (Naive),\nmoment-corrected (MC), and GMM estimators with equal (Equal), minimax (MM) and\nquasi-likelihood (QL) weights with nrep = 3 as measured by det(1000 \u00d7 MSErob).\nR\nU\nn\nTrue\nNaive\nMC\nGMM\nEqual\nMM\nQL\n\u03c1 = 0\nNormal\n250\n0.001\n14.862\n1.997\n1.972\n1.964\n1.976\n500\n0.000\n0.789\n0.043\n0.043\n0.043\n0.043\n1000\n0.000\n0.042\n0.001\n0.001\n0.001\n0.001\nt2.5\n250\n0.001\n13.456\n1.132\n1.087\n0.978\n1.056\n500\n0.000\n0.837\n0.022\n0.022\n0.020\n0.020\n1000\n0.000\n0.118\n0.001\n0.001\n0.001\n0.001\nCont.Normal\n250\n0.001\n21.877\n2.346\n2.379\n2.159\n2.282\n500\n0.000\n1.116\n0.053\n0.053\n0.049\n0.052\n1000\n0.000\n0.055\n0.002\n0.002\n0.001\n0.001\n\u03c1 = 0.5\nNormal\n250\n0.001\n51.724\n4.557\n4.554\n4.543\n4.555\n500\n0.000\n3.059\n0.145\n0.145\n0.145\n0.145\n1000\n0.000\n0.180\n0.005\n0.005\n0.005\n0.005\nt2.5\n250\n0.001\n32.805\n2.819\n2.777\n2.470\n2.635\n500\n0.000\n2.937\n0.081\n0.076\n0.069\n0.070\n1000\n0.000\n0.354\n0.003\n0.003\n0.003\n0.003\nCont.Normal\n250\n0.001\n44.184\n4.592\n4.594\n4.243\n4.462\n500\n0.000\n3.465\n0.149\n0.149\n0.135\n0.145\n1000\n0.000\n0.205\n0.004\n0.005\n0.004\n0.004\n33\nTable 10: Monte Carlo standard errors (MC-SE) and average of average of the bootstrap\nplug-in standard errors (Avg-SE) for the GMM estimators with the minimax weighting\nscheme in simulation settings I and III with nrep = 3 replicates and \u03c1 = 0.5.\nSetting\nU\nCoe\ufb00\nn = 500\nn = 1000\nMC-SE\nAvg-SE\nMC-SE\nAvg-SE\nII\nNormal\n\u02c6\u03b21\n0.030\n0.029\n0.021\n0.021\n\u02c6\u03b22\n0.028\n0.029\n0.022\n0.021\n\u02c6\u03b300\n0.043\n0.044\n0.031\n0.031\nt2.5\n\u02c6\u03b21\n0.023\n0.026\n0.022\n0.019\n\u02c6\u03b22\n0.027\n0.025\n0.019\n0.019\n\u02c6\u03b300\n0.035\n0.036\n0.027\n0.025\nIV\nNormal\n\u02c6\u03b21\n0.036\n0.033\n0.021\n0.022\n\u02c6\u03b22\n0.032\n0.030\n0.021\n0.022\n\u02c6\u03b300\n0.061\n0.049\n0.037\n0.035\n\u02c6\u03b31\n0.026\n0.027\n0.020\n0.019\n\u02c6\u03b32\n0.028\n0.028\n0.019\n0.019\nt2.5\n\u02c6\u03b21\n0.029\n0.032\n0.020\n0.021\n\u02c6\u03b22\n0.027\n0.027\n0.020\n0.020\n\u02c6\u03b300\n0.043\n0.046\n0.032\n0.030\n\u02c6\u03b31\n0.028\n0.026\n0.022\n0.018\n\u02c6\u03b32\n0.029\n0.025\n0.016\n0.018\n34\nReferences\nBuonaccorsi, J. P. (2010). Measurement Error: Models, Methods, and Applications. CRC\npress.\nCarroll, R. J., Ruppert, D., Stefanski, L. A., and Crainiceanu, C. M. (2006). Measurement\nError in Nonlinear Models: A Modern Perspective. Chapman and Hall/CRC.\nCarroll, R. J. and Stefanski, L. A. (1990). Approximate quasi-likelihood estimation in models\nwith surrogate predictors. Journal of the American Statistical Association, 85(411):652\u2013\n663.\nDelaigle, A. and Hall, P. (2016). Methodology for non-parametric deconvolution when the\nerror distribution is unknown. Journal of the Royal Statistical Society: Series B: Statistical\nMethodology, pages 231\u2013252.\nErickson, T., Jiang, C. H., and Whited, T. M. (2014). Minimum distance estimation of\nthe errors-in-variables model using linear cumulant equations. Journal of Econometrics,\n183(2):211\u2013221.\nGillard, J. (2014). Method of moments estimation in linear regression with errors in both\nvariables. Communications in Statistics-Theory and Methods, 43(15):3208\u20133222.\nHanfelt, J. J. and Liang, K.-Y. (1997). Approximate likelihoods for generalized linear errors-\nin-variables models. Journal of the Royal Statistical Society: Series B (Statistical Method-\nology), 59(3):627\u2013637.\nHansen, L. P. (1982). Large sample properties of generalized method of moments estimators.\nEconometrica: Journal of the Econometric Society, pages 1029\u20131054.\nHansen, L. P., Heaton, J., and Yaron, A. (1996). Finite-sample properties of some alternative\ngmm estimators. Journal of Business & Economic Statistics, 14(3):262\u2013280.\n35\nHigdon, R. and Schafer, D. W. (2001). Maximum likelihood computations for regression\nwith measurement error. Computational Statistics & Data Analysis, 35(3):283\u2013299.\nHoadley, B. (1971). Asymptotic properties of maximum likelihood estimators for the inde-\npendent not identically distributed case. The Annals of Mathematical Statistics, pages\n1977\u20131991.\nHu, F. and Kalb\ufb02eisch, J. D. (1997).\nEstimating equations and the bootstrap.\nLecture\nNotes-Monograph Series, pages 405\u2013416.\nNewey, W. K. and McFadden, D. (1994). Chapter 36: Large sample estimation and hypoth-\nesis testing. Handbook of Econometrics, 4:2111\u20132245.\nNghiem, L. and Potgieter, C. J. (2018). Density estimation in the presence of heteroscedastic\nmeasurement error of unknown type using phase function deconvolution.\nStatistics in\nMedicine, 37(25):3679\u20133692.\nNghiem, L. H., Byrd, M. C., and Potgieter, C. J. (2020). Estimation in linear errors-in-\nvariables models with unknown error distribution. Biometrika, 107(4):841\u2013856.\nP\u00a8otscher, B. M. and Prucha, I. R. (1989). A uniform law of large numbers for dependent and\nheterogeneous data processes. Econometrica: Journal of the Econometric Society, pages\n675\u2013683.\nReiers\u00f8l, O. (1941). Con\ufb02uence analysis by means of lag moments and other methods of\ncon\ufb02uence analysis. Econometrica: Journal of the Econometric Society, pages 1\u201324.\nRousseeuw, P. J. and Driessen, K. V. (1999). A fast algorithm for the minimum covariance\ndeterminant estimator. Technometrics, 41(3):212\u2013223.\nSong, W. (2021).\nNonparametric inference methods for berkson errors.\nIn Handbook of\nMeasurement Error Models, pages 271\u2013292. Chapman and Hall/CRC.\n36\nStefanski, L. A. and Carroll, R. J. (1987). Conditional scores and optimal scores for gener-\nalized linear measurement-error models. Biometrika, 74(4):703\u2013716.\nStefanski, L. A. and Cook, J. R. (1995). Simulation-extrapolation: the measurement error\njackknife. Journal of the American Statistical Association, 90(432):1247\u20131256.\nWald, A. (1940). The \ufb01tting of straight lines if both variables are subject to error. The\nAnnals of Mathematical Statistics, 11(3):284\u2013300.\nXue-Kun Song, P. (2000). Multivariate dispersion models generated from gaussian copula.\nScandinavian Journal of Statistics, 27(2):305\u2013320.\n37\n"}