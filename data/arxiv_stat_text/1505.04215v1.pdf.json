{"text": "An Analysis of Active Learning With Uniform Feature Noise\nAaditya Ramdas12\naramdas@cs.cmu.edu\nBarnab\u00b4as P\u00b4oczos2\nbapoczos@cs.cmu.edu\nAarti Singh2\naarti@cs.cmu.edu\nLarry Wasserman12\nlarry@stat.cmu.edu\nDepartment of Statistics1 and Machine Learning Department2\nCarnegie Mellon University\nAugust 6, 2018\nAbstract\nIn active learning, the user sequentially chooses values for feature X and an oracle returns\nthe corresponding label Y . In this paper, we consider the e\ufb00ect of feature noise in active\nlearning, which could arise either because X itself is being measured, or it is corrupted in\ntransmission to the oracle, or the oracle returns the label of a noisy version of the query\npoint. In statistics, feature noise is known as \u201cerrors in variables\u201d and has been studied\nextensively in non-active settings. However, the e\ufb00ect of feature noise in active learning has\nnot been studied before. We consider the well-known Berkson errors-in-variables model with\nadditive uniform noise of width \u03c3.\nOur simple but revealing setting is that of one-dimensional binary classi\ufb01cation setting\nwhere the goal is to learn a threshold (point where the probability of a + label crosses half).\nWe deal with regression functions that are antisymmetric in a region of size \u03c3 around the\nthreshold and also satisfy Tsybakov\u2019s margin condition around the threshold. We prove min-\nimax lower and upper bounds which demonstrate that when \u03c3 is smaller than the minimiax\nactive/passive noiseless error derived in Castro & Nowak (2007), then noise has no e\ufb00ect on\nthe rates and one achieves the same noiseless rates. For larger \u03c3, the un\ufb02attening of the\nregression function on convolution with uniform noise, along with its local antisymmetry\naround the threshold, together yield a behaviour where noise appears to be bene\ufb01cial. Our\nkey result is that active learning can buy signi\ufb01cant improvement over a passive strategy\neven in the presence of feature noise.\n1\nIntroduction\nActive learning is a machine learning paradigm where the algorithm interacts with a label-\nproviding oracle in a feedback driven loop where past training data (features queried and cor-\nresponding labels) are used to guide the design of subsequent queries. Typically, the oracle is\n1\narXiv:1505.04215v1  [stat.ML]  15 May 2015\nqueried with an exact feature value and the oracle returns the label corresponding precisely to\nthat feature value. However, in many scenarios, the feature value being queried can be noisy and\nit helps to analyze what would happen in such a setting. Such situations include noisy sensor\nmeasurements of features, corrupted transmission of data from source to storage, or just access\nto a limited noisy oracle.\nThe errors-in-variables model has been well studied in the statistical literature and their e\ufb00ect\ncan be profound.\nIn density estimation, Gaussian error causes the minimax rate to become\nlogarithmic in sample size instead of polynomial, see Fan (1991). For results in passive regression,\nrefer to Fan et al. (1993); Fuller (2009); Carroll et al. (2010), and for passive classi\ufb01cation,\nsee Loustau & Marteau (2012).\nHowever, classi\ufb01cation has not been studied in the Berkson\nmodel introduced below. Also, deconvolution estimators require the noise fourier transform to\nbe bounded away from zero, ruling out uniform noise. Finally, to the best of our knowledge,\nfeature noise has not been studied for active learning in any setting.\nThe classical errors in variables model has the graphical form W \u2190X \u2192Y , representing\nW = X + \u03b4 ,\nY = m(X) + \u03f5 .\nHere, the label Y depends on the feature X but we do not observe X; rather we observe the\nnoisy feature W. The Berkson errors in variables model is\nX = W + \u03b4 ,\nY = m(X) + \u03f5 .\nThe di\ufb00erence is that we start with an observed feature W and then noise is added to determine\nX. Graphically, this model is W \u2192X \u2192Y .\nIn this paper, we focus on the Berkson error model since it intuitively makes more sense for active\nlearning - it captures the idea that we request a label for feature W, but the oracle returns the\nlabel for X which is a corrupted version generated from W, i.e. the noise occurs between the\nlabel request and the oracle output. We use uniform noise since it yields insightful behavior and\nalso has not been addressed in the literature. We conjecture that qualitatively similar results\nhold for other symmetric error models.\n1.1\nSetup\nThreshold Classi\ufb01cation.\nLet X = [\u22121, 1], Y = {+, \u2212}, and f : X \u2192Y denote a classi\ufb01-\ncation rule. Assuming 0/1 loss, the risk of the classi\ufb01cation rule f is R(f) = E[1{f(X)\u0338=Y }] =\nP(f(X) \u0338= Y ). It is known that the Bayes optimal classi\ufb01er, the best measurable classi\ufb01er that\nminimizes the risk f \u2217= arg minf R(f), has the following form\nf \u2217(x) =\n(\n+\nif m(x) \u22651/2 ,\n\u2212\nif m(x) < 1/2 ,\nwhere m(x) = P(Y = +|X = x) is the unknown regression function. In what follows, we will\nconsider the case where the f \u2217is a threshold classi\ufb01er, i.e. there exists a unique t \u2208[\u22121, 1] with\nm(t) = 1/2 such that m(x) < 1/2 if x < t, and m(x) > 1/2 if x > t.\n2\nBerkson Error Model.\nThe model is:\n1. User chooses W and requests label.\n2. Oracle receives a noisy W namely X = W + U.\n3. Oracle returns Y where P(Y = +|X = x) = m(x).\nWe take the noise to be uniform: U \u223cUnif[\u2212\u03c3, \u03c3], where the noise width \u03c3 is known for\nsimplicity.\nSampling Strategies.\nIn passive sampling, assume that we are given a batch of wi \u223cUnif[\u22121, 1]\nand corresponding labels yi sampled independently of {wj}j\u0338=i and {yj}j\u0338=i. In this case, a strat-\negy S is just an estimator Sn : (W \u00d7 Y )n \u2192[\u22121, 1] that returns a guess bt of the threshold t on\nseeing {wi, yi}n\ni=1.\nIn active sampling we are allowed to sequentially choose wi = Si(w1, . . . , wi\u22121, y1, . . . , yi\u22121),\nwhere Si is a possibly random function of past queries and labels, where the randomness is\nindependent of queries and labels. In this case, a strategy A is a sequence of functions Si :\n(W \u00d7 Y )i\u22121 \u2192[\u22121, 1] returning query points and an estimator Sn : (W \u00d7 Y )n \u2192[\u22121, 1] that\nreturns a guess bt at the end.\nLet SP\nn , SA\nn be the set of all passive or active strategies (and estimators) with a total budget of\nn labels.\nTo avoid the issue of noise resulting in a point outside the domain, we make a (Q)uerying\nassumption:\n(Q). Querying within \u03c3 of the boundary is disallowed.\nLoss Measure.\nLet bt = bt(W n\n1 , Y n\n1 ) denote an estimator of t using n samples from a passive or\nactive strategy. Our task will be to estimate the location of t, where we measure accuracy of an\nestimator bt by a loss function which is the point error |bt \u2212t|.\nFunction Class.\nIn the analysis of rates for classi\ufb01cation (among others), it is common to\nuse the Tsybakov Noise/Margin Condition (see Tsybakov (2004)), to characterize the behavior\nof m(x) around the threshold t. Given constants c, C with C \u2265c, k \u22651, and noise level \u03c3, let\nP(c, C, k, \u03c3) be the set of regression functions m(x) that satisfy the following conditions (T,M,B)\nfor some threshold t:\n(T). |x \u2212t|k\u22121 \u2265|m(x) \u22121/2| \u2265c|x \u2212t|k\u22121 whenever |m(x) \u22121/2| \u2264\u03f50 for some constant \u03f50\n(M). m(t + \u03b4) \u22121/2 = 1/2 \u2212m(t \u2212\u03b4) for all \u03b4 \u2264\u03c3.\n(B). t is at least \u03c3 away from the boundary.\nOn adding noise U, the point where m \u22c6U (\u22c6means convolution) crosses half may di\ufb00er from\nt, the point where m crosses half. However, the antisymmetry assumption (M) and boundary\nassumption (B) together imply that the two thresholds are the same. Getting rid of (M,B) seems\nsubstantially di\ufb03cult.\nWhen \u03c3 = 0, (Q), (M) and (B) are vacuously satis\ufb01ed, and this is exactly the class of functions\nand strategies considered in Castro & Nowak (2007). Smaller k means that the regression function\n3\nis steeper, which makes it easier to estimate the threshold and classify future labels (cf. Steinwart\n& Scovel (2004)). k = 1 captures a discontinuous m(x) jumping at t.\nMinimax Risk.\nWe are interested in the minimax risk under the point error loss :\nRn(P(c, C, k, \u03c3)) = inf\nS\u2208Sn\nsup\nP \u2208P(c,C,k,\u03c3)\nE|bt \u2212t|\n(1)\nwhere Sn is the set of strategies accessing n samples. For brevity, RP\nn (k, \u03c3) or RA\nn (k, \u03c3) denotes\nrisk for (P)assive/(A)ctive sampling stratgies SP\nn , SA\nn .\nNotation \u227a, \u227b, \u224d, \u2aaf, \u2ab0.\nWe analyse minimax point error rates in di\ufb00erent regimes of \u03c3 as a\nfunction of n (or equivalently, for a given point error, we can analyse how the sample size n\ndepends on \u03c3) and we write \u03c3n for emphasis. In this paper, fn \u227agn means fn/gn \u21920, fn \u224dgn\nmeans c1gn \u2264fn \u2264c2gn where c1, c2 are constants, fn \u2aafgn means fn \u227agn or fn \u224dgn, fn \u2ab0gn\nmeans gn \u2aaffn and fn \u227bgn means gn \u227afn.\n2\nMain Result and Comparisions\nThe main result of this paper is as follows.\nTheorem 1. Under the Berkson error model, when given n labels sampled actively or passively\nwith assumption (Q), and when the true underlying regression function lies in P(c, C, k, \u03c3n) for\nknown k, \u03c3n, the minimax risk under the point error loss is:\n1. RP\nn (P(k, \u03c3)) \u224d\n(\nn\u2212\n1\n2k\u22121\nif \u03c3n \u227an\u2212\n1\n2k\u22121\n\u03c3\n\u2212(k\u22123\n2 )\nn\nq\n1\nn otherwise\n2. RA\nn (P(k, \u03c3)) \u224d\n(\nn\u2212\n1\n2k\u22122\nif \u03c3n \u227an\u2212\n1\n2k\u22122\n\u03c3\u2212(k\u22122)\nn\nq\n1\nn otherwise\nWhen k = 1, m(x) jumps at the threshold, and we interpret the quantity n\u2212\n1\n2k\u22122 as being\nexponentially small, i.e. being smaller than n\u2212p for any p. We also suppress logarithmic factors\nin n, \u03c3n. If the domain was [\u2212R, R], the corresponding passive rates are obtained by substituting\nn by n/R, but active rates remain the same upto logarithmic factors in R.\nRemark.\nIn this paper, we focus on learning the threshold t. This is relevant because the\nthreshold maybe of intrinsic interest, and also of interest for prediction if, for example, future\nqueries could be made with a di\ufb00erent noise model or can be obtained (with some cost) noise-free.\nSimilar results can be derived for 0/1-risk.\nZero Noise.\nWhen \u03c3 = 0, the assumptions (Q,B,M) are vacuously true, and our class P(c, C, k, 0)\nmatches the class P(c, C, k) considered in Castro & Nowak (2007), and our rates for \u03c3 = 0 i.e.\nn\u2212\n1\n2k\u22121 and n\u2212\n1\n2k\u22122 are precisely the passive and active minimax point error rates in Castro &\nNowak (2007).\n4\nSmall Noise.\nWhen the noise is small, we get what we expect - the risk does not change with\nnoise as long as the noise itself is smaller than the noiseless error. In other words, as long as\nthe noise is smaller than the noiseless error rate of n\u2212\n1\n2k\u22121 for passive learning, passive learners\nwill not really be able to notice this tiny noise, and the minimax rate remains n\u2212\n1\n2k\u22121 . Similarly,\nas long as the noise is smaller than the noiseless error rate of n\u2212\n1\n2k\u22122 for active learning, active\nlearners will not really be able to notice this tiny noise, and the minimax rate remains n\u2212\n1\n2k\u22121 .\nAlso, the passive rates vary smoothly - at the point when \u03c3n \u224dn\u2212\n1\n2k\u22121 , the rates for small and\nlarge noise coincide. Similarly, at the point when \u03c3n \u224dn\u2212\n1\n2k\u22122 , the aforementioned active rates\nfor small and large noise coincide.\nLarge Noise and Assumption (M).\nWhen the noise is large, we see a curious behaviour\nof the rates. When k > 2, the error rates seem to get smaller/better with larger noise for both\nactive and passive learning, and furthermore the noisy rates can also be better than the noiseless\nrate! This might seem to violate both the information processing inequality, and our intuition\nthat more noise shouldn\u2019t help estimation. Moreover, a noiseless active learner may be able to\nsimulate a noisy situation by adding noise and querying at the resulting point, and get better\nrates, violating lower bounds in Castro & Nowak (2007).\nHowever, we make the following crucial but subtle observation. Our claimed rates are not about\na \ufb01xed function class - due to assumption (M), the function class changes with \u03c3, and in fact\n(M) requires the antisymmetry of the regression function to hold over a larger region for larger\n\u03c3. This set of functions is actually getting smaller with larger \u03c3. Even though the functions can\nbehave quite arbitrarily outside (t \u2212\u03c3, t + \u03c3), this assumption (M) on a small region of size 2\u03c3\nactually helps us signi\ufb01cantly.\nGiven that there is no contradiction to the results of Castro & Nowak (2007) or more funda-\nmental information theoretic ideas, there is also an intuitive explanation of why assumption (M)\nhelps when we have large noise. As we will see in a later \ufb01gure, convolution with noise seems to\n\u201cstretch/un\ufb02atten\u201d the function around the threshold. Speci\ufb01cally, for larger k > 2, the regres-\nsion function can be quite \ufb02at around the threshold - convolution with noise makes it less \ufb02at\nand more linear - in fact it behaves linearly over a large region of width nearly 2\u03c3. This is true\nregardless of whether assumption (M) holds - however if (M) does not hold, then the convolved\nthreshold, which is the point where the convolved function crosses half, need not be the original\nthreshold t. While dropping assumption (M) will not hurt if we only want to \ufb01nd the convolved\nthreshold, but given that our aim is to estimate t, the problem of \ufb01guring out how much the\nthreshold shifted can be quite non-trivial.\nHence, large noise ensures a behaviour that is less \ufb02at and more linear around the threshold, and\nassumption (M) ensures that the threshold doesn\u2019t shift from t. Intuitively this is why (M) and\nlarge noise help, and technically there is no contradiction becasue the function class is getting\nprogressively simpler because of more controlled growth around the threshold.\nThe main takeaway is that in all settings, active learning yields a gain over passive sampling. We\nnow describe the upper and lower bounds that lead to Theorem 1. The case k = 1 is handled in\ndetail for intuitionb but proofs for k > 1 are in the Appendix.\n5\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.3\n0.4\n0.5\n0.6\n0.7\nQuery Domain\n\u03b7(x) in red, F(w) in blue\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.3\n0.4\n0.5\n0.6\n0.7\nQuery Domain\n\u03b7(x) in red, F(w) in blue\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.3\n0.4\n0.5\n0.6\n0.7\nQuery Domain\n\u03b7(x) in red, F(w) in blue\nFigure 1: Regression function \u03b7(x) (red) before and F(w) (blue) after convolution with noise.\nIn all 3 \ufb01gures, Tsybakov\u2019s margin condition holds for x \u2208[0.4, 0.6]. The top plot has a linear\nregression function (k = 2), and its two blue curves are for \u03c3n = 0.05 (narrow), 0.2 (wide), and\nthey show that a linear growth around t = 0.5 remains linear. The middle and bottom \ufb01gure are\nfor a \ufb02atter regression function with k = 4, and \u03c3n = 0.05, 0.2 respectively, plotted separately\nfor clarity. k = 4 is harder than for k = 2 because the red curve is \ufb02atter around t, making it\nharder to pinpoint the threshold. However, as one can see in both plots, noise actually helps by\nsmoothing it out and making it more linear. However, note that the e\ufb00ect of assumption (M)\ncannot be understated, due to which in all plots the threshold before and after noise cross half\nat the same point. The e\ufb00ect of noise when k = 1 can be seen in the following section.\n2.1\nSimulation of Noise Convolution\n2.2\nPaper Roadmap\nWe devote the next two sections to proving the lower and upper bounds, in that order, that lead\nto Theorem 1. While the proofs will be self-contained, we leave some detailed calculations to the\nappendix.\nFor easier readibility, we present lower bounds for k = 1 \ufb01rst to absorb the technique and then\nthe lower bounds for k > 1. In Section 2 we will prove\nTheorem 2 (Lower Bounds). Under the Berkson error model and assumption (Q),\n6\n1. For k = 1, the passive/active lower bounds are\ninf\nS\u2208SP\nn\nsup\nP \u2208P(1,\u03c3n)\nE|bt \u2212t| \u2ab0\n(\n1\nn\nif \u03c3n \u227a1\nn\np \u03c3n\nn\notherwise\ninf\nS\u2208SA\nn\nsup\nP \u2208P(1,\u03c3n)\nE|bt \u2212t| \u2ab0\n(\ne\u2212n\nif \u03c3n \u227ae\u2212n\n\u03c3n\n\u221an\notherwise\n2. For k > 1, the passive/active lower bounds are\ninf\nS\u2208SP\nn\nsup\nP \u2208P(k,\u03c3n)\nE|bt \u2212t| \u2ab0\n(\nn\u2212\n1\n2k\u22121 if \u03c3n \u227an\u2212\n1\n2k\u22121\n\u03c3\n\u2212(k\u22123\n2 )\nn\nq\n1\nn otherwise\ninf\nS\u2208SA\nn\nsup\nP \u2208P(k,\u03c3n)\nE|bt \u2212t| \u2ab0\n(\nn\u2212\n1\n2k\u22122 if \u03c3n \u227an\u2212\n1\n2k\u22122\n\u03c3\u2212(k\u22122)\nn\nq\n1\nn otherwise\nFollowing that, we again present active and passive algorithms for k = 1 \ufb01rst to gather intuition\nand then generalize them for k > 1. In Section 3 we will prove\nTheorem 3 (Upper Bounds). Under the Berkson error model and assumption (Q),\n1. For k = 1, a passive algorithm (WIDEHIST) and an active algorithm (ACTPASS) return\nbt s.t.\nsup\nP \u2208P(1,\u03c3n)\nE|bt \u2212t| \u2aaf\n(\n1\nn\nif \u03c3n \u227a1\nn\np \u03c3n\nn\notherwise\nsup\nP \u2208P(1,\u03c3n)\nE|bt \u2212t| \u2aaf\n(\ne\u2212n\nif \u03c3n \u227ae\u2212n\n\u03c3n\n\u221an\notherwise\n2. For k > 1, a passive algorithm (WIDEHIST) and an active algorithm (ACTPASS) return\nbt s.t.\nsup\nP \u2208P(k,\u03c3n)\nE|bt \u2212t| \u2aaf\n(\nn\u2212\n1\n2k\u22121 if \u03c3n \u227an\u2212\n1\n2k\u22121\n\u03c3\n\u2212(k\u22123\n2 )\nn\nq\n1\nn otherwise\nsup\nP \u2208P(k,\u03c3n)\nE|bt \u2212t| \u2aaf\n(\nn\u2212\n1\n2k\u22122 if \u03c3n \u227an\u2212\n1\n2k\u22122\n\u03c3\u2212(k\u22122)\nn\nq\n1\nn otherwise\n3\nLower Bounds\nTo derive lower bounds, we will follow the approach of Ibargimov & Hasminskii (1981); Tsybakov\n(2009) which were exempli\ufb01ed in lower bounds for active learning problems without feature noise\nin Castro & Nowak (2007, 2008). The standard methodology is to reduce the problem of classi-\n\ufb01cation in the class P(c, C, k, \u03c3) to one of hypothesis testing. Similar to Castro & Nowak (2007,\n7\n2008), it will su\ufb03ce to consider two hypotheses and use the following version of Fano\u2019s lemma\nfrom Tsybakov (2009) (Theorem 2.2).\nTheorem 4 (Tsybakov (2009)). Let F be a class of models. Associated with each f \u2208F we\nhave a probability measure Pf de\ufb01ned on a common probability space. Let d(., .) : F, F \u2192R\nbe a semi-distance. Let f0, f1 \u2208F be such that d(f0, f1) \u22652a, with a > 0. Also assume that\nKL(Pf0, Pf1) \u2264\u03b3, where KL denotes the Kullback-Leibler divergence. Then, the following bound\nholds:\ninf\nb\nf\nsup\nf\u2208F\nPf(d( bf, f) \u2265a)\n\u2265\ninf\nbt\nmax\nj\u2208{0,1} Pfj(d( bf, fj) \u2265a)\n\u2265\nmax\n \ne\u2212\u03b3\n4 , 1 \u2212\np \u03b3\n2\n2\n!\n=: \u03c1\nwhere the inf is taken with respect to the collection of all possible estimators of f based on a\nsample from Pf.\nCorollary 5. If \u03b3 is a constant, then \u03c1 is a constant, and by Markov\u2019s inequality, we would get\ninf\nb\nf\nsup\nf\u2208F\nEd( bf, f) \u2265\u03c1a\nand the minimax risk under loss d would be \u2ab0a.\nProof of Theorem 2, k = 1.\nChoose F = P(1, \u03c3n). Let Pt \u2208P(1, \u03c3n) denote a regression\nfunction with threshold at t. We choose the semi-metric to be the distance between thresholds,\ni.e. d(Pr, Ps) = |r \u2212s|. We now choose two such distributions with thresholds at least 2an apart\n(we use an to explicitly remind the reader that a will later be set to depend on n) - let them be\ndenoted Pt0 and Pt1 with t0 = \u2212an, t1 = an and\nPt(Y = +|X = x) =\n(\n0.5 \u2212c\nx < t ,\n0.5 + c\nx \u2265t .\nDue to addition of noise, we get convolved distributions P 0 = Pt0(Y |W) and P 1 := Pt1(Y |W).\nAs hinted by the above corollary, we will choose an so that KL(P 0, P 1) is bounded by a constant,\nto get a lower bound on risk \u2ab0an. This follows by the following argument from Castro & Nowak\n(2008).\n8\nThe KL(P 0, P 1) can be bounded as\nE1\nW,Y\n\u0014\nlog P 1(W n\n1 , Y n\n1 )\nP 0(W n\n1 , Y n\n1 )\n\u0015\n(2)\n=\nE1\nW,Y\n\u0014\nlog\nQ\ni P 1(Yi|Wi)P(Wi|W i\u22121\n1\n, Y i\u22121\n1\n)\nQ\ni P 0(Yi|Wi)P(Wi|W i\u22121\n1\n, Y i\u22121\n1\n)\n\u0015\n=\nE1\nW,Y\n\u0014\nlog\nQ\ni P 1(Yi|Wi)\nQ\ni P 0(Yi|Wi)\n\u0015\n(3)\n=\nX\ni\nE1\nW\n\u0014\nE1\nY\n\u0014\nlog P 1(Yi|Wi)\nP 0(Yi|Wi)\n\f\f\f W1, ..., Wn\n\u0015\u0015\n(4)\n\u2264\nn\nmax\nw\u2208[\u22121,1] E1\nY\n\u0014\nlog P 1(Y |W)\nP 0(Y |W)\n\f\f\f W = w\n\u0015\n(5)\n\u2aaf\nn\nmax\nw\u2208[\u22121,1](P 1(Y |w) \u2212P 0(Y |w))2\n(6)\nwhere (3) holds for active learning because the algorithm determines Wi when given {W i\u22121\n1\n, Y i\u22121\n1\n}\nand is independent of the model, and follows by the independence of future from past for passive\nlearning. (4) holds by law of iterated expectation. (5) is used for active learning but is not needed\nfor passive learning. (6) follows by an approximation\nKL(Ber(1/2 + p), Ber(1/2 + q)) \u2aaf(p \u2212q)2\nfor su\ufb03ciently small constants p, q.\nt0 \n1/2 \n0 \n\u03bb\t\n\r\n\u03bb\t\n\r\nP(Y=+|X=x) \nx  \n1/2-\u03bb\t\n\r\n1/2+\u03bb\t\n\r\nt1 \nm1\nm0\nt0 \n1/2 \n0 \nP(Y=+|W=w) \nx  \n1/2-\u03bb\t\n\r\n1/2+\u03bb\t\n\r\nt1 \n\u02dc\nm1\n\u02dc\nm0\n2\u03c3n\n2\u03c3n\nFigure 2: Regression functions before (top) and after (bottom) convolution with noise.\nFt(w) := Pt(Y |W = w) =\nR\nPt(Y |X)P(X|W = w)dX and a straightforward calculation reveals\nthat\nFt(w) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n0.5 \u2212c\nw \u2264t \u2212\u03c3n ,\n0.5 +\nc\n\u03c3n (w \u2212t)\nw \u2208[t \u2212\u03c3n, t + \u03c3n] ,\n0.5 + c\nw \u2265t + \u03c3n .\n(7)\nAs depicted in Fig.2, note the behavior before and after convolution with noise: (i) m(t) =\nF(t) = 1/2, hence F1(an) = 1/2 = F0(\u2212an) (ii) Both convolved regression functions grow\nlinearly for a region of width 2\u03c3n, and di\ufb00er only on a width of 2(\u03c3n +an); (iii) For a large region\n9\n[an \u2212\u03c3n, \u2212an + \u03c3n] of size 2(\u03c3n \u2212an), we have\n\f\fF1(w) \u2212F0(w)\n\f\f = 2anc/\u03c3n, a constant. Their\ngap varies when \u03c3n \u2ab0an as\n\f\fF0(w) \u2212F1(w)\n\f\f =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u0010\nw + an + \u03c3n\n\u0011\nc\n\u03c3n\nw \u2208[\u2212an \u2212\u03c3n, an \u2212\u03c3n]\n2an c\n\u03c3n\nw \u2208[an \u2212\u03c3n, \u2212an + \u03c3n]\n\u0010\n(an + \u03c3n) \u2212w\n\u0011\nc\n\u03c3n\nw \u2208[\u2212an + \u03c3n, an + \u03c3n]\n0\notherwise.\nWhen \u03c3n \u227aan,\n\f\fF1(w) \u2212F0(w)\n\f\f =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u0010\nw + an + \u03c3n\n\u0011\nc\n\u03c3n\nw \u2208[\u2212an \u2212\u03c3n, \u2212an + \u03c3n]\n2c\nw \u2208[\u2212an + \u03c3n, an \u2212\u03c3n]\n\u0010\n(an + \u03c3n) \u2212w\n\u0011\nc\n\u03c3n\nw \u2208[an \u2212\u03c3n, an + \u03c3n]\n0\notherwise.\nFor active learning, when \u03c3n \u2ab0an we note\nmax\nw\u2208[\u22121,1] |P 1(Y |w) \u2212P 0(Y |w)| = 2anc\n\u03c3n\nand get KL(P 0, P 1) \u2aafn a2\nn\n\u03c32n by Eq.(6). We choose an \u224d\u03c3n\n\u221an, which becomes our active minimax\nerror rate by Corollary 5 when \u03c3n \u2ab0an i.e. \u03c3n \u2ab0e\u2212n.\nSimilarly, if \u03c3n \u227aexp{\u2212n}, setting an \u224dexp{\u2212n} easily gives us an exponentially small lower\nbound.\nIn the passive setting, Eq.(5) does not apply. Since the two convolved distributions di\ufb00er only\non an interval of size 2(\u03c3n + an), the e\ufb00ective number of points falling in this interval would be\n\u224dn(\u03c3n + an).\nWhen \u03c3n \u2ab0an, a simple calculation shows\nKL(P 0, P 1) \u2aafn(\u03c3n + an)a2\nn\n\u03c32n\n\u224dna2\nn\n\u03c3n\n,\ngiving rise to a choice of an \u224dp \u03c3n\nn , which is the passive minimax rate when \u03c3n \u2ab0an i.e.\n\u03c3n \u2ab01\nn.\nWhen \u03c3n \u227a1\nn, a similar calculation shows\nKL(P 0, P 1) \u2aafn(\u03c3n + an)4c2 \u224dnan\ngiving rise to a choice of an \u224d1\nn, which is the passive minimax rate when \u03c3n \u2ab0an i.e. \u03c3n \u227a1\nn.\n\u25a0\nProof of Theorem 2, k > 1\nWe follow a very similar setup to the case k = 1. The di\ufb00erence\nwill lie in picking functions that are in P(c, C, k, \u03c3n) for general k \u0338= 1, and calculating the bounds\non KL divergence appropriately. However, for notational convenience, we will assume that the\n10\ndomain is shifted to [\u2212\u03c3n, 2 \u2212\u03c3n] instead of [\u22121, 1] and that the distance between thresholds is\nan instead of 2an. De\ufb01ne\nP0(Y |x) =\n(\n1/2 \u2212c|x|k\u22121 if x \u2208[\u2212\u03c3n, 0]\n1/2 + c|x|k\u22121 if x > 0\nP1(Y |x) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n1/2 \u2212c|x \u2212an|k\u22121 if x \u2208[\u2212\u03c3n, an]\n1/2 + c|x \u2212an|k\u22121 if x \u2208[an, \u03b2an + \u03c3n]\n1/2 + c|x|k\u22121\nif x > \u03b2an + \u03c3n\nwhere \u03b2 =\n1\n1\u2212(c/C)1/(k\u22121) \u22651 is a constant chosen such that P1 \u2208P(c, C, k, \u03c3n) (this fact is\nveri\ufb01ed explicitly in the Appendix).\nFor ease of notation, P0, P1 are understood to actually\nsaturate at 0, 1 if need be (i.e.\nwe are implicitly working with min{P0/1, 1}, etc).\nThe two\nthresholds are clearly at 0, an respectively, and after the point \u03b2an + \u03c3n, the two functions are\nthe same.\nContinuing the same notation as for k = 1, we let P i = Pi(Y |W) = Fi(w) for\ni = 0, 1.\nThe following claims hold true (Appendix).\n1. When \u03c3n \u2aafan, maxw |F1(w) \u2212F2(w)| \u224dak\u22121\nn\n.\n2. When \u03c3n \u2ab0an, maxw |F1(w) \u2212F2(w)| \u224d\u03c3k\u22122\nn\nan.\n3. As a subpart of the above cases, when \u03c3n \u224dan, maxw |F1(w) \u2212F2(w)| \u224d\u03c3k\u22122\nn\nan \u224dak\u22121\nn\nIf the above propositions are true, we can verify:\n1. In the \ufb01rst case, KL(P 0, P 1) \u2aafna2k\u22122\nn\n, hence an \u224dn\u2212\n1\n2k\u22122 is a lower bound when \u03c3n \u2aaf\nn\u2212\n1\n2k\u22122 .\n2. Otherwise, KL(P 0, P 1) \u2aafn\u03c32k\u22124\nn\na2\nn, hence an \u224d\n\u03c3\u2212(k\u22122)\nn\u221an\nis a lower bound when \u03c3n \u227b\nn\u2212\n1\n2k\u22122 .\nThe passive bounds follow by not just considering the maximum di\ufb00erence between |F1(w) \u2212\nF2(w)| but also the length of that di\ufb00erence, since it is directly proportional to the number of\npoints that may randomly fall in that region. Following the same calculations,\n1. When \u03c3n \u227aan, |F1(w) \u2212F2(w)| \u224dak\u22121\nn\nfor all w \u2208[0, \u03b2an + 2\u03c3n]. Hence KL(P 0, P 1) \u2aaf\nn(\u03b2an + 2\u03c3n)a2k\u22122\nn\n\u224dna2k\u22121\nn\nand an \u224dn\u2212\n1\n2k\u22121 is the minimax passive rate when \u03c3n \u227a\nn\u2212\n1\n2k\u22121 .\n2. When \u03c3n \u227ban, |F1(w) \u2212F2(w)| \u224d\u03c3k\u22122\nn\nan for all w \u2208[0, \u03b2an + 2\u03c3n]. Hence KL(P 0, P 1) \u2aaf\nn(\u03b2an+2\u03c3n)\u03c32k\u22124\nn\na2\nn and an \u224d\u03c3\n\u2212(k\u22123\n2 )\nn\nq\n1\nn is the minimax passive rate when \u03c3n \u227bn\u2212\n1\n2k\u22121 .\nas veri\ufb01ed from the Appendix calculation.\n\u25a0\n4\nUpper Bounds\nFor passive sampling, we present a modi\ufb01ed histogram estimator, WIDEHIST, when the noise\nlevel \u03c3n is larger than the noiseless minimax rate of 1/n.\nAssume for simplicity that the n\nsampled points on [\u22121, 1] are equally spaced to mimic a uniform distribution, lying at (2j\u22121)\n2n\n,\nj = 1, ..., n.\n11\nAlgorithm WIDEHIST.\n1. Divide [\u22121, 1] into m bins of width h > 2\nn so m = 2\nh < n. The ith bin covers [\u22121 + (i \u2212\n1)h, \u22121+ih], i \u2208{1, ..., m} and hence each bin has nh\n2 points. Let bi be the average number\nof positive labels in bin i of these nh\n2 points.\n2. Let bpi be the average of the bi\u2019s over a all bins within \u00b1\u03c3n/2 of bin i. We \u201cclassify\u201d regions\nwith bpi < 1/2 as being \u2212and bpi > 1/2 as being +, and return bt as the center of the \ufb01rst\nbin from left to right where bpi crosses half.\nObserve that we need not operate on [\u22121, 1] with n queries - WIDEHIST(D,B) could take as\ninputs any domain D and any query budget B. The argument below hinges on the fact that the\nconvolved regression function behaves linearly around t.\nProof of Theorem 3, k = 1, (Passive).\nLet i\u2217\u2208{1, ..., m} denote the true bin [(i\u2217\u22121)h, i\u2217h]\nthat contains t. Let bt be from bin bi, i.e. bpbi < 1/2 and bpbi+1 > 1/2. We will argue that bi is very\nclose to i\u2217, in which case the point error we su\ufb00er is |bi \u2212i\u2217|h. Speci\ufb01cally, we prove that all bins\nexcept I\u2217= {i\u2217\u22121, i\u2217, i\u2217+1} will be \u201cclassi\ufb01ed\u201d correctly with high probability. In other words,\nwe claim w.h.p. bpi < 1/2 if i < i\u2217\u22121 and bpi > 1/2 if i > i\u2217+ 1.\nIndeed, we can show (Appendix)\nFor i > i\u2217+ 2, E[bpi] \u2265E[bpi\u2217+2] \u22651/2 +\nc\n\u03c3n h\n(8)\nFor i < i\u2217\u22122, E[bpi] \u2264E[bpi\u2217\u22122] \u22641/2 \u2212\nc\n\u03c3n h\n(9)\nUsing Hoe\ufb00ding\u2019s inequality, we get that for bin i, Pr(|bpi \u2212pi| > \u03f5) \u22642 exp\n\b\n\u22122 n\u03c3n\n2 \u03f52\t\nTaking\nunion bound over all bins other than those in i\u2217\u22121, i\u2217, i\u2217+ 1 and setting \u03f5 =\nc\n\u03c3n h, we get\nPr(\u2200i\\I\u2217, |bpi \u2212pi| >\nc\n\u03c3n h) \u22642m exp\n\u001a\n\u22122 n\u03c3n\n2\n\u0010\nch\n\u03c3n\n\u00112\u001b\nSo we get bins i\\I\u2217correct andbi \u2208{i\u2217\u22121, i\u2217, i\u2217+ 1} with probability \u22651\u22122n exp\n\u001a\n\u2212n\u03c3n\n\u0010\nch\n\u03c3n\n\u00112\u001b\nsince m < n. Setting h = 1\nc\nq\n\u03c3n\nn log( 2n\n\u03b4 ) makes this hold with probability \u22651 \u2212\u03b4 so the point\nerror |bi \u2212i\u2217|h < 2h behaves like h \u2aafp \u03c3n\nn .\n\u25a0\nFor active sampling when the noise level \u03c3n is larger than the minimax noiseless rate e\u2212n, we\npresent a algorithm ACTPASS which makes its n queries on the domain [\u22121, 1] in E di\ufb00erent\nepochs/rounds. As a subroutine, it uses any optimal passive learning algorithm, like WIDE-\nHIST(D,B). In each round, ACTPASS runs WIDEHIST on progressively smaller domains D\nwith a restricted budget B. Hence it \u201cactivizes\u201d the WIDEHIST and achieves the optimal ac-\ntive rate in the process. This algorithm was inspired by a similar idea from Ramdas & Singh\n(2013).\nAlgorithm ACTPASS.\nLet E = \u2308log(1/\u03c3n)\u2309be the number of epochs and D1 = [\u22121, 1] denote the domain of \u201cradius\u201d\nR1 = 1 around t0 = 0. The budget of every epoch is a constant B = n/E. For epochs 1 \u2264e \u2264E,\ndo:\n1. Query for B labels uniformly on De.\n2. Let te = WIDEHIST(De, B) be the returned estimator using the most recent samples and\nlabels.\n12\n3. De\ufb01ne De+1 = [te \u22122\u2212e, te + 2\u2212e] \u2229[\u22121, 1] with a radius of at most Re+1 = 2\u2212e around te.\nRepeat.\nObserve that ACTPASS runs while Re > \u03c3n, since by design E \u2265log(1/\u03c3n) so \u03c3n \u22642\u2212E =\nRE+1.\nProof of Theorem 3, k = 2, (Active).\nThe analysis of ACTPASS proceeds in two stages\ndepending on the value of \u03c3n. Initially, when Re is large, it is possible that \u03c3n \u2aafRe/n and in\nthis phase, the passive algorithm WIDEHIST will behave as if it is in the noiseless setting since\nthe noise is smaller than its noiseless rate. However, after some point, when Re becomes quite\nsmall, \u03c3n \u2ab0Re/n is possible and then WIDEHIST will behave as if it is in the noisy setting since\nnoise is larger than its noiseless rate. Observe that it cannot stay in the \ufb01rst phase till the end\nof the algorithm, since the \ufb01rst phase runs while \u03c3n \u2aafRe/n but we know that \u03c3n > RE+1 by\nconstruction, so there must be an epoch where it switches phases, and ends the algorithm in its\nsecond phase.\nWe prove (by a separate induction in each epoch) that with high probability, the true threshold\nt will always lie inside the domain at the start of every epoch (this is clearly true before the \ufb01rst\nepoch). We claim:\n1. Before all e in phase one, t \u2208De w.h.p.\n2. Before all e in phase two, t \u2208De w.h.p.\nWe prove these in the Appendix. If these are true, then in the second phase, WIDEHIST is in\nthe large noise setting and it gets an error of\nq\nRe\u03c3n\nB\n. Hence the \ufb01nal error of the algorithm is\nq\nRE\u03c3n\nn/E \u224d\u03c3n\n\u221an.\n\u25a0\nProof of Theorem 3, k > 1.\nThe proofs for k > 1 are simply generalizations of those for\nk = 1. Again, we present concise arguments here for the settings where the algorithm can actually\ndetect noise, i.e. when the noise level is larger than the noiseless minimax rate (otherwise, one\ncan argue that algorithms which worked for the noiseless case will su\ufb03ce). In both cases, the\nalgorithm remains unchanged.\n1. We outline the proof for WIDEHIST when \u03c3n \u2ab0n\u2212\n1\n2k\u22121 . Using similar notation as before, we\nwill again show that if t is in bin i\u2217of width h < \u03c3n, then except for bins i\u2217\u22121, i\u2217, i\u2217+ 1, we\nwill \u201dclassify\u201d all other bins correct with high probability, by averaging over the n\u03c3n/2 points to\nthe left and right of that bin. Speci\ufb01cally, we claim\nFor i > i\u2217+ 2, E[bpi] \u2265E[bpi\u2217+2] \u22651/2 + \u03bb\u03c3k\u22122\nn\nh\n(10)\nFor i < i\u2217\u22122, E[bpi] \u2264E[bpi\u2217\u22122] \u22641/2 \u2212\u03bb\u03c3k\u22122\nn\nh\n(11)\nA similar use of Hoe\ufb00ding\u2019s inequality gives\nPr(\u2200i\\I\u2217, |bpi \u2212pi|\n>\n\u03bb\u03c3k\u22122\nn\nh) \u2264\n2m exp\n\b\n\u22122( n\u03c3n\n2R )h2\u03bb2\u03c32k\u22124\nn\n\t\n.\nArguing as before, w.h.p. we get a point error of h \u2aaf\nq\nR\n\u03c32k\u22123\nn\nn < \u03c3n when \u03c3n \u227bn\u2212\n1\n2k\u22121 .\n2. We outline the proof for ACTPASS when \u03c3n \u2ab0n\u2212\n1\n2k\u22122 . As before, the algorithm runs in two\nphases, and we will prove required properties within each phase by induction.\n13\nThe \ufb01rst phase is when Re is large and so \u03c3n may possibly be smaller than (Re/n)\n1\n2k\u22121 and\nWIDEHIST will achieve noiseless rates within each epoch. In the second phase, after Re has\nshrunk enough, \u03c3n will become larger than (Re/n)\n1\n2k\u22121 and WIDEHIST will achieve noisy rates\nin these epochs.\nOne can verify, as before, that the second phase must occur, by design. Intuitively, the second\nphase must occur because we make a \ufb01xed number of queries n/E \u224dn/ log n in a halving\ndomain size (equivalently we make geometrically increasing queries on a rescaled domain), and\nso relatively in successive epochs this noiseless error shrinks, and at some point \u03c3n becomes larger\nthan this shrinking noiseless error rate.\nAs before we make the following claims:\n1. Before all e in phase one t \u2208De w.h.p.\n2. Before all e in phase two t \u2208De w.h.p.\nThese are proved in the Appendix by induction.\nThe \ufb01nal point error is given by WIDEHIST in the last epoch as\nq\nRE\n\u03c32k\u22123\nn\nn/E \u224d\n1\n\u03c3k\u22122\nn\nq\n1\nn since\nRE \u224d\u03c3n and E \u224dlog n.\n5\nConclusion\nIn this paper, we propose a simple Berkson error model for one-dimensional threshold classi\ufb01-\ncation, inspired by the setup and model analysed in Castro & Nowak (2007, 2008), in which we\ncan analyse active learning with additive uniform feature noise. To the best of our knowledge,\nthis is the \ufb01rst attempt at jointly tackling feature noise and label noise in active learning.\nThis simple setting already yields interesting behaviour depending on the additive feature noise\nlevel and the label noise of the underlying regression function.\nFor both passive and active\nlearning, whenever the noise level is smaller than the minimax noiseless rate, the learner cannot\nnotice that there is noise, and will continue to achieve the noiseless rate.\nAs the noise gets\nlarger, the rates do depend on the noise level. Importantly, one can achieve better rates than\npassive learning in most scenarios, and we propose unique algorithms/estimators to achieve\ntight rates. The idea of \u201cactivizing\u201d passive algorithms, like algorithm ACTPASS did, seems\nespecially powerful and could carry forward to other settings beyond our paper and Ramdas &\nSingh (2013).\nThe immediate future work and most direct extension to this paper concerns the main weakness\nof the paper - the possibility of getting rid of Assumption (M), which is the only hurdle to a\nfair comparision with the noiseless setting. We would like to re-emphasize that at \ufb01rst glance,\nthe rates may be misleading and counterintuitive because it \u201cappears\u201d as if larger noise could\npossibly help estimation due to the presence of \u03c3n in the denominator for larger k.\nHowever, we point out once more that the class of functions is not constant over all \u03c3n - it\ndepends on \u03c3n, and in fact it gets \u201csmaller\u201d in some sense with larger \u03c3n because the assumption\n(M) becomes more stringent. This observation about the non-constant function class, along with\nthe fact that convolution with uniform noise seems to un\ufb02atten the regression function as shown\nin the \ufb01gures, together cause the rates to seemingly improve with larger noise levels.\n14\nAnalysing the case without (M) seems to be quite a challenging task since the noiseless and\nconvolved thresholds can be di\ufb00erent - we did attempt to formulate a few kernel-based estimators\nwith additional assumptions, but do not presently have tight bounds, and leave those for a future\nwork.\nAcknowledgements\nWe thank Rui Castro for detailed conversations about our model and results.\nThis work is\nsupported in part by NSF Big Data grant IIS-1247658.\nReferences\nCarroll, Raymond J, Ruppert, David, Stefanski, Leonard A, and Crainiceanu, Ciprian M.\nMeasurement error in nonlinear models: a modern perspective, volume 105. Chapman and\nHall/CRC, 2010.\nCastro, R. and Nowak, R. Minimax bounds for active learning. IEEE Transactions on Informa-\ntion Theory, 54(5):2339\u20132353, 2008.\nCastro, Rui M. and Nowak, Robert D. Minimax bounds for active learning. In Proceedings of\nthe 20th annual conference on Learning theory, COLT\u201907, pp. 5\u201319, Berlin, Heidelberg, 2007.\nSpringer-Verlag.\nFan, Jianqing. On the optimal rates of convergence for nonparametric deconvolution problems.\nThe Annals of Statistics, pp. 1257\u20131272, 1991.\nFan, Jianqing, Truong, Young K, et al. Nonparametric regression with errors in variables. The\nAnnals of Statistics, 21(4):1900\u20131925, 1993.\nFuller, Wayne A. Measurement error models, volume 305. Wiley, 2009.\nIbargimov, I. A. and Hasminskii, R. Z. Statistical Estimation. Asymptotic Theory. Springer-\nVerlag, 1981.\nLoustau, S\u00b4ebastien and Marteau, Cl\u00b4ement. Discriminant analysis with errors in variables. arXiv\npreprint arXiv:1201.3283, 2012.\nRamdas, Aaditya and Singh, Aarti. Algorithmic connections between active learning and stochas-\ntic convex optimization. In Algorithmic Learning Theory, pp. 339\u2013353. Springer, 2013.\nSteinwart, Ingo and Scovel, Clint. Fast rates to bayes for kernel machines. In Advances in neural\ninformation processing systems, pp. 1345\u20131352, 2004.\nTsybakov, A.B. Optimal aggregation of classi\ufb01ers in statistical learning. The Annals of Statistics,\n32(1):135\u2013166, 2004.\nTsybakov, Alexandre B. Introduction to Nonparametric Estimation. Springer Publishing Com-\npany, Incorporated, 1st edition, 2009. ISBN 0387790519, 9780387790510.\n15\nA\nJustifying Claims in the Lower Bounds\nApproximations:\n1. (x + y)k = xk(1 + y/x)k \u2248xk + kxk\u22121y when y \u227ax. Even when y \u2aafx, both terms are the\nsame order.\n2. (x \u2212y)k = xk(1 \u2212y/x)k \u2248xk \u2212kxk\u22121y when y \u227ax. Even when y \u2aafx both terms are the\nsame order.\n3. When y < x but not y \u227ax, by Taylor expansion of (1 + z)k around z = 0, we have\n(x + y)k = xk(1 + y/x)k = xk[1 + (1 + c)k\u22121y/x] = xk + Cxk\u22121y for some 0 < c < y/x < 1\nand some constant C. Similarly for (x \u2212y)k.\nLet\u2019s assume the boundary is at \u2212\u03c3 for easier calculations. (we denote an, \u03c3n as a, \u03c3 here).\nRemember\nm1(x) = 1/2 + cx|x|k\u22122 if x \u2265\u2212\u03c3\nm2(x) =\n(\n1/2 + c(x \u2212a)|x \u2212a|k\u22122\nif x < \u03b2a + \u03c3\nm1(x)\nif x \u2265\u03b2a + \u03c3\nwhere \u03b2 =\n1\n1\u2212(c/C)1/(k\u22121) \u22651 is such that m2 \u2208P(\u03ba, c, C, \u03c3). Clearly, when x < \u03b2a + \u03c3, m2\nsatis\ufb01es condition (T). So, we only need to verify that whenever x \u2265\u03b2a + \u03c3 we have\nm2(x) \u22121/2 = cxk\u22121\n\u2264\nC(x \u2212a)k\u22121\n(12)\nThis statement holds i\ufb00\n(c/C)1/(k\u22121) \u22641\u2212a/x \u21d4a/x \u22641\u2212(c/C)1/(k\u22121) \u21d4x \u2265\u03b2a, which\nholds for all \u03c3 \u22650, and hence m2 satis\ufb01es condition (T).\nProposition 1. When \u03c3 \u227aa, maxw |F1(w) \u2212F2(w)| \u224dak\u22121\nProposition 2. When \u03c3 \u227ba maxw |F1(w) \u2212F2(w)| \u224d\u03c3k\u22122a\nLet us now prove these two propositions, with detailed calculations in each case (note that when\n\u03c3 \u224da, then maxw |F1(w)\u2212F2(w)| \u224dak\u22121 \u224d\u03c3k\u22122a, and can be checked using our approximations\n1,2,3).\n1. When \u03c3 \u227aa, we will prove proposition 1. Remember that we can\u2019t query in \u2212\u03c3 \u2264w \u22640.\n(a) When 0 \u2264w \u2264\u03c3, we have\nF1(w) = (m1 \u22c6U)(w)\n=\nZ 0\nw\u2212\u03c3\n(1/2 \u2212cx|x|k\u22122)dx/2\u03c3 +\nZ w+\u03c3\n0\n(1/2 + cxk\u22121)dx/2\u03c3\n=\n1/2 +\nc\n2\u03c3k [(w + \u03c3)k \u2212(\u03c3 \u2212w)k]\n=\n1/2 +\nc\n2\u03c3k \u03c3k[(1 + w/\u03c3)k \u2212(1 \u2212w/\u03c3)k]\n\u2248\n1/2 + c\u03c3k\u22122w\nF2(w) = (m2 \u22c6U)(w)\n=\nZ w+\u03c3\nw\u2212\u03c3\n(1/2 \u2212c(x \u2212a)|x \u2212a|k\u22122)dx/2\u03c3\n=\n1/2 \u2212\nc\n2\u03c3k [(a \u2212w \u2212\u03c3)k \u2212(a + \u03c3 \u2212w)k]\n\u2248\n1/2 \u2212c(a \u2212w)k\u22121\n16\n[Boundaries: F1(0)\u22121\n2 = 0, F1(\u03c3)\u22121\n2 \u224d\u03c3k\u22121, F2(0)\u22121\n2 \u224d\u2212ak\u22121, F2(\u03c3)\u22121\n2 \u224d\u2212ak\u22121].\nF1(w) \u2212F2(w)\n\u2aaf\nak\u22121\n(b) When \u03c3 \u2264w \u2264a \u2212\u03c3\nF1(w) = (m1 \u22c6U)(w)\n=\nZ w+\u03c3\nw\u2212\u03c3\n(1/2 + cxk\u22121)dx/2\u03c3\n=\n1/2 +\nc\n2\u03c3k [(w + \u03c3)k \u2212(w \u2212\u03c3)k]\n\u2248\n1/2 + cwk\u22121\nF2(w) = (m2 \u22c6U)(w)\n=\nZ w+\u03c3\nw\u2212\u03c3\n(1/2 \u2212c(x \u2212a)|x \u2212a|k\u22122)dx/2\u03c3\n=\n1/2 \u2212\nc\n2\u03c3k [(a \u2212w \u2212\u03c3)k \u2212(a + \u03c3 \u2212w)k]\n\u2248\n1/2 \u2212c(a \u2212w)k\u22121\n[Boundaries: F1(\u03c3)\u22121\n2 \u224d\u03c3k\u22121, F1(a\u2212\u03c3)\u22121\n2 \u224dak\u22121, F2(\u03c3)\u22121\n2 \u224d\u2212ak\u22121, F2(a\u2212\u03c3)\u2212\n1\n2 \u224d\u2212\u03c3k\u22121].\nF1(w) \u2212F2(w)\n=\ncwk\u22121 + c(a \u2212w)k\u22121\n\u2264\nc(a \u2212\u03c3)k\u22121 + c(a \u2212\u03c3)k\u22121\n\u2aaf\nak\u22121\n(c) When a \u2212\u03c3 \u2264w \u2264a\nF1(w)\n\u2248\n1/2 + cwk\u22121\nF2(w)\n=\nZ a\nw\u2212\u03c3\n(1/2 \u2212c(x \u2212a)|x \u2212a|k\u22122)dx/2\u03c3 +\nZ w+\u03c3\na\n1/2 + c(x \u2212a)k\u22121dx/2\u03c3\n=\n1/2 \u2212\nc\n2\u03c3k [(a \u2212w + \u03c3)k \u2212(w + \u03c3 \u2212a)k]\n\u2248\n1/2 \u2212c\u03c3k\u22122(a \u2212w)\n[Boundaries: F1(a\u2212\u03c3) \u22121\n2 \u224dak\u22121, F1(a) \u22121\n2 \u224dak\u22121, F2(a\u2212\u03c3) \u22121\n2 \u224d\u2212\u03c3k\u22121, F2(a) \u2212\n1\n2 = 0]\nF1(w) \u2212F2(w)\n\u2248\ncwk\u22121 + c\u03c3k\u22122(a \u2212w)\n\u2264\ncak\u22121 + c\u03c3k\u22122\u03c3\n\u2aaf\nak\u22121\n(d) When a \u2264w \u2264a + \u03c3\nF1(w)\n\u2248\n1/2 + cwk\u22121\nF2(w)\n\u2248\n1/2 + c\u03c3k\u22122(a \u2212w)\n[Boundaries: F1(a) \u22121\n2 \u224dak\u22121, F1(a + \u03c3) \u22121\n2 \u224dak\u22121, F2(a) \u22121\n2 = 0, F2(a + \u03c3) \u22121\n2 \u224d\n\u03c3k\u22121]\nF1(w) \u2212F2(w) \u2aafak\u22121\n17\n(e) When a + \u03c3 \u2264w \u2264\u03b2a \u2212\u03c3\nF1(w)\n\u2248\n1/2 + cwk\u22121\nF2(w)\n=\nZ w+\u03c3\nw\u2212\u03c3\n1/2 + c(x \u2212a)k\u22121dx/2\u03c3\n=\n1/2 +\nc\n2\u03c3k [(w + \u03c3 \u2212a)k \u2212(w \u2212\u03c3 \u2212a)k]\n\u2248\n1/2 + c(w \u2212a)k\u22121\n[B: F1(a+\u03c3)\u22121\n2 \u224dak\u22121, F1(\u03b2a\u2212\u03c3)\u22121\n2 \u224dak\u22121, F2(a+\u03c3)\u22121\n2 \u224d\u03c3k\u22121, F2(\u03b2a\u2212\u03c3)\u22121\n2 \u224d\nak\u22121]\nF1(w) \u2212F2(w)\n\u2248\ncwk\u22121 \u2212c(w \u2212a)k\u22121\n\u2264\nc(\u03b2a \u2212\u03c3)k\u22121 + c\u03c3k\u22121\n\u2264\nc(\u03b2k\u22121 + 1)ak\u22121\n\u2aaf\nak\u22121\n(f) When \u03b2a \u2212\u03c3 \u2264w \u2264\u03b2a + \u03c3\nF1(w)\n\u2248\n1/2 + cwk\u22121\nF2(w)\n=\nZ \u03b2a\nw\u2212\u03c3\n1/2 + c(x \u2212a)k\u22121dx/2\u03c3 +\nZ w+\u03c3\n\u03b2a\n1/2 + xk\u22121dx/2\u03c3\n=\n1/2 +\nc\n2\u03c3k [(\u03b2a \u2212a)k \u2212(w \u2212\u03c3 \u2212a)k + (w + \u03c3)k \u2212(\u03b2a)k]\n[F1(\u03b2a\u2212\u03c3)\u22121\n2 \u224dak\u22121, F1(\u03b2a+\u03c3)\u22121\n2 \u224dak\u22121, F2(\u03b2a\u2212\u03c3)\u22121\n2 \u224dak\u22121, F2(\u03b2a+\u03c3)\u22121\n2 \u224d\nak\u22121]\nF1(w) \u2212F2(w)\n=\ncwk\u22121 +\nc\n2\u03c3k [(\u03b2k \u2212(\u03b2 \u22121)k)ak + (w \u2212\u03c3 \u2212a)k \u2212(w \u2212\u03c3)k]\n\u2264\nc(\u03b2 + 1)k\u22121ak\u22121 +\nc\n2\u03c3k [(\u03b2a)k \u2212(\u03b2a \u22122\u03c3)k] \u2212\nc\n2\u03c3k [(\u03b2 \u22121)kak \u2212((\u03b2 \u22121)a \u2212\u03c3)k]\n\u2248\nc(\u03b2 + 1)k\u22121ak\u22121 +\nc\n2\u03c3k [k(\u03b2a)k\u221212\u03c3] \u2212\nc\n2\u03c3k [k(\u03b2 \u22121)k\u22121ak\u22121\u03c3]\n=\ncak\u22121[(\u03b2 + 1)k\u22121 + \u03b2k\u22121 \u22121\n2(\u03b2 \u22121)k\u22121]\n\u224d\nak\u22121\n(g) When \u03b2a + \u03c3 \u2264w \u2264\u03b2a + 2\u03c3\nF1(w)\n=\n1/2 +\nc\n2\u03c3k [(w + \u03c3)k \u2212(w \u2212\u03c3)k]\nF2(w)\n=\nZ \u03b2a+\u03c3\nw\u2212\u03c3\n1/2 + c(x \u2212a)k\u22121dx/2\u03c3 +\nZ w+\u03c3\n\u03b2a+\u03c3\n1/2 + cxk\u22121dx/2\u03c3\n=\n1/2 +\nc\n2\u03c3k [(\u03b2a + \u03c3 \u2212a)k \u2212(w \u2212\u03c3 \u2212a)k + (w + \u03c3)k \u2212(\u03b2a + \u03c3)k]\n18\n[F1(\u03b2a+\u03c3)\u22121\n2 \u224dak\u22121, F1(\u03b2a+2\u03c3)\u22121\n2 \u224dak\u22121, F2(\u03b2a+\u03c3)\u22121\n2 \u224dak\u22121, F2(\u03b2a+2\u03c3)\u22121\n2 \u224d\nak\u22121]\nF1(w) \u2212F2(w)\n=\nc\n2\u03c3k [(\u03b2a + \u03c3)k \u2212(\u03b2a + \u03c3 \u2212a)k + (w \u2212\u03c3 \u2212a)k \u2212(w \u2212\u03c3)k]\n\u2248\nc\n2\u03c3k [(\u03b2a + \u03c3)k\u22121ka \u2212(w \u2212\u03c3)k\u22121ka]\n\u2264\nca\n2\u03c3 [(\u03b2a + \u03c3)k\u22121 \u2212(\u03b2a)k\u22121]\n\u2248\nca\n2\u03c3 [(\u03b2a)k\u22121(1 + (k \u22121)\u03c3\n\u03b2a\n) \u2212(\u03b2a)k\u22121]\n=\nak\u22121[c\u03b2k\u22122(k \u22121)/2]\n\u224d\nak\u22121\n(h) When w \u2265\u03b2a + 2\u03c3\nF1(w) = F2(w)\nThat completes the proof of the \ufb01rst claim.\n2. When \u03c3 \u227ba, we will prove the second proposition.\n(a) When \u2212\u03c3 \u2264w \u22640, we are not allowed to query here.\n(b) When 0 < w \u2264\u03b2a\nF1(w) = (m1 \u22c6U)(w)\n=\nZ 0\nw\u2212\u03c3\n(1/2 \u2212cx|x|k\u22122)dx/2\u03c3 +\nZ w+\u03c3\n0\n(1/2 + cxk\u22121)dx/2\u03c3\n=\n1/2 +\nc\n2\u03c3k [(w + \u03c3)k \u2212(\u03c3 \u2212w)k]\n=\n1/2 +\nc\n2\u03c3k \u03c3k[(1 + w/\u03c3)k \u2212(1 \u2212w/\u03c3)k]\n\u2248\n1/2 + c\u03c3k\u22122w\nSimilarly F2(w) \u22481/2 + c\u03c3k\u22122(w \u2212a)\n[Boundaries: F1(0)\u22121\n2 = 0, F1(\u03b2a)\u22121\n2 \u224d\u03c3k\u22122a, F2(0)\u22121\n2 \u224d\u2212\u03c3k\u22122a, F2(\u03b2a) \u224d\u03c3k\u22122a]\nF1(w) \u2212F2(w) \u224d\u03c3k\u22122\nn\na.\n(c) When \u03b2a \u2264w \u2264\u03c3\nF1(w) =\n=\nZ 0\nw\u2212\u03c3\n(1/2 \u2212cx|x|k\u22122)dx/2\u03c3 +\nZ w+\u03c3\n0\n(1/2 + cxk\u22121)dx/2\u03c3\n=\n1/2 +\nc\n2\u03c3k [(w + \u03c3)k \u2212(\u03c3 \u2212w)k]\n=\n1/2 +\nc\n2\u03c3k \u03c3k[(1 + w/\u03c3)k \u2212(1 \u2212w/\u03c3)k]\n\u2248\n1/2 + c\u03c3k\u22122w\n19\nF2(w)\n=\nZ a\nw\u2212\u03c3\n(1/2 \u2212c(x \u2212a)|x \u2212a|k\u22122)dx\n2\u03c3 +\nZ \u03b2a+\u03c3\na\n(1/2 + c(x \u2212a)k\u22121)dx\n2\u03c3 +\nZ w+\u03c3\n\u03b2a+\u03c3\n1/2 + cxk\u22121 dx\n2\u03c3\n=\n1/2 +\nc\n2\u03c3k [\u2212(\u03c3 + a \u2212w)k + (\u03b2a + \u03c3 \u2212a)k + (w + \u03c3)k \u2212(\u03b2a + \u03c3)k]\n\u2248\n1/2 +\nc\n2\u03c3k [\u2212\u03c3k(1 \u2212k(w \u2212a)\n\u03c3\n) + \u03c3k(1 + k(\u03b2 \u22121)a\n\u03c3\n) + \u03c3k(1 + kw\n\u03c3 ) \u2212\u03c3k(1 + k\u03b2a\n\u03c3 )]\n=\n1/2 + c\n2\u03c3k\u22122[w \u2212a + (\u03b2 \u22121)a + w \u2212\u03b2a]\n=\n1/2 + c\u03c3k\u22122(w \u2212a)\n[Boundaries: F1(\u03b2a) \u22121\n2 \u224d\u03c3k\u22122a, F1(\u03c3) \u22121\n2 \u224d\u03c3k\u22121, F2(\u03b2a) \u224d\u03c3k\u22122a, F2(\u03c3) \u22121\n2 \u224d\n\u2212\u03c3k\u22122a]\nF1(w) \u2212F2(w) \u224d\u03c3k\u22122a\nSpeci\ufb01cally, verify the boundary at \u03c3\nF1(\u03c3) \u2212F2(\u03c3)\n=\nc\n2\u03c3k [ak \u2212(\u03b2a + \u03c3 \u2212a)k + (\u03b2a + \u03c3)k]\n=\nc\n2\u03c3k [ak \u2212\u03c3k(1 + k \u03b2a \u2212a\n\u03c3\n) + \u03c3k(1 + k \u03b2a\n\u03c3 )]\n=\nc\n2\u03c3k [ak + k\u03c3k\u22121a]\n\u2264\nc\u03c3k\u22122a\n(d) When \u03c3 \u2264w \u2264a + \u03c3\nF1(w)\n=\nZ w+\u03c3\nw\u2212\u03c3\n(1/2 + cxk\u22121)dx/2\u03c3\n=\n1/2 +\nc\n2\u03c3k [(w + \u03c3)k \u2212(w \u2212\u03c3)k]\nF2(w)\n=\nZ a\nw\u2212\u03c3\n(1/2 \u2212c(x \u2212a)|x \u2212a|k\u22122)dx\n2\u03c3 +\nZ \u03b2a+\u03c3\na\n(1/2 + c(x \u2212a)k\u22121)dx\n2\u03c3 +\nZ w+\u03c3\n\u03b2a+\u03c3\n1/2 + cxk\u22121 dx\n2\u03c3\n=\n1/2 +\nc\n2\u03c3k [\u2212(\u03c3 + a \u2212w)k + (\u03b2a + \u03c3 \u2212a)k + (w + \u03c3)k \u2212(\u03b2a + \u03c3)k]\nF1(w) \u2212F2(w)\n=\nc\n2\u03c3k [(\u03c3 + a \u2212w)k \u2212(\u03b2a + \u03c3 \u2212a)k \u2212(w \u2212\u03c3)k + (\u03b2a + \u03c3)k]\nDi\ufb00erentiating the above term with respect to w, gives\nc\n2\u03c3[\u2212(\u03c3 + a \u2212w)k\u22121 \u2212(w \u2212\n\u03c3)k\u22121] \u22640 because \u03c3 \u2264w \u2264a + \u03c3 and hence F1(w) \u2212F2(w) is decreasing with w. We\nalready saw F1(\u03c3) \u2212F2(\u03c3) \u2264c\u03c3k\u22122a. We can also verify that at the other boundary,\nF1(a + \u03c3) \u2212F2(a + \u03c3)\n=\nc\n2\u03c3k [\u2212(\u03b2a + \u03c3 \u2212a)k \u2212ak + (\u03b2a + \u03c3)k]\n=\nc\n2\u03c3k [\u2212ak \u2212\u03c3k(1 + k \u03b2a \u2212a\n\u03c3\n) + \u03c3k(1 + k \u03b2a\n\u03c3 )]\n=\nc\n2\u03c3k [\u2212ak + k\u03c3k\u22121a]\n\u2264\nc\n2\u03c3k\u22122a\n20\n(e) When \u03c3 + a \u2264w \u2264\u03b2a + \u03c3\nF1(w)\n=\nZ w+\u03c3\nw\u2212\u03c3\n(1/2 + cxk\u22121)dx/2\u03c3\n=\n1/2 +\nc\n2\u03c3k [(w + \u03c3)k \u2212(w \u2212\u03c3)k]\nF2(w)\n=\nZ \u03b2a+\u03c3\nw\u2212\u03c3\n(1/2 + c(x \u2212a)k\u22121)dx\n2\u03c3 +\nZ w+\u03c3\n\u03b2a+\u03c3\n1/2 + cxk\u22121 dx\n2\u03c3\n=\n1/2 +\nc\n2\u03c3k [(\u03b2a + \u03c3 \u2212a)k \u2212(w \u2212\u03c3 \u2212a)k + (w + \u03c3)k \u2212(\u03b2a + \u03c3)k]\nF1(w) \u2212F2(w)\n=\nc\n2\u03c3k [(w \u2212\u03c3 \u2212a)k \u2212(\u03b2a + \u03c3 \u2212a)k \u2212(w \u2212\u03c3)k + (\u03b2a + \u03c3)k]\nDi\ufb00erentiating with respect to w gives\nc\n2\u03c3[(w \u2212\u03c3 \u2212a)k\u22121 \u2212(w \u2212\u03c3)k\u22121] \u22640 because\nw\u2212\u03c3\u2212a \u2264w\u2212\u03c3 and so F1\u2212F2 is decreasing with w. We know F1(a+\u03c3)\u2212F2(a+\u03c3) \u2264\nc\n2\u03c3k\u22122a, and we can verify at the other boundary that\nF1(\u03b2a + \u03c3) \u2212F2(\u03b2a + \u03c3)\n=\nc\n2\u03c3k [(\u03b2a \u2212a)k \u2212(\u03b2a + \u03c3 \u2212a)k \u2212(\u03b2a)k + (\u03b2a + \u03c3)k]\n\u2248\nc\n2\u03c3k [(\u03b2a \u2212a)k \u2212(\u03b2a)k \u2212\u03c3k(1 + k \u03b2a \u2212a\n\u03c3\n) + \u03c3k(1 + k \u03b2a\n\u03c3 )]\n=\nc\n2\u03c3k [(\u03b2a \u2212a)k \u2212(\u03b2a)k + k\u03c3k\u22121a]\n\u2264\nc\n2\u03c3k\u22122a\n(f) When \u03b2a + \u03c3 \u2264w \u2264\u03b2a + 2\u03c3\nF1(w) = 1/2 +\nc\n2\u03c3k [(w + \u03c3)k \u2212(w \u2212\u03c3)k]\nF2(w)\n=\nZ \u03b2a+\u03c3\nw\u2212\u03c3\n1/2 + c(x \u2212a)k\u22121dx/2\u03c3 +\nZ w+\u03c3\n\u03b2a+\u03c3\n1/2 + cxk\u22121dx/2\u03c3\n=\n1/2 +\nc\n2k\u03c3 [(\u03b2a + \u03c3 \u2212a)k \u2212(w \u2212\u03c3 \u2212a)k + (w + \u03c3)k \u2212(\u03b2a + \u03c3)k]\nHence\nF1(w) \u2212F2(w)\n=\nc\n2\u03c3k [(\u03b2a + \u03c3)k \u2212(\u03b2a + \u03c3 \u2212a)k + (w \u2212\u03c3 \u2212a)k \u2212(w \u2212\u03c3)k]\n\u2248\nc\n2\u03c3k [(\u03b2a + \u03c3)k\u22121ka \u2212(w \u2212\u03c3)k\u22121ka]\n\u2264\nca\n2\u03c3 [(\u03b2a + \u03c3)k\u22121 \u2212(\u03b2a)k\u22121]\n\u2248\nc/2\u03c3k\u22122a\n\u224d\n\u03c3k\u22122a\n21\nAlternately, by the same argument as in the previous case, di\ufb00erentiating with respect\nto w gives\nc\n2\u03c3[(w \u2212\u03c3 \u2212a)k\u22121 \u2212(w \u2212\u03c3)k\u22121] \u22640 because w \u2212\u03c3 \u2212a \u2264w \u2212\u03c3 and so\nF1 \u2212F2 is decreasing with w. We know F1(\u03b2a + \u03c3) \u2212F2(\u03b2a + \u03c3) \u2264c\n2\u03c3k\u22122a, and we\ncan verify at the other endpoint that\nF1(\u03b2a + 2\u03c3) \u2212F2(\u03b2a + 2\u03c3)\n=\n0\n(g) When w \u2265\u03b2a + 2\u03c3, F1(w) = F2(w)\nThat completes the proof of the second proposition.\nB\nConvolved Regression Function, Justifying Eqs.(8-11)\nFor ease of presentation, let us assume the threshold is at 0, and de\ufb01ne m \u2208P(c, C, k, \u03c3) as\nm(x) =\n(\n1/2 + f(x) + \u2206(x) if x \u22650\n1/2 \u2212f(x) if x < 0\nDue to assumption (M), \u2206(x) must be 0 when 0 \u2264x \u2264\u03c3. Hence, the Taylor expansion of \u2206(x)\naround x = \u03c3 looks like\n\u2206(x) = (x \u2212\u03c3)\u2206\u2032(\u03c3) + (x \u2212\u03c3)2\u2206\u2032\u2032(\u03c3) + ...\nIf one represents, as before, F(x) = m \u22c6U, then directly from the de\ufb01nitions, it follows for \u03b4 > 0\nthat\nF(\u03b4) \u2212F(0) =\nZ \u03c3+\u03b4\n\u03c3\n(1/2 + f(z) + \u2206(z)) dz\n2\u03c3 \u2212\nZ \u2212\u03c3+\u03b4\n\u2212\u03c3\n(1/2 \u2212f(z)) dz\n2\u03c3\nIn particular, due to the form (T) of m, let f = c1|x|k\u22121 for some c \u2264c1 \u2264C (we could also\nbreak f into parts where it has di\ufb00erent c1s but this is a technicality and does not change the\nbehaviour). Then\nF(\u03b4) \u2212F(0)\n=\nc1\n2k\u03c3 [(xk)\u03c3+\u03b4\n\u03c3\n\u2212(xk)\u2212\u03c3+\u03b4\n\u2212\u03c3\n] +\nZ \u03b4+\u03c3\n\u03c3\n[(z \u2212\u03c3)\u2206\u2032(\u03c3) + (z \u2212\u03c3)2\u2206\u2032\u2032(\u03c3) + ...] dz\n2\u03c3\n=\nc1\n2k\u03c3 [(\u03c3 + \u03b4)k \u2212\u03c3k + (\u2212\u03c3 + \u03b4)k \u2212(\u2212\u03c3)k] + [(z \u2212\u03c3)2]\u03c3+\u03b4\n\u03c3\n4\u03c3\n\u2206\u2032(\u03c3) + ...\n\u2248\nc1\u03c3k\u22122\u03b4 + \u03b42\n4\u03c3 \u2206\u2032(\u03c3) + o(\u03b42)\nThus we get behaviour of the form\nF(t + h) \u22651/2 + c\u03c3k\u22122h\nOne can derive similar results when \u03b4 < 0.\nThe claims about WIDEHIST immediately follow from the above, but we can make them a little\nmore explicit. First note that F(w) = 1/2+ c\n\u03c3n (w\u2212t) for w close to t (in fact for w \u2208[t\u2212\u03c3, t+\u03c3]),\nas seen in Section 1 of this Appendix. Consider a bin just outside the bins i\u2217\u22121, i\u2217, i\u2217+ 1, for\n22\ninstance bin i = i\u2217+ 2 centered at bi (note bi \u2265t + h), and let J be the set of points j that fall\nwithin bi \u00b1 \u03c3/2. De\ufb01ne\nbpi =\n1\nn\u03c3/2R\nX\nj\u2208J\nI(Yj = +)\nwhere Yj \u2208{\u00b11} are observations at points j \u2208J. Now, we have, since P(Yj = +) = F(j)\nE[bpi]\n=\n1\nn\u03c3/2R\nX\nj\u2208J\nF(j)\n=\n1\nn\u03c3/2R\n\uf8ee\n\uf8f0X\nj\u2208J\n1/2 +\nc\n\u03c3n (Xj \u2212t)\n\uf8f9\n\uf8fb\n\u2248\n1/2 + 1\n\u03c3\nZ bi\u2212t+\u03c3/2\nbi\u2212t\u2212\u03c3/2\nc\n\u03c3n zdz\n=\n1/2 +\nc\n2\u03c32\n\u0002\n(bi \u2212t + \u03c3/2)2 \u2212(bi \u2212t \u2212\u03c3/2)2\u0003\n=\n1/2 +\nc\n\u03c3n (bi \u2212t)\n\u2265\n1/2 +\nc\n\u03c3n h\nC\nJustifying Claims in the Active Upper Bounds\nPhase 1 (k = 1). In the \ufb01rst phase of the algorithm, it is possible that \u03c3 \u2aafRe/n but \u2ab0Ree\u2212n\n- in other words the noise may be small enough that passive learning cannot make out that we\nare in the errors-in-variables setting, and then the passive estimator will get a point error of C1Re\nn/E\nin each of those epochs (as if there is no feature noise). This point error is to the best point in\nepoch e, which we can prove by induction is the true threshold t with high probability. Since\nit trivially holds in the \ufb01rst epoch (t \u2208D1 = [\u22121, 1]), we assume that it is true in epoch e \u22121.\nThen, in epoch e, the true threshold t is still the best point if the estimator xe\u22121 of epoch e \u22121\nwas within Re of t, or in other words if |xe\u22121\u2212t| \u2264Re. This would de\ufb01nitely hold if C1Re\u22121\nn/E\n\u2264Re\ni.e. n \u22652C1E = 2C1\u2308log(1/\u03c3)\u2309, which is true since \u03c3 \u227bexp{\u2212n/2C1}. However, the algorithm\ncannot stay in this phase of \u03c3 \u2aafRe/n this until the last epoch since \u03c3 > RE+1 = RE/2.\nPhase 2 (k = 1). When \u03c3 \u2ab0Re/n, WIDEHIST gets an estimation error of C2\nq\nRe\u03c3\nn/E in epoch\ne. This error is the distance to the best point in epoch e, which is t by the following similar\ninduction. In epoch e, t is still the best point only if |xe\u22121 \u2212t| \u2264Re, i.e. C2\n2\nRe\u22121\u03c3\nn/E\n\u2264R2\ne i.e.\nnRe \u22652C2\n2E\u03c3 which holds since Re > \u03c3 for all e \u2264E and since n \u22652C2\n2E (\u03c3 \u227bexp{\u2212n/2C2\n2}\nimplies E \u2264n/2C2\n2).\nThe \ufb01nal error of the algorithm is is\nq\nRE\u03c3\nn/E = \u02dcO( \u03c3\n\u221an) since RE < 2\u03c3.\nExplanation for k > 1\nAssume \u03c3 \u227bn\u2212\n1\n2k\u22122 , otherwise active learning won\u2019t notice the feature\nnoise, and so log(1/\u03c3) \u2264\nlog n\n(2k\u22122). Choose total epochs E = \u2308log( 1\n\u03c3)\u2309\u2264\nlog n\n(2k\u22122) \u2264C log n for some\nC. In each epoch of length n/E in a region of radius Re = 2\u2212e+1, we get a passive bound of\nC1\nq\nRe\n\u03c32k\u22123n/E whenever \u03c3 > ( Re\nn )\n1\n2k\u22121 . (This must happen at some e \u2264E = \u2308log( 1\n\u03c3)\u2309because\nRE = 2\u2212E+1 < 2\u03c3 < \u03c3\u03c32k\u22122n since \u03c3 \u227bn\u2212\n1\n2k\u22122 and hence in the last epoch \u03c3 > ( RE\nn )\n1\n2k\u22121 .)\n23\nBy the same logic as for k = 1, we need to verify that |xe\u22121 \u2212t| \u2264Re so that if t was in the\nsearch space in epoch e \u22121 then it remains the in the search space in epoch e, i.e. we want to\nverify C2\n1\nRe\u22121\n\u03c32k\u22123n/E \u2264R2\ne \u21d4\u03c32k\u22122Re \u22652C2\n1E\nn\n\u03c3 which is true since Re \u2265\u03c3 and \u03c32k\u22122 > 2C2\n1E/n .\n(By choice of E = \u2308log( 1\n\u03c3)\u2309, Re \u2265RE \u2265\u03c3 \u2265RE+1 . Since \u03c3 \u227bn\u2212\n1\n2k\u22122 we get \u03c32k\u22122 > 2C2\n1E/n\nsince E \u2264C log n .)\nThe \ufb01nal point error is given by the passive algorithm in the last epoch as\nq\nRE\n\u03c32k\u22123n/E ; since\nRE < 2\u03c3 and E \u2264C log n, this becomes \u2aaf\n1\n\u03c3k\u22122\nq\n1\nn .\n24\n"}