{"text": "arXiv:1412.7216v1  [math.ST]  22 Dec 2014\nAn {\u21131, \u21132, \u2113\u221e}-Regularization Approach to\nHigh-Dimensional Errors-in-variables Models\nAlexandre Belloni\nThe Fuqua School of Business\nDuke University\ne-mail: abn5@duke.edu\nMathieu Rosenbaum\nLaboratoire de Probabilit\u00b4es et Mod`eles Al\u00b4eatoires,\nUniversit\u00b4e Pierre et Marie Curie (Paris 6), and\nCentre de Recherche en Economie et Statistique,\nENSAE-Paris Tech\ne-mail: mathieu.rosenbaum@upmc.fr\nAlexandre B. Tsybakov\nCentre de Recherche en Economie et Statistique,\nENSAE-Paris Tech\ne-mail: alexandre.tsybakov@ensae.fr\nAbstract: Several new estimation methods have been recently proposed for the linear regres-\nsion model with observation error in the design. Di\ufb00erent assumptions on the data generating\nprocess have motivated di\ufb00erent estimators and analysis. In particular, the literature considered\n(1) observation errors in the design uniformly bounded by some \u00af\u03b4, and (2) zero mean indepen-\ndent observation errors. Under the \ufb01rst assumption, the rates of convergence of the proposed\nestimators depend explicitly on \u00af\u03b4, while the second assumption has been applied when an es-\ntimator for the second moment of the observational error is available. This work proposes and\nstudies two new estimators which, compared to other procedures for regression models with\nerrors in the design, exploit an additional \u2113\u221e-norm regularization. The \ufb01rst estimator is appli-\ncable when both (1) and (2) hold but does not require an estimator for the second moment of\nthe observational error. The second estimator is applicable under (2) and requires an estimator\nfor the second moment of the observation error. Importantly, we impose no assumption on the\naccuracy of this pilot estimator, in contrast to the previously known procedures. As the recent\nproposals, we allow the number of covariates to be much larger than the sample size. We estab-\nlish the rates of convergence of the estimators and compare them with the bounds obtained for\nrelated estimators in the literature. These comparisons show interesting insights on the interplay\nof the assumptions and the achievable rates of convergence.\n1. Introduction\nSeveral new estimation methods have been recently proposed for the linear regression model with\nobservation error in the design. Such problems arise in a variety of applications, see [7, 6, 9, 10]. In\nthis work we consider the following regression model with observation error in the design:\ny\n=\nX\u03b8\u2217+ \u03be,\nZ\n=\nX + W.\nHere the random vector y \u2208Rn and the random n \u00d7 p matrix Z are observed, the n \u00d7 p matrix X is\nunknown, W is an n \u00d7 p random noise matrix, and \u03be \u2208Rn is a random noise vector. The vector of\nunknown parameters of interest is \u03b8\u2217which is assumed to belong to a given convex subset \u0398 of Rp\ncharacterizing some prior knowledge about \u03b8\u2217(potentially \u0398 = Rp). Similarly to the recent literature\non this topic, we consider the setting where the dimension p can be much larger than the sample size\n1\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n2\nn and the vector \u03b8\u2217is s-sparse, which means that it has not more than s non-zero components.\nThe need for new estimators under errors in the design arises from the fact that standard estimators\n(e.g. Lasso and Dantzig selector) might become unstable, see [7]. To deal with this framework, various\nassumptions have been considered, leading to di\ufb00erent estimators.\nA classical assumption in the literature is a uniform boundedness condition on the errors in the design,\nnamely,\n|W|\u221e\u2264\u00af\u03b4 almost surely,\n(1)\nwhere |\u00b7|q denotes the \u2113q-norm for 1 \u2264q \u2264\u221e. Note that this assumption allows for various dependences\nbetween the errors in the design. In this setting, the Matrix Uncertainty selector (MU selector), which\nis robust to the presence of errors in the design, is proposed in [7]. The MU selector \u02c6\u03b8MU is de\ufb01ned\nas a solution of the minimization problem\nmin{|\u03b8|1 : \u03b8 \u2208\u0398,\n\f\f 1\nnZT (y \u2212Z\u03b8)\n\f\f\n\u221e\u2264\u00b5|\u03b8|1 + \u03c4},\n(2)\nwhere the parameters \u00b5 and \u03c4 depend on the level of the noises of W and \u03be respectively. Under\nappropriate choices of these parameters and suitable assumptions on X, it was shown in [7] that with\nprobability close to 1,\n|\u02c6\u03b8MU \u2212\u03b8\u2217|q \u2264Cs1/q{\u00af\u03b4 + \u00af\u03b42}|\u03b8\u2217|1 + Cs1/q\nr\nlog p\nn ,\n1 \u2264q \u2264\u221e.\n(3)\nHere and in what follows we denote by the same symbol C di\ufb00erent positive constants that do not\ndepend on \u03b8\u2217, s, n, p, \u00af\u03b4. The result (3) implies consistency as the sample size n tends to in\ufb01nity\nprovided that the error in the design goes to zero su\ufb03ciently fast to o\ufb00set s1/q|\u03b8\u2217|1, and the number\nof variables p and the sparsity s of \u03b8\u2217do not grow too fast relative to the sample size n.\nAn alternative assumption considered in the literature is that the entries of the random matrix W are\nindependent with zero mean, the values\n\u03c32\nj = 1\nn\nn\nX\ni=1\nE(W 2\nij), j = 1, . . . , p,\nare \ufb01nite, and data-driven estimators \u02c6\u03c32\nj of \u03c32\nj are available converging with an appropriate rate. This\nassumption motivated the idea to compensate the bias of using the observable ZT Z instead of the\nunobservable XT X in (2) thanks to the estimates of \u03c32\nj . This compensated MU selector, introduced\nin [8] and denoted as \u02c6\u03b8cMU, is de\ufb01ned as a solution of the minimization problem\nmin{|\u03b8|1 : \u03b8 \u2208\u0398,\n\f\f 1\nnZT (y \u2212Z\u03b8) + bD\u03b8\n\f\f\n\u221e\u2264\u00b5|\u03b8|1 + \u03c4},\nwhere bD is the diagonal matrix with entries \u02c6\u03c32\nj and \u00b5 > 0 and \u03c4 > 0 are constants chosen according\nto the level of the noises and the accuracy of the \u02c6\u03c32\nj .\nRates of convergence of the compensated MU selector were established in [8]. Importantly, the com-\npensated MU selector can be consistent as the sample size n increases even if the error in the design\ndoes not vanish. This is in contrast to the case of the MU selector, where the bounds are small only\nif the bound on the design error \u00af\u03b4 is small. In particular, under regularity conditions, when \u03b8\u2217is\ns-sparse, it is shown in [8] that with probability close to 1\n|\u02c6\u03b8cMU \u2212\u03b8\u2217|q \u2264Cs1/q\nr\nlog p\nn (|\u03b8\u2217|1 + 1),\n1 \u2264q \u2264\u221e.\n(4)\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n3\nUnder the same alternative assumption, a conic programming based estimator \u02c6\u03b8C has been recently\nproposed and analyzed in [1]. The estimator \u02c6\u03b8C is de\ufb01ned as the \ufb01rst component of any solution of\nthe optimization problem\nmin\n(\u03b8,t)\u2208Rp\u00d7R+{|\u03b8|1 + \u03bbt : \u03b8 \u2208\u0398,\n\f\f 1\nnZT (y \u2212Z\u03b8) + bD\u03b8\n\f\f\n\u221e\u2264\u00b5t + \u03c4, |\u03b8|2 \u2264t},\n(5)\nwhere \u03bb, \u00b5 and \u03c4 are some positive tuning constants. Akin to \u02c6\u03b8cMU, this estimator compensates\nfor the bias by using the estimators \u02c6\u03c32\nj of \u03c32\nj . However it exploits a combination of \u21131 and \u21132-norm\nregularization to be more adaptive. It was shown to attain a bound as in (4) and to be computationally\nfeasible since it is cast as a tractable convex optimization problem (a second order cone programming\nproblem). Moreover, under mild additional conditions, with probability close to 1, the estimator (5)\nachieves improved bounds of the form\n|\u02c6\u03b8C \u2212\u03b8\u2217|q \u2264Cs1/q\nr\nlog p\nn (|\u03b8\u2217|2 + 1),\n1 \u2264q \u2264\u221e,\n(6)\nprovided that \u02c6D converges to D in sup-norm with the rate\np\n(log p)/n. It is shown in [1] that the rate\nof convergence in (6) is minimax optimal in the considered model.\nThere have been other approaches to the errors-in-variables model, usually exploiting some knowledge\nabout the vector \u03b8\u2217, see [6, 9, 2, 3]. Assuming |\u03b8\u2217|1 is known, [6] proposed an estimator \u02c6\u03b8\u2032 de\ufb01ned\nas the solution of a non-convex program which can be well approximated by an iterative relaxation\nprocedure. In the case where the entries of the regression matrix X are zero-mean subgaussian and\n\u03b8\u2217is s-sparse, under appropriate assumptions, it is shown in [6] that for the error in \u21132-norm (q = 2),\n|\u02c6\u03b8\u2032 \u2212\u03b8\u2217|2 \u2264C(\u03b8\u2217)s1/2\nr\nlog p\nn (|\u03b8\u2217|2 + 1),\n(7)\nwith probability close to 1. Here, the value C(\u03b8\u2217) depends on \u03b8\u2217, so that there is no guarantee that\nthe estimator attains the optimal bound as in (6). Assuming that the sparsity s of \u03b8\u2217is known and\nthe non-zero components of \u03b8\u2217are separated from zero in the way that\n|\u03b8\u2217\nj | \u2265C\nr\nlog p\nn (|\u03b8\u2217|2 + 1),\nan orthogonal matching pursuit algorithm to estimate \u03b8\u2217is introduced in [2, 3]. Focusing as in [6] on\nthe particular case where the entries of the regression matrix X are zero-mean subgaussian, it is shown\nin [2, 3] that this last estimator satis\ufb01es a bound analogous to (6), as well as a consistent support\nrecovery result.\nThe main purpose of this work is to show that an additional regularization term based on the \u2113\u221e-\nnorm leads to improved rates of convergence in several situations. We propose two new estimators for\n\u03b8\u2217. The \ufb01rst proposal is applicable under a new combination of the assumptions mentioned above.\nNamely, we assume that the components of the errors in the design are uniformly bounded by \u00af\u03b4 as in\n(1), and that the rows of W are independent and with zero mean. However, we will neither assume\nthat a data-driven estimator \u02c6D is available, nor that speci\ufb01c features of \u03b8\u2217are known (e.g. s or |\u03b8\u2217|1).\nThe estimator is de\ufb01ned as a solution of a regularized optimization problem which uses simultane-\nously \u21131, \u21132, and \u2113\u221eregularization functions. It can be cast as a convex optimization problem and\nthe solution can be easily computed. We study its rates of convergence in various norms in Section\n3. One of the conclusions is that for \u00af\u03b4 \u226b\np\n(log p)/n the new estimator has improved rates of con-\nvergence compared to the MU selector. Furthermore, note that the conic estimator \u02c6\u03b8C studied in [1]\ncan be also applied. Indeed, our setting can be embedded into that of [1] with bD being the identically\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n4\nzero p \u00d7 p matrix, which means that we have an estimator of each \u03c32\nj with an error bounded by \u00af\u03b42.\nComparing the bounds yields that the conic estimator \u02c6\u03b8C achieves the same rate as our new estimator\nif \u00af\u03b4 is smaller than or of the order\n\u0000(log p)/n\n\u00011/4. However, there is no bound for \u02c6\u03b8C available when\n\u00af\u03b4 \u226b\n\u0000(log p)/n\n\u00011/4.\nThe second estimator we propose applies to the same setting as in [1]. The idea of taking advantage\nof an additional \u2113\u221e-norm regularization can be used to improve the conic estimator \u02c6\u03b8C of [1] when-\never the rate of convergence of the estimator bD for \u03c32\nj , j = 1, . . . , p, is slower than\np\n(log p)/n. This\nmotivates us to propose and analyze a modi\ufb01cation of the conic estimator. We derive new rates of\nconvergence that can lead to improvements. However, we acknowledge that in the case considered in\n[1], where the rate of convergence of bD is\np\n(log p)/n, there is no gain in the rates of convergence\nwhen using the additional \u2113\u221e-norm regularization.\nThe paper is organized as follows. Section 2 contains the notation, main assumptions and some prelim-\ninary lemmas needed to determine threshold constants in the algorithms. The de\ufb01nition and properties\nof our \ufb01rst estimator are given in Section 3 whereas those of our second procedure can be found in\nSection 4. Section 5 contains simulation results. Some auxiliary lemmas are relegated to an appendix.\n2. Notation, assumptions, and preliminary lemmas\nIn this section, we introduce the assumptions which will be required to derive the rates of convergence\nof the proposed estimators. One set of conditions pertains to the design matrix and the second to the\nerrors in the model. We also state preliminary lemmas related to the stochastic error terms. We start\nby introducing some notation.\n2.1. Notation\nLet J \u2282{1, . . ., p} be a set of integers. We denote by |J| the cardinality of J. For a vector \u03b8 =\n(\u03b81, . . . , \u03b8p) in Rp, we denote by \u03b8J the vector in Rp whose jth component satis\ufb01es (\u03b8J)j = \u03b8j if j \u2208J,\nand (\u03b8J)j = 0 otherwise. For \u03b3 > 0, the random variable \u03b7 is said to be sub-gaussian with variance\nparameter \u03b32 (or shortly \u03b3-sub-gaussian) if, for all t \u2208R,\nE[exp(t\u03b7)] \u2264exp(\u03b32t2/2).\nA random vector \u03b6 \u2208Rp is said to be sub-gaussian with variance parameter \u03b32 if the inner products\n(\u03b6, v) are \u03b3-sub-gaussian for any v \u2208Rp with |v|2 = 1.\n2.2. Design matrix\nThe performance of the estimators that we consider below is in\ufb02uenced by the properties of the Gram\nmatrix\n\u03a8 = 1\nnXTX.\nWe will assume that:\n(A1) The matrix X is deterministic.\nIn order to characterize the behavior of the design matrix, we set\nm2 =\nmax\nj=1,...,p\n1\nn\nn\nX\ni=1\nX2\nij,\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n5\nwhere the Xij are the elements of matrix X and we consider the sensitivity characteristics related to\nthe Gram matrix \u03a8. For u > 0, de\ufb01ne the cone\nCJ(u) =\n\b\n\u2206\u2208Rp : |\u2206Jc|1 \u2264u|\u2206J|1\n\t\n,\nwhere J is a subset of {1, . . . , p}. For q \u2208[1, \u221e] and an integer s \u2208[1, p], the \u2113q-sensitivity (cf. [4]) is\nde\ufb01ned as follows:\n\u03baq(s, u) =\nmin\nJ: |J|\u2264s\n\u0010\nmin\n\u2206\u2208CJ(u): |\u2206|q=1 |\u03a8\u2206|\u221e\n\u0011\n.\nLike in [4], we use here the sensitivities to derive the rates of convergence of estimators under sparsity.\nImportantly, as shown in [4], the approach based on sensitivities is more general than that based\non the restricted eigenvalue or the coherence conditions, see also [8, 5, 1]. In particular, under those\nconditions, we have \u03baq(s, u) \u2265c s\u22121/q for some constant c > 0, which implies the usual optimal bounds\nfor the errors.\n2.3. Disturbances\nNext we turn to the error W in the design and the error \u03be in the regression equation. We will make\nthe following assumptions.\n(A2) The elements of the random vector \u03be are independent zero-mean sub-gaussian random variables\nwith variance parameter \u03c32.\n(A3) The rows wi, i = 1, . . . , n, of the noise matrix W are independent zero-mean sub-gaussian\nrandom vectors with variance parameter \u03c32\n\u2217. Furthermore, W is independent of \u03be.\n2.4. Bounds on the stochastic error terms\nWe now state some useful lemmas from [1] and [8] that provide bounds to various stochastic error\nterms that play a role in our analysis. We state them here because they introduce the thresholds \u03b4i, \u03b4\u2032\ni\nthat will be used in the de\ufb01nition of the estimators. In what follows, D is the diagonal matrix with\ndiagonal elements \u03c32\nj , j = 1, . . . , p, and for a square matrix A, we denote by Diag{A} the matrix with\nthe same dimensions as A, the same diagonal elements, and all o\ufb00-diagonal elements equal to zero.\nLemma 1. Let 0 < \u03b5 < 1 and assume (A1)-(A3). Then, with probability at least 1 \u2212\u03b5 (for each\nevent),\n\f\f 1\nnXT W\n\f\f\n\u221e\u2264\u03b41(\u03b5),\n\f\f 1\nnXT \u03be\n\f\f\n\u221e\u2264\u03b42(\u03b5),\n\f\f 1\nnW T \u03be\n\f\f\n\u221e\u2264\u03b43(\u03b5),\n\f\f 1\nn(W T W \u2212Diag{W T W})\n\f\f\n\u221e\u2264\u03b44(\u03b5),\n\f\f 1\nnDiag{W T W} \u2212D\n\f\f\n\u221e\u2264\u03b45(\u03b5),\nwhere\n\u03b41(\u03b5) = \u03c3\u2217\nr\n2m2 log(2p2/\u03b5)\nn\n,\n\u03b42(\u03b5) = \u03c3\nr\n2m2 log(2p/\u03b5)\nn\n,\n\u03b43(\u03b5) = \u03b45(\u03b5) = \u00af\u03b4(\u03b5, 2p),\n\u03b44(\u03b5) = \u00af\u03b4(\u03b5, p(p \u22121)),\nand for an integer N,\n\u00af\u03b4(\u03b5, N) = max\n \n\u03b30\nr\n2 log(N/\u03b5)\nn\n, 2 log(N/\u03b5)\nt0n\n!\n,\nwhere \u03b30, t0 are positive constants depending only on \u03c3, \u03c3\u2217.\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n6\nLemma 2. Let 0 < \u03b5 < 1, \u03b8\u2217\u2208Rp and assume (A1)-(A3). Then, with probability at least 1 \u2212\u03b5,\n\f\f 1\nnXTW\u03b8\u2217\f\f\n\u221e\u2264\u03b4\u2032\n1(\u03b5)|\u03b8\u2217|2,\nwhere \u03b4\u2032\n1(\u03b5) = \u03c3\u2217\nq\n2m2 log(2p/\u03b5)\nn\n. In addition, with probability at least 1 \u2212\u03b5,\n\f\f 1\nn(W T W \u2212Diag{W T W})\u03b8\u2217\f\f\n\u221e\u2264\u03b4\u2032\n4(\u03b5)|\u03b8\u2217|2,\nwhere\n\u03b4\u2032\n4(\u03b5) = max\n \n\u03b32\nr\n2 log(2p/\u03b5)\nn\n, 2 log(2p/\u03b5)\nt2n\n!\n,\nand \u03b32, t2 are positive constants depending only on \u03c3\u2217.\nThe proofs of Lemmas 1 and 2 can be found in [8] and [1] respectively.\n3. {\u21131, \u21132, \u2113\u221e}-MU selector\nIn this section, we de\ufb01ne and analyze our \ufb01rst estimator. It can be seen as a compromise between the\nMU selector (2) and the conic estimator (5) achieved thanks to an additional \u2113\u221e-norm regularization.\nIn the setting that we consider now, the estimate \u02c6D is not available but the rows of the design error\nmatrix W are independent with mean 0, and its entries are uniformly bounded. Formally, in this\nsection, we make the following assumption.\n(A4) Almost surely, |W|\u221e\u2264\u00af\u03b4.\nThus, Assumptions (A1)-(A4) imply the assumptions in [7]. However, they neither imply or are im-\nplied by the assumptions in [8]. That is, it is an intermediary set of conditions relative to the original\nassumptions for the MU selector in [7] and to those for the compensated MU selector in [8]. Impor-\ntantly, we do not assume that there are some accurate estimators of the \u03c32\nj .\nWe consider the estimator \u02c6\u03b8 such that (\u02c6\u03b8, \u02c6t, \u02c6u) \u2208Rp \u00d7 R+ \u00d7 R+ is a solution of the following mini-\nmization problem\nmin\n\u03b8,t,u{|\u03b8|1 + \u03bbt + \u03bdu : \u03b8 \u2208\u0398,\n\f\f 1\nnZT (y \u2212Z\u03b8)\n\f\f\n\u221e\u2264\u00b5t + \u00af\u03b42u + \u03c4, |\u03b8|2 \u2264t, |\u03b8|\u221e\u2264u},\n(8)\nwhere \u03bb > 0 and \u03bd > 0 are tuning constants and the minimum is taken over (\u03b8, t, u) \u2208Rp \u00d7 R+ \u00d7 R+.\nThis estimator \u02c6\u03b8 will be further referred to as the {\u21131, \u21132, \u2113\u221e}-MU selector.\nThe estimator above attempts to mimic the conic estimator (5) without an estimator bD for \u03c32\nj , j =\n1, . . . , p. In order to make \u03b8\u2217feasible for (8), the contribution of the unknown term 1\nnDiag(W T W)\u03b8\u2217\nneeds to be bounded. This is precisely the role of the extra term \u00af\u03b42u in the constraint since |\u03b8|\u221e\u2264u\nand | 1\nnDiag(W T W)|\u221e\u2264\u00af\u03b42 almost surely. Note that the use of u and t instead of |\u03b8|\u221eand |\u03b8|2 in the\nconstraint makes (8) a convex programming problem.\nThis new estimator exploits Assumptions (A2)-(A4) to achieve a rate of convergence that is interme-\ndiary relative to the rate of the MU selector and to that of the conic estimator.\nSet \u00b5 = \u03b4\u2032\n1(\u03b5) + \u03b4\u2032\n4(\u03b5) and \u03c4 = \u03b42(\u03b5) + \u03b43(\u03b5). Note that \u00b5 and \u03c4 are of order\np\n(log p)/n. The next\ntheorem summarizes the performance of the estimator de\ufb01ned by solving (8).\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n7\nTheorem 1. Let Assumptions (A1)-(A4) hold. Assume that the true parameter \u03b8\u2217is s-sparse and\nbelongs to \u0398. Let 0 < \u03b5 < 1, 1 \u2264q \u2264\u221eand 0 < \u03bb, \u03bd < \u221e, and let \u02c6\u03b8 be the {\u21131, \u21132, \u2113\u221e}-MU selector.\nIf \u03baq(s, 1 + \u03bb + \u03bd) \u2265cs\u22121/q for some constant c > 0 then, with probability at least 1 \u22127\u03b5,\n|\u02c6\u03b8 \u2212\u03b8\u2217|q \u2264Cs1/q\nr\nlog(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|1 + 1) + Cs1/q\u00af\u03b42|\u03b8\u2217|1,\n(9)\nfor some constants C > 0 and c\u2032 > 0 (here we set s1/\u221e= 1).\nIf in addition, \u00af\u03b42 +\np\nlog(p/\u03b5)/n \u2264c1\u03ba1(s, 1 + \u03bb + \u03bd) for some small enough constant c1 then, with\nthe same probability we have\n|\u02c6\u03b8 \u2212\u03b8\u2217|q \u2264Cs1/q\nr\nlog(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|2 + 1) + Cs1/q\u00af\u03b42|\u03b8\u2217|\u221e\n(10)\nfor some constants C > 0 and c\u2032 > 0.\nUnder the same assumptions with q = 1, the prediction error admits the following bound, with the\nsame probability:\n1\nn\n\f\fX(\u02c6\u03b8 \u2212\u03b8\u2217)\n\f\f2\n2\n\u2264\nCslog(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|2 + 1)2 + Cs\u00af\u03b44|\u03b8\u2217|2\n\u221e.\n(11)\nProof. We proceed in three steps. Step 1 establishes initial relations and the fact that \u2206= \u02c6\u03b8 \u2212\u03b8\u2217\nbelongs to CJ(1 + \u03bb + \u03bd). Step 2 provides a bound on | 1\nnXTX\u2206|\u221e. Step 3 establishes the rates of\nconvergence stated in the theorem. We work on the event of probability at least 1 \u22127\u03b5 where all the\ninequalities in Lemmas 1 and 2 are realized. Throughout the proof, J = {j : \u03b8\u2217\nj \u0338= 0}. We often make\nuse of the inequalities |\u03b8|\u221e\u2264|\u03b8|2 \u2264|\u03b8|1, \u2200\u03b8 \u2208Rp.\nStep 1. We \ufb01rst note that\n| 1\nnZT (y \u2212Z\u03b8\u2217)|\u221e\n\u2264| 1\nnZT \u03be|\u221e+ | 1\nnZT W\u03b8\u2217|\u221e\n\u2264\u03b42(\u03b5) + \u03b43(\u03b5) + | 1\nnZT W\u03b8\u2217|\u221e\n(12)\nwith probability at least 1 \u22122\u03b5 by Lemma 1. Next, Lemma 2 and the fact that, due to (1), we have\n| 1\nnDiag(W T W)|\u221e\u2264\u00af\u03b42 imply\n| 1\nnZT W\u03b8\u2217|\u221e\n\u2264| 1\nnXT W\u03b8\u2217|\u221e+ | 1\nnW T W\u03b8\u2217|\u221e\n\u2264| 1\nnXT W\u03b8\u2217|\u221e+ | 1\nn(W T W \u2212Diag(W T W))\u03b8\u2217|\u221e+ | 1\nnDiag(W T W)\u03b8\u2217|\u221e\n\u2264\u03b4\u2032\n1(\u03b5)|\u03b8\u2217|2 + \u03b4\u2032\n4(\u03b5)|\u03b8\u2217|2 + \u00af\u03b42|\u03b8\u2217|\u221e.\n(13)\nCombining (12) and (13) we get that (\u03b8, t, u) = (\u03b8\u2217, |\u03b8\u2217|2, |\u03b8\u2217|\u221e) is feasible for the problem (8), so that\n|\u02c6\u03b8|1 + \u03bb|\u02c6\u03b8|2 + \u03bd|\u02c6\u03b8|\u221e\u2264|\u02c6\u03b8|1 + \u03bb\u02c6t + \u03bd\u02c6u \u2264|\u03b8\u2217|1 + \u03bb|\u03b8\u2217|2 + \u03bd|\u03b8\u2217|\u221e.\n(14)\nFrom (14) we easily obtain\n|\u02c6\u03b8Jc|1 \u2264(1 + \u03bb + \u03bd)|\u02c6\u03b8J \u2212\u03b8\u2217|1.\nArguments similar to (14) lead to\n\u02c6t \u2212|\u03b8\u2217|2 \u2264|\u2206|1 + \u03bd|\u2206|\u221e\n\u03bb\n\u2264(1 + \u03bd)\n\u03bb\n|\u2206|1 and \u02c6u \u2212|\u03b8\u2217|\u221e\u2264|\u2206|1 + \u03bb|\u2206|2\n\u03bd\n\u2264(1 + \u03bb)\n\u03bd\n|\u2206|1.\nStep 2. We have\n| 1\nnXT X\u2206|\u221e\n\u2264| 1\nnZTX\u2206|\u221e+ | 1\nnW T X\u2206|\u221e\n\u2264| 1\nnZTZ\u2206|\u221e+ | 1\nnZT W\u2206|\u221e+ | 1\nnW T X\u2206|\u221e\n\u2264| 1\nnZT(y \u2212Z\u03b8\u2217)|\u221e+ | 1\nnZT (y \u2212Z \u02c6\u03b8)|\u221e+ | 1\nnZTW\u2206|\u221e+ | 1\nnW T X\u2206|\u221e.\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n8\nThe results of Step 1 and of Lemmas 1 and 2 imply the following bounds\n| 1\nnZT (y \u2212Z\u03b8\u2217)|\u221e\n\u2264\u00b5|\u03b8\u2217|2 + \u00af\u03b42|\u03b8\u2217|\u221e+ \u03c4,\n| 1\nnZT (y \u2212Z \u02c6\u03b8)|\u221e\n\u2264\u00b5\u02c6t + \u00af\u03b42\u02c6u + \u03c4\n\u2264\u00b5|\u03b8\u2217|2 + \u00af\u03b42|\u03b8\u2217|\u221e+ \u03c4 + {\u00b5(1 + \u03bd)/\u03bb + \u00af\u03b42(1 + \u03bb)/\u03bd}|\u2206|1,\n| 1\nnW T X\u2206|\u221e\n\u2264\u03b41|\u2206|1,\n| 1\nnZT W\u2206|\u221e\n\u2264| 1\nnXTW\u2206|\u221e+ | 1\nn(W T W \u2212Diag(W T W))\u2206|\u221e+ | 1\nnDiag(W T W)\u2206|\u221e\n\u2264\u03b41|\u2206|1 + \u03b44|\u2206|1 + \u00af\u03b42|\u2206|\u221e.\nThese relations and the inequality |\u2206|\u221e\u2264|\u2206|1 yield that\n| 1\nnXT X\u2206|\u221e\u22642\u00b5|\u03b8\u2217|2 + 2\u00af\u03b42|\u03b8\u2217|\u221e+ 2\u03c4 + (\u00af\u03b42{(1 + \u03bb + \u03bd)/\u03bd} + {(1 + \u03bd)/\u03bb}\u00b5 + 2\u03b41 + \u03b44)|\u2206|1.\nStep 3. Next note that |\u2206|1 \u2264|\u02c6\u03b8|1 + |\u03b8\u2217|1 \u2264(2 + \u03bb + \u03bd)|\u03b8\u2217|1. Letting\n\u03b7 = (\u00af\u03b42{(1 + \u03bb + \u03bd)/\u03bd} + {(1 + \u03bd)/\u03bb}\u00b5 + 2\u03b41 + \u03b44),\nwe have\n| 1\nnXTX\u2206|\u221e\u22642\u03c4 + (2\u00b5 + 2\u00af\u03b42 + (2 + \u03bb + \u03bd)\u03b7)|\u03b8\u2217|1.\nBy the de\ufb01nition of the \u2113q-sensitivity,\n| 1\nnXT X\u2206|\u221e\u2265\u03baq(s, 1 + \u03bb + \u03bd)|\u2206|q.\nNow, (9) follows by combining the last two displays and the assumption on \u03baq(s, 1 + \u03bb + \u03bd). To prove\n(10), we use that\n| 1\nnXT X\u2206|\u221e\n\u22642\u00b5|\u03b8\u2217|2 + 2\u00af\u03b42|\u03b8\u2217|\u221e+ 2\u03c4 + \u03b7|\u2206|1\n\u22642\u00b5|\u03b8\u2217|2 + 2\u00af\u03b42|\u03b8\u2217|\u221e+ 2\u03c4 + \u03b7| 1\nnXT X\u2206|\u221e/\u03ba1(s, 1 + \u03bb + \u03bd).\nUnder our conditions, \u03b7/\u03ba1(s, 1 + \u03bb + \u03bd) \u2264c\u2032 for some 0 < c\u2032 < 1. Thus, we have\n| 1\nnXT X\u2206|\u221e\u2264c\n\u0000\u00b5|\u03b8\u2217|2 + \u00af\u03b42|\u03b8\u2217|\u221e+ \u03c4\n\u0001\n,\n(15)\nwhich implies (10) in view of the de\ufb01nition of the \u2113q-sensitivity and the assumption on \u03baq(s, 1+\u03bb+\u03bd).\nTo show (11), note \ufb01rst that\n1\nn |X\u2206|2\n2 \u22641\nn\n\f\fXT X\u2206\n\f\f\n\u221e|\u2206|1.\n(16)\nBy (10) with q = 1,\n|\u2206|1 \u2264Cs\nr\nlog(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|2 + 1) + Cs\u00af\u03b42|\u03b8\u2217|\u221e.\nCombining this inequality with (15) and (16) proves (11). \u25a1\nRemark 1. We have stated Theorem 1 under Assumption (A4) to make the analysis streamlined with\nthe previous literature, see [7]. However, inspection of the proofs shows that a more general condition\ncan be used. The results of Theorem 1 hold with probability at least 1\u22127\u03b5\u2212\u03b5\u2032 if instead of Assumption\n(A4) we require W to satisfy:\n| 1\nnDiag(W T W)|\u221e\u2264\u00af\u03b42\nwith probability at least 1 \u2212\u03b5\u2032, for some \u03b5\u2032 > 0.\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n9\nCompared to [7], the results in Theorem 1 exploit the zero mean condition on the noise matrix W.\nAs in [7], the estimator is consistent as \u00af\u03b4 goes to zero. In order to compare the rates in Theorem 1\nwith those for the MU selector, we recall that, by Theorem 3 in [7], the MU selector satis\ufb01es\n|\u02c6\u03b8MU \u2212\u03b8\u2217|q \u2264Cs1/q\nr\nlog(c\u2032p/\u03b5)\nn\n+ Cs1/q(\u00af\u03b4 + \u00af\u03b42)|\u03b8\u2217|1\nwith probability close to 1. While both rates share some terms, a term of order s1/q\u00af\u03b4|\u03b8\u2217|1 appears only\nin the rate for the MU selector whereas a term of the order s1/qp\nlog(c\u2032p/\u03b5)/n|\u03b8\u2217|1 appears only for\nthe {\u21131, \u21132, \u2113\u221e}-MU selector. Therefore, the improvement upon the original MU selector is achieved\nwhenever \u00af\u03b4 \u226b\np\nlog(c\u2032p/\u03b5)/n.\nIf the additional condition \u00af\u03b42 +\np\nlog(c\u2032p/\u03b5)/n \u2264c1\u03ba1(s, 1 + \u03bb + \u03bd) holds, we can use the bound (10)\nand a better accuracy is achieved by the proposed estimator. In particular, |\u03b8\u2217|1 no longer drives the\nrate of convergence. The impact of \u00af\u03b4 on this rate is in the term\ns1/q\u00af\u03b42|\u03b8\u2217|\u221e\ninstead of\ns1/q(\u00af\u03b4 + \u00af\u03b42)|\u03b8\u2217|1\n(17)\nfor the MU selector. Furthermore, the rate of convergence of the new estimator also has a term of\nthe form |\u03b8\u2217|2s1/qp\nlog(c\u2032p/\u03b5)/n. Thus the new estimator obtains a better accuracy by exploiting\nadditional assumptions together with the fact that \u00af\u03b4|\u03b8\u2217|1 is of larger order than\np\nlog(c\u2032p/\u03b5)/n|\u03b8\u2217|2,\nwhich holds whenever \u00af\u03b4 \u226b\np\nlog(c\u2032p/\u03b5)/n. Finally, the impact of going down from the \u21131-norm to the\n\u21132- or \u2113\u221e-norms is not negligible neither. For example, if all non-zero components of \u03b8\u2217are equal to\nthe same constant a > 0, we have |\u03b8\u2217|1 = sa while |\u03b8\u2217|2 = a\u221as, and |\u03b8\u2217|\u221e= a. Then, the comparison\nin (17) is reduces to comparing\ns1/q\u00af\u03b42\nversus\ns1+1/q(\u00af\u03b4 + \u00af\u03b42),\nfeaturing the maximum contrast between the two rates.\nFinally, note that the conic estimator \u02c6\u03b8C studied in [1] can be also applied under the assumptions of\nthis section. Indeed, our setting can be embedded into that of [1] with bD being the identically zero\np \u00d7 p matrix, which means that we have an estimator of each \u03c32\nj with an error bounded by b = \u00af\u03b42.\nThe results in [1] assume b = C\np\n(log p)/n but they do not apply to designs with b of larger order.\nComparing the bound (10) in Theorem 1 to the bound (6) yields that the conic estimator \u02c6\u03b8C achieves\nthe same rate as our new estimator whenever \u00af\u03b4 is smaller than or of the order\n\u0000(log p)/n\n\u00011/4. However,\nthere is no bound for \u02c6\u03b8C available when \u00af\u03b4 \u226b\n\u0000(log p)/n\n\u00011/4.\n4. {\u21131, \u21132, \u2113\u221e}-compensated MU selector\nIn this section, we discuss a modi\ufb01cation of the conic estimator proposed in [1]. We introduce an\nadditional \u2113\u221e-norm regularization to better adapt to the estimation error in bD. As discussed in the\nintroduction, this is bene\ufb01cial when the rate of convergence of bD to D is slower than\np\n(log p)/n,\nwhich is not covered by [1]. Here we consider the same assumptions as in [1] with the only di\ufb00erence\nthat now we allow for any rate of convergence of bD to D. Thus, we replace Assumption (A4) by the\nfollowing assumption on the availability of estimators for \u03c32\nj , j = 1, . . . , p.\n(A5) There exist statistics \u02c6\u03c32\nj and positive numbers b(\u03b5) such that for any 0 < \u03b5 < 1, we have\nP\n\u0002\nmax\nj=1,...,p |\u02c6\u03c32\nj \u2212\u03c32\nj | \u2265b(\u03b5)\n\u0003\n\u2264\u03b5.\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n10\nIn what follows, we \ufb01x \u03b5 and set\n\u00b5 = \u03b4\u2032\n1(\u03b5) + \u03b4\u2032\n4(\u03b5),\n\u03c4 = \u03b42(\u03b5) + \u03b43(\u03b5) and \u03b2 = b(\u03b5) + \u03b45(\u03b5).\nWe are particularly interested in cases where \u03b2 is of larger order than\np\n(log p)/n. To de\ufb01ne the\nestimator, we consider the following minimization problem:\nmin\n\u03b8,t,u{|\u03b8|1 + \u03bbt + \u03bdu : \u03b8 \u2208\u0398,\n\f\f 1\nnZT (y \u2212Z\u03b8) + bD\u03b8\n\f\f\n\u221e\u2264\u00b5t + \u03b2u + \u03c4, |\u03b8|2 \u2264t, |\u03b8|\u221e\u2264u}.\n(18)\nHere, \u03bb > 0 and \u03bd > 0 are tuning constants and the minimum is taken over (\u03b8, t, u) \u2208Rp \u00d7 R+ \u00d7 R+.\nLet (\u02c6\u03b8, \u02c6t, \u02c6u) be a solution of (18). We take \u02c6\u03b8 as estimator of \u03b8\u2217and we call it the {\u21131, \u21132, \u2113\u221e}-\ncompensated MU selector. The rates of convergence of this estimator are given in the next theorem.\nTheorem 2. Let Assumptions (A1)-(A3), and (A5) hold. Assume that the true parameter \u03b8\u2217is\ns-sparse and belongs to \u0398. Let 0 < \u03b5 < 1 and 1 \u2264q \u2264\u221e. Suppose also that\n\u03baq(s, 1 + \u03bb + \u03bd) \u2265cs\u22121/q\n(19)\nfor some constant c > 0 and that\ns \u2264c1 min{\np\nn/ log(p/\u03b5), 1/b(\u03b5)},\n(20)\nfor some small enough constant c1 > 0. Let \u02c6\u03b8 be the {\u21131, \u21132, \u2113\u221e}-compensated MU selector. Then, with\nprobability at least 1 \u22128\u03b5,\n|\u02c6\u03b8 \u2212\u03b8\u2217|q\n\u2264\nCs1/q\nr\nlog(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|2 + 1) + Cs1/qb(\u03b5)|\u03b8\u2217|\u221e,\n(21)\nfor some constants C > 0 and c\u2032 > 0 (here we set s1/\u221e= 1).\nUnder the same assumptions with q = 1, the prediction error admits the following bound, with the\nsame probability:\n1\nn\n\f\fX(\u02c6\u03b8 \u2212\u03b8\u2217)\n\f\f2\n2\n\u2264\nCslog(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|2 + 1)2 + Csb2(\u03b5)|\u03b8\u2217|2\n\u221e.\n(22)\nProof. Throughout the proof, we assume that we are on the event of probability at least 1 \u22128\u03b5 where\nthe results of Lemmas 3, 4 and 5 in the Appendix hold. Property (32) in Lemma 4 implies that\n\u2206= \u02c6\u03b8 \u2212\u03b8\u2217is in the cone CJ(1 + \u03bb + \u03bd), where J = {j : \u03b8\u2217\nj \u0338= 0}. Therefore, by the de\ufb01nition of the\n\u2113q-sensitivity and Lemma 5, we have\n\u03baq(s, 1 + \u03bb + \u03bd)|\u2206|q \u2264\n\f\f 1\nnXT X\u2206\n\f\f\n\u221e\u2264\u00b50 + \u00b51|\u02c6\u03b8 \u2212\u03b8\u2217|1 + \u00b52|\u03b8\u2217|2 + \u00b5\u221e|\u03b8\u2217|\u221e,\nwhere \u00b50 and \u00b52 are of order\nq\n1\nn log(c\u2032p/\u03b5), and \u00b51 and \u00b5\u221eare of order\nq\n1\nn log(c\u2032p/\u03b5) + b(\u03b5). Using\nagain (32), we have\n|\u2206|1\n=\n|\u2206Jc|1 + |\u2206J|1 \u2264(2 + \u03bb + \u03bd)|\u2206J|1\n\u2264\n(2 + \u03bb + \u03bd)s1\u22121/q|\u2206J|q \u2264(2 + \u03bb + \u03bd)s1\u22121/q|\u2206|q.\nIt follows that\n(\u03baq(s, 1 + \u03bb + \u03bd) \u2212(2 + \u03bb + \u03bd)\u00b51s1\u22121/q)|\u2206|q \u2264\u00b50 + \u00b52|\u03b8\u2217|2 + \u00b5\u221e|\u03b8\u2217|\u221e,\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n11\nwhich implies, by (19),\n(c \u2212(2 + \u03bb + \u03bd)\u00b51s)s\u22121/q|\u2206|q \u2264\u00b50 + \u00b52|\u03b8\u2217|2 + \u00b5\u221e|\u03b8\u2217|\u221e,\nin view of the assumptions of the theorem. Recall that \u00b51 \u2264a{\np\nlog(c\u2032p/\u03b5)/n + b(\u03b5)}, where a > 0\nis a constant. Therefore, since we assume that s \u2264c1 min{\np\nn/ log(p/\u03b5), 1/b(\u03b5)}, (21) follows if c1 is\nsmall enough.\nTo prove (22), we use (16). Remark that from (21) with q = 1, we have\n|\u2206|1 \u2264Cs\nr\nlog(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|2 + 1) + Csb(\u03b5)|\u03b8\u2217|\u221e.\nLemma 5 in the Appendix yields\n\f\f 1\nnXT X\u2206\n\f\f\n\u221e\u2264\u00b50 + \u00b51|\u02c6\u03b8 \u2212\u03b8\u2217|1 + \u00b52|\u03b8\u2217|2 + \u00b5\u221e|\u03b8\u2217|\u221e.\n(23)\nCombining the above bound for |\u2206|1 and (23), we get\n1\nn |X\u2206|2\n2 \u2264C s log(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|2 + 1)2 + Csb2(\u03b5)|\u03b8\u2217|2\n\u221e\nsince \u00b51s \u2264C\u2032\u2032 for some constant C\u2032\u2032 > 0 under our assumptions. This proves (22). \u25a1\nTheorem 2 generalizes the results in [1] to estimators bD that converge with rate b(\u03b5) of larger order\nthan\np\n(log p)/n. At the same time, if b(\u03b5) is smaller than\np\n(log p)/n, both the conic estimator \u02c6\u03b8C\nof [1] and the {\u21131, \u21132, \u2113\u221e}-compensated MU selector achieve the same rate of convergence.\nFor such designs that condition (20) does not hold, the conclusions of Theorem 2 need to be slightly\nmodi\ufb01ed as shown in the next theorem.\nTheorem 3. Let Assumptions (A1)-(A3), and (A5) hold. Assume that the true parameter \u03b8\u2217is s-\nsparse and belongs to \u0398. Let 0 < \u03b5 < 1 and 1 \u2264q \u2264\u221e. Let \u02c6\u03b8 be the {\u21131, \u21132, \u2113\u221e}-compensated MU\nselector. Then, with probability at least 1 \u22128\u03b5,\n|\u02c6\u03b8 \u2212\u03b8\u2217|q\n\u2264\nC\n\u03baq(s, 1 + \u03bb + \u03bd)\n(r\nlog(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|1 + 1) + b(\u03b5)|\u03b8\u2217|1\n)\n,\n(24)\nfor some constants C > 0 and c\u2032 > 0, and the prediction error admits the following bound, with the\nsame probability:\n1\nn\n\f\fX(\u02c6\u03b8 \u2212\u03b8\u2217)\n\f\f2\n2 \u2264C min\n( log(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|1 + 1)2 + b2(\u03b5)|\u03b8\u2217|2\n1\n\u03ba1(s, 1 + \u03bb + \u03bd)\n,\nr\nlog(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|1 + 1)2 + b(\u03b5)|\u03b8\u2217|2\n1\n)\n. (25)\nProof. Again, throughout the proof, we assume that we are on the event of probability at least 1 \u22128\u03b5\nwhere the results of Lemmas 3, 4 and 5 in the Appendix hold. Property (32) in Lemma 4 implies that\n\u2206= \u02c6\u03b8 \u2212\u03b8\u2217is in the cone CJ(1 + \u03bb + \u03bd), where J = {j : \u03b8\u2217\nj \u0338= 0}. Since\n|\u2206|1 \u2264|\u02c6\u03b8|1 + |\u03b8\u2217|1 \u2264{|\u03b8\u2217|1 + \u03bb|\u03b8\u2217|2 + \u03bd|\u03b8\u2217|\u221e} + |\u03b8\u2217|1 \u2264(2 + \u03bb + \u03bd)|\u03b8\u2217|1,\n(26)\nwe obtain\n\f\f 1\nnXT X\u2206\n\f\f\n\u221e\u2264\u00b50 + \u00b51|\u2206|1 + \u00b52|\u03b8\u2217|2 + \u00b5\u221e|\u03b8\u2217|\u221e\u2264\u00b50 + (\u00b51 + \u00b52 + \u00b5\u221e)(2 + \u03bb + \u03bd)|\u03b8\u2217|1.\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n12\nTherefore\n\u03baq(s, 1 + \u03bb + \u03bd)|\u2206|q \u2264\u00b50 + (\u00b51 + \u00b52 + \u00b5\u221e)(2 + \u03bb + \u03bd)|\u03b8\u2217|1,\nwhich implies (24). Note also that, due to (16), the above displays immediately imply the bound on\nthe prediction risk given by the second term under the minimum in (25). The \ufb01rst term under the\nminimum in (25) is obtained by combining (16), (24) with q = 1, and (26). \u25a1\n5. Simulations\nThis section aims to illustrate the \ufb01nite sample performance of the proposed estimators. We will focus\non the {\u21131, \u21132, \u2113\u221e}-compensated MU selector only. We consider the following data generating process\nyi = xT\ni \u03b8\u2217+ \u03bei,\nzi = xi + wi.\nHere, \u03bei, wi, xi are independent and \u03bei \u223cN(0, \u03c32), wi \u223cN(0, \u03c32\n\u2217Ip\u00d7p), xi \u223cN(0, \u03a3) where Ip\u00d7p is the\nidentity matrix and \u03a3 is p \u00d7 p matrix with elements \u03a3ij = \u03c1|i\u2212j|. We consider the vector of unknown\nparameters \u03b8\u2217= 1.25(1, 1, 1, 1, 1, 0, . . ., 0)T . We set \u03c3 = 0.128, \u03c32\n\u2217= 0.5, and \u03c1 = 0.25. We assume\nthat \u03c3 is known and we set \u02c6D = D = \u03c32\n\u2217Ip\u00d7p. The penalty parameters are set as \u03c4 = \u03c3\np\nlog(p/\u03b5)/n,\nb(\u03b5) = \u03c32\n\u2217\np\nlog(p/\u03b5)/n, for \u03b5 = 0.05.\nIn our \ufb01rst set of simulations, we illustrate the \ufb01nite sample performance of the proposed estimator\nby setting \u03bb = \u03bd \u2208{0.25, 0.5, 0.75, 1}. The {\u21131, \u21132, \u2113\u221e}-compensated MU selector will be denoted by\n{\u21131, \u21132, \u2113\u221e}. We compare its performance with other recent proposals in the literature, namely the\nconic estimator (denoted as Conic (\u03bb) for \u03bb = 0.25, 0.5, 0.75, 1), and the Compensated MU selector\n(cMU). We also provide the (infeasible) Dantzig selector which knows X (Dantzig X) and the Dantzig\nselector that uses only Z (Dantzig Z) as additional benchmark for the performance.\nn = 300 and p = 10\nn = 300 and p = 50\nMethod (\u03bb = \u03bd)\nBias\nRMSE\nPR\nBias\nRMSE\nPR\nDantzig X\n0.0265486\n0.0321528\n0.0349530\n0.0301636\n0.0349420\n0.0386731\nDantzig Z\n0.5892699\n0.6218173\n0.7118256\n0.6032541\n0.7246990\n0.7526539\ncMU\n0.6002801\n0.6526144\n0.7375240\n0.6684987\n0.7074681\n0.8148175\nConic (0.25)\n1.9261733\n1.9567318\n2.3165088\n1.9952936\n2.0190105\n2.4085353\n{\u21131, \u21132, \u2113\u221e}(0.25)\n1.7922416\n1.8349666\n2.1453927\n1.9035308\n1.9325326\n2.2875796\nConic (0.5)\n0.3184083\n0.4161670\n0.4326569\n0.3668194\n0.4395404\n0.4781078\n{\u21131, \u21132, \u2113\u221e} (0.5)\n0.2137347\n0.3505829\n0.3382480\n0.3489980\n0.4491837\n0.4605638\nConic (0.75)\n0.3179691\n0.4158134\n0.4322128\n0.3668194\n0.4395404\n0.4781078\n{\u21131, \u21132, \u2113\u221e} (0.75)\n0.2085334\n0.3459298\n0.3330411\n0.2699453\n0.3786945\n0.3896168\nConic (1)\n0.3179691\n0.4158134\n0.4322128\n0.3661721\n0.4390614\n0.4773173\n{\u21131, \u21132, \u2113\u221e} (1)\n0.2078373\n0.3455287\n0.3324356\n0.2483137\n0.3691060\n0.3736929\nTable 1\nSimulation results for 100 replications. For each estimator we provide average bias (Bias), average root-mean squared\nerror (RMSE), and average prediction risk (PR).\nTables 1 and 2 provide the performance of the proposed estimator when \u03bb = \u03bd and the performance\nof various benchmarks. As discussed in the literature, ignoring the error-in-variables issue can lead to\nworse performance as seen from the performance of Dantzig Z compared to the (infeasible) Dantzig X.\nThe conic estimator performs better than the compensated MU selector (cMU) when \u03bb \u2208{0.5, 0.75, 1}.\nThe comparison of the proposed estimator and the conic estimator is easier to establish as we can\nparametrize them by \u03bb (as we set \u03bb = \u03bd). In this case the conic estimator penalizes more aggressively\nthe uncertainty of not knowing \u03c32\nj . In essentially all cases1 the proposed estimator yields improvements.\nThe introduction of \u2113\u221e-norm regularization seems to alleviate regularization bias. Nonetheless, when\n1The conic estimator performs slightly better only with respect to RMSE in the case of \u03bb = 0.5. For all other\nparameters and metrics, the proposed estimator performs slightly better or substantially better.\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n13\nn = 300 and p = 100\nn = 300 and p = 300\nMethod (\u03bb = \u03bd)\nBias\nRMSE\nPR\nBias\nRMSE\nPR\nDantzig X\n0.0317776\n0.0366155\n0.0403419\n0.0344617\n0.0387848\n0.0436396\nDantzig Z\n0.6039890\n0.8364059\n0.7910512\n0.6334052\n1.0775665\n0.8824695\ncMU\n0.6908240\n0.7359536\n0.8472447\n0.7228791\n0.7653174\n0.8843476\nConic (0.25)\n2.0196204\n2.0428152\n2.4429977\n2.0833543\n2.0985979\n2.5281871\n{\u21131, \u21132, \u2113\u221e}(0.25)\n1.9363225\n1.9646153\n2.3321286\n2.0016163\n2.0247679\n2.4181903\nConic (0.5)\n0.5032353\n0.6479385\n0.6390150\n0.6809176\n0.8886359\n0.8367831\n{\u21131, \u21132, \u2113\u221e} (0.5)\n0.4170439\n0.5207218\n0.5436218\n0.4694103\n0.5507253\n0.5975351\nConic (0.75)\n0.3849631\n0.4699933\n0.5082582\n0.4195124\n0.4964321\n0.5428568\n{\u21131, \u21132, \u2113\u221e} (0.75)\n0.3250997\n0.4312186\n0.4512656\n0.3869566\n0.4747343\n0.5104562\nConic(1)\n0.3811186\n0.4673239\n0.5043246\n0.4047078\n0.4846393\n0.5271225\n{\u21131, \u21132, \u2113\u221e} (1)\n0.2907918\n0.4155573\n0.4242000\n0.3573025\n0.4569624\n0.4819208\nTable 2\nSimulation results for 100 replications. For each estimator we provide average bias (Bias), average root-mean squared\nerror (RMSE), and average prediction risk (PR).\nsetting \u03bb = 0.25 both the conic estimator and the proposed estimator fail in the experiment. This\nfailure occurs by not having enough penalty to control t \u2212|\u03b8|2 and u \u2212|\u03b8|\u221ewhich leads to a large\nright hand side \u00b5t + \u03b2u + \u03c4 in the constraint\n\f\f 1\nnZT (y \u2212Z\u03b8) + bD\u03b8\n\f\f\n\u221e\u2264\u00b5t + \u03b2u + \u03c4\nin (18) and similarly the right hand side \u00b5t + \u03c4 in (5). In turn, this leads to substantial regularization\nbias and therefore under\ufb01tting. In fact, detailed inspection of estimators in that case reveals that\ncoe\ufb03cients are very close to zero for both the conic and the proposed estimator.\nIn the second set of simulations, we explore the performance of the proposed estimator for the case\n\u03bb \u0338= \u03bd. Moreover, we also study a modi\ufb01ed estimator that contains safeguard constraints. These\nconstraints aim to mitigate the problem discussed above. The safeguard constraints are described in\nRemark 2 below. We denote by {\u21131, \u21132, \u2113\u221e}\u2217the estimator computed with the safeguards.\nRemark 2 (Safeguard Constraints). In order to further bound t and u, we can add constraints that\nexploit that | \u00b7 |q \u2264| \u00b7 |1 for q \u22651. Therefore, the constraints\n\u03b8 = \u03b8+ \u2212\u03b8\u2212,\n\u03b8+ \u22650,\n\u03b8\u2212\u22650,\nw =\np\nX\nj=1\n{\u03b8+ + \u03b8\u2212},\nt \u2264w,\nand u \u2264w\npreserve the convexity of the optimization problem and can potentially yield additional performance.\nWe consider the same design as before and we explore some combinations of values\n(\u03bb, \u03bd) \u2208{0.25, 0.5, 0.75, 1} \u00d7 {0.25, 0.5, 0.75, 1}\nfor both proposed estimators (with and without the safeguard constraints).\nTables 3 and 4 show the performance for di\ufb00erent values of \u03bb and \u03bd. We note that these parameters\nseem to have di\ufb00erent impact on the \ufb01nite sample performance even if \u03bb + \u03bd is kept constant. Impor-\ntantly, we observe that the addition of safeguard constraints virtually always leads to improvements\nalthough small (even zero sometimes) for most of the tested parameter values. In the case \u03bb < \u03bd,\nusing safeguard constraints makes almost no di\ufb00erence and overall performance of both estimators is\nbetter. In contrast, the estimators perform worse when \u03bb > \u03bd and the safeguard constraints lead to\nimprovements. Finally, as expected, the safeguard constraints improve substantially the performance\nwhen \u03bb = \u03bd = 0.25. In that case, the performance becomes comparable to that of the cMU estimator.\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n14\nn = 300 and p = 10\nn = 300 and p = 50\nMethod (\u03bb, \u03bd)\nBias\nRMSE\nPR\nBias\nRMSE\nPR\n{\u21131, \u21132, \u2113\u221e} (1,1)\n0.2078373\n0.3455287\n0.3324356\n0.2483137\n0.3691060\n0.3736929\n{\u21131, \u21132, \u2113\u221e}\u2217(1,1)\n0.2078373\n0.3455287\n0.3324356\n0.2483137\n0.3691060\n0.3736929\n{\u21131, \u21132, \u2113\u221e} (1,0.5)\n0.2534465\n0.3997941\n0.3725479\n0.5214272\n0.7086267\n0.6514348\n{\u21131, \u21132, \u2113\u221e}\u2217(1,0.5)\n0.2392491\n0.3623416\n0.3569492\n0.3980543\n0.4729990\n0.5112310\n{\u21131, \u21132, \u2113\u221e} (0.5,1)\n0.2077228\n0.3455690\n0.3322088\n0.2448911\n0.3690180\n0.3723095\n{\u21131, \u21132, \u2113\u221e}\u2217(0.5,1)\n0.2077228\n0.3455690\n0.3322088\n0.2448911\n0.3690180\n0.3723095\n{\u21131, \u21132, \u2113\u221e} (0.75,0.75)\n0.2085334\n0.3459298\n0.3330411\n0.2699453\n0.3786945\n0.3896168\n{\u21131, \u21132, \u2113\u221e}\u2217(0.75,0.75)\n0.2085334\n0.3459297\n0.3330411\n0.2699453\n0.3786945\n0.3896168\n{\u21131, \u21132, \u2113\u221e} (0.25,1)\n0.2078663\n0.3458796\n0.3322444\n0.2439496\n0.3684173\n0.3715836\n{\u21131, \u21132, \u2113\u221e}\u2217(0.25,1)\n0.2078663\n0.3458796\n0.3322444\n0.2439496\n0.3684173\n0.3715836\n{\u21131, \u21132, \u2113\u221e} (0.5,0.5)\n0.2137347\n0.3505829\n0.3382480\n0.3489980\n0.4491837\n0.4605638\n{\u21131, \u21132, \u2113\u221e}\u2217(0.5,0.5)\n0.2137347\n0.3505827\n0.3382479\n0.3382218\n0.4225007\n0.4490958\n{\u21131, \u21132, \u2113\u221e} (0.25,0.5)\n0.2114159\n0.3502938\n0.3369809\n0.3188151\n0.4086438\n0.4313163\n{\u21131, \u21132, \u2113\u221e}\u2217(0.25,0.5)\n0.2114159\n0.3502938\n0.3369809\n0.3188151\n0.4086438\n0.4313163\n{\u21131, \u21132, \u2113\u221e} (0.25,0.25)\n1.7922416\n1.8349666\n2.1453927\n1.9035308\n1.9325326\n2.2875796\n{\u21131, \u21132, \u2113\u221e}\u2217(0.25,0.25)\n0.5477221\n0.6050091\n0.6780050\n0.6151622\n0.6574460\n0.7535672\nTable 3\nSimulation results for 100 replications. For each estimator we provide average bias (Bias), average root-mean squared\nerror (RMSE), and average prediction risk (PR).\nn = 300 and p = 100\nn = 300 and p = 300\nMethod (\u03bb, \u03bd)\nBias\nRMSE\nPR\nBias\nRMSE\nPR\n{\u21131, \u21132, \u2113\u221e} (1,1)\n0.2907918\n0.4155573\n0.4242000\n0.3573025\n0.4569624\n0.4819208\n{\u21131, \u21132, \u2113\u221e}\u2217(1,1)\n0.2907918\n0.4155573\n0.4242000\n0.3573084\n0.4569653\n0.4819268\n{\u21131, \u21132, \u2113\u221e} (1,0.5)\n0.6707248\n0.8687948\n0.8260765\n1.0995021\n1.2843061\n1.3224733\n{\u21131, \u21132, \u2113\u221e}\u2217(1,0.5)\n0.4713115\n0.5469680\n0.5998890\n0.5813572\n0.6440090\n0.7214057\n{\u21131, \u21132, \u2113\u221e} (0.5,1)\n0.2813842\n0.4123716\n0.4183949\n0.3434854\n0.4501870\n0.4705890\n{\u21131, \u21132, \u2113\u221e}\u2217(0.5,1)\n0.2813842\n0.4123716\n0.4183949\n0.3434113\n0.4501763\n0.4705578\n{\u21131, \u21132, \u2113\u221e} (0.75,0.75)\n0.3250997\n0.4312186\n0.4512656\n0.3869566\n0.4747343\n0.5104562\n{\u21131, \u21132, \u2113\u221e}\u2217(0.75,0.75)\n0.3250997\n0.4312186\n0.4512656\n0.3869382\n0.4747230\n0.5104387\n{\u21131, \u21132, \u2113\u221e} (0.25,1)\n0.2791982\n0.4113275\n0.4166136\n0.3392525\n0.4482318\n0.4674070\n{\u21131, \u21132, \u2113\u221e}\u2217(0.25,1)\n0.2790174\n0.4111578\n0.4163974\n0.3386830\n0.4478506\n0.4667958\n{\u21131, \u21132, \u2113\u221e} (0.5,0.5)\n0.4170439\n0.5207218\n0.5436218\n0.4694103\n0.5507253\n0.5975351\n{\u21131, \u21132, \u2113\u221e}\u2217(0.5,0.5)\n0.3977208\n0.4819926\n0.5218939\n0.4590288\n0.5311414\n0.5863702\n{\u21131, \u21132, \u2113\u221e} (0.25,0.5)\n0.3726889\n0.4645454\n0.4983916\n0.4324230\n0.5082528\n0.5569916\n{\u21131, \u21132, \u2113\u221e}\u2217(0.25,0.5)\n0.3718829\n0.4639374\n0.497499\n0.4357035\n0.5115011\n0.5608083\n{\u21131, \u21132, \u2113\u221e} (0.25,0.25)\n1.9363225\n1.9646153\n2.3321286\n2.0016163\n2.0247679\n2.4181903\n{\u21131, \u21132, \u2113\u221e}\u2217(0.25,0.25)\n0.6365329\n0.6854681\n0.7851421\n0.6669880\n0.7127657\n0.8198798\nTable 4\nSimulation results for 100 replications. For each estimator we provide average bias (Bias), average root-mean squared\nerror (RMSE), and average prediction risk (PR).\nEssentially, the safeguard constraints help to avoid severe under\ufb01tting. They are very helpful when\nthe performance is below of what can be achieved. Nonetheless, we recommend to keep them in all\ncases as it does not impact negatively the estimator and the additional computational burden seems\nminimal.\nAppendix: Auxiliary lemmas\nIn what follows, we write for brevity \u03b4i = \u03b4i(\u03b5), \u03b4\u2032\ni = \u03b4\u2032\ni(\u03b5), and we set \u2206= \u02c6\u03b8 \u2212\u03b8\u2217, J = {j : \u03b8\u2217\nj \u0338= 0}.\nLemma 3. Assume (A1)-(A3) and (A5). Then with probability at least 1 \u22126\u03b5, the pair (\u03b8, t, u) =\n(\u03b8\u2217, |\u03b8\u2217|2, |\u03b8\u2217|\u221e) belongs to the feasible set of the minimization problem (18).\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n15\nProof. First, note that ZT (y \u2212Z\u03b8\u2217) + n bD\u03b8\u2217is equal to\n\u2212XTW\u03b8\u2217+ XT \u03be + W T \u03be \u2212(W T W \u2212Diag{W T W})\u03b8\u2217\n\u2212(Diag{W TW} \u2212nD)\u03b8\u2217+ n( bD \u2212D)\u03b8\u2217.\nBy de\ufb01nition of \u03b4i and b, with probability at least 1 \u22124\u03b5, we have\n| 1\nnXT\u03be|\u221e+ | 1\nnW T \u03be|\u221e\u2264\u03b42 + \u03b43\n(27)\n|( 1\nnDiag{W T W} \u2212D)\u03b8\u2217|\u221e\u2264| 1\nnDiag{W TW} \u2212D|\u221e|\u03b8\u2217|\u221e\u2264\u03b45|\u03b8\u2217|\u221e\n(28)\n|( bD \u2212D)\u03b8\u2217|\u221e\u2264b(\u03b5)|\u03b8\u2217|\u221e,\n(29)\nwhere in (28) and (29) we have used that the considered matrices are diagonal. Also, by Lemma 2,\nwith probability at least 1 \u22122\u03b5, we have\n| 1\nnXTW\u03b8\u2217|\u221e\u2264\u03b4\u2032\n1|\u03b8\u2217|2\n(30)\n| 1\nn(W T W \u2212Diag{W T W})\u03b8\u2217|\u221e\u2264\u03b4\u2032\n4|\u03b8\u2217|2.\n(31)\nCombining the decomposition of ZT (y \u2212Z\u03b8\u2217) + n bD\u03b8\u2217together with (27)-(31), we \ufb01nd that\n\f\f 1\nnZT (y \u2212Z\u03b8\u2217) + bD\u03b8\u2217\f\f\n\u221e\u2264\u00b5|\u03b8\u2217|2 + b|\u03b8\u2217|\u221e+ \u03c4,\nwith probability at least 1 \u22126\u03b5, which implies the lemma. \u25a1\nLemma 4. Let \u02c6\u03b8 be the {\u21131, \u21132, \u2113\u221e}-compensated MU-selector. Assume (A1)-(A3) and (A5). Then\nwith probability at least 1 \u22126\u03b5 (on the same event as in Lemma 3), we have\n|(\u02c6\u03b8 \u2212\u03b8\u2217)Jc|1 \u2264(1 + \u03bb + \u03bd)|(\u02c6\u03b8 \u2212\u03b8\u2217)J|1,\n(32)\n\u02c6t \u2212|\u03b8\u2217|2 \u2264{(1 + \u03bd)/\u03bb}|\u02c6\u03b8 \u2212\u03b8\u2217|1 and \u02c6u \u2212|\u03b8\u2217|\u221e\u2264{(1 + \u03bb)/\u03bd}|\u02c6\u03b8 \u2212\u03b8\u2217|1.\n(33)\nProof. Set \u2206= \u02c6\u03b8 \u2212\u03b8\u2217. On the event of Lemma 3, (\u03b8\u2217, |\u03b8\u2217|2, |\u03b8\u2217|\u221e) belongs to the feasible set of the\nminimization problem (5). Consequently,\n|\u02c6\u03b8|1 + \u03bb|\u02c6\u03b8|2 + \u03bd|\u02c6\u03b8|\u221e\u2264|\u02c6\u03b8|1 + \u03bb\u02c6t + \u03bd\u02c6u \u2264|\u03b8\u2217|1 + \u03bb|\u03b8\u2217|2 + \u03bd|\u03b8\u2217|\u221e.\n(34)\nThis implies\n|\u2206Jc|1 + \u03bb|\u2206Jc|2 + \u03bd|\u2206Jc|\u221e\u2264|\u2206J|1 + \u03bb|\u2206J|2 + \u03bd|\u2206J|\u221e\u2264(1 + \u03bb + \u03bd)|\u2206J|1,\nand so\n|\u2206Jc|1 \u2264(1 + \u03bb + \u03bd)|\u2206J|1.\nand (32) follows. To prove (33), it su\ufb03ces to note that (34) implies\n\u03bb\u02c6t \u2264|\u03b8\u2217|1 \u2212|\u02c6\u03b8|1 + \u03bb|\u03b8\u2217|2 + \u03bd|\u03b8\u2217|\u221e\u2212\u03bd\u02c6u \u2264|\u02c6\u03b8 \u2212\u03b8\u2217|1 + \u03bb|\u03b8\u2217|2 + \u03bd|\u02c6\u03b8 \u2212\u03b8\u2217|\u221e\nand the result follows since |\u02c6\u03b8|\u221e\u2264\u02c6u and |\u02c6\u03b8 \u2212\u03b8\u2217|\u221e\u2264|\u02c6\u03b8 \u2212\u03b8\u2217|1. Similar calculations yield the bound\nfor \u02c6u. \u25a1\nLemma 5. Let \u02c6\u03b8 be the {\u21131, \u21132, \u2113\u221e}-compensated MU-selector. Assume (A1)-(A3) and (A5). Then,\non a subset of the event of Lemma 3 having probability at least 1 \u22128\u03b5, we have\n\f\f 1\nnXT X(\u02c6\u03b8 \u2212\u03b8\u2217)\n\f\f\n\u221e\u2264\u00b50 + \u00b51|\u02c6\u03b8 \u2212\u03b8\u2217|1 + \u00b52|\u03b8\u2217|2 + \u00b5\u221e|\u03b8\u2217|\u221e,\nwhere \u00b50 = \u03c4 + \u03b42 + \u03b43, \u00b51 = 2\u03b41 + \u03b44 + \u03b45 + b(\u03b5) + {(1 + \u03bd)/\u03bb}\u00b5 + {(1 + \u03bb)/\u03bd}\u03b2, \u00b52 = \u00b5 + \u03b4\u2032\n1,\n\u00b5\u221e= \u03b2 + b(\u03b5) + \u03b4\u2032\n4 + \u03b45.\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n16\nNote that \u00b50 and \u00b52 are of order\nq\n1\nn log(c\u2032p/\u03b5), and \u00b51 and \u00b5\u221eare of order\nq\n1\nn log(c\u2032p/\u03b5) + b(\u03b5).\nProof. Throughout the proof, we assume that we are on the event of probability at least 1 \u22126\u03b5\nwhere inequalities (27) \u2013 (31) hold and (\u03b8\u2217, |\u03b8\u2217|2, |\u03b8\u2217|\u221e) belongs to the feasible set of the minimization\nproblem (18). We have\n| 1\nnXTX\u2206|\u221e\u2264| 1\nnZT (Z \u02c6\u03b8 \u2212y) \u2212bD\u02c6\u03b8|\u221e+ |( 1\nnZT W \u2212D)\u02c6\u03b8|\u221e\n+|( bD \u2212D)\u02c6\u03b8|\u221e+ | 1\nnZT \u03be|\u221e+ | 1\nnW T X\u2206|\u221e.\nUsing the fact that (\u02c6\u03b8, \u02c6t, \u02c6u) belongs to the feasible set of the minimization problem (5) together with\n(33), we obtain\n| 1\nnZT (Z \u02c6\u03b8 \u2212y) \u2212bD\u02c6\u03b8|\u221e\u2264\u00b5\u02c6t + \u03b2\u02c6u + \u03c4\n\u2264{(1 + \u03bd)/\u03bb}\u00b5|\u2206|1 + \u00b5|\u03b8\u2217|2 + {(1 + \u03bb)/\u03bd}\u03b2|\u2206|1 + \u03b2|\u03b8\u2217|\u221e+ \u03c4.\nUsing that \u02c6\u03b8 = \u03b8\u2217+ \u2206, Assumption (A5) together with (29) yields that\n| 1\nnXTX\u2206|\u221e\u2264{(1 + \u03bd)/\u03bb}\u00b5|\u2206|1 + \u00b5|\u03b8\u2217|2 + {(1 + \u03bb)/\u03bd}\u03b2|\u2206|1 + \u03b2|\u03b8\u2217|\u221e+ \u03c4\n+ |( 1\nnZT W \u2212D)\u02c6\u03b8|\u221e+ |( bD \u2212D)\u02c6\u03b8|\u221e+ | 1\nnZT \u03be|\u221e+ | 1\nnW T X\u2206|\u221e\n\u2264{(1 + \u03bd)/\u03bb}\u00b5|\u2206|1 + \u00b5|\u03b8\u2217|2 + {(1 + \u03bb)/\u03bd}\u03b2|\u2206|1 + \u03b2|\u03b8\u2217|\u221e+ \u03c4\n+ |( 1\nnZT W \u2212D)\u02c6\u03b8|\u221e+ b(\u03b5)|\u03b8\u2217|\u221e+ b(\u03b5)|\u2206|1 + \u03b42 + \u03b43 + | 1\nnW T X\u2206|\u221e.\nNow remark that |( 1\nnZT W \u2212D)\u02c6\u03b8|\u221e\u2264|( 1\nnZTW \u2212D)\u2206|\u221e+ |( 1\nnZT W \u2212D)\u03b8\u2217|\u221e. In view of Lemma 2\nand (28), on the initial event of probability at least 1 \u22126\u03b5,\n|( 1\nnZT W \u2212D)\u03b8\u2217|\u221e\n\u2264| 1\nn(W T W \u2212Diag{W T W})\u03b8\u2217|\u221e+ |( 1\nnDiag{W TW} \u2212D)\u03b8\u2217|\u221e+ | 1\nnXTW\u03b8\u2217|\u221e\n\u2264(\u03b4\u2032\n4 + \u03b45)|\u03b8\u2217|\u221e+ \u03b4\u2032\n1|\u03b8\u2217|2.\n(35)\nMoreover, we have\n|( 1\nnZT W \u2212D)\u2206|\u221e\u2264| 1\nnZT W \u2212D|\u221e|\u2206|1\n\u2264\n\u0000| 1\nn(W T W \u2212Diag{W T W})|\u221e+ | 1\nnDiag{W T W} \u2212D|\u221e+ | 1\nnXT W|\u221e\n\u0001\n|\u2206|1.\nTherefore,\n|( 1\nnZT W \u2212D)\u2206|\u221e\u2264(\u03b41 + \u03b44 + \u03b45)|\u2206|1,\n(36)\nwith probability at least 1 \u22128\u03b5 (since we intersect the initial event of probability at least 1 \u22126\u03b5 with\nthe event of probability at least 1 \u22122\u03b5 where the bounds \u03b41 and \u03b44 hold for the corresponding terms).\nNext, on the same event of probability at least 1 \u22128\u03b5,\n| 1\nnW T X\u2206|\u221e\u2264| 1\nnXT W|\u221e|\u2206|1 \u2264\u03b41|\u2206|1.\n(37)\nTo complete the proof, it su\ufb03ces to plug (35) \u2013 (37) in the last inequality for | 1\nnXT X\u2206|\u221eand to\nobtain\n| 1\nnXTX\u2206|\u221e\u2264[2\u03b41 + \u03b44 + \u03b45 + b(\u03b5) + {(1 + \u03bd)/\u03bb}\u00b5 + {(1 + \u03bb)/\u03bd}\u03b2]|\u2206|1\n+ {\u00b5 + \u03b4\u2032\n1}|\u03b8\u2217|2 + {\u03b2 + b(\u03b5) + \u03b4\u2032\n4 + \u03b45}|\u03b8\u2217|\u221e+ \u03c4 + \u03b42 + \u03b43.\n\u25a1\nBelloni, Rosenbaum and Tsybakov/{\u21131, \u21132, \u2113\u221e}-Regularization to EIV Models\n17\nAcknowledgement\nThe work of the third author is supported by GENES, and by the French National Research Agency\n(ANR) as part of Idex Grant ANR -11- IDEX-0003-02, Labex ECODEC (ANR - 11-LABEX-0047),\nand IPANEMA grant (ANR-13-BSH1-0004-02).\nReferences\n[1] A. Belloni, M. Rosenbaum and A. B. Tsybakov (2014) Linear and conic programming estimators\nin high-dimensional errors-in-variables models. arxiv:1408.0241\n[2] Y. Chen and C. Caramanis (2012) Orthogonal matching pursuit with noisy and missing data:\nLow and high-dimensional results. arxiv:1206.0823\n[3] Y. Chen and C. Caramanis (2013) Noisy and missing data regression: Distribution-oblivious\nsupport recovery. Proc. of International Conference on Machine Learning (ICML).\n[4] E. Gautier and A. B. Tsybakov (2011) High-dimensional instrumental variables regression and\ncon\ufb01dence sets. arxiv:1105.2454\n[5] E. Gautier and A. B. Tsybakov (2013) Pivotal estimation in high-dimensional regression via\nlinear programming. In: Empirical Inference \u2013 A Festschrift in Honor of Vladimir N. Vapnik,\nB.Sch\u00a8olkopf, Z. Luo, V. Vovk eds., 195 - 204. Springer, New York e.a.\n[6] P-L. Loh and M. J. Wainwright (2012) High-dimensional regression with noisy and missing data:\nProvable guarantees with non-convexity. Annals of Statistics, 40, 1637\u20131664.\n[7] M. Rosenbaum and A.B. Tsybakov (2010) Sparse recovery under matrix uncertainty. Annals of\nStatistics, 38, 2620\u20132651.\n[8] M. Rosenbaum and A.B. Tsybakov (2013) Improved matrix uncertainty selector. In: From Proba-\nbility to Statistics and Back: High-Dimensional Models and Processes \u2013 A Festschrift in Honor of\nJon A. Wellner, M.Banerjee et al. eds. IMS Collections, vol.9, 276\u2013290, Institute of Mathematical\nStatistics.\n[9] \u00d8. S\u00f8rensen, A. Frigessi and M. Thoresen (2012) Measurement error in Lasso: Impact and likeli-\nhood bias correction. arxiv:1210.5378\n[10] \u00d8. S\u00f8rensen, A. Frigessi and M. Thoresen (2014) Covariate selection in high-dimensional gener-\nalized linear models with measurement error. arxiv:1407.1070\n"}