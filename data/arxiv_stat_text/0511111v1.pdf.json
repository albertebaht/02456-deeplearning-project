{"text": "arXiv:math/0511111v1  [math.ST]  4 Nov 2005\nNONPARAMETRIC ESTIMATION OF THE REGRESSION FUNCTION IN\nAN ERRORS-IN-VARIABLES MODEL\nF. COMTE\u2217,1 AND M.-L. TAUPIN2\nAbstract. We consider the regression model with errors-in-variables where we observe n i.i.d.\ncopies of (Y, Z) satisfying Y = f(X) + \u03be, Z = X + \u03c3\u03b5, involving independent and unobserved\nrandom variables X, \u03be, \u03b5. The density g of X is unknown, whereas the density of \u03c3\u03b5 is completely\nknown. Using the observations (Yi, Zi), i = 1, \u00b7 \u00b7 \u00b7 , n, we propose an estimator of the regression\nfunction f, built as the ratio of two penalized minimum contrast estimators of \u2113= fg and g,\nwithout any prior knowledge on their smoothness. We prove that its L2-risk on a compact set\nis bounded by the sum of the two L2(R)-risks of the estimators of \u2113and g, and give the rate\nof convergence of such estimators for various smoothness classes for \u2113and g, when the errors \u03b5\nare either ordinary smooth or super smooth. The resulting rate is optimal in a minimax sense\nin all cases where lower bounds are available.\nOctober 2004. Revised September 2005.\nMSC 2000 Subject Classi\ufb01cation. Primary 62G08, 62G07. Secondary 62G05, 62G20.\nKeywords and phrases. Adaptive estimation. Errors-in-variables. Density deconvolution.\nMinimax estimation. Nonparametric regression. Projection estimators.\n1. Introduction\nWe consider that we observe n independent and identically distributed (i.i.d.) copies of (Y, Z)\nsatisfying the following errors-in-variables regression model\n(\nY = f(X) + \u03be\nZ = X + \u03c3\u03b5,\n(1.1)\ninvolving independent and unobserved, random variables X, \u03be, \u03b5 and an unknown regression\nfunction f. The unobserved Xi\u2019s, have common unknown density denoted by g. The errors\n\u03b5i\u2019s have common known density f\u03b5, and \u03c3 is the known noise level. We assume moreover\nthat all random variables have \ufb01nite variance. Our aim is to estimate the regression function\nf on a compact set denoted by A, by using the observations (Yi, Zi) for i = 1, . . . , n, without\nany prior knowledge, neither on the smoothness of f nor on the smoothness of the density g.\n1 Universit\u00b4e Paris V, MAP5, UMR CNRS 8145, 45 rue des Saints-P`eres, 75 270 PARIS cedex 06, France.\nemail: fabienne.comte@univ-paris5.fr.\n2 IUT de Paris V et Universit\u00b4e d\u2019Orsay, Laboratoire de Probabilit\u00b4es, Statistique et Mod\u00b4elisation, UMR 8628,\nB\u02c6atiment 425,91405 Orsay Cedex, France.- email: Marie-Luce.Taupin@math.u-psud.fr.\n1\n2\nF. COMTE AND M.-L. TAUPIN\nIn nonparametric errors-in-variables regression models, two factors determine the estimation\naccuracy of the regression function: \ufb01rst, the smoothness of the function f to be estimated,\nand second the smoothness of the errors density f\u03b5. As in the deconvolution framework, the\nworst rates of convergence are obtained for the smoother errors density f\u03b5. In this context,\ntwo classes of errors are considered: \ufb01rst the so called ordinary smooth errors with polynomial\ndecay of their Fourier transform and second, the super smooth errors with Fourier transform\nhaving an exponential decay.\nMany papers deal with parametric or semi-parametric estimation in errors in variables mod-\nels, but we only mention here previous known results in the general nonparametric case. In this\ncontext most of the proposed estimators are some Nadaraya-Watson kernel type estimators,\nconstructed as the ratio of two deconvolution kernel type estimators, see e.g. Fan et al. (1991),\nFan and Masry (1992), Fan and Truong (1993), Masry (1993), Truong (1991), Ioannides and\nAlevizos (1997). One assumption usually done in all those works, is that the regularity of the\nregression function f and the regularity of the density g of the design are equal. In partic-\nular, when the regression function f and the density g admit kth-order derivatives, Fan and\nTruong (1993) give upper and lower bounds of the minimax risk for quadratic pointwise risk\nand for Lp-risk on compact sets for ordinary and super smooth errors \u03b5.\nIn a slightly di\ufb00erent way, Koo and Lee (1998) propose an estimation method based on B-\nspline, when the errors are ordinary smooth. This method also relates to estimation of the\nregression function as a ratio of two estimators.\nTo our knowledge, all previous papers consider that the regression function and the density\ng belong to the same smoothness class and that this common class is known.\nWe propose here an estimation procedure of f, that does not require any prior knowledge\non the regularity of the unknown functions f and g. Our estimation procedure is based on the\nclassical idea that the regression function f at point x can be written as the ratio\nf(x) = E(Y |X = x) =\nR\nyfX,Y (x, y)dy\ng(x)\n= (fg)(x)\ng(x) ,\nwith fX,Y the joint density of (X, Y ). Hence f is estimated by a ratio of an adaptive estimator\n\u02dc\u2113of \u2113= fg and of an adaptive estimator \u02dcg of g, both of them being built by minimization\nof penalized contrast functions. The contrasts are determined by projection methods and the\npenalizations give an automatic choice of the relevant projection spaces.\nWe give upper bounds on the L2-risk on a compact set for the regression function f as well\nas for the L2(R)-risk of the density g when the errors are either ordinary or super smooth.\nWe show in particular that the L2-risk on a compact set of our estimator \u02dcf of f is bounded\nby the sum of the risks of \u02dc\u2113and \u02dcg. The rate of convergence of \u02dcf is thus given by the slower\nrate between the rate of the adaptive estimation of g and the rate of the adaptive estimation\nof \u2113= fg. The resulting estimator automatically reaches the minimax rates in standard cases\nNONPARAMETRIC ESTIMATION IN AN ERROR-IN-VARIABLES MODEL\n3\nwhere lower bounds are available. The other cases are intensively discussed. In other words, our\nprocedure provides an adaptive estimator, in the sense that its construction does not require\nany prior knowledge on the smoothness of f nor g, which seems often optimal.\nThe paper is organized as follows. In Section 2, we describe the estimators. Section 3 is\ndevoted to the presentation of the upper bounds for the resulting L2-risks with some discussions\nabout the optimality in the minimax sense of the estimators. All proofs and technical lemmas\nare gathered in Section 4.\n2. Description of the estimators\nFor u and v in L2(R), u\u2217is the Fourier transform of u with u\u2217(x) =\nR\neitxu(t)dt, u \u2217v\nis the convolution product, u \u2217v(x) =\nR\nu(y)v(x \u2212y)dy, and < u, v >=\nR\nu(x)v(x)dx with\nzz = |z|2. The quantities \u2225u\u22251, \u2225u\u22252, \u2225u\u2225\u221eand \u2225u\u2225\u221e,K denote \u2225u\u22251 =\nR\n|u(x)|dx, \u2225u\u22252\n2 =\nR\n|u(x)|2dx, \u2225u\u2225\u221e= supx\u2208R |u(x)|, \u2225u\u2225\u221e,K = supx\u2208K |u(x)|.\nSubsequently we assume that f\u03b5 \u2208L2(R), f \u2217\n\u03b5 \u2208L2(R) with f \u2217\n\u03b5 (x) \u0338= 0 for all x \u2208R.\n2.1. Projection spaces. Consider \u03d5(x) = sin(\u03c0x)/(\u03c0x), and \u03d5m,j(x) = \u221aDm\u03d5(Dmx \u2212j).\nHere, we take Dm = m and m \u2208Mn = {1, \u00b7 \u00b7 \u00b7 , mn}, but when Dm = 2m, the basis {\u03d5m,j}j\u2208Z\nis known as the Shannon basis. It is well known (see for instance Meyer (1990), p.22), that\n{\u03d5m,j}j\u2208Z is an orthonormal basis of the space Sm of square integrable functions having a\nFourier transform with compact support contained in [\u2212\u03c0Dm, \u03c0Dm] , that is\nSm = Vect{\u03d5m,j, j \u2208Z} = {f \u2208L2(R), with supp(f \u2217) contained in [\u2212\u03c0Dm, \u03c0Dm]}.\nSince the orthogonal projection of g and \u2113on Sm, gm and \u2113m, gm = P\nj\u2208Z am,j(g)\u03d5m,j and\n\u2113m = P\nj\u2208Z am,j(\u2113)\u03d5m,j with am,j(g) =< \u03d5m,j, g >, and am,j(\u2113) =< \u03d5m,j, \u2113>, involve in\ufb01nite\nsums, we consider in practice, the truncated spaces S(n)\nm de\ufb01ned as\nS(n)\nm = Vect {\u03d5m,j, |j| \u2264kn}\nwhere kn is an integer to be chosen later. The family {\u03d5m,j}|j|\u2264kn is an orthonormal basis of\nS(n)\nm , and the orthogonal projection of g and \u2113on S(n)\nm\ndenoted by g(n)\nm\nand \u2113(n)\nm , are given by\ng(n)\nm = P\n|j|\u2264kn am,j(g)\u03d5m,j and \u2113(n)\nm = P\n|j|\u2264kn am,j(\u2113)\u03d5m,j.\n2.2. Construction of the minimum contrast estimators. For r \u2208R and d > 0, we denote\nby r(d) = sign(r) min(|r|, d), and thus de\ufb01ne the trimmed estimator of f by\n\u02c6f \u02d8m\u2113, \u02d8mg = (\u02c6\u2113\u02d8m\u2113/\u02c6g \u02d8mg)(an),\n(2.1)\nwith an being suitably chosen, \u02d8m\u2113and \u02d8mg minimizing the L2(R) risks of \u02c6\u2113\u02d8m\u2113the projection\nestimator on a space S(n)\n\u02d8m\u2113, and of \u02c6g \u02d8mg the projection estimator on a space S(n)\n\u02d8mg, de\ufb01ned as\nfollows.\n4\nF. COMTE AND M.-L. TAUPIN\nThe estimator of \u2113, is de\ufb01ned by\n(2.2)\n\u02c6\u2113m = arg min\nt\u2208S(n)\nm\n\u03b3n,\u2113(t),\nwith \u03b3n,\u2113de\ufb01ned, for t \u2208S(n)\nm , by\n(2.3)\n\u03b3n,\u2113(t) = \u2225t\u22252 \u22122n\u22121\nn\nX\ni=1\n(Yiu\u2217\nt(Zi)) with ut(x) = (2\u03c0)\u22121t\u2217(\u2212x)/f \u2217\n\u03b5 (\u2212x),\nthat is \u02c6\u2113m = P\n|j|\u2264kn \u02c6am,j(\u2113)\u03d5m,j with \u02c6am,j(\u2113) = n\u22121 Pn\ni=1 Yiu\u2217\n\u03d5m,j(Zi).\nBy using Parseval and inverse Fourier formulas, we get that\nE(Y1u\u2217\nt(Z1)) = E(f(X1)u\u2217\nt(Z1)) = \u27e8u\u2217\nt \u2217f\u03b5, fg\u27e9= 1\n2\u03c0\u27e8f \u2217\n\u03b5 t\u2217/f \u2217\n\u03b5 , (fg)\u2217\u27e9= 1\n2\u03c0\u27e8t\u2217, (fg)\u2217\u27e9= \u27e8t, \u2113\u27e9.\nTherefore, we \ufb01nd that E(\u03b3n,\u2113(t)) = \u2225t\u22252\n2 \u22122\u27e8\u2113, t\u27e9= \u2225t \u2212\u2113\u22252\n2 \u2212\u2225\u2113\u22252\n2 which is minimal when\nt = \u2113. This shows that \u03b3n,\u2113(t) suits well for the estimation of \u2113= fg.\nBy using the estimation procedure described in Comte et al. (2005a), the estimator of g on\nS(n)\nm is de\ufb01ned by \u02c6gm = P\n|j|\u2264kn \u02c6am,j(g)\u03d5m,j with \u02c6am,j(g) = n\u22121 Pn\ni=1 u\u2217\n\u03d5m,j(Zi), that is\n\u02c6gm = arg min\nt\u2208S(n)\nm\n\u03b3n,g(t)\n(2.4)\nwith \u03b3n,g de\ufb01ned, for t \u2208S(n)\nm by \u03b3n,g(t) = \u2225t\u22252\n2 \u22122n\u22121 Pn\ni=1 u\u2217\nt(Zi), with ut de\ufb01ned in (2.3).\nRemark 2.1. The use of r(d) avoids the problems that may occur when \u02c6gm2 takes small values.\n2.3. Construction of the minimum penalized contrast estimators. In order to construct\nthe minimum penalized contrast estimators, and especially to de\ufb01ne the penalty functions, we\nneed to precise the behavior of f \u2217\n\u03b5 , described as follows. We assume that, for all x in R,\n\u03ba0(x2 + 1)\u2212\u03b1/2 exp{\u2212\u03b2|x|\u03c1} \u2264|f \u2217\n\u03b5 (x)| \u2264\u03ba\u2032\n0(x2 + 1)\u2212\u03b1/2 exp{\u2212\u03b2|x|\u03c1}.\n(A1)\nOnly the left-hand side of (A1) is required to de\ufb01ne the penalty function and for upper bounds.\nThe right-hand side is needed when we consider lower bounds and the question of optimality\nin a minimax sense. When \u03c1 = 0, \u03b1 has to be such that \u03b1 > 1/2 . When \u03c1 = 0 in (A1), the\nerrors are usually called \u201cordinary smooth\u201d errors, and \u201csuper smooth\u201d errors when \u03c1 > 0. The\nstandard examples are the following: Gaussian or Cauchy distributions are super smooth of\norder (\u03b1 = 0, \u03c1 = 2) and (\u03b1 = 0, \u03c1 = 1) respectively, and the double exponential distribution\nis ordinary smooth (\u03c1 = 0) of order \u03b1 = 2.\nBy convention, we set \u03b2 = 0 when \u03c1 = 0 and we assume that \u03b2 > 0 when \u03c1 > 0. In the same\nway, if \u03c3 = 0, the Xi\u2019s are directly observed without noise and we set \u03b2 = \u03b1 = \u03c1 = 0.\nUnder the assumption (A1), the regression function f is estimated by \u02dcf de\ufb01ned as\n(2.5)\n\u02dcf = (\u02dc\u2113/\u02dcg)(an),\nNONPARAMETRIC ESTIMATION IN AN ERROR-IN-VARIABLES MODEL\n5\nwhere \u02dc\u2113is the adaptive estimator de\ufb01ned by\n(2.6)\n\u02dc\u2113= \u02c6\u2113\u02c6m\u2113with \u02c6m\u2113= arg min\nm\u2208Mn,\u2113\nh\n\u03b3n,\u2113(\u02c6\u2113m) + pen\u2113(m)\ni\n,\n\u02dcg is the adaptive estimator de\ufb01ned as in Comte et al. (2005a), by\n(2.7)\n\u02dcg = \u02c6g \u02c6mg with \u02c6mg = arg\nmin\nm\u2208Mn,g\n\u0002\n\u03b3n,g(\u02c6gm) + peng(m)\n\u0003\n,\nwhere Mn,\u2113and Mn,g are some restrictions of Mn given below, and where pen\u2113and peng are\ndata driven penalty functions given by\npen\u2113(m) = \u03ba\u2032(\u03bb1 + \u00b52)[1 + \u02c6m2(Y )]\u02dc\u0393(m)/n,\npeng(m) = \u03ba(\u03bb1 + \u00b51)\u02dc\u0393(m)/n,\n(2.8)\nwith\n(2.9)\n\u02c6m2(Y ) = 1\nn\nn\nX\ni=1\nY 2\ni ,\nand\n\u02dc\u0393(m) = D2\u03b1+max(1\u2212\u03c1,min((1+\u03c1)/2,1))\nm\nexp{2\u03b2\u03c3\u03c1(\u03c0Dm)\u03c1}.\nThe constants \u03bb1, \u00b51 and \u00b52 are some known constants, only depending on f\u03b5 and \u03c3 (assumed\nto be known), to be de\ufb01ned later (see (3.4), (3.8) and (3.9)), and \u03ba and \u03ba\u2032 are some numerical\nconstants.\nRemark 2.2. First note that the penalty functions in (2.8) have the same form with di\ufb00erent\nconstants. More precisely, in both cases, the penalties are of order D2\u03b1+1\u2212\u03c1\nm\nexp(2\u03b2\u03c3\u03c1(\u03c0Dm)\u03c1) if\n0 \u2264\u03c1 \u22641/3, D2\u03b1+(1+\u03c1)/2\nm\nexp(2\u03b2\u03c3\u03c1(\u03c0Dm)\u03c1) if 1/3 \u2264\u03c1 \u22641 and of order D2\u03b1+1\nm\nexp(2\u03b2\u03c3\u03c1(\u03c0Dm)\u03c1)\nif \u03c1 \u22651.\nSecond, the constants involve \u03ba and \u03ba\u2032, universal numerical constants, as well as constants\n\u03bb1, \u00b51, \u00b52 related to the known errors density f\u03b5. Any constant greater than any well chosen\nconstant also suits for theoretical results. In practice, such constants are usually calibrated by\nsome intensive simulation studies. We refer to Comte et al. (2005a, 2005b) for further details\non penalty calibration as well as for details on the implementation of such estimators in density\ndeconvolution problems.\n3. Rates of convergence and adaptivity\n3.1. Assumptions. We consider Model (1.1) under (A1) and the following additional assump-\ntions.\n\u2113\u2208L2(R) and \u2113\u2208L =\n\u001a\n\u03c6 such that\nZ\nx2\u03c62(x)dx \u2264\u03baL < \u221e\n\u001b\n,\n(A2)\nf \u2208FG = {\u03c6 such that sup\nx\u2208G\n|\u03c6(x)| \u2264\u03ba\u221e,G < \u221e}, where G is the support of g.\n(A3)\ng \u2208L2(R) and g \u2208G = {\u03c6, density, such that\nZ\nx2\u03c62(x)dx < \u03baG < \u221e}.\n(A4)\nThere exist g0, g1 positive constants such that for all x \u2208A, g0 \u2264g(x) \u2264g1.\n(A5)\n6\nF. COMTE AND M.-L. TAUPIN\nNote that we do not assume that g is compactly supported but only that f is bounded on\nthe support of g. It follows that if g is compactly supported then f has to be bounded on a\ncompact set. But if g has R as support then the regression function has to be bounded on R.\nWe estimate f only on a compact set denoted by A. Hence, the assumption (A5) implies that\nA \u2282G and therefore under (A3) and (A5), f is bounded on A. The assumptions (A3) and\n(A4) imply that (A2) holds, with \u03baL = \u03ba2\n\u221e,G\u03baG.\nClassically, the slowest rate of convergence for estimating f and g are obtained for super\nsmooth errors density. In particular, when f\u03b5 is the Gaussian density the minimax rate of\nconvergence obtained by Fan and Truong (1993) when f and g have the same H\u00a8olderian type\nregularity is of order a power of ln(n). Nevertheless, those rates can be improved by some\nadditional regularity conditions on f and g described as follows.\nSa,r,B(C1) = {\u03c8 \u2208L2(R) :\nsuch that\nZ +\u221e\n\u2212\u221e\n|\u03c8\u2217(x)|2(x2 + 1)a exp{2B|x|r}dx \u2264C1},\n(R1)\nfor a, r, B, C1 some nonnegative real numbers. The smoothness class in (R1) is classically con-\nsidered in nonparametric estimation, especially in deconvolution. When r = 0, this corresponds\nto Sobolev spaces of order a. The densities belonging to Sa,r,B(C1) with r > 0, B > 0 are in\ufb01n-\nitely many times di\ufb00erentiable, admit analytic continuation on a \ufb01nite width strip when r = 1\nand on the whole complex plane if r = 2.\n3.2. Risks bounds for the minimum contrast estimators. We start by presenting some\ngeneral bound for the risk.\nProposition 3.1. Consider the estimators \u02c6\u2113Dm = \u02c6\u2113m and \u02c6gDm = \u02c6gm of \u2113and g de\ufb01ned by (2.2)\nand (2.4). Let \u2206(m) = Dm\u03c0\u22121 R \u03c0Dm\n0\n|f \u2217\n\u03b5 (Dmx\u03c3)|\u22122dx. Then, under (A2) and (A4),\n(3.1)\nE(\u2225\u2113\u2212\u02c6\u2113m\u22252\n2) \u2264\u2225\u2113\u2212\u2113m\u22252\n2 + 2E(Y 2\n1 )\u2206(m)/n + (\u03baL+ \u2225\u2113\u22251)D2\nm/kn\nand\n(3.2)\nE(\u2225g \u2212\u02c6gm\u22252\n2) \u2264\u2225g \u2212gm\u22252\n2 + 2\u2206(m)/n + (\u03baG + 1)D2\nm/kn.\nAs in deconvolution problems, the variance term \u2206(m)/n depends on the rate of decay of\nthe Fourier transform f \u2217\n\u03b5 , with larger variance for fast decreasing f \u2217\n\u03b5 . Under (A1), the variance\nterm is bounded in the following way\n\u2206(m) \u2264\u03bb1\u0393(m)\nwhere\n\u0393(m) = D2\u03b1+1\u2212\u03c1\nm\nexp(2\u03b2\u03c3\u03c1(\u03c0Dm)\u03c1),\n(3.3)\nwith\n\u03bb1 = (\u03c32\u03c02 + 1)\u03b1/(\u03c0\u03c1\u03ba2\n0R(\u03b2, \u03c3, \u03c1)) with R(\u03b2, \u03c3, \u03c1) = 1I\u03c1=0 + 2\u03b2\u03c1\u03c3\u03c11I0<\u03c1\u22641 + 2\u03b2\u03c3\u03c11I\u03c1>1,\n(3.4)\nNONPARAMETRIC ESTIMATION IN AN ERROR-IN-VARIABLES MODEL\n7\nIn order to ensure that \u0393(m)/n is bounded, we only consider models such that \u03c0Dm = m \u2264mn\nin Mn = {1, \u00b7 \u00b7 \u00b7 , mn} with\nmn \u2264\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n\u03c0\u22121n1/(2\u03b1+1)\nif \u03c1 = 0\n\u03c0\u22121\n\u0014ln(n)\n2\u03b2\u03c3\u03c1 + 2\u03b1 + 1 \u2212\u03c1\n2\u03c1\u03b2\u03c3\u03c1\nln\n\u0012ln(n)\n2\u03b2\u03c3\u03c1\n\u0013\u00151/\u03c1\nif \u03c1 > 0.\n(3.5)\nLastly, the bias terms \u2225\u2113\u2212\u2113m\u22252\n2 and \u2225g \u2212gm\u22252\n2 depend, as usual, on the smoothness of the\nfunctions \u2113and g. They have the expected order for classical smoothness classes since they\nrelate to the distance between g and the classes of entire functions having Fourier transform\ncompactly supported on [\u2212\u03c0Dm, \u03c0Dm] (see Ibragimov and Hasminskii (1983)).\nSince \u2113m and gm are the orthogonal projections of \u2113and g on Sm, when \u2113belongs Sa\u2113,r\u2113,B\u2113(\u03baa\u2113)\nand g belongs Sag,rg,Bg(\u03baag) de\ufb01ned by (R1), then\n(3.6)\n\u2225\u2113\u2212\u2113m\u22252\n2 = (2\u03c0)\u22121\nZ\n|x|\u2265\u03c0Dm\n|\u2113\u2217|2(x)dx \u2264[\u03baa\u2113/(2\u03c0)](D2\nm\u03c02 + 1)\u2212a\u2113exp{\u22122B\u2113\u03c0r\u2113Dr\u2113\nm},\nand the same holds for \u2225gm \u2212g \u22252\n2 with (a\u2113, B\u2113, r\u2113) replaced by (ag, Bg, rg).\nCorollary 3.1. Under (A1), (A2) and (A4), let \u0393(m) and \u03bb1 being de\ufb01ned in (3.3) and (3.4).\nAssume that kn \u2265n, that \u2113belongs to Sa\u2113,r\u2113,B\u2113(\u03baa\u2113) and that g belongs to Sag,rg,Bg(\u03baag) de\ufb01ned\nby (R1). Then\nE(\u2225\u2113\u2212\u02c6\u2113m\u22252\n2) \u2264\u03baa\u2113\n2\u03c0 (D2\nm\u03c02 + 1)\u2212a\u2113e\u22122B\u2113\u03c0r\u2113D\nr\u2113\nm + 2\u03bb1E(Y 2\n1 )\u0393(m)/n + D2\nm(\u03baL+ \u2225\u2113\u22251)/n,\nand\nE(\u2225g \u2212\u02c6gm\u22252\n2) \u2264\u03baag\n2\u03c0 (D2\nm\u03c02 + 1)\u2212age\u22122Bg\u03c0rg D\nrg\nm + 2\u03bb1\u0393(m)/n + (\u03baG + 1)D2\nm/n.\nRemark 3.1. We point out that the {\u03d5m,j} are R-supported (and not compactly supported)\nand hence, we obtain estimations of \u2113and g on the whole line and not only on a compact set\nas for usual projection estimators. This is a great advantage of this basis even if, due to the\ntruncation |j| \u2264kn, it induces the residual terms D2\nm(\u03baL+ \u2225\u2113\u22251)/kn and D2\nm(\u03baG + 1)/kn, in\nthe upper bounds of the risks. The most important thing is that the choice of kn does not\nin\ufb02uence the other terms. Consequently, we can \ufb01nd a relevant choice of kn (kn \u2265n under\n(A2) and (A4)), that makes those additional terms unconditionally negligible with respect to\nthe bias and variance terms. The condition kn \u2265n allows us to construct truncated spaces S(n)\nm\nusing O(n) basis vectors and hence to use a tractable and fast algorithm. The choice of larger\nkn, independent of \u2113and g, does not change the e\ufb03ciency of our estimator from a statistical\npoint of view but will only change the speed of the algorithm from a practical point of view.\n8\nF. COMTE AND M.-L. TAUPIN\nf\u03b5\n\u03c1 = 0\n\u03c1 > 0\nordinary smooth\nsuper smooth\ng\nr\u2113= 0\nSobolev(s)\n\u03c0D \u02d8m\u2113= O(n1/(2\u03b1+2a\u2113+1))\nrate = O(n\u22122a\u2113/(2\u03b1+2a\u2113+1))\n\u03c0D \u02d8m\u2113= [ln(n)/(2\u03b2\u03c3\u03c1 + 1)]1/\u03c1\nrate = O((ln(n))\u22122a\u2113/\u03c1)\nr\u2113> 0\nC\u221e\n\u03c0D \u02d8m\u2113= [ln(n)/2B\u2113]1/r\u2113\nrate = O\n \nln(n)(2\u03b1+1)/r\u2113\nn\n!\n\u03c0D \u02d8m\u2113implicit solution of\nD \u02d8m\u2113\n2\u03b1+2a\u2113+1\u2212r\u2113e2\u03b2\u03c3\u03c1(\u03c0D \u02d8\nm\u2113)\u03c1+2B(\u03c0D \u02d8\nm\u2113)r\u2113\n= O(n)\nTable 1. Best choices of D \u02d8m\u2113minimizing E(\u2225\u2113\u2212\u02c6\u2113m\u22252\n2) and resulting rates for \u02c6\u2113\u02d8m\u2113.\nFor the case r\u2113> 0 and \u03c1 > 0, the choice \u03c0D \u02d8m\u2113= [ln(n)/(2\u03b2\u03c3\u03c1 + 1)]1/\u03c1 leads to a rate which\nis faster than any power of ln(n) and slower than any power of n. For instance if r\u2113= \u03c1, the\nrate is of order [ln(n)]bn\u2212B\u2113/(B\u2113+\u03b2\u03c3\u03c1) with b = [\u22122a\u2113\u03b2\u03c3\u03c1 + (2\u03b1 \u2212r\u2113+ 1)B\u2113]/[r\u2113(\u03b2\u03c3\u03c1 + B\u2113)].\nThe same table holds for g, by replacing (a\u2113, B\u2113, r\u2113) by (ag, Bg, rg). For D \u02d8mg chosen in the\nsame way as D \u02d8m\u2113in Table 1, the rate of convergence of \u02c6g \u02d8mg is the minimax rate of convergence,\nas given in Fan (1991a) for rg = 0, in Butucea (2004) for rg > 0 and \u03c1 = 0 and in Butucea and\nTsybakov (2004) for 0 < rg < \u03c1 and ag = 0.\nThe rate of convergence of \u02c6f \u02d8m\u2113, \u02d8mg is given by the following proposition.\nProposition 3.2. Under (A1), (A2), (A3), (A4), and (A5), assume that g belongs to some\nspace Sag,rg,Bg(\u03baag) de\ufb01ned by (R1) with ag > 1/2 if rg = 0. Let \u02c6f \u02d8m\u2113, \u02d8mg be de\ufb01ned by (2.1), with\n\u02d8m\u2113and \u02d8mg such that D \u02d8m\u2113and D \u02d8mg minimize the risks E(\u2225\u2113\u2212\u02c6\u2113m\u22252\n2) and E(\u2225g\u2212\u02c6gm\u22252\n2) respectively.\nIf an = nk for k > 0, and kn \u2265n3/2, then, for n great enough and C0 = Kg\u22122\n0 (1 + g1g\u22122\n0 \u03ba\u221e,G),\n(3.7)\nE\u2225( \u02c6f \u02d8m\u2113, \u02d8mg \u2212f)1IA\u22252\n2 \u2264C0[E(\u2225\u2113\u2212\u02c6\u2113\u02d8m\u2113\u22252\n2) + E(\u2225g \u2212\u02c6g \u02d8mg\u22252\n2)] + o(n\u22121).\nIf ag \u22641/2 then we only have a result of type \u2225(f \u2212\u02c6f \u02d8m\u2113, \u02d8mg)1IA\u22252\n2 = Op(\u2225\u2113\u2212\u02c6\u2113\u02d8m\u2113\u22252\n2 +\u2225g \u2212\u02c6g \u02d8mg\u22252\n2).\nAlso note that the result holds when the constant \u03ba\u221e,G is replaced by \u2225f \u2225\u221e,A if f is bounded\non the compact set A.\nThe performance of \u02c6f \u02d8m\u2113, \u02d8mg is given by the worst performance between the one of \u02c6\u2113\u02d8m\u2113and the\none of \u02c6g \u02d8mg. Let us be more precise in some examples. Under the assumptions of Proposition\n3.2:\n\u2022 If the \u03b5i\u2019s are ordinary smooth,\nNONPARAMETRIC ESTIMATION IN AN ERROR-IN-VARIABLES MODEL\n9\n\u2013 If r\u2113= rg = 0 and \u03c0D \u02d8m\u2113= O\n\u0000n1/(2a\u2113+2\u03b1+1)\u0001\nand \u03c0D \u02d8mg = O\n\u0000n1/(2ag+2\u03b1+1)\u0001\n, then\nE(\u2225(f \u2212\u02c6f \u02d8m\u2113, \u02d8mg)1IA\u22252\n2) \u2264O(n\u22122a\u2217/(2a\u2217+2\u03b1+1))\nwith\na\u2217= inf(a\u2113, ag).\n\u2013 If r\u2113> 0, rg > 0, \u03c0D \u02d8m\u2113= (ln(n)/2B)1/r\u2113and \u03c0D \u02d8mg = (ln(n)/2B)1/rg, then\nE(\u2225(f \u2212\u02c6f \u02d8m\u2113, \u02d8mg)1IA\u22252\n2) \u2264O\n\u0012ln(n)(2\u03b1+1)/r\u2217\nn\n\u0013\nwith\nr\u2217= inf(r\u2113, rg).\n\u2022 If the \u03b5i\u2019s are super smooth and r\u2113= rg = 0, \u03c0D \u02d8m\u2113= \u03c0D \u02d8mg = [ln(n)/(2\u03b2\u03c3\u03c1 + 1)]1/\u03c1,\nthen\nE(\u2225(f \u2212\u02c6f \u02d8m\u2113, \u02d8mg)1IA\u22252\n2) \u2264O([ln(n)]\u22122a\u2217/\u03c1)\nwith\na\u2217= inf(a\u2113, ag).\nSince \u2113= fg, the smoothness properties of \u2113are related to those of f and of g.\nWhen \u2113belongs to Sa\u2113,0,B\u2113(\u03baa\u2113) and g belongs to Sag,0,Bg(\u03baag) with a\u2113= ag, then the resulting\nrate is the minimax rate given in Fan and Truong (1993) for H\u00a8olderian regression functions and\ndensities with the same regularity. It follows that our estimator seems then optimal in that\ncase. It is easy to see that the estimator is also optimal if ag \u2265a\u2113, that is when the density g is\nsmoother than the regression function f. But the optimality of the rate of \u02c6f \u02d8m\u2113, \u02d8mg when a\u2113> ag,\nthat is when the regression function f is smoother than g, remains an open question. This is\na known drawback of Nadaraya-Watson type estimators for regression functions, constructed\nas ratio of estimators.\nIn \u201cclassical\u201d regression models, when the Xi\u2019s are observed, a lot\nof methods, like local polynomial estimators, mean square estimators..., avoid the need of\nregularity conditions on g for the estimation of f. The point is that standard methods solving\nthe regression problem do not seem to work in the errors-in-variables model and it is an open\nproblem to build an estimator of f that does not require the estimation of the density g.\n\u00bfFrom the above results we see that the choice of the dimensions D \u02d8m\u2113and D \u02d8mg that realize\nthe best trade-o\ufb00between the squared bias and the variance terms depends on the unknown\nregularity coe\ufb03cients of the functions \u2113and g. In the next section we provide the upper bounds\nof the risks of the penalized estimators, constructed without such smoothness knowledge.\n3.3. Risks bounds of the minimum penalized contrast estimators: adaptation.\nTheorem 3.1. Under the assumptions (A1), (A2) and (A4), let\n(3.8)\n\u00b51 =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n0\nif \u03c1 < 1/3\n\u03b2(\u03c3\u03c0)\u03c1\u03bb1/2\n1 (\u03b1, \u03ba0, \u03b2, \u03c3, \u03c1)(1 + \u03c32\u03c02)\u03b1/2\u03ba\u22121\n0 (2\u03c0)\u22121/2\nif 1/3 \u2264\u03c1 \u22641,\n\u03b2(\u03c3\u03c0)\u03c1\u03bb1(\u03b1, \u03ba0, \u03b2, \u03c3, \u03c1)\nif \u03c1 > 1.\nand\n(3.9)\n\u00b52 = \u00b511I{0\u2264\u03c1<1/3}\u222a{\u03c1>1} + \u00b51\u2225f\u03b5\u222521I{1/3\u2264\u03c1\u22641}.\n10\nF. COMTE AND M.-L. TAUPIN\nLet kn \u2265n, \u02dc\u2113= \u02c6\u2113\u02c6m\u2113and \u02dcg = \u02c6g \u02c6mg be de\ufb01ned by (2.6) and (2.7) and with pen\u2113and peng given\nby (2.8), for \u03ba and \u03ba\u2032 two universal numerical constants and 1 \u2264m \u2264mn, mn satisfying (3.5)\nand, if \u03c1 > 0,\nmn \u2264\u03c0\u22121\n\u0014ln(n)\n2\u03b2\u03c3\u03c1 + 2\u03b1 + min[(1/2 + \u03c1/2), 1]\n2\u03c1\u03b2\u03c3\u03c1\nln\n\u0012ln(n)\n2\u03b2\u03c3\u03c1\n\u0013\u00151/\u03c1\n.\n(3.10)\n1) Adaptive estimation of g. (Comte et al. (2005a)).\nThen \u02dcg satis\ufb01es E(\u2225g\u2212\u02dcg\u22252\n2) \u2264K infm\u2208Mn,g\n\u0002\n\u2225g \u2212gm\u22252\n2 + D2\nm(\u03baG + 1)/n + peng(m)\n\u0003\n+c/n where\nK is a constant and c is a constant depending on f\u03b5 and Ag.\n2) Adaptive estimation of \u2113. Under the assumption (A3), if E|\u03be1|8 < \u221ethen \u02dc\u2113satis\ufb01es\nE(\u2225\u2113\u2212\u02dc\u2113\u22252\n2) \u2264K\u2032\ninf\nm\u2208Mn,\u2113\n\u0002\n\u2225\u2113\u2212\u2113m\u22252\n2 + D2\nm(\u03baL+ \u2225\u2113\u22251)/n + E(pen\u2113(m))\n\u0003\n+ c\u2032/n\nwhere K\u2032 is a constant and c\u2032 is a constant depending on f\u03b5, \u03baL, and \u2225\u2113\u22251.\nRemark 3.2. In Theorem 3.1, the penalty is random since it involves the term \u02c6m2(Y ), instead\nof the unknown quantity E(Y 2\n1 ) which appears \ufb01rst. The only price to pay for this substitution is\nthe moment condition E|\u03be1|8 < \u221einstead of E|\u03be1|6 < \u221eif E(Y 2\n1 ) was in the penalty. Moreover,\nthe term E(pen\u2113(m)) in the bound is equal to pen\u2113(m) with \u02c6m2(Y ) replaced by E(Y 2\n1 ).\nRemark 3.3. According to Remark 2.2, the penalty functions are of order \u0393(m)/n if 0 \u2264\u03c1 \u2264\n1/3, of order D3\u03c1/2\u22121/2\nm\n\u0393(m)/n if 1/3 \u2264\u03c1 \u22641 and of order D\u03c1\nm\u0393(m)/n if \u03c1 \u22651. When \u03c1 > 1/3,\nthe penalty functions pen\u2113(m) and pen\u2113(m) have not exactly the order of the variance \u0393(m)/n,\nbut a loss of order Dmin[(3\u03c1/2\u22121/2),\u03c1]\nm\noccurs, that is of order D(3\u03c1\u22121)/2\nm\nif 1/3 < \u03c1 \u22641 and of order\nD\u03c1\nm if \u03c1 > 1.\nRemark 3.4. Rates of convergence of \u02dcg. The rate of convergence of \u02dcg is the rate of\nconvergence of \u02c6g \u02d8mg when 0 \u2264\u03c1 \u22641/3 or when \u03c1 > 1/3 and rg = 0 or rg < \u03c1. And there is\na logarithmic loss, as a price to pay for adaptation when rg \u2265\u03c1 > 1/3. We refer to Comte et\nal. (2005a) for further comments on the optimality in a minimax sense of \u02dcg.\nRemark 3.5. Rates of convergence of \u02dc\u2113. The rates, similar to the rates of \u02dcg, are easy to\ndeduce from Theorem 3.1 as soon as \u2113= fg belongs to some smoothness class, but the procedure\ncan reach the rate of \u02c6\u2113\u02d8m\u2113, that uses the unknown smoothness parameter. If pen\u2113(m) has the\nsame order as the variance order \u0393(m)/n, then Theorem 3.1 guarantees an automatic trade-o\ufb00\nbetween the squared bias term \u2225\u2113\u2212\u2113m\u22252\n2 and the variance term, up to some multiplicative\nconstant. Else, there is some loss due to the adaptation. Let us be more precise.\nIf 0 \u2264\u03c1 \u22641/3, the errors \u03b5i\u2019s are ordinary smooth or super smooth with \u03c1 \u22641/3. If \u2113\nsatis\ufb01es (R1), the squared bias is bounded by applying (3.6) which combined with the value of\npen\u2113(m), of order \u0393(m)/n (see (3.3)) gives that the estimator \u02dcg automatically reaches the best\nrate achievable by the estimator \u02c6\u2113\u02d8m\u2113, as given in Table 1.\nNONPARAMETRIC ESTIMATION IN AN ERROR-IN-VARIABLES MODEL\n11\nIf \u03c1 > 1/3 the penalty function pen\u2113(m) is slightly bigger than the variance order \u0393(m)/n.\nThe rate of convergence remains the best rate if the bias \u2225\u2113\u2212\u2113m\u22252\n2 is the dominating term\nin the trade-o\ufb00between \u2225\u2113\u2212\u2113m\u22252\n2 and pen\u2113(m). When r\u2113= 0 and \u03c1 > 0, the rate of order\n(ln(n))\u22122a\u2113/\u03c1 is given by the bias term, and the loss in the penalty function does not change\nthe rate of the adaptive estimator \u02dc\u2113, which remains the best achievable rate E \u2225\u2113\u2212\u02c6\u2113\u02d8m\u2113\u22252\n2. In\nthe same way, when 0 < r\u2113< \u03c1, the rate is given by the bias term and thus this loss does not\na\ufb00ect the rate of convergence of \u02dc\u2113either.\nLet us now focus our discussion on the case where pen\u2113(m) can be the dominating term in\nthe trade-o\ufb00between \u2225\u2113\u2212\u2113m\u22252\n2 and pen\u2113(m), that is when r\u2113\u2265\u03c1 > 1/3. In that case, there\nis a loss of order Dmin[(3\u03c1/2\u22121/2),\u03c1]\nm\nin the penalty function, compared to the variance term. But\nthis happens in cases where the order of the optimal Dm is less than (ln n)1/\u03c1 and consequently\nthe loss in the rate is at most of order ln n, when the rate is faster than logarithmic: therefore\nthe loss appears only in cases where it can be seen as negligible.\nIn particular, there is no price to pay for the adaptation if the \u03bei\u2019s are Gaussian and the \u03b5i\u2019s\nare ordinary smooth. Indeed, in that case, the rate of convergence of the penalized estimator \u02dc\u2113,\nwithout any knowledge on \u2113or g, is the same as the rate given by the non penalized estimator\n\u02c6\u2113\u02d8m\u2113, requiring the knowledge of smoothness parameters. But, if both the \u03bei\u2019s and the \u03b5i\u2019s are\nGaussian, then \u03c1 = 2 and a logarithmic negligible loss appears in the rate of \u02dc\u2113compared to the\nrate of \u02c6\u2113\u02d8m\u2113.\nTheorem 3.2. Adaptive estimation of f. Under the assumptions (A1), (A2), (A3), (A4)\nand (A5), let \u02dcf be de\ufb01ned by (2.5) with \u02dcg and \u02dc\u2113be de\ufb01ned in (2.7) and (2.6) with \u02c6mg \u2208Mn,g\nsatisfying (3.5) and (3.10), Dmn,g \u2264(n/ ln(n))1/(2\u03b1+2) and \u02c6m\u2113\u2208Mn,\u2113satisfying (3.5) and\n(3.10). Assume that g belongs to some space Sag,rg,Bg(\u03baag) de\ufb01ned by (R1) with ag > 1/2 if\nrg = 0, and that E|\u03be1|8 < \u221e. If kn \u2265n3/2, an = nk for k > 0, for n large enough, C0 = 8Kg\u22122\n0\nand C1 = 4K\u2032g\u22122\n0 (2g2\n1 + 1)\u03ba2\n\u221e,G, then\nE(\u2225(f \u2212\u02dcf)1IA\u22252\n2)\n\u2264\nC0\ninf\nm\u2208Mn,\u2113[\u2225\u2113\u2212\u2113m\u22252\n2 + D2\nm(\u03baL+ \u2225\u2113\u22251)/n + pen\u2113(m)]\n+C1\ninf\nm\u2208Mn,g[\u2225g \u2212gm\u22252\n2 + D2\nm(\u03baG + 1)/n + peng(m)] + c/n\n(3.11)\nwhere K and K\u2032 are constants depending on f\u03b5, and c is a constant depending on f\u03b5, f and g.\nAs in Theorem 2.1, if ag \u22641/2 then it may happen that D \u02dcmg \u2265n1/(2\u03b1+2), and in this case we\nonly have a result in probability: \u2225(f \u2212\u02dcf)1IA\u22252\n2 = Op(\u2225\u2113\u2212\u02dc\u2113\u22252\n2 + \u2225g \u2212\u02dcg\u22252\n2). Moreover, the result\nholds when the constant \u03ba\u221e,G is replaced by \u2225f \u2225\u221e,A if f is bounded on the compact set A.\nAlso note that the remark 3.1 is still valid for all adaptive estimators.\n12\nF. COMTE AND M.-L. TAUPIN\nComments about the resulting rates for estimating f. First the rate of convergence\nof \u02dcf is given by the worst rate of convergence between the rate of \u02dc\u2113and \u02dcg. Obviously all the\ncomments about \u02c6f \u02d8m\u2113, \u02d8mg, related to this fact keep holding here.\nWhen 0 \u2264\u03c1 < 1/3 or when r\u2113\u2264\u03c1 and rg \u2264\u03c1, then \u02dcf achieves the rate of convergence of\n\u02c6f \u02d8m\u2113, \u02d8mg, given by the worst rate of convergence between E \u2225\u02c6\u2113\u02d8m\u2113\u2212\u2113\u22252\n2 and E \u2225\u02c6g \u02d8mg \u2212g \u22252\n2. And\nwhen rg > \u03c1 > 1/3 or r\u2113> \u03c1 > 1/3, there is a logarithmic loss in the rate of convergence of \u02dcf\ncompared to the rate of convergence of \u02c6f \u02d8m\u2113, \u02d8mg.\nSince the regularity of \u2113is by de\ufb01nition the regularity of fg, the rate of convergence of \u02dc\u2113in fact\ndepends on smoothness properties of f and g. As a consequence, if \u2113and g belong respectively\nto Sa\u2113,r\u2113,Bf(\u03baa\u2113) and Sag,rg,Bg(\u03baag), then the rate of convergence of \u02dcf is the rate of \u02c6f \u02d8m\u2113, \u02d8mg when\n0 \u2264\u03c1 \u22641/3. According to Fan and Truong (1993), this rate seems the minimax rate when\na\u2113\u2264ag and r\u2113= rg = 0. In the other cases, the question of the optimality in a minimax sense\nremains open. Even if the regression function is smoother than g and 0 \u2264\u03c1 \u22641/3, the rate of\nconvergence of \u02dcf has the order of the rate of convergence of \u02c6f \u02d8m\u2113, \u02d8mg, but we do not know if the\nrate of \u02c6f \u02d8m\u2113, \u02d8mg is the minimax rate (see comments following Theorem 2.1). When \u03c1 > 1/3, a\nloss appears between the rate of convergence of \u02dcf and the rate of convergence of \u02c6f \u02d8m\u2113, \u02d8mg. This\nloss only appears, when r\u2113> \u03c1 or rg > \u03c1 (see the comments after Theorem 3.1), in cases where\nit is negligible with respect to the rate.\nRemark 3.6. Obviously, the resulting rates for all estimators depend on the noise level \u03c3. The\n\ufb01rst point is to note that if \u03c3 = 0, then by convention B = 0 = \u03c1 = 0, \u03bb = 1, and Z = X is\nobserved. In that case, \u0393(m)/n of order Dm/n has the expected order for the variance term\nin \u201cusual regression\u201d, when the explanatory variables are observed, and the same holds for the\npenalties pen\u2113and peng. This order Dm/n is the expected penalty order for density estimation\nand nonparametric regression estimation, when there is one model per dimension, as in our\ncase.\nThe second point is to note that if \u03c3 is small, then the procedure automatically selects a\ndimension Dm closed to the dimension that would be selected in \u201cusual\u201d density estimation\nand nonparametric regression estimation.\nConcluding remarks\nOur estimation procedure provides an adaptive estimator in the sense that its construction\ndoes not require any prior knowledge on the smoothness parameters of the regression function\nf and of the density g. This estimation procedure allows to consider various smoothness classes\nfor the regression function and for the density g when the errors are either ordinary smooth or\nsuper smooth, and to give upper bounds for the risk in all the cases.\nThe resulting rates of convergence for the estimation of f are given by the worst between\nthe rate for the estimation of fg and the rate for the estimation of g. Nevertheless, they are\nNONPARAMETRIC ESTIMATION IN AN ERROR-IN-VARIABLES MODEL\n13\nthe minimax rates in cases where lower bounds are available. In the other cases, the resulting\nrates are in most cases the best rates achievable if the smoothness parameters were known.\nSome logarithmic loss, negligible compared to the order of the rate, appears, as a price to pay\nfor the adaptation, when both the errors density f\u03b5 and fg are super smooth with f\u03b5 strictly\nsmoother than fg. This logarithmic loss appears when the in\ufb02uence of the noise \u03c3\u03b5 dominates\nthe smoothness properties of f and g.\n4. Proofs\n4.1. Proof of Proposition 3.1. By applying De\ufb01nition (2.2), for any m belonging to Mn,\n\u02c6\u2113m satis\ufb01es \u03b3n,\u2113(\u02c6\u2113m) \u2212\u03b3n,\u2113(\u2113(n)\nm ) \u22640. Denoting by \u03bdn(t) the centered empirical process,\n(4.1)\n\u03bdn(t) = 1\nn\nn\nX\ni=1\n(Yiu\u2217\nt(Zi) \u2212\u27e8t, \u2113\u27e9) ,\nand by using that t 7\u2192u\u2217\nt is linear we get the following decomposition\n\u03b3n,\u2113(t) \u2212\u03b3n,\u2113(s) = \u2225t \u2212\u2113\u22252\n2 \u2212\u2225s \u2212\u2113\u22252\n2 \u22122\u03bdn(t \u2212s)\n(4.2)\nand therefore, since by Pythagoras Theorem, \u2225\u2113\u2212\u2113(n)\nm \u22252\n2 = \u2225\u2113\u2212\u2113m\u22252\n2 +\u2225\u2113\u2212\u2113(n)\nm \u22252\n2, we infer that\n\u2225\u2113\u2212\u02c6\u2113m\u22252\n2 \u2264\u2225\u2113\u2212\u2113m\u22252\n2 + \u2225\u2113m \u2212\u2113(n)\nm \u22252\n2 + 2\u03bdn(\u02c6\u2113m \u2212\u2113(n)\nm ). Using that \u02c6am,j(\u2113) \u2212am,j(\u2113) = \u03bdn(\u03d5m,j),\nwe get\n(4.3)\n\u03bdn(\u02c6\u2113m \u2212\u2113(n)\nm ) =\nX\n|j|\u2264kn\n(\u02c6am,j(\u2113) \u2212am,j(\u2113))\u03bdn(\u03d5m,j) =\nX\n|j|\u2264kn\n[\u03bdn(\u03d5m,j)]2,\nand consequently\nE\u2225\u2113\u2212\u02c6\u2113m\u22252\n2 \u2264\u2225\u2113\u2212\u2113m\u22252\n2 + \u2225\u2113m \u2212\u2113(n)\nm \u22252\n2 + 2\nX\nj\u2208Z\nVar[\u03bdn(\u03d5m,j)].\n(4.4)\nNow, since the (Yi, Zi)\u2019s are independent, Var[\u03bdn(\u03d5m,j)] = n\u22121Var[Y1u\u2217\n\u03d5m,j(Z1)], and, arguing as\nin Comte et al. (2005a), by using Parseval\u2019s formula we get that\nX\nj\u2208Z\nVar[\u03bdn(\u03d5m,j)] \u2264n\u22121 \u2225\nX\nj\u2208Z\n|u\u2217\n\u03d5m,j|2 \u2225\u221eE(Y 2\n1 ) \u2264E(Y 2\n1 )\u2206(m)/n.\n(4.5)\nwhere \u2206is de\ufb01ned in Proposition (3.1). Let us study the residual term \u2225\u2113m \u2212\u2113(n)\nm \u22252\n2, by simply\nwritting that\n\u2225\u2113m \u2212\u2113(n)\nm \u22252\n2 =\nX\n|j|\u2265kn\na2\nm,j(\u2113) \u2264(sup\nj\njam,j(\u2113))2 X\n|j|\u2265kn\nj\u22122.\n14\nF. COMTE AND M.-L. TAUPIN\nNow by de\ufb01nition\njam,j(\u2113) = j\np\nDm\nZ\n\u03d5(Dmx \u2212j)\u2113(x)dx\n\u2264\nD3/2\nm\nZ\n|x||\u03d5(Dmx \u2212j)||\u2113(x)|dx +\np\nDm\nZ\n|Dmx \u2212j||\u03d5(Dmx \u2212j)||\u2113(x)|dx\n\u2264\nD3/2\nm\n\u0012Z\n|\u03d5(Dmx \u2212j)|2dx\n\u00131/2\n\u03ba1/2\nL +\np\nDm sup\nx |x\u03d5(x)|\u2225\u2113\u22251.\nConsequently jam,j \u2264Dm\u2225\u03d5\u22252\u03ba1/2\nL\n+ \u221aDm\u2225\u2113\u22251/\u03c0, and \u2225\u2113m \u2212\u2113(n)\nm \u22252\n2 \u2264\u03ba(\u03baL + \u2225\u2113\u22252\n1)D2\nm/kn. \u25a1\n4.2. Proof of Proposition 3.2. The proof of Proposition 3.2 being rather similar to the proof\nof Theorem 3.2 is omitted. We refer to Comte and Taupin (2004) for further details.\n4.3. Proof of Theorem 3.1. We only prove the result with E(Y 2) in the penalty instead\nof \u02c6m2(Y ) and refer to Comte and Taupin (2004) for the complete proof with \u02c6m2(Y ), as an\napplication of Rosenthal\u2019s inequality (see Rosenthal (1970)).\nFor the study of \u02dc\u2113, the main di\ufb03culty compared to the study of \u02dcg comes from the unbounded\nnoise \u03bei. By de\ufb01nition, \u02dc\u2113satis\ufb01es that for all m \u2208Mn,\u2113, \u03b3n,\u2113(\u02dc\u2113)+pen\u2113( \u02c6m) \u2264\u03b3n,\u2113(\u2113(n)\nm )+pen\u2113(m).\nTherefore, by applying (4.2) we get that\n\u2225\u02dc\u2113\u2212\u2113\u22252\n2\u2264\u2225\u2113\u2212\u2113(n)\nm \u22252\n2 +2\u03bdn(\u02dc\u2113\u2212\u2113(n)\nm ) + pen\u2113(m) \u2212pen\u2113( \u02c6m).\n(4.6)\nNext, we use that if t = t1 +t2 with t1 in Sm and t2 in Sm\u2032, then t is such that t\u2217has its support\nin [\u2212\u03c0Dmax(m,m\u2032), \u03c0Dmax(m,m\u2032)] and therefore t belongs to Sm\u2217where m\u2217= max(m, m\u2032). Denote\nby Bm,m\u2032(0, 1) the set\nBm,m\u2032(0, 1) = {t \u2208S(n)\nmax(m,m\u2032) / \u2225t\u22252 = 1}.\nIt follows that\n|\u03bdn(\u02dc\u2113\u2212\u2113(n)\nm )| \u2264\u2225\u02dc\u2113\u2212\u2113(n)\nm \u22252\nsup\nt\u2208Bm, \u02c6\nm(0,1)\n|\u03bdn(t)|,\nwhere \u03bdn(t) is de\ufb01ned by (4.1). Consequently, by using that 2ab \u2264x\u22121a2 + xb2\n\u2225\u02dc\u2113\u2212\u2113\u22252\n2\n\u2264\n\u2225\u2113(n)\nm \u2212\u2113\u22252\n2 + 1\nx\u2225\u02dc\u2113\u2212\u2113(n)\nm \u22252\n2 + x\nsup\nt\u2208Bm, \u02c6\nm(0,1)\n\u03bd2\nn(t) + pen\u2113(m) \u2212pen\u2113( \u02c6m)\nand therefore, writing that \u2225\u02dc\u2113\u2212\u2113(n)\nm \u22252\n2 \u2264(1 + y\u22121)\u2225\u02dc\u2113\u2212\u2113\u22252\n2 + (1 + y)\u2225\u2113\u2212\u2113(n)\nm \u22252\n2, with y =\n(x + 1)/(x \u22121) for x > 1, we infer that\n\u2225\u02dc\u2113\u2212\u2113\u22252\n2 \u2264\n\u0012x + 1\nx \u22121\n\u00132\n\u2225\u2113\u2212\u2113(n)\nm \u22252\n2 + x(x + 1)\nx \u22121\nsup\nt\u2208Bm, \u02c6\nm(0,1)\n\u03bd2\nn(t) + x + 1\nx \u22121(pen\u2113(m) \u2212pen\u2113( \u02c6m)).\nNONPARAMETRIC ESTIMATION IN AN ERROR-IN-VARIABLES MODEL\n15\nChoose some positive function p\u2113(m, m\u2032) such that xp\u2113(m, m\u2032) \u2264pen\u2113(m) + pen\u2113(m\u2032). Then,\nby denoting by \u03bax = (x + 1)/(x \u22121),\n\u2225\u02dc\u2113\u2212\u2113\u22252\n2\n\u2264\n\u03ba2\nx\u2225\u2113\u2212\u2113(n)\nm \u22252\n2 + x\u03bax[\nsup\nt\u2208Bm, \u02c6\nm(0,1)\n|\u03bdn|2(t) \u2212p(m, \u02c6m)]+\n+\u03bax (xp\u2113(m, \u02c6m) + pen\u2113(m) \u2212pen\u2113( \u02c6m))\n(4.7)\nthat is\n\u2225\u02dc\u2113\u2212\u2113\u22252\n2 \u2264\u03ba2\nx\u2225\u2113\u2212\u2113(n)\nm \u22252\n2 + 2\u03baxpen\u2113(m) + x\u03baxWn( \u02c6m),\n(4.8)\nwhere\n(4.9)\nWn(m\u2032) = [\nsup\nt\u2208Bm,m\u2032(0,1)\n|\u03bdn(t)|2 \u2212p\u2113(m, m\u2032)]+.\nThe main point of the proof lies in studying Wn(m\u2032), more precisely in \ufb01nding p\u2113(m, m\u2032) such\nthat\n(4.10)\nE(Wn( \u02c6m)) \u2264\nX\nm\u2032\u2208Mn,\u2113\nE(Wn(m\u2032))) \u2264C/n,\nwhere C is a constant. In this case, combining (4.8) and (4.10) we infer that, for all m in Mn,\u2113,\nE\u2225\u2113\u2212\u02dc\u2113\u22252\n2 \u2264\u03ba2\nx\u2225\u2113\u2212\u2113(n)\nm \u22252\n2 + 2\u03baxpen\u2113(m) + x\u03baxC/n,\nwhich can also be written\n(4.11)\nE\u2225\u2113\u2212\u02dc\u2113\u22252\n2 \u2264Cx\ninf\nm\u2208Mn,\u2113\n\u0002\n\u2225\u2113\u2212\u2113m\u22252\n2 + pen\u2113(m)\n\u0003\n+ CxC\u2032/n,\nwhere Cx = max(\u03ba2\nx, 2\u03bax) suits, when kn \u2265n, and (3.5) and (3.10) hold. It remains thus to\n\ufb01nd p\u2113(m, m\u2032) such that (4.10) holds.\nThe process Wn(m\u2032) is studied by using the decomposition of \u03bdn(t) = \u03bdn,1(t) + \u03bdn,2(t) with\n\u03bdn,1(t) = 1\nn\nn\nX\ni=1\n(f(Xi)u\u2217\nt(Zi) \u2212\u27e8t, \u2113\u27e9) and \u03bdn,2(t) = 1\nn\nn\nX\ni=1\n\u03beiu\u2217\nt(Zi).\n(4.12)\nIt follows that Wn(m\u2032) \u22642Wn,1(m\u2032) + 2Wn,2(m\u2032) where for i = 1, 2,\n(4.13) Wn,i(m\u2032) = [\nsup\nt\u2208Bm,m\u2032 (0,1)\n|\u03bdn,i(t)|2\u2212pi(m, m\u2032)]+, and p\u2113(m, m\u2032) = 2p1(m, m\u2032)+2p2(m, m\u2032).\n\u2022 Study of Wn,1.\nSince under (A3), f is bounded on the support of g, we apply a standard Talagrand\u2019s (1996)\ninequality (see Lemma 4.1 below that can be a fortiori applied to identically distributed vari-\nables):\n16\nF. COMTE AND M.-L. TAUPIN\nLemma 4.1. Let U1, . . . , Un be independent random variables and \u03bdn(r) = (1/n) Pn\ni=1[r(Ui) \u2212\nE(r(Ui))] for r belonging to a countable class R of uniformly bounded measurable functions.\nThen for \u01eb > 0\n(4.14)\nE\n\u0014\nsup\nr\u2208R\n|\u03bdn(r)|2 \u22122(1 + 2\u01eb)H2\n\u0015\n+\n\u22646\nK1\n\u0012v\nne\u2212K1\u01eb nH2\nv\n+\n8M2\n1\nK1n2C2(\u01eb)e\u2212K1C(\u01eb)\u221a\u01eb\n\u221a\n2\nnH\nM1\n\u0013\n,\nwith C(\u01eb) = \u221a1 + \u01eb \u22121, K1 is a universal constant, and where\nsup\nr\u2208R\n\u2225r\u2225\u221e\u2264M1,\nE\n\u0012\nsup\nr\u2208R\n|\u03bdn(r)|\n\u0013\n\u2264H,\nsup\nr\u2208R\n1\nn\nn\nX\ni=1\nVar(r(Ui)) \u2264v.\nThe inequality (4.14) is a straightforward consequence of Talagrand\u2019s (1996) inequality given\nin Ledoux (1996) (or Birg\u00b4e and Massart (1998)). Therefore\n(4.15)\nE[\nsup\nt\u2208Bm,m\u2032(0,1)\n|\u03bdn,1(t)|2 \u22122(1 + 2\u01eb1)H2\n1]+ \u2264\u03ba1\n\u0012v1\nn e\u2212K1\u01eb1\nnH2\n1\nv1 + M2\n1\nn2 e\u2212K2\u221a\u01eb1C(\u01eb1) nH1\nM1\n\u0013\n,\nwhere K2 = K1/\n\u221a\n2 and H1, v1 and M1 are de\ufb01ned by E(supt\u2208Bm,m\u2032 (0,1) |\u03bdn,1(t)|2) \u2264H2\n1,\nsup\nt\u2208Bm,m\u2032(0,1)\nVar(f(X1)u\u2217\nt(Z1)) \u2264v1, and\nsup\nt\u2208Bm,m\u2032 (0,1)\n\u2225f(X1)u\u2217\nt(Z1)\u2225\u221e\u2264M1.\nAccording to (3.3) and (4.5), we propose to take\n(4.16)\nM1 = M1(m, m\u2032) = \u03ba\u221e,G\np\n\u03bb1\u0393(m\u2217).\nFor v1, denoting by Pj,k, the quantity Pj,k(m) = E\nh\nf 2(X1)u\u2217\n\u03d5m,j(Z1)u\u2217\n\u03d5m,k(\u2212Z1)\ni\n, write\nsup\nt\u2208Bm,m\u2032(0,1)\nVar(f(X1)u\u2217\nt(Z1))\n\u2264\n(\nX\nj,k\u2208Z\n|Pj,k(m\u2217)|2)1/2.\nArguing as in Comte et al. (2005a), let us de\ufb01ne \u22062(m, \u03a8) by\n\u22062(m, \u03a8) = D2\nm\nZ Z \f\f\f\f\n\u03d5\u2217(x)\u03d5\u2217(y)\nf \u2217\u03b5 (Dmx)f \u2217\u03b5 (Dmy)\u03a8\u2217(Dm(x \u2212y))\n\f\f\f\f\n2\ndxdy \u2264\u03bb2\n2(\u2225\u03a8\u22252)\u03932\n2(m\u2217),\nwith\n(4.17)\n\u03932(m\u2217) = D2\u03b1+min[(1/2\u2212\u03c1/2),(1\u2212\u03c1)]\nm\u2217\nexp{2\u03b2\u03c3\u03c1(\u03c0Dm\u2217)\u03c1}\nand \u03bb2(\u2225\u03a8\u22252) = \u03bb2(\u03b1, \u03ba0, \u03b2, \u03c3, \u03c1, \u2225\u03a8\u22252) given by\n(4.18)\n\u03bb2(\u2225\u03a8\u22252) =\n(\n\u03bb1(\u03b1, \u03ba0, \u03b2, \u03c3, \u03c1)\nif \u03c1 > 1,\n\u03ba\u22121\n0 (2\u03c0)\u22121/2\u03bb1/2\n1 (\u03b1, \u03ba0, \u03b2, \u03c3, \u03c1)(1 + \u03c32\u03c02)\u03b1/2\u2225\u03a8\u22252\nif \u03c1 \u22641.\nNow, write Pj,k as\nPj,k(m)\n=\nZZ\nf 2(x)u\u2217\n\u03d5m,j(x + y)u\u2217\n\u03d5m,k(\u2212(x + y))g(x)f\u03b5(y)dxdy\nNONPARAMETRIC ESTIMATION IN AN ERROR-IN-VARIABLES MODEL\n17\nthat is\nPj,k(m)\n= Dm\nZZ\nf 2(x)\nZZ\ne\u2212i(x+y)uDm \u03d5\u2217(u)eiju\nf \u2217\u03b5 (Dmu)ei(x+y)vDm \u03d5\u2217(v)eikv\nf \u2217\u03b5 (Dmv)dudvg(x)f\u03b5(y)dxdy\n= Dm\nZZ eiju+ikv\u03d5\u2217(u)\u03d5\u2217(v)\nf \u2217\u03b5 (Dmu)f \u2217\u03b5 (Dmv)\n\u0012ZZ\ne\u2212i(x+y)(u\u2212v)Dmf 2(x)g(x)f\u03b5(y)dxdy\n\u0013\ndudv\n=\nDm\nZZ eiju+ikv\u03d5\u2217(u)\u03d5\u2217(v)\nf \u2217\n\u03b5 (Dmu)f \u2217\n\u03b5 (Dmv) [(f 2g) \u2217f\u03b5]\u2217((u \u2212v)Dm)dudv.\nBy applying Parseval\u2019s formula we get that P\nj,k |Pj,k(m)|2 equals\nD2\nm\nZ Z \f\f\f\f\n\u03d5\u2217(u)\u03d5\u2217(v)\nf \u2217\u03b5 (Dmu)f \u2217\u03b5 (Dmv)[(f 2g) \u2217f\u03b5]\u2217((u \u2212v)Dm)\n\f\f\f\f\n2\ndudv = \u22062(m, (f 2g) \u2217f\u03b5).\nSince \u2225(f 2g) \u2217f\u03b5\u22252 \u2264\u2225f 2g\u22252\u2225f\u03b5\u22252 = E1/2(f 2(X1))\u2225f\u03b5\u22252, and \u03bb2(\u2225f 2g\u22252\u2225f\u03b5\u22252) \u2264\u00b52, by using\nthe de\ufb01nition of \u00b52 given in (3.8), we propose to take\n(4.19)\nv1 = v1(m, m\u2032) = \u00b52\u03932(m\u2217).\nLastly, we have E[supt\u2208Bm,m\u2032(0,1) |\u03bdn,1(t)|2] \u2264E(f 2(X1))\u03bb1\u0393(m\u2217)/n and thus we propose to take\nH2\n1 = H2\n1(m, m\u2032) = E(f 2(X1))\u03bb1\u0393(m\u2217)/n.\n(4.20)\nIt follows from (4.15), (4.16), (4.19) and (4.20) that if\np1(m, m\u2032) = 2(1 + 2\u01eb1)H2\n1 = 2(1 + 2\u01eb1)E(f 2(X1))\u03bb1\u0393(m\u2217)/n\nthen\nE(Wn,1(m\u2032))\n\u2264\nE\n\"\nsup\nt\u2208Bm,m\u2032(0,1)\n|\u03bdn,1(t)|2 \u22122(1 + 2\u01eb1)H2\n1\n#\n+\n\u2264A1(m\u2217) + B1(m\u2217)\n(4.21)\nwith\nA1(m)\n=\nK3\n\u00b52\u03932(m)\nn\nexp\n\u0012\n\u2212K1\u01eb1E(f 2(X1)) \u03bb1\u0393(m)\n\u00b52\u03932(m)\n\u0013\n(4.22)\nand B1(m)\n=\nK3\n\u03ba2\n\u221e,G\u03bb1\u0393(m)\nn2\nexp\n(\n\u2212K2\n\u221a\u01eb1C(\u01eb1)\np\nE(f 2(X1))\n\u03ba\u221e,G\n\u221an\n)\n.\n(4.23)\nSince \u2200m \u2208Mn,\u2113, \u0393(m) \u2264n and |Mn,\u2113| \u2264n, there exist some constants K4 and c such that\nX\nm\u2208Mn,\u2113\nB1(m\u2217) \u2264K3\u2225f\u22252\n\u221e,G\u03bb1 exp[\u2212K4\np\nE(f 2(X1))\u221an/\u03ba\u221e,G] \u2264c/n.\nLet us now come to the study of A1(m\u2217).\n18\nF. COMTE AND M.-L. TAUPIN\n1) Case 0 \u2264\u03c1 < 1/3. In that case, \u03c1 \u2264(1/2 \u2212\u03c1/2)+ and the choice \u01eb1 = 1/2 ensures the\nconvergence of P\nm\u2032\u2208Mn,\u2113A1(m\u2217). Indeed, if we denote by \u03c8 = 2\u03b1 + min[(1/2 \u2212\u03c1/2), (1 \u2212\u03c1)],\n\u03c9 = (1/2 \u2212\u03c1/2)+, K\u2032 = \u03ba2\u03bb1/\u00b52, then for a, b \u22651, we infer that\nmax(a, b)\u03c8e2\u03b2\u03c3\u03c1\u03c0\u03c1 max(a,b)\u03c1e\u2212K\u2032\u03be2 max(a,b)\u03c9\u2264\n(a\u03c8e2\u03b2\u03c3\u03c1\u03c0\u03c1a\u03c1 + b\u03c8e2\u03b2\u03c3\u03c1\u03c0\u03c1b\u03c1)e\u2212(K\u2032\u03be2/2)(a\u03c9+b\u03c9)\nis bounded by\na\u03c8e2\u03b2\u03c3\u03c1\u03c0\u03c1a\u03c1e\u2212(K\u2032\u03be2/2)a\u03c9e\u2212(K\u2032\u03be2/2)b\u03c9 + b\u03c8e2\u03b2\u03c3\u03c1\u03c0\u03c1b\u03c1e\u2212(K\u2032\u03be2/2)b\u03c9).\n(4.24)\nSince the function a 7\u2192a\u03c8e2\u03b2\u03c3\u03c1\u03c0\u03c1a\u03c1e\u2212(K\u2032\u03be2/2)a\u03c9 is bounded on R+ by a constant, depending\non \u03b1, \u03c1 and K\u2032 only, and since Ak\u03c1 \u2212\u03b2k\u03c9 \u2264\u2212(\u03b2/2)k\u03c9 for any k \u22651, it follows that\nP\nm\u2032\u2208Mn,\u2113A1(m\u2217) \u2264C/n.\n2) Case \u03c1 = 1/3. In that case, \u03c1 = (1/2 \u2212\u03c1/2)+, and \u03c9 = \u03c1. We choose \u01eb1 = \u01eb1(m, m\u2032)\nsuch that 2\u03b2\u03c3\u03c1\u03c0\u03c1D\u03c1\nm\u2217\u2212K\u2032E(f 2(X1))\u01eb1D\u03c1\nm\u2217= \u22122\u03b2\u03c3\u03c1\u03c0\u03c1D\u03c1\nm\u2217that is, since K\u2032 = K1\u03bb1/\u00b52,\n\u01eb1 = \u01eb1(m, m\u2032) = (4\u03b2\u03c3\u03c1\u03c0\u03c1\u00b52)/(K1\u03bb1E(f 2(X1))).\n3) Case \u03c1 > 1/3. In that case, \u03c1 > (1/2 \u2212\u03c1/2)+. Bearing in mind the inequality (4.24) we\nchoose \u01eb1 = \u01eb1(m, m\u2032) such that 2\u03b2\u03c3\u03c1\u03c0\u03c1D\u03c1\nm\u2217\u2212K\u2032E(f 2(X1))\u01eb1D\u03c9\nm\u2217= \u22122\u03b2\u03c3\u03c1\u03c0\u03c1D\u03c1\nm\u2217that is, since\nK\u2032 = K1\u03bb1/\u00b52, \u01eb1 = \u01eb1(m, m\u2032) = (4\u03b2\u03c3\u03c1\u03c0\u03c1\u00b52)/(K1\u03bb1E(f 2(X1)))D\u03c1\u2212\u03c9\nm\u2217.\nThese choices ensure that P\nm\u2032\u2208Mn,\u2113A1(m\u2217) is less than C/n.\n\u2022 Study of Wn,2.\nDenote by\nH2\n\u03be(m, m\u2032) = (n\u22121\nn\nX\ni=1\n\u03be2\ni )\u03bb1\u0393(m\u2217)/n,\n(4.25)\nwith (n\u22121 Pn\ni=1 \u03be2\ni )\u03bb1\u0393(m)/n = (n\u22121 Pn\ni=1 \u03be2\ni \u2212\u03c32\n\u03be)\u03bb1\u0393(m)/n + \u03c32\n\u03be\u03bb1\u0393(m)/n bounded by\n(n\u22121\nn\nX\ni=1\n\u03be2\ni \u2212\u03c32\n\u03be)1I{n\u22121| Pn\ni=1(\u03be2\ni \u2212\u03c32\n\u03be)|>\u03c32\n\u03be/2}\u03bb1\u0393(m)/n + 3\u03c32\n\u03be\u03bb1\u0393(m)/(2n).\nConsequently H2\n\u03be(m, m\u2032) \u2264H\u03be,1(m, m\u2032) + H\u03be,2(m, m\u2032) where\nH\u03be,1(m, m\u2032) = (n\u22121\nn\nX\ni=1\n\u03be2\ni \u2212\u03c32\n\u03be)1I{n\u22121| Pn\ni=1 \u03be2\ni \u2212\u03c32\n\u03be|>\u03c32\n\u03be/2}\u03bb1\u0393(m\u2217)/n and H\u03be,2(m, m\u2032) = 3\u03c32\n\u03be\u03bb1\u0393(m\u2217)/(2n).\nBy applying (4.12) we infer that E[supt\u2208Bm,m\u2032(0,1) |\u03bdn,2(t)|2 \u2212p2(m, m\u2032)]+ is bounded by\nE[2\nsup\nt\u2208Bm,m\u2032(0,1)\n(n\u22121\nn\nX\ni=1\n\u03bei(u\u2217\nt(Zi) \u2212\u27e8t, g)\u27e9)2 \u22124(1 + 2\u01eb2)H2\n\u03be(m, m\u2032)]+ + 2\u2225g\u22252\n2E[(n\u22121\nn\nX\ni=1\n\u03bei)2]\n+ E[4(1 + 2\u01eb2)H2\n\u03be(m, m\u2032) \u2212p2(m, m\u2032)]+,\nNONPARAMETRIC ESTIMATION IN AN ERROR-IN-VARIABLES MODEL\n19\nthat is\n(4.26)\nE[\nsup\nt\u2208Bm,m\u2032 (0,1)\n|\u03bdn,2(t)|2 \u2212p2(m, m\u2032)]+\n\u22642E[\nsup\nt\u2208Bm,m\u2032(0,1)\n(n\u22121\nn\nX\ni=1\n\u03bei(u\u2217\nt(Zi) \u2212\u27e8t, g\u27e9))2 \u22122(1 + 2\u01eb2)H2\n\u03be(m, m\u2032)]+ + 2\u2225g\u22252\n2\u03c32\n\u03be/n\n+ 4(1 + 2\u01eb2)E|H\u03be,1(m, m\u2032)| + E[4(1 + 2\u01eb2)H\u03be,2(m, m\u2032) \u2212p2(m, m\u2032)]+.\nSince we only consider dimensions Dm such that \u0393(m)/n is bounded by some constant \u03ba, we\nget that for some p \u22652, E|H\u03be,1(m, m\u2032)| is bounded by\n\u03ba\u03bb1E[| 1\nn\nn\nX\ni=1\n\u03be2\ni \u2212\u03c32\n\u03be|1I{n\u22121| Pn\ni=1(\u03be2\ni \u2212\u03c32\n\u03be)|>\u03c32\n\u03be/2}] \u2264\u03ba\u03bb12p\u22121E[|n\u22121\nn\nX\ni=1\n\u03be2\ni \u2212\u03c32\n\u03be|p]/\u03c32(p\u22121)\n\u03be\nAccording to Rosenthal\u2019s inequality (see Rosenthal (1970)), we \ufb01nd that, for \u03c3p\n\u03be,p := E(|\u03be|p), \u03c32\n\u03be,2 =\n\u03c32\n\u03be,\nE|n\u22121\nn\nX\ni=1\n\u03be2\ni \u2212\u03c32\n\u03be|p \u2264C\u2032(p)\n\u0000\u03c32p\n\u03be,2pn1\u2212p + \u03c32p\n\u03be,4n\u2212p/2\u0001\n.\nNow, the assumption (A1) implies that \u03b1 > 1/2, therefore |Mn| \u2264\u221an and consequently,\nby choosing p = 3 this leads to P\nm\u2032\u2208Mn E|H\u03be,1(m, m\u2032)| \u2264C(\u03c3\u03be,6, \u03c3\u03be)/n. The last term of the\ninequality (4.26) vanishes as soon as\n(4.27)\np2(m, m\u2032) = 4(1 + 2\u01eb2)H\u03be,2(m, m\u2032) = 6(1 + 2\u01eb2)\u03bb1\u03c32\n\u03be\u0393(m\u2217)/n.\nFor this choice of p2(m, m\u2032), the inequality (4.26) becomes E[supt\u2208Bm, \u02c6\nm(0,1) |\u03bdn,2(t)|2\u2212p2(m, \u02c6m)]+\nis less than\n2\nX\nm\u2032\u2208Mn,\u2113\nE[\nsup\nt\u2208Bm,m\u2032 (0,1)\n(n\u22121\nn\nX\ni=1\n\u03bei(u\u2217\nt(Zi) \u2212\u27e8t, g\u27e9))2 \u22122(1 + 2\u01eb2)H2\n\u03be(m, m\u2032)]+\n+ 2\u2225g\u22252\n2\u03c32\n\u03be/n + 4C(1 + 2\u01eb2)/n.\nThen we apply the following Lemma to reach the same kind of result as (4.15) for Wn,1.\nLemma 4.2. Under the assumptions of Theorem 3.1, if E|\u03be1|6 < \u221e, then for some given\n\u01eb2 > 0:\n(4.28)\nX\nm\u2032\u2208Mn,\u2113\nE\n\uf8ee\n\uf8f0\nsup\nt\u2208Bm,m\u2032(0,1)\n \n1\nn\nn\nX\ni=1\n\u03bei(u\u2217\nt(Zi) \u2212\u27e8t, g\u27e9\n!2\n\u22122(1 + 2\u01eb2)H2\n\u03be(m, m\u2032)\n\uf8f9\n\uf8fb\n+\n\u2264K1\n\uf8f1\n\uf8f2\n\uf8f3\nX\nm\u2032\u2208Mn,\u2113\n\u0014\u03c32\n\u03be\u00b52\u03932(m\u2217)\nn\nexp\n\u0012\n\u2212K1\u01eb2\n\u03bb1\u0393(m\u2217)\n\u00b52\u03932(m\u2217)\n\u0013\u0015\n+\n\u0012\n1 + ln4(n)\n\u221an\n\u0013 1\nn\n\uf8fc\n\uf8fd\n\uf8fe,\n20\nF. COMTE AND M.-L. TAUPIN\nwhere \u00b52 and \u03932(m) are de\ufb01ned by (3.8) and (4.17) and K1 is a constant depending on the\nmoments of \u03be. The constant \u00b52 can be replaced by \u03bb2(\u2225h\u22252) where \u03bb2 is de\ufb01ned by (4.18).\nBy analogy with (4.22) we denote by\n(4.29)\nA2(m\u2217) = K1\u03c32\n\u03be\nn\n\u00b52\u03932(m\u2217) exp\n\u0012\n\u2212K1\u01eb2\n\u03bb1\u0393(m\u2217)\n\u00b52\u03932(m\u2217)\n\u0013\n= K1\u03c32\n\u03be\u00b52\u03932(m\u2217)\nn\nexp\n\u0012\n\u2212\u03ba2\u01eb2\n\u03bb1\n\u00b52\nD(1/2\u2212\u03c1/2)+\nm\u2217\n\u0013\n.\nWith p2(m, m\u2032) given by (4.27), by gathering (4.15) and (4.28), we \ufb01nd, for Wn,2 de\ufb01ned by\n(4.13),\nE(Wn,2( \u02c6m)) \u2264K\nX\nm\u2032\u2208Mn\nA2(m\u2217) + C(1 + ln(n)6/n)/n + K\u2032/n.\nThe sum P\nm\u2032\u2208Mn A2(m\u2217) is bounded in the same way as the sum P\nm\u2032\u2208Mn A1(m\u2217) with \u01eb2 =\n\u01eb1 = 1/2 if 0 \u2264\u03c1 < 1/3 and \u01eb1(m, m\u2032) replaced by \u01eb2 = \u01eb2(m, m\u2032) = E(f 2(X1))\u01eb1(m, m\u2032), when\n\u03c1 \u22651/3 that is \u01eb2(m, m\u2032) = (4\u03b2\u03c3\u03c1\u03c0\u03c1\u00b52)/(K1\u03bb1)D\u03c1\u2212\u03c9\nm\u2217. These choices ensure that P\nm\u2032\u2208Mn,\u2113A2(m\u2217)\nis less than C/n. The result follows by taking as announced in (4.13), p\u2113(m, m\u2032) = 2p1(m, m\u2032)+\n2p2(m, m\u2032), that is p\u2113(m, m\u2032) = 4[(1 + 2\u01eb1(m, m\u2032))E(f 2(X1)) + 3(1 + 2\u01eb2(m, m\u2032))\u03c32\n\u03be]\u03bb1\u0393(m\u2217)/n,\nand more precisely if 0 \u2264\u03c1 < 1/3,\np\u2113(m, m\u2032) = 24E(Y 2\n1 )\u03bb1\u0393(m\u2217)/n,\n(4.30)\nand if \u03c1 \u22651/3,\np\u2113(m, m\u2032) = 4[3E(Y 2\n1 ) + 32\u03b2\u03c3\u03c1\u03c0\u03c1\u00b52D\u03c1\u2212\u03c9\nm\u2217/k1\u03bb1]\u03bb1\u0393(m\u2217)/n.\n(4.31)\nConsequently if 0 \u2264\u03c1 < 1/3, we take pen\u2113(m) = \u03baE(Y 2\n1 )\u03bb1\u0393(m)/n, and if \u03c1 \u22651/3 we take\npen\u2113(m) = \u03ba[E(Y 2\n1 ) + \u03b2\u03c3\u03c1\u03c0\u03c1\u00b52D\u03c1\u2212\u03c9\nm\n/k1\u03bb1]\u03bb1\u0393(m)/n, for some numerical constants \u03ba. Note\nthat for \u03c1 = 1/3, \u03c1 \u2212\u03c9 = 0 and the second penalty has the same order as the \ufb01rst one with a\ndi\ufb00erent multiplicative constant.\n\u25a1\n4.4. Proof of Lemma 4.2, by using a conditioning argument. We work conditionally to\nthe \u03bei\u2019s and E\u03be and P\u03be denote the conditional expectations and probability for \ufb01xed \u03be1, . . . , \u03ben.\nWe apply Lemma 4.1 with ft(\u03bei, Zi) = \u03beiu\u2217\nt(Zi), conditionally to the \u03bei\u2019s to the random\nvariables (\u03be1, Z1), . . . , (\u03ben, Zn) which are independent but non identically distributed since the\n\u03bei\u2019s are \ufb01xed constants. Let Qj,k = E[u\u2217\n\u03d5m,j(Z1)u\u2217\n\u03d5m,k(\u2212Z1)]. Straightforward calculations give\nthat for H\u03be(m, m\u2032) de\ufb01ned in (4.25) we have\nE2\n\u03be[\nsup\nt\u2208Bm,m\u2032(0,1)\nn\u22121\nn\nX\ni=1\n\u03bei(u\u2217\nt(Zi) \u2212\u27e8t, g\u27e9)] \u2264H2\n\u03be(m, m\u2032).\nAgain, arguing as in Comte et al. (2005a), P\nj,k |Qj,k|2 \u2264\u22062(m, h) \u2264\u03bb2(\u2225h\u22252)\u03932(m, \u2225f\u03b5\u22252)\nwith \u2225h\u22252 \u2264\u2225f\u03b5\u22252, where \u22062(m, h) is de\ufb01ned by (4.17), \u03bb2 by (4.18), \u03932(m) by (4.17), \u00b52 by\nNONPARAMETRIC ESTIMATION IN AN ERROR-IN-VARIABLES MODEL\n21\n(3.8). We now write that\nsup\nt\u2208Bm,m\u2032(0,1)\n(n\u22121\nn\nX\ni=1\nVar\u03be(\u03beiu\u2217\nt(Zi)))\n\u2264\n(n\u22121\nn\nX\ni=1\n\u03be2\ni )\u00b52\u03932(m\u2217, \u2225f\u03b5\u22252)\nand thus we take\nv\u03be(m, m\u2032) = (n\u22121\nn\nX\ni=1\n\u03be2\ni )\u00b52\u03932(m\u2217, \u2225f\u03b5\u22252).\nLastly, since\nsup\nt\u2208Bm,m\u2032 (0,1)\n\u2225ft\u2225\u221e\u22642 max\n1\u2264i\u2264n |\u03bei|\np\n\u2206(m\u2217) \u22642 max\n1\u2264i\u2264n |\u03bei|\np\n\u03bb1\u0393(m\u2217)\nwe take M1,\u03be(m, m\u2032) = 2 max1\u2264i\u2264n |\u03bei|\np\n\u03bb1\u0393(m\u2217). By applying Lemma 4.1, we get for some\nconstants \u03ba1, \u03ba2, \u03ba3\nE\u03be[\nsup\nt\u2208Bm,m\u2032(0,1)\n\u03bd2\nn,1(t) \u22122(1 + 2\u01eb)H2\n\u03be]+ \u2264\u03ba1\n\"\n\u00b52\u03932(m\u2217)\nn2\n(\nn\nX\ni=1\n\u03be2\ni ) exp\n\u001a\n\u2212\u03ba2\u01eb \u03bb1\u0393(m\u2217)\n\u00b52\u03932(m\u2217)\n\u001b\n+\u03bb1\u0393(m\u2217)\nn2\n( max\n1\u2264i\u2264n \u03be2\ni ) exp\n(\n\u2212\u03ba3\n\u221a\u01ebC(\u01eb)\npPn\ni=1 \u03be2\ni\nmaxi |\u03bei|\n)#\nTo relax the conditioning, it su\ufb03ces to integrate with respect to the law of the \u03bei\u2019s the above\nexpression. The \ufb01rst term in the bound simply becomes:\n\u03c32\n\u03be\u00b52\u03932(m\u2217) exp[\u2212\u03ba2\u01eb\u03bb1\u0393(m\u2217)/(\u00b52\u03932(m\u2217)])/n\nand has the same order as in the case of bounded variables. The second term is bounded by\n(4.32)\n\u03bb1\u0393(m\u2217)\nn2\nE\n\"\n(max |\u03bei|2) exp\n \n\u2212\u03ba3\n\u221a\u01ebC(\u01eb)\npPn\ni=1 \u03be2\ni\nmax1\u2264i\u2264n |\u03bei|\n!#\n.\nSince we only consider dimensions Dm such that the penalty term is bounded, we have\n\u0393(m)/n \u2264K and the sum of the above terms for m \u2208Mn,\u2113and |Mn,\u2113| \u2264n is less than\n\u03bb1E\n\"\u0012\nmax\n1\u2264i\u2264n \u03be2\ni\n\u0013\nexp\n \n\u2212\u03ba3\n\u221a\u01ebC(\u01eb)\npPn\ni=1 \u03be2\ni\nmax1\u2264i\u2264n |\u03bei|\n!#\n.\nWe need to study when such a term is less than c/n for some constant c. We bound maxi |\u03bei|\nby b on the set {maxi |\u03bei| \u2264b} and the exponential by 1 on the set {maxi |\u03bei| \u2265b} and by\n22\nF. COMTE AND M.-L. TAUPIN\ndenoting \u00b5\u01eb = \u03ba3\n\u221a\u01ebC(\u01eb), this yields\nE\n\"\nmax\n1\u2264i\u2264n \u03be2\ni exp\n \n\u2212\u00b5\u01eb\ns\nPn\ni=1 \u03be2\ni\nmax1\u2264i\u2264n \u03be2\ni\n!#\n\u2264\nb2E\n \nexp(\u2212\u00b5\u01eb\npPn\ni=1 \u03be2\ni\nb\n)\n!\n+ E\n\u0012\nmax\n1\u2264i\u2264n \u03be2\ni 1I{max1\u2264i\u2264n |\u03bei|\u2265b}\n\u0013\n\u2264\nb2\n\"\nE\n\u0010\nexp(\u2212\u00b5\u01eb\nq\nn\u03c32\n\u03be/(2b2)\n\u0011\n+ P\n \n| 1\nn\nn\nX\ni=1\n\u03be2\ni \u2212\u03c32\n\u03be| \u2265\u03c32\n\u03be/2\n!#\n+ b\u2212rE( max\n1\u2264i\u2264n |\u03bei|r+2)\n\u2264\nb2e\u2212\u00b5\u01eb\n\u221an\u03c3\u03be/(\n\u221a\n2b) + b22p\u03c3\u22122p\n\u03be\nE\n \f\f\f\f\f\n1\nn\nn\nX\ni=1\n\u03be2\ni \u2212\u03c32\n\u03be\n\f\f\f\f\f\np!\n+ b\u2212rE( max\n1\u2264i\u2264n |\u03bei|r+2).\nAgain by applying Rosenthal\u2019s inequality (see Rosenthal (1970)), we get that\nE\n\"\nmax\n1\u2264i\u2264n \u03be2\ni exp\n \n\u2212\u00b5\u01eb\ns\nPn\ni=1 \u03be2\ni\nmax1\u2264i\u2264n \u03be2\ni\n!#\n\u2264b2e\u2212\u00b5\u01eb\n\u221an\u03c3\u03be/(\n\u221a\n2b) + b2 2p\n\u03c32p\n\u03be\nC(p)\nnp [nE(|\u03be2\n1 \u2212\u03c32\n\u03be|p) + (nE(\u03be4\n1))p/2] + nE(|\u03be1|r+2)b\u2212r\nalso bounded by\nb2e\u2212\u00b5\u01eb\n\u221an\u03c3\u03be/(\n\u221a\n2b) + C\u2032(p)b2\u03c32p\n\u03be,2p2p\u03c3\u22122p\n\u03be\n[n1\u2212p + n\u2212p/2] + n\u03c3r+2\n\u03be,r+2b\u2212r.\nSince E|\u03be1|6 < \u221e, we take p = 3, r = 4, b = \u03c3\u03be\n\u221a\u01ebC(\u01eb)\u03ba3\n\u221an/[2\n\u221a\n2(ln(n) \u2212ln ln n)] and for any\nn \u22653, and for C1 and C2 some constants depending on the moments of \u03be, we \ufb01nd that\nE\n\uf8f1\n\uf8f2\n\uf8f3\n\u0012\nmax\n1\u2264i\u2264n \u03be2\ni\n\u0013\nexp\n\uf8eb\n\uf8ed\u2212\u03ba3\n\u221a\u01ebC(\u01eb)\nv\nu\nu\nt\nn\nX\ni=1\n\u03be2\ni / max\n1\u2264i\u2264n \u03be2\ni\n\uf8f6\n\uf8f8\n\uf8fc\n\uf8fd\n\uf8fe\u2264C1\n\u221an + C2\n\u0012ln4(n)\n\u221an\n\u0013 1\n\u221an.\nThen the sum over Mn,\u2113with cardinality less than \u221an of the terms in (4.32) is bounded by\nC(1 + ln(n)4/\u221an)/n for some constant C, by using again that \u0393(m\u2217)/n is bounded.\n4.5. Proof of Theorem 3.2. Let \u02dcEn be the event \u02dcEn = {\u2225g\u2212\u02dcg \u2225\u221e,A\u2264g0/2}. Since g(x) \u2265g0\nfor any x in A, then, on \u02dcEn, \u02dcg(x) \u2265g0/2 also for any x in A. It follows that\nE\u2225(f \u2212\u02dcf)1IA1I \u02dcEn\u22252\n2 \u22648g\u22122\n0 E\u2225\u02dc\u2113\u2212\u2113\u22252\n2 + 8\u2225\u2113\u22252\n\u221e,Ag\u22124\n0 E\u2225\u02dcg \u2212g\u22252\n2,\n(4.33)\nwhere \u2225\u2113\u2225\u221e,A \u2264g1\u03ba\u221e,G. Using that \u2225\u02dcf\u2225\u221e,A \u2264an, we obtain\nE[\u2225(f \u2212\u02dcf)1IA1I \u02dcEcn\u22252\n2] \u22642(a2\nn + \u2225f\u22252\n\u221e,A)\u03bb(A)P( \u02dcEc\nn),\n(4.34)\nwhere \u03bb(A) =\nR\nA dx. It follows that for \u02c6m\u2113= \u02c6m\u2113(n), \u02c6mg = \u02c6mg(n), if anP( \u02dcEc\nn) = o(n\u22121), then\n(3.11) is proved by applying Theorem 3.1. We now come to the study of P( \u02dcEc\nn) by writing that\nP( \u02dcEc\nn) = P (\u2225g \u2212\u02dcg\u2225\u221e> g0/2) = P\n\u0010\n\u2225g \u2212g(n)\n\u02c6mg + g(n)\n\u02c6mg \u2212\u02dcg\u2225\u221e> g0/2\n\u0011\n. By applying Lemma 4.3:\nNONPARAMETRIC ESTIMATION IN AN ERROR-IN-VARIABLES MODEL\n23\nLemma 4.3. Let g belongs to Sag,\u03bdg,Bg(\u03baag) de\ufb01ned by (R1) with ag > 1/2. Then for t \u2208Sm,\n\u2225t\u2225\u221e\u2264\u221aDm\u2225t\u22252 and \u2225g \u2212gm\u2225\u221e\u2264(2\u03c0)\u22121\u221a\u03c0Dm((\u03c0Dm)2 + 1)\u2212ag/2 exp(\u2212Bg|\u03c0Dm|rg)A1/2\ng .\nand by arguing as for \u2225\u2113m \u2212\u2113(n)\nm \u22252\n2, we get that \u2225g \u2212g(n)\n\u02c6mg\u2225\u221e\u2264\u2225g \u2212g \u02c6mg\u2225\u221e+ \u2225g \u02c6mg \u2212g(n)\n\u02c6mg\u2225\u221e\nalso bounded by\np\n\u03ba(\u03baG + 1)D3/2\n\u02c6mg /\np\nkn + (2\u03c0)\u22121q\n\u03c0D \u02c6mg((\u03c0D \u02c6mg)2 + 1)\u2212ag/2 exp(\u2212Bg|\u03c0D \u02c6mg|\u03bdg)A1/2\ng .\nConsequently, \u2225g\u2212g(n)\n\u02c6mg\u2225\u221etends to zero as soon as g belongs to some space Sag,\u03bdg,Bg(\u03baag) de\ufb01ned\nby (R1) with ag > 1/2 if rg = 0 and since kn \u2265n3/2 and D \u02c6mg = o(\u221an) for \u03b1 > 1/2. It follows\nthat for n large enough, \u2225g \u2212g(n)\n\u02c6mg\u2225\u221e\u2264g0/4 and consequently P( \u02dcEc\nn) \u2264P[\u2225g(n)\n\u02c6mg \u2212\u02dcg\u2225\u221e> g0/4].\nBy applying again Lemma 4.3, since g(n)\n\u02c6mg \u2212\u02dcg belongs to S \u02c6mg, we get that\nP( \u02dcEc\nn) \u2264P[\u2225g(n)\n\u02c6mg \u2212\u02dcg\u22252 > g0/(4\nq\nD \u02c6mg)].\n(4.35)\nIn this context, we have\n\u2225g(n)\n\u02c6mg \u2212\u02dcg \u02c6mg\u22252\n2\n=\nX\n|j|\u2264kn\n(\u02c6a \u02c6mg,j \u2212a \u02c6mg,j)2 =\nX\n|j|\u2264kn\n\u03bd2\nn,g(\u03d5 \u02c6mg,j) =\nsup\nt\u2208B \u02c6\nmg (0,1)\n\u03bd2\nn,g(t).\n(4.36)\nConsequently,\nP( \u02dcEc\nn)\n\u2264\nP[\nsup\nt\u2208B \u02c6\nmg (0,1)\n|\u03bdn,g(t)| \u2265g0/(4\nq\nD \u02c6mg)] \u2264sup\nm\u2208Mn\nP[\nsup\nt\u2208B \u02c6\nmg (0,1)\n|\u03bdn,g(t)| \u2265g0/(4\np\nDm)]\n\u2264\nX\nm\u2208Mn\nP[\nsup\nt\u2208B \u02c6\nmg (0,1)\n|\u03bdn,g(t)| \u2265g0/(4\np\nDm)].\nWe apply Talagrand\u2019s (1996) inequality as given in Birg\u00b4e and Massart (1998), to get that if\nwe take \u03bb = g0/(8\u221aDm) and if we ensure 2H < g0/(8\u221aDm), then P[supt\u2208Bm(0,1) |\u03bdn,g(t)| \u2265\ng0/(4\u221aDm)] \u22643 exp\n\u0002\n\u2212K\u2032\n1n\n\u0000min[(Dmv)\u22121, (M1\n\u221aDm)\u22121]\n\u0001\u0003\n. This yields\nP( \u02dcEc\nn)\n\u2264\nK\nX\nm\u2208Mn\n{exp[\u2212K\u2032\n1n/(M1\np\nDm)] + exp[\u2212K\u2032\n1n/(Dmv)]}.\n(4.37)\nSince we only consider Dm such that Dm \u2264\u221an,\nan|Mn| exp[\u2212K\u2032\n1n/(M1\np\nDm)] \u2264an|Mn| exp(\u2212K\u201dn1/4) = o(n\u22121).\nWe only consider Dm such that \u0393(m)/n tends to zero. Consequently, when \u03c1 > 0 then Dm \u2264\n(ln n/(2\u03b2\u03c3\u03c1 + 1))1/\u03c1 which combined with the fact that v \u2264D2\u03b1+1\u2212\u03c1\nm\nexp(2\u03b2\u03c3\u03c1\u03c0\u03c1D\u03c1\nm) gives that\nan|Mn| exp (\u2212K\u2032\n1n/(Dmv)) = o(1/n).\nWhen \u03c1 = 0, then v = \u00b51D2\u03b1+1/2\nm\nand consequently, as Dm \u2264(n/ ln(n))1/(2\u03b1+1) \u2264n1/(2\u03b1+1),\nexp(\u2212K\u2032\n1n/(Dmv)) \u2264exp(\u2212K\u201dn/(D2\u03b1+3/2\nm\n)) \u2264exp(\u2212K\u201dn1/(4(\u03b1+1))).\nAnalogously, \u221aDmH \u22641/\np\nln(n) in the worst case corresponding to \u03c1 = 0, for Dm \u2264\n(n/ ln(n))1/(2\u03b1+2), tends to zero and therefore is bounded by g0/8 for n great enough.\nWe\n24\nF. COMTE AND M.-L. TAUPIN\nconclude that if we only consider Dm such that Dm \u2264n1/(2\u03b1+2) then anP( \u02dcEc\nn) = o(1/n), and\nthe result follows by applying the inequalities (4.33) and (4.34).\n\u25a1\nProof of Lemma 4.3. For t \u2208Sm, written as t(x) = P\nj\u2208Z\u27e8t, \u03d5m,j\u27e9\u03d5m,j(x) and |t(x)|2 \u2264\nP\nj\u2208Z |\u27e8t, \u03d5m,j\u27e9|2 P\nj\u2208Z |(\u03d5\u2217\nm,j)\u2217(\u2212x)|2/(2\u03c0)2 with by applying Parseval\u2019s Formula\nX\nj\u2208Z\n|\u27e8t, \u03d5m,j\u27e9|2 X\nj\u2208Z\n|(\u03d5\u2217\nm,j)\u2217(\u2212x)|2/(2\u03c0)2 = \u2225t\u22252\n2Dm\nZ\n\u03d5\u2217(u)2du/(2\u03c0) = Dm\u2225t\u22252\n2.\nLet b such that 1/2 < b < ag. Since \u2225g \u2212gm\u2225\u221e\u2264(2\u03c0)\u22121 R\n|x|\u2265\u03c0Dm |g\u2217(x)|dx we get that\n\u2225g \u2212gm\u2225\u221e\n\u2264\n(2\u03c0)\u22121((\u03c0Dm)2 + 1)\u2212(ag\u2212b)/2e\u2212Bg|\u03c0Dm|rg Z\n|x|\u2265\u03c0Dm\n|g\u2217(x)|(x2 + 1)(ag\u2212b)/2eBg|x|rgdx\nalso bounded by\n1\n2\u03c0((\u03c0Dm)2 + 1)\u2212(ag\u2212b)/2 exp(\u2212Bg|\u03c0Dm|rg)\u03ba1/2\nag\nsZ\n|x|\u2265\u03c0Dm\n(x2 + 1)\u2212bdx\n\u2264(2\u03c0)\u22121((\u03c0Dm)2 + 1)\u2212(ag\u2212b)/2 exp(\u2212Bg|\u03c0Dm|rg)\u03ba1/2\nag (\u03c0Dm)1/2\u2212b\n\u2264(2\u03c0)\u22121p\n\u03c0Dm((\u03c0Dm)2 + 1)\u2212ag/2 exp(\u2212Bg|\u03c0Dm|rg)\u03ba1/2\nag .\n\u25a1\nReferences\n[1] Birg\u00b4e, L. and Massart, P. (1998) Minimum contrast estimators on sieves : exponential bounds and rates of\nconvergence. Bernoulli 4 329-375.\n[2] Butucea, C. (2004) Deconvolution of supersmooth densities with smooth noise. The Canadian Journal of\nStatistics 32(2). 181\u2013192\n[3] Butucea, C. and Tsybakov, A.B. (2004) Sharp optimality and some e\ufb00ects of dominating bias in density\ndeconvolution, Preprint LPMA-898, http://www.proba.jussieu.fr/mathdoc/preprints/index.html#2004.\n[4] Carroll, R.J. (1999) Nonparametric regression in the presence of measurement error. Biometrika 86 (3)\n541-554\n[5] Comte, F. and Taupin, M.-L. (2004). Nonparametric estimation of the regression function in an errors-in-\nvariables model. Preprint MAP5 2004-20, http://www.math-info.univ-paris5.fr/map5/publis/.\n[6] Comte, F., Rozenholc, Y. and Taupin, M.-L. (2005a). Penalized contrast estimator for density deconvolu-\ntion. Revised version of the Preprint MAP5 2003-2, http://www.math-info.univ-paris5.fr/map5/publis/.\n[7] Comte, F., Rozenholc, Y. and Taupin, M.-L. (2005b). Finite sample penalization in adaptive deconvolution.\nPreprint MAP5 2005-11, http://www.math-info.univ-paris5.fr/map5/publis/.\n[8] Fan, J. (1991a) On the optimal rates of convergence for nonparametric deconvolution problems. Ann.\nStatist. 19, 1257-1272.\n[9] Fan, J. (1991b). Global behaviour of deconvolution kernel estimates. Statist. Sinica 1 (2) 541-551.\n[10] Fan, J. and Gijbels, I. (1996) Local polynomial modelling and its applications. Monographs on Statistics\nand Applied Probability. 66. London: Chapman & Hall.\nNONPARAMETRIC ESTIMATION IN AN ERROR-IN-VARIABLES MODEL\n25\n[11] Fan, J. and Masry, M. (1992) Multivariate regression estimation with errors-in-variables: asymptotic nor-\nmality for mixing processes. J. Multivariate Anal. 43, 237-272.\n[12] Fan, J. and Truong, Y.K. (1993) Nonparametric regression with errors-in-variables. Ann. Statist. 21, 4,\n1900-1925.\n[13] Fan, J., Truong, Y.K. and Wang, Y. (1991) Nonparametric function estimation involving errors-in-variables.\nNonparametric Functional Estimation and Related Topics 613-627.\n[14] Ioannides, D.A. and Alevizos, P. D. (1997) Nonparametric regression with errors in variables and applica-\ntions. Statist. Probab. Lett. 32, 35-43.\n[15] Koo, J.-Y. and Lee K.-W. (1998) B-spline estimation of regression functions with errors in variable. Statist.\nProbab. Lett. 40, 57-66.\n[16] Ledoux, M. (1996) On Talagrand\u2019s deviation inequalities for product measures. ESAIM Probab. Statist.,\n63-87.\n[17] Masry, E. (1993). Multivariate regression estimation with errors-in-variables for stationary processes. Non-\nparametric statistics. 3, 13-36.\n[18] Rosenthal, H.P. (1970) On the subspaces of Lp, (p > 2), spanned by sequences of independent random\nvariables. Israel J. Math. 8, 273\u2013303.\n[19] Talagrand, M. (1996) New concentration inequalities in product spaces. Invent. Math. 126, 505-563.\n[20] Tsybakov, A.B. (1986) Robust reconstruction of functions by the local-approximation method. Problems\nof Information Transmission 22, 133-146.\n"}