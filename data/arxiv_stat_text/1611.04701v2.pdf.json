{"text": "Electronic Journal of Statistics\nVol. 0 (2017) 1\u201399\nISSN: 1935-7524\nErrors-in-variables models with dependent\nmeasurements\nMark Rudelson\u2217and Shuheng Zhou\u2020\nDepartment of Mathematics, Department of Statistics\nUniversity of Michigan, Ann Arbor, MI 48109\ne-mail: rudelson@umich.edu; shuhengz@umich.edu\nAbstract:\nSuppose that we observe y \u2208Rn and X \u2208Rn\u00d7m in the following errors-in-\nvariables model:\ny\n=\nX0\u03b2\u2217+ \u03f5\nX\n=\nX0 + W\nwhere X0 is an n \u00d7 m design matrix with independent subgaussian row vectors,\n\u03f5 \u2208Rn is a noise vector and W is a mean zero n \u00d7 m random noise matrix with\nindependent subgaussian column vectors, independent of X0 and \u03f5. This model is sig-\nni\ufb01cantly different from those analyzed in the literature in the sense that we allow\nthe measurement error for each covariate to be a dependent vector across its n ob-\nservations. Such error structures appear in the science literature when modeling the\ntrial-to-trial \ufb02uctuations in response strength shared across a set of neurons.\nUnder sparsity and restrictive eigenvalue type of conditions, we show that one is\nable to recover a sparse vector \u03b2\u2217\u2208Rm from the model given a single observation ma-\ntrix X and the response vector y. We establish consistency in estimating \u03b2\u2217and obtain\nthe rates of convergence in the \u2113q norm, where q = 1, 2 for the Lasso-type estimator,\nand for q \u2208[1, 2] for a Dantzig-type Conic programming estimator. We show error\nbounds which approach that of the regular Lasso and the Dantzig selector in case the\nerrors in W are tending to 0. We analyze the convergence rates of the gradient descent\nmethods for solving the nonconvex programs and show that the composite gradient de-\nscent algorithm is guaranteed to converge at a geometric rate to a neighborhood of the\nglobal minimizers: the size of the neighborhood is bounded by the statistical error in\nthe \u21132 norm. Our analysis reveals interesting connections between computational and\nstatistical ef\ufb01ciency and the concentration of measure phenomenon in random matrix\ntheory. We provide simulation evidence illuminating the theoretical predictions.\nMSC 2010 subject classi\ufb01cations: Primary 60K35, 60K35; secondary 60K35.\nKeywords and phrases: Errors-in-variable models, measurement error data, subgaus-\nsian concentration, matrix variate distributions, nonconvexity.\nContents\n1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n\u2217Mark Rudelson is partially supported by NSF grant DMS 1161372 and USAF Grant FA9550-14-1-\n0009.\n\u2020Shuheng Zhou is supported in part by NSF under Grant DMS-1316731 and Elizabeth Caroline Crosby\nResearch Award from the Advance Program at the University of Michigan.\n1\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\narXiv:1611.04701v2  [stat.ML]  1 Apr 2017\nRudelson and Zhou/Errors-in-variables\n2\n1.1\nThe model and the method . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.2\nGradient descent algorithms\n. . . . . . . . . . . . . . . . . . . . . .\n6\n1.3\nOur contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.4\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2\nAssumptions and preliminary results . . . . . . . . . . . . . . . . . . . . .\n11\n3\nMain results on the statistical error . . . . . . . . . . . . . . . . . . . . . .\n16\n3.1\nRegarding the MA constant . . . . . . . . . . . . . . . . . . . . . . .\n19\n3.2\nDiscussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4\nImproved bounds when the measurement errors are small . . . . . . . . . .\n22\n4.1\nOracle results on the Lasso-type estimator . . . . . . . . . . . . . . .\n24\n4.2\nOracle results on the Conic programming estimator . . . . . . . . . .\n27\n5\nOptimization error on the gradient descent algorithm\n. . . . . . . . . . . .\n28\n5.1\nDiscussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n6\nProof of theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n6.1\nStochastic error terms . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n6.2\nOutline for proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . .\n33\n6.3\nImproved bounds for the corrected Lasso estimator . . . . . . . . . .\n35\n6.4\nOutline for proof of Theorem 4\n. . . . . . . . . . . . . . . . . . . .\n35\n6.5\nImproved bounds for the DS-type estimator . . . . . . . . . . . . . .\n37\n7\nLower and Upper RE conditions . . . . . . . . . . . . . . . . . . . . . . .\n37\n8\nNumerical results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n8.1\nRelative error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n8.2\nCorrected Lasso via GD versus Conic programming estimator\n. . . .\n43\n8.3\nSensitivity to tuning parameters\n. . . . . . . . . . . . . . . . . . . .\n44\n8.4\nStatistical and optimization error in Gradient Descent . . . . . . . . .\n49\n9\nProof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n10 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n11 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n12 Proof of Theorem 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n13 Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n14 Proof of Theorem 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n14.1 Proof of Corollary 10 . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n15 Proof of Theorem 12\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n16 Conclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\nAcknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\nA Outline\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\nB\nProof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\nC\nSome auxiliary results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\nC.1\nProof of Lemma 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nC.2\nProof of Lemma 11 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\nD Proof of Corollary 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\nE\nProof of Corollary 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\nF\nProof of Lemma 15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\nF.1\nComparing the two type of RE conditions in Theorems 3 and 4 . . . .\n75\nG Proof of Theorem 16\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\nH Proofs for the Lasso-type estimator . . . . . . . . . . . . . . . . . . . . . .\n78\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n3\nH.1\nProof of Lemma 17 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\nH.2\nProof of Lemma 18 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\nI\nProofs for the Conic Programming estimator . . . . . . . . . . . . . . . . .\n81\nI.1\nProof of Lemma 19 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\nI.2\nProof of Lemma 20 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\nI.3\nProof of Lemma 21 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\nJ\nProof for Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\nJ.1\nProof of Lemma 22 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\nJ.2\nProof of Lemma 23 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\nJ.3\nProof of Lemma 24 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\nK Some geometric analysis results\n. . . . . . . . . . . . . . . . . . . . . . .\n86\nL\nProof of Corollary 25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\nM Proof of Theorem 26\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\nN Proof of Lemma 32 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\nO Proof of Lemmas 40 and 41 and Corollary 42 . . . . . . . . . . . . . . . .\n94\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n96\n1. Introduction\nThe matrix variate normal model has a long history in psychology and social sciences.\nIn recent years, it is becoming increasingly popular in biology and genomics, neu-\nroscience, econometric theory, image and signal processing, wireless communication,\nand machine learning; see for example [15, 22, 17, 52, 5, 54, 18, 2, 26] and references\ntherein. We call the random matrix X, which contains n rows and m columns a sin-\ngle data matrix, or one instance from the matrix variate normal distribution. We say\nthat an n \u00d7 m random matrix X follows a matrix normal distribution with a sep-\narable covariance matrix \u03a3X = A \u2297B and mean M \u2208Rn\u00d7m, which we write\nXn\u00d7m \u223cNn,m(M, Am\u00d7m \u2297Bn\u00d7n). This is equivalent to say vec { X } follows a\nmultivariate normal distribution with mean vec { M } and covariance \u03a3X = A \u2297B.\nHere, vec { X } is formed by stacking the columns of X into a vector in Rmn. In-\ntuitively, A describes the covariance between columns of X, while B describes the\ncovariance between rows of X. See [15, 22] for more characterization and examples.\nIn this paper, we introduce the related sum of Kronecker product models to encode the\ncovariance structure of a matrix variate distribution. The proposed models and methods\nincorporate ideas from recent advances in graphical models, high-dimensional regres-\nsion model with observation errors, and matrix decomposition. Let Am\u00d7m, Bn\u00d7n be\nsymmetric positive de\ufb01nite covariance matrices. Denote the Kronecker sum of A =\n(aij) and B = (bij) by\n\u03a3\n=\nA \u2295B := A \u2297In + Im \u2297B\n=\n\uf8ee\n\uf8ef\uf8ef\uf8f0\na11In + B\na12In\n. . .\na1mIn\na21In\na22In + B\n. . .\na2mIn\n. . .\nam1In\nam2In\n. . .\nammIn + B\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n(mn)\u00d7(mn)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n4\nwhere In is an n \u00d7 n identity matrix. This covariance model arises naturally from the\ncontext of errors-in-variables regression model de\ufb01ned as follows.\nSuppose that we observe y \u2208Rn and X \u2208Rn\u00d7m in the following model:\ny\n=\nX0\u03b2\u2217+ \u03f5\n(1.1a)\nX\n=\nX0 + W\n(1.1b)\nwhere X0 is an n \u00d7 m design matrix with independent row vectors, \u03f5 \u2208Rn is a noise\nvector and W is a mean zero n \u00d7 m random noise matrix, independent of X0 and \u03f5,\nwith independent column vectors \u03c91, . . . , \u03c9m.\nIn particular, we are interested in the additive model of X = X0 + W such that\nvec { X } \u223cN(0, \u03a3)\nwhere\n\u03a3 = A \u2295B := A \u2297In + Im \u2297B\n(1.2)\nwhere we use one covariance component A \u2297In to describe the covariance of matrix\nX0 \u2208Rn\u00d7m, which is considered as the signal matrix, and the other component Im\u2297B\nto describe that of the noise matrix W \u2208Rn\u00d7m, where E\u03c9j \u2297\u03c9j = B for all j,\nwhere \u03c9j denotes the jth column vector of W. Our focus is on deriving the statistical\nproperties of two estimators for estimating \u03b2\u2217in (1.1a) and (1.1b) despite the presence\nof the additive error W in the observation matrix X. We will show that our theory and\nanalysis works with a model much more general than that in (1.2), which we will de\ufb01ne\nin Section 1.1.\nBefore we go on to de\ufb01ne our estimators, we now use an example to motiviate (1.2) and\nits subgaussian generalization in (1.4). Suppose that there are n patients in a particular\nstudy, for which we use X0 to model the \u201csystolic blood pressure\u201d and W to model the\nseasonal effects. In this case, X models the fact that among the n patients we measure,\neach patient has its own row vector of observed set of blood pressures across time,\nand each column vector in W models the seasonal variation on top of the true signal\nat a particular day/time. Thus we consider X as measurement of X0 with W being\nthe observation error. That is, we model the seasonal effects on blood pressures across\na set of patients in a particular study with a vector of dependent entries. Thus W is a\nmatrix which consists of repeated independent sampling of spatially dependent vectors,\nif we regard the individuals as having spatial coordinates, for example, through their\ngeographic locations. We will come back to discuss this example in Section 1.4.\n1.1. The model and the method\nWe \ufb01rst need to de\ufb01ne an independent isotropic vector with subgaussian marginals as\nin De\ufb01nition 1.1. For a vector y = (y1, . . . , yp) in Rp, denote by \u2225y\u22252 =\nqP\nj y2\nj the\nlength of y.\nDe\ufb01nition 1.1. Let Y be a random vector in Rp\n1. Y is called isotropic if for every y \u2208Rp, E\n\u0010\n| \u27e8Y, y \u27e9|2\u0011\n= \u2225y\u22252\n2.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n5\n2. Y is \u03c82 with a constant \u03b1 if for every y \u2208Rp,\n\u2225\u27e8Y, y \u27e9\u2225\u03c82 := inf{t : E\n\u0000exp( \u27e8Y, y \u27e92/t2)\n\u0001\n\u22642} \u2264\u03b1 \u2225y\u22252 .\n(1.3)\nThe \u03c82 condition on a scalar random variable V is equivalent to the subgaussian tail\ndecay of V , which means P (|V | > t) \u22642 exp(\u2212t2/c2), for all t > 0.\nThroughout this paper, we use \u03c82 vector, a vector with subgaussian marginals and\nsubgaussian vector interchangeably.\nThe model. Let Z be an n \u00d7 m random matrix with independent entries Zij satisfying\nEZij = 0, 1 = EZ2\nij \u2264\u2225Zij\u2225\u03c82 \u2264K. Let Z1, Z2 be independent copies of Z. Let\nX = X0 + W\n(1.4)\nsuch that X0 = Z1A1/2 is the design matrix with independent subgaussian row vectors,\nand W = B1/2Z2 is a random noise matrix with independent subgaussian column\nvectors.\nAssumption (A1) allows the covariance model in (1.2) and its subgaussian variant\nin (1.4) to be identi\ufb01able.\n(A1) We assume tr(A) = m is a known parameter, where tr(A) denotes the trace of\nmatrix A.\nIn the Kronecker sum model, we could assume we know tr(B), in order not to assume\nknowing tr(A). Assuming one or the other is known is unavoidable as the covariance\nmodel is not identi\ufb01able otherwise. Moreover, by knowing tr(A), we can construct an\nestimator for tr(B):\nbtr(B)\n=\n1\nm\n\u0000\u2225X\u22252\nF \u2212ntr(A)\n\u0001\n+\nand de\ufb01ne\nb\u03c4B := 1\nn btr(B) \u22650\n(1.5)\nwhere (a)+ = a \u22280 and \u2225X\u22252\nF := P\ni\nP\nj X2\nij. We \ufb01rst introduce the corrected Lasso\nestimator, adapted from those as considered in [30].\nSuppose that btr(B) is an estimator for tr(B); for example, as constructed in (1.5). Let\nb\u0393\n=\n1\nnXT X \u22121\nn\nbtr(B)Im\nand b\u03b3 =\n1\nnXT y.\n(1.6)\nFor a chosen penalization parameter \u03bb \u22650, and parameters b0 and d, we consider the\nfollowing regularized estimation with the \u21131-norm penalty,\nb\u03b2\n=\narg min\n\u03b2:\u2225\u03b2\u22251\u2264b0\n\u221a\nd\n1\n2\u03b2T b\u0393\u03b2 \u2212\u27e8b\u03b3, \u03b2 \u27e9+ \u03bb\u2225\u03b2\u22251,\n(1.7)\nwhich is a variation of the Lasso [48] or the Basis Pursuit [12] estimator. Although in\nour analysis, we set b0 \u2265\u2225\u03b2\u2217\u22252 and d = |supp(\u03b2\u2217)| :=\n\f\f{j : \u03b2\u2217\nj \u0338= 0}\n\f\f for simplicity,\nin practice, both b0 and d are understood to be parameters chosen to provide an upper\nbound on the \u21132 norm and the sparsity of the true \u03b2\u2217.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n6\nFor a vector \u03b2 \u2208Rm, denote by \u2225\u03b2\u2225\u221e:= maxj |\u03b2j|. Recently, [3] discussed the\nfollowing conic programming compensated matrix uncertainly (MU) selector , which\nis a variant of the Dantzig selector [6, 35, 36]. Adapted to our setting, it is de\ufb01ned as\nfollows. Let \u03bb, \u00b5, \u03c4 > 0,\nb\u03b2\n=\narg min\n\b\n\u2225\u03b2\u22251 + \u03bbt : (\u03b2, t) \u2208\u03a5\n\t\nwhere\n(1.8)\n\u03a5\n=\nn\n(\u03b2, t) : \u03b2 \u2208Rm,\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\n\r\r\r\n\u221e\u2264\u00b5t + \u03c9, \u2225\u03b2\u22252 \u2264t\no\nwhere b\u03b3 and b\u0393 are as de\ufb01ned in (1.6) with \u00b5 \u223c\nq\nlog m\nn\n, \u03c9 \u223c\nq\nlog m\nn\n. We refer to this\nestimator as the Conic programming estimator from now on.\n1.2. Gradient descent algorithms\nIn order to obtain fast, approximate solutions to the optimization goal as in (1.10), we\nadopt the computational framework of [1, 30], namely, the composite gradient descent\nmethod due to Nesterov [34] to analyze our computational and statistical errors in an\nintegrated manner. First we denote the population and empirical loss functions by\nL(\u03b2) = 1\n2\u03b2T \u03a3x\u03b2 \u2212\u03b2\u2217T \u03a3x\u03b2\nand\nLn(\u03b2) = 1\n2\u03b2T b\u0393\u03b2 \u2212b\u03b3T \u03b2\n(1.9)\nrespectively. We consider regularizers that are separable across all coordinates and\nwrite\n\u03c1\u03bb(\u03b2) =\nm\nX\ni=1\n\u03c1\u03bb(\u03b2i).\nThroughout this paper, we denote by\n\u03c6(\u03b2) = 1\n2\u03b2T b\u0393\u03b2 \u2212b\u03b3T \u03b2 + \u03c1\u03bb(\u03b2).\nFrom the formulation (1.7), the corrected linear regression estimator is given by mini-\nmizing the penalized loss function \u03c6(\u03b2) subject to the constraint that g(\u03b2) \u2264R:\nb\u03b2 \u2208\narg min\n\u03b2\u2208Rm,g(\u03b2)\u2264R\n\u001a1\n2\u03b2T b\u0393\u03b2 \u2212b\u03b3T \u03b2 + \u03c1\u03bb(\u03b2)\n\u001b\n(1.10)\nwhere g(\u03b2) is a convex function, which is allowed to be identical to \u2225\u03b2\u22251 and R is a\nsecond tuning parameter that is chosen to con\ufb01ne the solution b\u03b2 within the \u21131 ball of\nradius R, while at the same time ensuring that \u03b2\u2217is a feasible solution. The gradient\ndescent method generates a sequence {\u03b2t}\u221e\nt=0 of iterates by \ufb01rst initializing to some\nparameter \u03b20 \u2208Rm, and then for t = 0, 1, 2, . . ., applying the recursive updates:\n\u03b2t+1 =\narg min\n\u03b2\u2208Rm,g(\u03b2)\u2264R\n\u001a\nLn(\u03b2t) + \u27e8\u2207Ln(\u03b2t), \u03b2 \u2212\u03b2t \u27e9+ \u03b6\n2\n\r\r\u03b2 \u2212\u03b2t\r\r2\n2 + \u03c1\u03bb(\u03b2)\n\u001b\n(1.11)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n7\nwhere \u03b6 is the step size parameter.\nMore generally, we consider loss function Ln : Rm \u2192R and \u03c1\u03bb which are possibly\nnonconvex and consider the regularized M-estimator of the form\nb\u03b2 \u2208\narg min\n\u03b2\u2208Rm,g(\u03b2)\u2264R\n{Ln(\u03b2; X) + \u03c1\u03bb(\u03b2)}\n(1.12)\nwhere \u03c1\u03bb : Rm \u2192R is a regularizer depending on a tuning parameter \u03bb > 0. Because\nof this potential nonconvexity, we also include a side constraint in the form of g(\u03b2) \u2264\nR, where\ng(\u03b2) := 1\n\u03bb\nn\n\u03c1\u03bb(\u03b2) + \u00b5\n2 \u2225\u03b2\u22252\n2\no\n(1.13)\nso that this choice of g is convex for properly chosen parameter \u00b5 \u22650 for a class of\nweakly convex penalty functions \u03c1 [51]; See Assumption 1 in [31] where properties of\ng and \u03c1\u03bb are stated in terms of the univariate function \u03c1\u03bb : R \u2192R and the parameter\n\u00b5 \u22650. While our results hold for the general nonconvex penalty \u03c1\u03bb that is weakly con-\nvex in the sense that (1.13) holds for some parameter \u00b5 > 0, we focus our discussion\nto the choice of \u03c1\u03bb(\u03b2) = \u03bb \u2225\u03b2\u22251 and \u00b5 = 0 in the present paper.\n1.3. Our contributions\nWe provide a uni\ufb01ed analysis of the rates of convergence for both the corrected Lasso\nestimator (1.7) and the Conic programming estimator (1.8), which is a Dantzig selector-\ntype, although under slightly different conditions. We will show the rates of conver-\ngence in the \u2113q norm for q = 1, 2 for estimating a sparse vector \u03b2\u2217\u2208Rm in the\nmodel (1.1a) and (1.1b) using the corrected Lasso estimator (1.7) in Theorems 3 and 6,\nand the Conic programming estimator (1.8) in Theorems 4 and 7 for 1 \u2264q \u22642. We\nalso show bounds on the predictive errors for the Conic programming estimator. The\nbounds we derive in Theorems 3 and 4 focus on cases where the errors in W are not too\nsmall in their magnitudes in the sense that \u03c4B := tr(B)/n is bounded from below. For\nthe extreme case when \u03c4B approaches 0, one hopes to recover bounds close to those\nfor the regular Lasso or the Dantzig selector since the effect of the noise in matrix W\non the procedure becomes negligible. We show in Theorems 6 and 7 that this is indeed\nthe case. These results are new to the best of our knowledge.\nLet Z1, Z2 be independent subgaussian random matrices with independent entries (cf. (1.4)).\nIn Theorems 3 to 7, we consider the regression model in (1.1a) and (1.1b) with sub-\ngaussian random design, where X0 = Z1A1/2 is a subgaussian random matrix with\nindependent row vectors, and W = B1/2Z2 is an n \u00d7 m random noise matrix with\nindependent column vectors, This model is signi\ufb01cantly different from those analyzed\nin the literature. For example, unlike the present work, the authors in [30] apply The-\norem 16 which states a general result on statistical convergence properties of the es-\ntimator (1.7) to cases where W is composed of independent subgaussian row vectors,\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n8\nwhen the row vectors of X0 are either independent or follow a Gaussian vector auto-\nregressive model. See also [35, 36, 3] for the corresponding results on the compensated\nMU selectors, variations on the Conic programming estimator (1.8).\nThe second key difference between our framework and the existing work is that we as-\nsume that only one observation matrix X with the single measurement error matrix W\nis available. Assuming (A1) allows us to estimate EW T W as required in the estima-\ntion procedure (1.6) directly, given the knowledge that W is composed of independent\ncolumn vectors. In contrast, existing work needs to assume that the covariance matrix\n\u03a3W :=\n1\nnEW T W of the independent row vectors of W or its functionals are either\nknown a priori, or can be estimated from a dataset independent of X, or from replicated\nX measuring the same X0; see for example [35, 36, 3, 30, 10]. Although the model we\nconsider is different from those in the literature, the identi\ufb01ability issue, which arises\nfrom the fact that we observe the data under an additive error model, is common. Such\nrepeated measurements are not always available or costly to obtain in practice [10]. We\nwill explore such tradeoffs in future work.\nA noticeable exception is the work of [11], which deals with the scenario when the\nnoise covariance is not assumed to be known. We now elaborate on their result, which\nis a variant of the orthogonal matching pursuit (OMP) algorithm [49, 50]. Their sup-\nport recovery result, that is, recovering the support set of \u03b2\u2217, applies only to the case\nwhen both signal matrix and the measurement error matrix have isotropic subgaussian\nrow vectors. In other words, they assume independence among both rows and columns\nin X (X0 and W). Moreover, their algorithm requires the knowledge of the sparsity\nparameter d, which is the number of non-zero entries in \u03b2\u2217, as well as a \u03b2min con-\ndition: minj\u2208supp \u03b2\u2217\n\f\f\u03b2\u2217\nj\n\f\f = \u2126\n\u0012q\nlog m\nn\n(\u2225\u03b2\u2217\u22252 + 1)\n\u0013\n. Under these conditions, they\nrecover essentially the same \u21132-error bounds as in the current work, and [30], where\nthe covariance \u03a3W is assumed to be known.\nFinally, we present in Theorems 2 and 9 the optimization error for the gradient descent\nalgorithms in solving (1.12) and more speci\ufb01cally (1.7). Let b\u03b2 be a global optimizer\nof (1.12). Let \u03bbmax(A) and \u03bbmin(A) be the largest and smallest eigenvalues, and \u03ba(A)\nbe the condition number for matrix A. Let 0 < \u03ba < 1 be a contraction factor to be de-\n\ufb01ned in (2.11). Similar to the work of [1, 30], we show that the geometric convergence\nis not guaranteed to an arbitrary precision, but only to an accuracy related to statistical\nprecision of the problem, measured by the \u21132 error: \u2225b\u03b2 \u2212\u03b2\u2217\u22252\n2 =: \u03b52\nstat between the\nglobal optimizer b\u03b2 and the true parameter \u03b2\u2217.\nMore precisely, our analysis guarantees geometric convergence of the sequence {\u03b2t}\u221e\nt=0\nto a parameter \u03b2\u2217up to a neighborhood of radius de\ufb01ned through the statistical error\nbound \u03b52\nstat\n\u03b42 \u224d\n\u03b52\nstat\n1 \u2212\u03ba\nd log m\nn\n,\nwhere \u03ba is a contraction coef\ufb01cient to be de\ufb01ned (2.11), so that for all t \u2265T \u2217(\u03b4) as\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n9\nin (2.17), \u03b1\u2113\u224d\u03bbmin(A) and \u03b1u \u224d\u03bbmax(A),\n\r\r\r\u03b2t \u2212b\u03b2\n\r\r\r\n2\n2 \u22644\u03b42\n\u03b1\u2113\n+ \u03b1\u2113\u03b52\nstat\n4\n+\n4\u03b44\nb2\n0\u03b1\u2113\u03bbmax(A) = O(\u03b52\nstat)\nfor \u03bb, \u03b6 \u2265\u03b1u appropriately chosen, R = eO(\nq\nn\nlog m) and n = e\u2126(d log m), where the\neO(\u00b7) and e\u2126(\u00b7) symbols hide spectral parameters regarding A and B. To quantify such\nresults, we \ufb01rst need to introduce some conditions in Section 2. See Theorem 2 and\nCorollary 10 for the precise conditions and statements.\n1.4. Discussion\nThe theory on matrix variate normal data show that having replicates will allow one to\nestimate more complicated graphical structures and achieve faster rates of convergence\nunder less restrictive assumptions [56]. Our consistency results in the present work\ndeal with only a single random matrix following the model (1.4), assuming that tr(A)\nis known. With replicates, this assumption can be lifted off immediately. Assume there\nexists a replicate\ne\nX = X0 + f\nW,\n(1.14)\nthen we can use e\nX \u2212X = f\nW \u2212W to estimate B using existing methods. The ra-\ntionale for considering such an option is one may have a repeated measurement of X0\nfor which the errors W and f\nW follow the same error distribution. Such external data\nor knowledge of the noise distribution is needed in order to do inference under such\nadditive measurement error model [10].\nThe second key modeling question is: would each row vector in W for a particular\npatient across all time points be a correlated normal or subgaussian vector as well? It is\nour conjecture that combining the newly developed techniques, namely, the concentra-\ntion of measure inequalities we have derived in the current framework with techniques\nfrom existing work [56], we can handle the case when W follows a matrix normal dis-\ntribution with a separable covariance matrix \u03a3W = C \u2297B, where C is an m \u00d7 m\npositive semi-de\ufb01nite covariance matrix. Moreover, for this type of \u201dseasonal effects\u201d\nas the measurement errors, the time varying covariance model would make more sense\nto model W, which we elaborate in the second example.\nIn neuroscience applications, population encoding refers to the information contained\nin the combined activity of multiple neurons [27]. The relationship between population\nencoding and correlations is complicated and is an area of active investigation, see for\nexample [40, 13]. It becomes more often that repeated measurements (trials) simulta-\nneously recorded across a set of neurons and over an ensemble of stimuli are available.\nIn this context, one can use a random matrix X0 \u223cNn,m(\u00b5, A \u2297B) which follows a\nmatrix-variate normal distribution, or its subgaussian correspondent, to model the en-\nsemble of mean response variables, e.g., the membrane potential, corresponding to the\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n10\ncross-trial average over a set of experiments. Here we use A to model the task corre-\nlations and B to model the baseline correlation structure among all pairs of neurons at\nthe signal level. It has been observed that the onset of stimulus and task events not only\nchange the cross-trial mean response in \u00b5, but also alter the structure and correlation of\nthe noise for a set of neurons, which correspond to the trial-to-trial \ufb02uctuations of the\nneuron responses. We use W to model such task-speci\ufb01c trial-to-trial \ufb02uctuations of a\nset of neurons recorded over the time-course of a variety of tasks. Models as in (1.1a)\nand (1.1b) are useful in predicting the response of set of neurons based on the cur-\nrent and past mean responses of all neurons. Moreover, we could incorporate non-i.i.d.\nnon-Gaussian W = [w1, . . . , wm] with wt = B1/2(t)z(t), where z(1), . . . , z(m) are\nindependent isotropic subgaussian random vectors and B(t) \u227b0 for all t, to model the\ntime-varying correlated noise as observed in the trial-to-trial \ufb02uctuations. It is possi-\nble to combine the techniques developed in the present paper with those in [57, 56] to\ndevelop estimators for A, B and the time varying B(t), which is itself an interesting\ntopic, however, beyond the scope of the current work.\nIn summary, oblivion in \u03a3W and a general dependency condition in the data matrix\nX are not simultaneously allowed in existing work. In contrast, while we assume that\nX0 is composed of independent subgaussian row vectors, we allow rows of W to be\ndependent, which brings dependency to the row vectors of the observation matrix X.\nIn the current paper, we focus on the proof-of-the-concept on using the Kronecker\nsum covariance and additive model to model two way dependency in data matrix X,\nand derive bounds in statistical and computational convergence for (1.7) and (1.8). In\nsome sense, we are considering a parsimonious model for \ufb01tting observation data with\ntwo-way dependencies: we use the signal matrix to encode column-wise dependency\namong covariates in X, and error matrix W to explain its row-wise dependency. When\nreplicates of X or W are available, we are able to study more sophisticated models and\ninference problems, some of which are described earlier in this section.\nWe leave the investigation of this more general modeling framework and relevant\nstatistical questions to future work. We refer to [10] for an excellent survey of the\nclassical as well as modern developments in measurement error models. In future\nwork, we will also extend the estimation methods to the settings where the covari-\nates are measured with multiplicative errors which are shown to be reducible to the\nadditive error problem as studied in the present work [36, 30]. Moreover, we are in-\nterested in applying the analysis and concentration of measure results developed in\nthe current paper and in our ongoing work to the more general contexts and set-\ntings where measurement error models are introduced and investigated; see for ex-\nample [16, 8, 44, 24, 20, 45, 9, 7, 14, 46, 25, 28, 47, 53, 23, 29, 32, 2, 43, 41, 42] and\nreferences therein.\nNotation. Let e1, . . . , ep be the canonical basis of Rp. For a set J \u2282{1, . . . , p}, denote\nEJ = span{ej : j \u2208J}. For a matrix A, we use \u2225A\u22252 to denote its operator norm. For\na set V \u2282Rp, we let conv V denote the convex hull of V . For a \ufb01nite set Y , the car-\ndinality is denoted by |Y |. Let Bp\n1, Bp\n2 and Sp\u22121 be the unit \u21131 ball, the unit Euclidean\nball and the unit sphere respectively. For a matrix A = (aij)1\u2264i,j\u2264m, let \u2225A\u2225max =\nmaxi,j |aij| denote the entry-wise max norm. Let \u2225A\u22251 = maxj\nPm\ni=1 |aij| denote the\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n11\nmatrix \u21131 norm. The Frobenius norm is given by \u2225A\u22252\nF = P\ni\nP\nj a2\nij. Let |A| denote\nthe determinant and tr(A) be the trace of A. The operator or \u21132 norm \u2225A\u22252\n2 is given\nby \u03bbmax(AAT ). For a matrix A, denote by r(A) the effective rank tr(A)/ \u2225A\u22252. Let\n\u2225A\u22252\nF /\u2225A\u22252\n2 denote the stable rank for matrix A. We write diag(A) for a diagonal ma-\ntrix with the same diagonal as A. For a symmetric matrix A, let \u03a5(A) = (\u03c5ij) where\n\u03c5ij = I(aij \u0338= 0), where I(\u00b7) is the indicator function. Let I be the identity matrix.\nFor two numbers a, b, a \u2227b := min(a, b) and a \u2228b := max(a, b). For a function\ng : Rm \u2192R, we write \u2207g to denote a gradient or subgradient, if it exists. We write\na \u224db if ca \u2264b \u2264Ca for some positive absolute constants c, C which are independent\nof n, m or sparsity parameters. Let (a)+ := a \u22280. We write a = O(b) if a \u2264Cb for\nsome positive absolute constants C which are independent of n, m or sparsity parame-\nters. The absolute constants C, C1, c, c1, . . . may change line by line.\n2. Assumptions and preliminary results\nWe will now de\ufb01ne some parameters related to the restricted and sparse eigenvalue\nconditions that are needed to state our main results. We also state a preliminary result\nin Lemma 1 regarding the relationships between the two conditions in De\ufb01nitions 2.1\nand 2.2.\nDe\ufb01nition 2.1. (Restricted eigenvalue condition RE(s0, k0, A)). Let 1 \u2264s0 \u2264p,\nand let k0 be a positive number. We say that a q \u00d7 p matrix A satis\ufb01es RE(s0, k0, A)\ncondition with parameter K(s0, k0, A) if for any \u03c5 \u0338= 0,\n1\nK(s0, k0, A) :=\nmin\nJ\u2286{1,...,p},\n|J|\u2264s0\nmin\n\u2225\u03c5Jc\u22251\u2264k0\u2225\u03c5J\u22251\n\u2225A\u03c5\u22252\n\u2225\u03c5J\u22252\n> 0.\n(2.1)\nwhere \u03c5J represents the subvector of \u03c5 \u2208Rp con\ufb01ned to a subset J of {1, . . . , p}.\nIt is clear that when s0 and k0 become smaller, this condition is easier to satisfy. We\nalso consider the following variation of the baseline RE condition.\nDe\ufb01nition 2.2. (Lower-RE condition) [30] The matrix \u0393 satis\ufb01es a Lower-RE condi-\ntion with curvature \u03b1 > 0 and tolerance \u03c4 > 0 if\n\u03b8T \u0393\u03b8 \u2265\u03b1 \u2225\u03b8\u22252\n2 \u2212\u03c4 \u2225\u03b8\u22252\n1 \u2200\u03b8 \u2208Rm.\nwhere \u2225\u03b8\u22251 := P\nj |\u03b8j|. As \u03b1 becomes smaller, or as \u03c4 becomes larger, the Lower-RE\ncondition is easier to be satis\ufb01ed.\nLemma 1. Suppose that the Lower-RE condition holds for \u0393 := AT A with \u03b1, \u03c4 > 0\nsuch that \u03c4(1 + k0)2s0 \u2264\u03b1/2. Then the RE(s0, k0, A) condition holds for A with\n1\nK(s0, k0, A) \u2265\nr\u03b1\n2 > 0.\nAssume that RE((k0 + 1)2, k0, A) holds. Then the Lower-RE condition holds for \u0393 =\nAT A with\n\u03b1 =\n1\n(k0 + 1)K2(s0, k0, A) > 0\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n12\nwhere s0 = (k0 + 1)2, and \u03c4 > 0 which satis\ufb01es\n\u03bbmin(\u0393) \u2265\u03b1 \u2212\u03c4s0/4.\n(2.2)\nThe condition above holds for any \u03c4 \u2265\n4\n(k0+1)3K2(s0,k0,A) \u22124\u03bbmin(\u0393)\n(k0+1)2 .\nThe \ufb01rst part of Lemma 1 means that, if k0 is \ufb01xed, then smaller values of \u03c4 guarantee\nRE(s0, k0, A) holds with larger s0, that is, a stronger RE condition. The second part\nof the Lemma implies that a weak RE condition implies that the Lower-RE (LRE)\nholds with a large \u03c4. On the other hand, if one assumes RE((k0 + 1)2, k0, A) holds\nwith a large value of k0 (in other words, a strong RE condition), this would imply\nLRE with a small \u03c4. In short, the two conditions are similar but require tweaking the\nparameters. Weaker RE condition implies LRE condition holds with a larger \u03c4, and\nLower-RE condition with a smaller \u03c4, that is, stronger LRE implies stronger RE. We\nprove Lemma 1 in Section 9.\nDe\ufb01nition 2.3. (Upper-RE condition) [30] The matrix \u0393 satis\ufb01es an upper-RE condi-\ntion with smoothness e\u03b1 > 0 and tolerance \u03c4 > 0 if\n\u03b8T \u0393\u03b8 \u2264e\u03b1 \u2225\u03b8\u22252\n2 + \u03c4 \u2225\u03b8\u22252\n1 \u2200\u03b8 \u2208Rm.\nDe\ufb01nition 2.4. De\ufb01ne the largest and smallest d-sparse eigenvalue of a p \u00d7 q matrix\nA to be\n\u03c1max(d, A)\n:=\nmax\nt\u0338=0;d\u2212sparse \u2225At\u22252\n2/ \u2225t\u22252\n2 , where\nd < p,\n(2.3)\nand\n\u03c1min(d, A)\n:=\nmin\nt\u0338=0;d\u2212sparse \u2225At\u22252\n2/ \u2225t\u22252\n2 .\n(2.4)\nBefore stating some general result for the optimization program (1.12) and its impli-\ncations for the Lasso-type estimator (1.7) in terms of statistical and optimization er-\nrors, we need to introduce some more notation and the following assumptions. Let\namax = maxi aii and bmax = maxi bii be the maximum diagonal entries of A and B\nrespectively. In general, under (A1), one can think of \u03bbmin(A) \u22641 and for s \u22651,\n1 \u2264amax \u2264\u03c1max(s, A) \u2264\u03bbmax(A),\n(2.5)\nwhere \u03bbmax(A) denotes the maximum eigenvalue of A.\n(A2) The minimal eigenvalue \u03bbmin(A) of the covariance matrix A is bounded: 1 \u2265\n\u03bbmin(A) > 0.\n(A3) Moreover, we assume that the condition number \u03ba(A) is upper bounded by\nO\n\u0010q\nn\nlog m\n\u0011\nand \u03c4B = O(\u03bbmax(A)).\nThroughout the rest of the paper, s0 \u226532 is understood to be the largest integer chosen\nsuch that the following inequality still holds:\n\u221as0\u03d6(s0) \u2264\u03bbmin(A)\n32C\nr\nn\nlog m where \u03d6(s0) := \u03c1max(s0, A) + \u03c4B\n(2.6)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n13\nwhere we denote by \u03c4B = tr(B)/n and C is to be de\ufb01ned. Denote by\nMA = 64C\u03d6(s0)\n\u03bbmin(A)\n\u226564C.\n(2.7)\nThroughout this paper, we denote by A0 the event that the modi\ufb01ed gram matrix b\u0393 as\nde\ufb01ned in (1.6) satis\ufb01es the Lower as well as Upper RE conditions with\ncurvature\n\u03b1 = 5\n8\u03bbmin(A), smoothness e\u03b1 = 11\n8 \u03bbmax(A)\nand tolerance\n384C2\u03d6(s0)2\n\u03bbmin(A)\nlog m\nn\n\u2264\u03c4 := \u03bbmin(A) \u2212\u03b1\ns0\n\u2264396C2\u03d62(s0 + 1)\n\u03bbmin(A)\nlog m\nn\nfor \u03b1, e\u03b1 and \u03c4 as de\ufb01ned in De\ufb01nitions 2.2 and 2.3, and C, s0, \u03d6(s0) in (2.6).\nTo bound the optimization errors, we show that the corrected linear regression loss\nfunction (1.9) satis\ufb01es the following Restricted Strong Convexity (RSC) and Restricted\nSmoothness (RSM) conditions when the sample size and effective rank of matrix B\nsatisfy certain lower bounds (cf. Theorem 3); namely, for all vectors \u03b20, \u03b21 \u2208Rm and\nT (\u03b21, \u03b20)\n:=\nLn(\u03b21) \u2212Ln(\u03b20) \u2212\u27e8\u2207Ln(\u03b20), \u03b21 \u2212\u03b20 \u27e9,\nwe show that for some parameters (\u03b1\u2113, \u03c4\u2113(Ln)) and (\u03b1u, \u03c4u(Ln)),\nT (\u03b21, \u03b20)\n\u2265\n\u03b1\u2113\n2 \u2225\u03b21 \u2212\u03b20\u22252\n2 \u2212\u03c4\u2113(Ln) \u2225\u03b21 \u2212\u03b20\u22252\n1\nand\n(2.8)\nT (\u03b21, \u03b20)\n\u2264\n\u03b1u\n2 \u2225\u03b21 \u2212\u03b20\u22252\n2 + \u03c4u(Ln) \u2225\u03b21 \u2212\u03b20\u22252\n1 .\n(2.9)\nApplied to (1.12), the composite gradient descent procedure of [34] produces a se-\nquence of iterates {\u03b2t}\u221e\nt=0 via the updates\n\u03b2t+1 =\narg min\n\u03b2\u2208Rm,g(\u03b2)\u2264R\n(\n1\n2\n\r\r\r\r\u03b2 \u2212\n\u0012\n\u03b2t \u2212\u2207Ln(\u03b2t)\n\u03b6\n\u0013\r\r\r\r\n2\n2\n+ \u03c1\u03bb(\u03b2)\n\u03b6\n)\n(2.10)\nwhere 1\n\u03b6 is the step size. Let \u03bd\u2113= 64d\u03c4\u2113(Ln) and \u00af\u03b1\u2113:= \u03b1\u2113\u2212\u03bd\u2113. We show that the\ncomposite gradient updates exhibit a type of globally geometric convergence in terms\nof the compound contraction coef\ufb01cient\n\u03ba\n=\n1 \u2212\u00af\u03b1\u2113\n4\u03b6 + \u03f1\n1 \u2212\u03f1\n,\nwhere\n\u03f1 := 2\u03bd(d, m, n)\n\u03b1\u2113\u2212\u03bd\u2113\n:= 128d\u03c4u(Ln)\n\u00af\u03b1\u2113\n(2.11)\nwhere \u03bd\u2113< \u03b1\u2113/C for some C > 1 to be speci\ufb01ed. Let \u03c4(Ln) = \u03c4\u2113(Ln) \u2228\u03c4u(Ln).\nDe\ufb01ne\n\u03be\n:=\n2\u03c4(Ln)\n1 \u2212\u03f1\n\u0012 \u00af\u03b1\u2113\n4\u03b6 + 2\u03f1 + 5\n\u0013\n> 10\u03c4(Ln).\n(2.12)\nFor simplicity, we present in Theorem 2 the case for \u03c1\u03bb(\u03b2) = \u03bb \u2225\u03b2\u22251 only.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n14\nTheorem 2. Consider the optimization program (1.12) for a radius R such that \u03b2\u2217is\nfeasible. Let g(\u03b2) = 1\n\u03bb\u03c1\u03bb(\u03b2) where \u03c1\u03bb(\u03b2) = \u03bb \u2225\u03b2\u22251. Suppose that the loss function\nLn satis\ufb01es the RSC/RSM conditions (2.8) and (2.9) with parameters (\u03b1\u2113, \u03c4\u2113(Ln)) and\n(\u03b1u, \u03c4u(Ln)) respectively. Let \u03f1, \u03ba and \u03be be de\ufb01ned as in (2.11) and (2.12) respectively.\nSuppose that the regularization parameter is chosen such that for \u03b6 \u2265\u03b1u\n\u03bb\n\u2265\nmax\n\u001a\n12 \u2225\u2207Ln(\u03b2\u2217)\u2225max , 16R\u03be\n(1 \u2212\u03ba)\n\u001b\n.\n(2.13)\nSuppose that \u03ba < 1. Suppose that b\u03b2 is a global minimizer of (1.12). Then for any step\nsize parameter \u03b6 \u2265\u03b1u and tolerance parameter\n\u03b42\n\u2265\nc\u03b52\nstat\n1 \u2212\u03ba\nd log m\nn\n=: \u00af\u03b42,\nwhere\n\u03b52\nstat =\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2\n2 ,\n(2.14)\nthe following hold for all t \u2265T \u2217(\u03b4)\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2)\n\u2264\n\u03b42,\nand for \u03f52 = 16\u03b44\n\u03bb2\n\u22274R2,\n(2.15)\n\r\r\r\u03b2t \u2212b\u03b2\n\r\r\r\n2\n2\n\u2264\n2\n\u00af\u03b1\u2113\n\u0000\u03b42 + 4\u03bd\u03b52\nstat + 4\u03c4(Ln)\u03f52\u0001\n,\n(2.16)\nwhere \u03bd = 64d\u03c4(Ln), \u03c4(Ln) \u224dlog m\nn\n, and\nT \u2217(\u03b4) = 2 log( \u03c6(\u03b20)\u2212\u03c6(b\u03b2)\n\u03b42\n)\nlog(1/\u03ba)\n+ log log\n\u0012\u03bbR\n\u03b42\n\u0013 \u0012\n1 +\nlog 2\nlog(1/\u03ba)\n\u0013\n.\n(2.17)\nWe prove Theorem 2 in Section B. Theorem 2 is similar in spirit to the main result\nTheorem 2 in [1] that deals with a convex loss function, and Theorem 3 in [31] on a\nsimilar setting to the present work. Compared to [31], we simpli\ufb01ed the condition on \u03bb\nby not imposing an upper bound. Moreover, we present re\ufb01ned analysis on the sample\nrequirement and illuminate its dependence upon the condition number \u03ba(A) and the\ntolerance parameter \u03c4 when applied to the corrected linear regression problem (1.10).\nIt is understood throughout the paper that for the same C as in (2.7),\n\u03c4 \u224d\u03c40\nlog m\nn\n,\nwhere\n\u03c40 \u224d400C2\u03d6(s0 + 1)2\n\u03bbmin(A)\n\u2248M 2\nA\u03bbmin(A)/10\n(2.18)\nand it is helpful to consider MA as being upper bounded by O(\u03ba(A)) in view of (2.5)\nand (A3). Toward this end, we prove in Section 5 that under event A0\u2229B0, the RSC and\nRSM conditions as stated in Theorem 2 hold with \u03b1\u2113\u224d\u03bbmin(A) and \u03b1u \u224d\u03bbmax(A)\nand \u03c4\u2113(Ln) = \u03c4u(Ln) \u224d\u03c4; then we have for all t \u2265T \u2217(\u03b4) as de\ufb01ned in (2.17) and for\n\u03b42 \u224d\n\u03b52\nstat\n1\u2212\u03ba\nd log m\nn\n,\n\r\r\r\u03b2t \u2212b\u03b2\n\r\r\r\n2\n2\n\u2264\n4\n\u03b1\u2113\n\u03b42 + \u03b1\u2113\n4 \u03b52\nstat + O\n\u0012\u03b42\u03b52\nstat\nb2\n0\n\u0013\n,\n(2.19)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n15\nwhere 0 < \u03ba < 1 so long as \u03b6 \u224d\u03bbmax(A) and n = \u2126(\u03ba(A)M 2\nAd log m).\nWe now check the conditions on \u03bb in Theorem 2. First, we note that both types of\nconditions on \u03bb are also required in the present paper for the statistical error bounds\nshown in Theorems 3 and 6. We state in Theorem 16 a deterministic result from [30]\non the statistical error for the corrected linear model, which requires that\n\u03bb \u22652 \u2225\u2207Ln(\u03b2\u2217)\u2225max and\n\u03bb \u22654b0\n\u221a\nd\u03c4 \u224d4R\u03c4 for \u03c4 := \u03c40\nlog m\nn\n(2.20)\nas de\ufb01ned in (2.18) and d\u03c4 \u2264\u03b1\u2113\n32 in order to obtain the statistical error bound for the\ncorrected linear model at the order of\n\u03b52\nstat =\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2\n2 \u224d400\n\u03b12\n\u2113\n\u03bb2d.\n(2.21)\nUnder suitable conditions on the sample size n and the effective rank of matrix B to\nbe stated in Theorem 3, we show that for the loss function (1.9), the RSC and RSM\nconditions hold under event A0 (cf. Lemma 15) following the Lower and Upper-RE\nconditions as derived in Lemma 15,\n\u00af\u03b1\u2113\u2248\u03b1\u2113\u224d\u03bbmin(A)\n2\n,\n\u03b1u \u224d3\u03bbmax(A)\n2\n,\nand\n\u03c4(Ln) \u224d\u03c4.\nCompared with the lower bound imposed on \u03bb as in (2.20) that we use to derive statis-\ntical error bounds, the penalty now involves a term\n\u03be\n1\u2212\u03ba that crucially depends on the\ncondition number \u03ba(A) in (2.13); Assuming that \u03b6 \u2265\u03b1u, then the second condition in\n(2.13) on \u03bb implies that\n\u03bb\n=\n\u2126(R\u03c4(Ln)\u03ba(A))\ngiven\n\u03be\n1 \u2212\u03ba\n\u2265\n40\u03c4(Ln) \u03b6\n\u00af\u03b1\u2113\n+ 2\u03c4(Ln) \u224d\u03c4\u03ba(A),\n(2.22)\nwhich now depends explicitly on the condition number \u03ba(A) in addition to the radius\nR \u224db0\n\u221a\nd and the tolerance parameter \u03c4. This is expected given that both RSC and\nRSM conditions are needed in order to derive the computational convergence bounds,\nwhile for the statistical error, we only require the RSC (Lower RE) condition to hold.\nRemarks. Consider the regression model in (1.1a) and (1.1b) with independent random\nmatrices X0, W as in (1.4), and an error vector \u03f5 \u2208Rn independent of X0, W, with\nindependent entries \u03f5j satisfying E\u03f5j = 0 and \u2225\u03f5j\u2225\u03c82 \u2264M\u03f5. Theorem 12 and its\ncorollaries provide an upper bound on the \u2113\u221enorm of the gradient \u2207Ln(\u03b2\u2217) = b\u0393\u03b2\u2217\u2212\nb\u03b3 of the loss function in the corrected linear model, where b\u0393 and b\u03b3 are as de\ufb01ned\nin (1.6). Let\nD\u2032\n0 = \u2225B\u22251/2\n2\n+ a1/2\nmax,\nand\nDoracle = 2(\u2225A\u22251/2\n2\n+ \u2225B\u22251/2\n2\n).\n(2.23)\nSpecializing to the case of corrected linear models, we have by Corollary 14, on event\nB0 as de\ufb01ned therein,\n\u2225\u2207Ln(\u03b2\u2217)\u2225\u221e=\n\r\r\rb\u0393\u03b2\u2217\u2212b\u03b3\n\r\r\r\n\u221e\u2264\u03c8\nr\nlog m\nn\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n16\nwhere \u03c8 := C0D\u2032\n0K\n\u0010\nM\u03f5 + \u03c4 +/2\nB\nK \u2225\u03b2\u2217\u22252\n\u0011\nand \u03c4 +/2\nB\n= \u03c4 1/2\nB + Doracle\n\u221am\nfor D\u2032\n0, Doracle\nas de\ufb01ned in (2.23).\nThe bound (2.15) characterizes the excess loss \u03c6(\u03b2t)\u2212\u03c6(b\u03b2) for solving (1.7) using the\ncomposite gradient algorithm; moreover, for any iterate \u03b2t such that (2.15) holds, the\nfollowing bound on the optimization error \u03b2t \u2212b\u03b2 follows immediately:\n\r\r\r\u03b2t \u2212b\u03b2\n\r\r\r\n2\n2\n\u2264\n2\n\u00af\u03b1\u2113\n\u0012\n\u03b42 + 4\u03bd\u03b52\nstat + 64\u03c4\u2113(Ln)\u03b44\n\u03bb2\n\u0013\n,\nwhere \u03bd = 64d\u03c4(Ln) and 4\u03c4(Ln)\u03f52 = 64\u03c4(Ln) \u03b44\n\u03bb2 by de\ufb01nition of \u03f52 in view of\n(2.21). Finally, we note that Theorem 2 holds for a class of weakly convex penalties\nas considered in [31] with suitable adaptation of RSC and parameters and conditions\nto involve \u00b5, following exactly the same sequence of arguments. Notable examples of\nsuch weakly convex penalty functions are SCAD [19] and MCP [55].\nThe rest of the paper is organized as follows. In Section 3, we present two main re-\nsults in Theorems 3 and 4. In Section 4, we state more precise results which improve\nupon Theorems 3 and 4; these results are more precise in the sense that our bounds and\npenalty parameters now take tr(B), the parameter that measures the magnitudes of er-\nrors in W, into consideration. In Section 5, we show that the RSC and RSM conditions\nhold for the corrected linear loss function and present our computational convergence\nbounds with regard to (1.7) in Theorem 9 and Corollary 10. In Section 6, we outline the\nproof of the main theorems. In particular, we outline the proofs for Theorems 3, 4, 6\nand 7 in Section 6, 6.3 and 6.5 respectively. In Section 7, we show a deterministic result\nas well as its application to the random matrix b\u0393 \u2212A for b\u0393 as in (1.6) with regards to\nthe upper and Lower RE conditions. In Section 8, we present results from numerical\nsimulations designed to validate the theoretical predictions in previous sections. The\ntechnical details of proofs are collected at the end of the paper. We prove Theorem 3\nin Section 10. We prove Theorem 4 in Section 11. We prove Theorems 6 and 7 in Sec-\ntion 12 and Section 13 respectively. We defer the proof of Theorem 2 to Section B. The\npaper concludes with a discussion of the results in Section 16. We list a set of symbols\nwe use throughout the paper in Table 1. Additional proofs and theoretical results are\ncollected in the Appendix.\n3. Main results on the statistical error\nIn this section, we will state our main results in Theorems 3 and 4 where we consider\nthe regression model in (1.1a) and (1.1b) with random matrices X0, W \u2208Rn\u00d7m as\nde\ufb01ned in (1.4). For the corrected Lasso estimator, we are interested in the case where\nthe smallest eigenvalue of the column-wise covariance matrix A does not approach 0\ntoo quickly and the effective rank of the row-wise covariance matrix B is bounded\nfrom below (cf. (3.2)). More precisely, (A2) thus ensures that the Lower-RE condition\nas in De\ufb01nition 2.2 is not vacuous. (A3) ensures that (2.6) holds for some s0 \u22651.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n17\nTABLE 1\nsymbols we used throughout the proof\nSymbol\nDe\ufb01nition\n\u03b1\ncurvature: \u03b1 := 5\n8 \u03bbmin(A)\n\u03b1\u2113\nLower RE/ RSC curvature parameter: \u03b1\u2113= \u03b1\n\u03b1u\nUpper RE/ RSM parameter \u03b1u \u224d3\n2 \u03bbmax(A)\n\u00af\u03f5stat\n\u00af\u03f5stat = 8\n\u221a\nd\u03b5stat where \u03b5stat =\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2\n\u03c40\n\u03c40 \u224d400C2\u03d6(s0+1)2\n\u03bbmin(A)\n\u03c4 = \u03bbmin(A)\u2212\u03b1\ns0\ntolerance parameter \u03c4 = \u03c40\nlog m\nn\nin Lower/Upper RE conditions\n\u03c4B\n\u03c4B = tr(B)/n\ns0 \u22651\nthe largest integer chosen such that the following inequality still holds:\n\u221as0\u03d6(s0) \u2264\u03bbmin(A)\n32C\nq\nn\nlog m\n\u03d6(s0)\n\u03c1max(s0, A) + \u03c4B\n\u03c4\u2113(Ln)\ntolerance parameter in RSC condition: \u03c4\u2113(Ln) \u224d\u03c40 log m/n\n\u03c4u(Ln)\ntolerance parameters in RSM condition: \u03c4u(Ln) \u224d\u03c40 log m/n\n\u03bd\u2113\n\u03bd\u2113= 64d\u03c4\u2113(Ln) < \u03b1\u2113\n60\n\u03bd(d, m, n)\n\u03bd(d, m, n) = 64d\u03c4u(Ln)\n\u00af\u03b1\u2113\neffective RSC coef\ufb01cient \u00af\u03b1\u2113= \u03b1\u2113\u2212\u03bd\u2113\n\u03c6(\u03b2)\nloss function: \u03c6(\u03b2) = 1\n2 \u03b2T b\u0393\u03b2 \u2212b\u03b3T + \u03c1\u03bb(\u03b2)\n\u2207Ln(\u03b2)\nGradient of the loss function b\u0393\u03b2 \u2212b\u03b3\n\u03c1n\n\u03c1n = C0K\nq\nlog m\nn\nrm,n\nrm,n = 2C0K2\nq\nlog m\nmn\n\u03b6\nstep size parameter: \u03b6 \u2265\u03b1u = 11\u03bbmax/8\n\u03f1\ncontraction parameter \u03f1 := 2\u03bd(d,m,n)\n\u00af\u03b1\u2113\n= 128d\u03c4u(Ln)\n\u00af\u03b1\u2113\n< \u00af\u03b1\u2113\n8\u03b6\n\u03ba\ncontraction coef\ufb01cient as \u03ba := (1 \u2212\u00af\u03b1\u2113\n4\u03b6 + \u03f1)(1 \u2212\u03f1)\u22121 < 1\n\u03b42\ntolerance parameter in computational errors \u03b42 \u2265c\u03b52\nstat\n1\u2212\u03ba\nd log p\nn\nMA\nMA = 64C\u03d6(s0)\n\u03bbmin(A) where \u03d6(s0) = \u03c1max(s0, A) + \u03c4B.\nM+\nM+ = 32C\u03d6(s0+1)\n\u03bbmin(A)\nwhere \u03d6(s0 + 1) = \u03c1max(s0 + 1, A) + \u03c4B.\n\u03be\n\u03be = 2(\u03c4\u2113(Ln) \u2228\u03c4u(Ln))\n\u0010\n\u00af\u03b1\u2113\n4\u03b6 + 2\u03f1 + 5\n\u0011\n(1 \u2212\u03f1)\u22121\nV\nV = 3eM3\nA/2\nThroughout this paper, for the corrected Lasso estimator, we will use the expression\n\u03c4 := \u03bbmin(A) \u2212\u03b1\ns0\n, where \u03b1 = 5\n8\u03bbmin(A)\nand\ns0 \u224d\n4n\nM 2\nA log m\nwhere MA is as de\ufb01ned in (2.7). Let\nD0 = \u221a\u03c4B + a1/2\nmax\nand\nD2 = 2(\u2225A\u22252 + \u2225B\u22252).\n(3.1)\nTheorem 3. (Estimation for the corrected Lasso estimator) Consider the regression\nmodel in (1.1a) and (1.1b) with independent random matrices X0, W as in (1.4), and\nan error vector \u03f5 \u2208Rn independent of X0, W, with independent entries \u03f5j satisfying\nE\u03f5j = 0 and \u2225\u03f5j\u2225\u03c82 \u2264M\u03f5. Set n = \u2126(log m). Suppose n \u2264(V/e)m log m, where\nV is a constant which depends on \u03bbmin(A), \u03c1max(s0, A) and tr(B)/n. Suppose m is\nsuf\ufb01ciently large.\nSuppose (A1), (A2) and (A3) hold. Let C0, c\u2032, c2, c3 > 0 be some absolute constants.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n18\nSuppose that \u2225B\u22252\nF / \u2225B\u22252\n2 \u2265log m. Suppose that c\u2032K4 \u22641 and\nr(B) := tr(B)\n\u2225B\u22252\n\u2265\n16c\u2032K4\nn\nlog m log Vm log m\nn\n.\n(3.2)\nLet b0, \u03c6 be numbers which satisfy\nM 2\n\u03f5\nK2b2\n0\n\u2264\u03c6 \u22641.\n(3.3)\nAssume that the sparsity of \u03b2\u2217satis\ufb01es for some 0 < \u03c6 \u22641\nd := |supp(\u03b2\u2217)| \u2264c\u2032\u03c6K4\n40M 2\n+\nn\nlog m < n/2,\n(3.4)\nwhere\nM+ = 32C\u03d6(s0 + 1)\n\u03bbmin(A)\n(3.5)\nfor \u03d6(s0 + 1) = \u03c1max(s0 + 1, A) + \u03c4B.\nLet b\u03b2 be an optimal solution to the corrected Lasso estimator as in (1.7) with\n\u03bb \u22654\u03c8\nr\nlog m\nn\nwhere\n\u03c8 := C0D2K (K \u2225\u03b2\u2217\u22252 + M\u03f5) .\n(3.6)\nThen for any d-sparse vectors \u03b2\u2217\u2208Rm, such that\n\u03c6b2\n0 \u2264\u2225\u03b2\u2217\u22252\n2 \u2264b2\n0,\n(3.7)\nwe have with probability at least 1\u22124 exp\n\u0010\n\u2212\nc3n\nM 2\nA log m log\n\u0010\nVm log m\nn\n\u0011\u0011\n\u22122 exp\n\u0010\n\u22124c2n\nM 2\nAK4\n\u0011\n\u2212\n22/m3,\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2 \u226420\n\u03b1 \u03bb\n\u221a\nd\nand\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n1 \u226480\n\u03b1 \u03bbd.\nWe give an outline of the proof of Theorem 3 in Section 6.2. We prove Theorem 3 in\nSection 10. We defer discussions on conditions appearing Theorem 3 in Section 3.2.\nFor the Conic programming estimator, we impose a restricted eigenvalue condition as\nformulated in [4, 38] on A and assume that the sparsity of \u03b2\u2217is bounded by o(\np\nn/ log m).\nThese conditions will be relaxed in Section 4 where we allow \u03c4B to approach 0.\nTheorem 4. Suppose (A1) holds. Set 0 < \u03b4 < 1. Suppose that n < m \u226aexp(n)\nand 1 \u2264d0 < n. Let \u03bb > 0 be the same parameter as in (1.8). Suppose that\n\u2225B\u22252\nF / \u2225B\u22252\n2 \u2265log m. Suppose that the sparsity of \u03b2\u2217is bounded by\nd0 := |supp(\u03b2\u2217)| \u2264c0\np\nn/ log m\n(3.8)\nfor some constant c0 > 0. Suppose\nn\n\u2265\n2000dK4\n\u03b42\nlog\n\u001260em\nd\u03b4\n\u0013\nwhere\n(3.9)\nd\n=\n2d0 + 2d0amax\n16K2(2d0, 3k0, A1/2)(3k0)2(3k0 + 1)\n\u03b42\n.\n(3.10)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n19\nConsider the regression model in (1.1a) and (1.1b) with X0, W as in (1.4) and an error\nvector \u03f5 \u2208Rn, independent of X0, W, with independent entries \u03f5j satisfying E\u03f5j = 0\nand \u2225\u03f5j\u2225\u03c82 \u2264M\u03f5. Let b\u03b2 be an optimal solution to the Conic programming estimator\nas in (1.8) with input (b\u03b3, b\u0393) as de\ufb01ned in (1.6). Recall \u03c4B := tr(B)/n. Choose for\nD0, D2 as in (3.1) and\n\u00b5 \u224dD2K2\nr\nlog m\nn\nand\n\u03c9 \u224dD0KM\u03f5\nr\nlog m\nn\n.\nThen with probability at least 1 \u2212\nc\u2032\nm2 \u22122 exp(\u2212\u03b42n/2000K4),\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\nq \u2264CD2K2d1/q\n0\nr\nlog m\nn\n\u0012\n\u2225\u03b2\u2217\u22252 + M\u03f5\nK\n\u0013\n(3.11)\nfor 2 \u2265q \u22651. Under the same assumptions, the predictive risk admits the following\nbounds with the same probability as above,\n1\nn\n\r\r\rX(b\u03b2 \u2212\u03b2\u2217)\n\r\r\r\n2\n2 \u2264C\u2032D2\n2K4d0\nlog m\nn\n\u0012\n\u2225\u03b2\u2217\u22252 + M\u03f5\nK\n\u00132\nwhere c\u2032, C0, C, C\u2032 > 0 are some absolute constants.\nWe give an outline of the proof of Theorem 4 in Section 6 while leaving the detailed\nproof in Section 11.\n3.1. Regarding the MA constant\nDenote by\nMA = 64C\u03d6(s0)\n\u03bbmin(A)\n\u224d\u03c1max(s0, A) + \u03c4B\n\u03bbmin(A)\n\u2022 (A3) ensures that MA and M+ are upper bounded by the condition number of A:\n\u03ba(A) := \u03bbmax(A)\n\u03bbmin(A) = O\n\u0010q\nn\nlog m\n\u0011\ngiven that \u03c4B := tr(B)/n = O(\u03bbmax(A)).\n\u2022 So the condition (3.4) in Theorem 3 allows d \u224dn/ log m in the optimal set-\nting when the condition number \u03ba(A) is understood to be a constant. As \u03ba(A)\nincreases, the conservative worst case upper bound on d needs to be adjusted\ncorrespondingly. Moreover, this adjustment is also crucial in order to ensure the\ncomposite gradient algorithm to converge in the sense of Theorem 2. We will\nillustrate such dependencies on \u03ba(A) in numerical examples in Section 8.\n\u2022 The condition \u03c4B = O(\u03bbmax(A)) puts an upper bound on how large the mea-\nsurement error in W can be. We do not allow the measurement error to over-\nwhelm the signal entirely. When \u03c4B \u21920, we recover the ordinary Lasso bound\nin [4], which we elaborate in the next two sections.\nThroughout this paper, we assume that MA \u224dM+, where recall M+ = 32C\u03d6(s0+1)\n\u03bbmin(A)\n.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n20\n3.2. Discussions\nThroughout our analysis, we set the parameter b0 \u2265\u2225\u03b2\u2217\u22252 and d = |supp(\u03b2\u2217)| :=\n\f\f{j : \u03b2\u2217\nj \u0338= 0}\n\f\f for the corrected Lasso estimator. In practice, both b0 and d are un-\nderstood to be parameters chosen to provide an upper bound on the \u21132 norm and the\nsparsity of the true \u03b2\u2217. The parameter 0 < \u03c6 < 1 is a parameter that we use to describe\nthe gap between \u2225\u03b2\u2217\u22252\n2 and its upper bound b2\n0. Denote the Signal-to-noise ratio by\nS/N := K2 \u2225\u03b2\u2217\u22252\n2/M 2\n\u03f5 , where\nN := M 2\n\u03f5 and\n\u03c6K2b2\n0 \u2264S := K2 \u2225\u03b2\u2217\u22252\n2 \u2264K2b2\n0.\nThe two conditions (3.3) and (3.7) on b0 and \u03c6 imply that N \u2264K2\u03c6b2\n0 \u2264S. Notice that\nthis could be restrictive if \u03c6 is small. We will show in Section 6.2 that condition (3.3)\nis not needed in order for the \u2113p, p = 1, 2 errors as stated in the Theorem 3 to hold.\nIt was indeed introduced so as to further simplify the expression for the condition on\nd as shown in (3.4). Therefore we provide slightly more general conditions on d in\n(6.9) in Lemma 17, where (3.3) is not required. We introduce the parameter \u03c6 so that\nthe conditions on d depend on \u03c6 and b2\n0 rather than the true signal \u2225\u03b2\u2217\u22252 (cf. Proof of\nLemmas 17 and 18). It will also become clear in the sequel from the proof of Lemma 17\n(cf. (H.4)) that we could use \u2225\u03b2\u2217\u22252 rather than its the lower bound b2\n0\u03c6 in the expression\nfor d. However, we choose to state the condition on d as in Theorem 3 for clarity of our\nexposition. See also Theorem 6 and Lemma 18.\nIn fact, we prove that Theorem 3 holds with N = M 2\n\u03f5 and S = \u03c6K2b2\n0 in arbitrary\norders, so long as conditions (3.2) and (3.4) or (6.9) hold. For both cases, we require\nthat \u03bb \u224d(\u2225A\u22252 + \u2225B\u22252)K\n\u221a\nS + N\nq\nlog m\nn\nas expressed in (3.6). That is, when either\nthe noise level M\u03f5 or the signal strength K \u2225\u03b2\u2217\u2225increases, we need to increase \u03bb\ncorrespondingly; moreover, when N dominates the signal K2 \u2225\u03b2\u2217\u22252\n2, we have for d \u224d\n1\nM 2\nA\nn\nlog m as in (3.4),\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2 / \u2225\u03b2\u2217\u22252 = OP\n \nD2K2\nr\nN\nS\n1\n\u03d6(s0 + 1)\n!\n,\nwhich eventually becomes a vacuous bound when N \u226bS. This bound appears a bit\ncrude as it does not entirely discriminate between the noise, measurement error, and the\nsignal strength. We further elaborate on the relationships among these three elements\nin Section 4. We will then present an improved bound in Theorem 6.\n1. The choice of \u03bb for the Lasso estimator and parameters \u00b5, \u03c9 for the DS-type\nestimator satisfy\n\u03bb \u224d\u00b5 \u2225\u03b2\u2217\u22252 + \u03c9.\nThis relationship is made clear through Theorem 16 regarding the corrected\nLasso estimator, which follows from Theorem 1 by [30], and Lemmas 19 and 22\nfor the Conic programming estimator. The penalty parameter \u03bb is chosen to\nbound\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221efrom above, which is in turn bounded in Theorem 12. See\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n21\nCorollaries 13 and 14, which are the key results in proving Theorems 3, 4, 6,\nand 7.\n2. Throughout our analysis of Theorems 3 and 4, our error bounds are stated in a\nway assuming the errors in W are suf\ufb01ciently large in the sense that these bounds\nare optimal only when \u03c4B is bounded from below by some absolute constant. For\nexample, when \u2225B\u22252 is bounded away from 0, the lower bound on the effective\nrank r(B) = tr(B)/ \u2225B\u22252 implies that \u03c4B must also be bounded away from 0.\nMore precisely, by the condition on the effective rank as in (3.2), we have\n\u03c4B = tr(B)\nn\n\u2265\n16c\u2032K4 \u2225B\u22252\nlog m log Vm log m\nn\nwhere V = 3eM 3\nA/2.\nLater, we will state our results with \u03c4B = tr(B)/n > 0 being explicitly in-\ncluded in the error bounds as well as the penalization parameters and sparsity\nconstraints.\n3. In view of the main Theorems 3 and 4, at this point, we do not really think one\nestimator is preferable to the other. While the \u2113q error bounds we obtain for the\ntwo estimators are at the same order for q = 1, 2, the conditions under which\nthese error bounds are obtained are somewhat different. In Theorem 4, we only\nrequire that RE(2d0, 3k0, A1/2) holds for k0 = 1 + \u03bb where \u03bb \u224d1, while in\nTheorem 3 we need the minimal eigenvalue of A to be bounded from below,\nnamely, we need to assume that (A2) holds. As mentioned earlier, (A2) ensures\nthat the Lower-RE condition as in De\ufb01nition 2.2 is not vacuous while (A3) en-\nsures that (2.6) holds for some s0 \u22651. Th condition (3.2) on the effective rank\nof the row-wise covariance matrix B is also needed to establish the Lower and\nUpper RE conditions in Lemma 15 for the corrected Lasso estimator. Moreover,\nfor the sparsity parameter d0 in (3.8), we show in Lemma 34 that (A2) is a suf-\n\ufb01cient condition for a type of RE(2d0, 3k0) condition to hold on non positive\nde\ufb01nite b\u0393 as de\ufb01ned in (1.6). See also Theorem 26.\n4. In some sense, the assumptions in Theorem 3 appear to be slightly stronger, while\nat the same time yielding correspondingly stronger results in the following sense:\nThe corrected Lasso procedure can recover a sparse model using O(log m) num-\nber of measurements per nonzero component despite the measurement error in\nX and the stochastic noise \u03f5, while the Conic programming estimator allows\nonly d \u224d\np\nn/ log m to achieve the error rate at the same order as the cor-\nrected Lasso estimator. Hence, while Conic programming estimator is conceptu-\nally more adaptive by not \ufb01xing an upper bound on \u2225\u03b2\u2217\u22252 a priori, the price we\npay seems to be a more stringent upper bound on the sparsity level.\n5. We note that following Theorem 2 as in [3], one can show that without the rela-\ntively restrictive sparsity condition (3.8), a bound similar to that in (3.11) holds,\nhowever, with \u2225\u03b2\u2217\u22252 being replaced by \u2225\u03b2\u2217\u22251, so long as the sample size sat-\nis\ufb01es the condition as in (4.9). However, we show in Theorem 7 in Section 6.5\nthat this restriction on the sparsity can be relaxed for the Conic programming\nestimator (1.8), when we make a different choice for the parameter \u00b5 based on a\nmore re\ufb01ned analysis.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n22\nResults similar to Theorems 3 and 4 have been derived in [30, 3], however, under\ndifferent assumptions on the distribution of the noise matrix W. When W is a random\nmatrix with i.i.d. subgaussian noise, our results in Theorems 3 and 4 will essentially\nrecover the results in [30] and [3]. We compare with their results in Section 4 in case\nB = \u03c4BI after we present our improved bounds in Theorems 6 and 7. We refer to the\npaper of [3] for a concise summary of these and some earlier results.\nFinally, one reviewer asked about the dependence of the tuning parameter on properties\nof A and B, namely parameters D0 = \u221a\u03c4B + a1/2\nmax, D\u2032\n0 = \u2225B\u22251/2\n2\n+ a1/2\nmax and\nD2 = \u2225A\u22252 + \u2225B\u22252. We now state in Lemma 5 a sharp bound on estimating \u03c4B using\nb\u03c4B as in (1.5), which will provide a natural plug-in estimate for parameters such as D0\nthat involve \u03c4B.\nLemma 5. Let m \u22652. Let X be de\ufb01ned as in (1.4) and b\u03c4B be as de\ufb01ned in (1.5).\nDenote by \u03c4B = tr(B)/n and \u03c4A = tr(A)/m. Suppose that n \u2228(r(A)r(B)) > log m.\nDenote by B6 the event such that\n|b\u03c4B \u2212\u03c4B|\n\u2264\n2C0K2\nr\nlog m\nmn\n\u0012\u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221an\n\u0013\n=: D1rm,m,\nwhere D1 = \u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221an\nand rm,m = 2C0K2\nq\nlog m\nmn . Then P (B6) \u22651 \u2212\n3\nm3 .\nIf we replace \u221alog m with log m in the de\ufb01nition of event B6, then we can drop the\ncondition on n or r(A)r(B) = tr(A)\n\u2225A\u22252\ntr(B)\n\u2225B\u22252 to achieve the same bound on event B6.\nIn an earlier version of the present work by the same authors [39], we presented the\nrate of convergence for using the corrected gram matrix bB :=\n1\nmXXT \u2212tr(A)\nm Im to\nestimate B and proved isometry properties in the operator norm once the effective rank\nof A is suf\ufb01ciently large compared to n; one can then use such estimated bB and its\noperator norm in D2 and D\u2032\n0. See Theorem 21 and Corollary 22 therein. As mentioned,\nwe use the estimated b\u03c4B (cf. Lemma 5) in D0. The dependencies on A, \u2225\u03b2\u2217\u22252 and \u03f5 are\nknown problems in the Lasso and corrected Lasso literature; see [4, 30]. For example,\nthe RE condition as stated in De\ufb01nition 2.1 and its subgaussian concentration properties\nas shown [38] clearly depend on unknown parameter amax related to covariance matrix\nA. See Theorem 27 in the present paper. We prove Lemma 5 in Section C.1. Lemma 5\nprovides the powerful technical insight and one of the key ingredients leading to the\ntight analysis in Theorems 6 and 7 for the corrected Lasso estimator (1.7) as well as\nthe Conic programming estimator (1.8) in Section 4, where we also present theory for\nwhich the dependency on \u2225A\u22252 becomes extremely mild.\n4. Improved bounds when the measurement errors are small\nAlthough the conclusions of Theorems 3 and 4 apply to cases when \u2225B\u22252 \u21920, the\nerror bounds are not as tight as the bounds we are about to derive in this section. So far,\nwe have used more crude approximations on the error bounds in terms of estimating\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221efor the sake of reducing the amount of unknown parameters we need to\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n23\nconsider. The bounds we derive in this section take the magnitudes of the measurement\nerrors in W into consideration. As such, we allow the error bounds to depend on the\nparameter \u03c4B explicitly, which become much tighter as \u03c4B becomes smaller. For the\nextreme case when \u03c4B approaches 0, one hopes to recover a bound close to the regular\nLasso or the Dantzig selector as the effect of the noise on the procedure should become\nnegligible. We show in Theorems 6 and 7 that this is indeed the case. Denote by\n\u03c4 +/2\nB\n:= \u221a\u03c4B + Doracle\n\u221am , where Doracle = 2(\u2225A\u22251/2\n2\n+ \u2225B\u22251/2\n2\n). (4.1)\nWe \ufb01rst state a more re\ufb01ned result for the Lasso-type estimator, for which we now only\nrequire that\n\u03bb \u224d(a1/2\nmax + \u2225B\u22251/2\n2\n)K\np\nN + \u03c4BS\nr\nlog m\nn\n.\nThat is, we replace\n\u221a\nN + S in \u03bb (3.6) now with \u221aN + \u03c4BS, which leads to signi\ufb01cant\nimprovement on the rates of convergence for estimating \u03b2\u2217when \u03c4B \u21920.\nTheorem 6. Suppose all conditions in Theorem 3 hold, except that we drop (3.3) and\nreplace (3.6) with\n\u03bb \u22654\u03c8\nr\nlog m\nn\n, where\n\u03c8 := C0D\u2032\n0K\n\u0010\nM\u03f5 + \u03c4 +/2\nB\nK \u2225\u03b2\u2217\u22252\n\u0011\n(4.2)\nfor D\u2032\n0 and \u03c4 +/2\nB\nas de\ufb01ned in (2.23) and (4.1) respectively. Let c\u2032, \u03c6, b0, M\u03f5, K and\nM+ be as de\ufb01ned in Theorem 3. Let \u03c4 +\nB = (\u03c4 +/2\nB\n)2.\nSuppose that for 0 < \u03c6 \u22641 and CA :=\n1\n160M 2\n+ ,\nd\n:=\n|supp(\u03b2\u2217)| \u2264CA\nn\nlog m {c\u2032c\u2032\u2032D\u03c6 \u22278} =: \u00afd0,\nwhere\n(4.3)\nc\u2032\u2032 = \u2225B\u22252 + amax\n\u03d6(s0 + 1)2\nand\nD\u03c6 = K2M 2\n\u03f5\nb2\n0\n+ \u03c4 +\nB K4\u03c6\n(4.4)\nThen for any d-sparse vectors \u03b2\u2217\u2208Rm, such that \u03c6b2\n0 \u2264\u2225\u03b2\u2217\u22252\n2 \u2264b2\n0, we have\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2 \u226420\n\u03b1 \u03bb\n\u221a\nd\nand\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n1 \u226480\n\u03b1 \u03bbd\n(4.5)\nwith probability at least 1\u22124 exp\n\u0010\n\u2212\nc3n\nM 2\nA log m log\n\u0010\nVm log m\nn\n\u0011\u0011\n\u22122 exp\n\u0010\n\u22124c2n\nM 2\nAK4\n\u0011\n\u2212\n22/m3.\nWe give an outline for the proof of Theorem 6 in Section 6.3, and show the actual proof\nin Section 12.\nWe next state in Theorem 7 an improved bounds for the Conic programming esti-\nmator (1.8), which dramatically improve upon those in Theorem 4 when \u03c4B is small,\nwhere an \u201coracle\u201d rate for estimating \u03b2\u2217with the Conic programming estimator b\u03b2 (1.8)\nis de\ufb01ned and the predictive error \u2225Xv\u22252\n2 when \u03c4B = o(1) is derived.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n24\nLet C0 satisfy (H.6) for c as de\ufb01ned in Theorem 31. Throughout the rest of the paper,\nwe denote by:\n\u03c1n\n=\nC0K\nr\nlog m\nn\nand rm,m = 2C0K2\nr\nlog m\nmn ;\n(4.6)\n\u03c4 \u2020/2\nB\n=\n(\u03c4 1/2\nB\n+ 3\n2C6r1/2\nm,m)\nand\n\u03c4 \u2021\nB \u224d2\u03c4B + 3C2\n6rm,m.\n(4.7)\nTheorem 7. Let D0 = \u221a\u03c4B +a1/2\nmax, and D\u2032\n0, Doracle be as de\ufb01ned in (2.23). Let C6 \u2265\nDoracle. Let \u03c1n and rm,m be as de\ufb01ned in (4.6). Suppose all conditions in Theorem 4\nhold, except that we replace the condition on d as in (3.8) with the following.\nSuppose that the sample size n and the size of the support of \u03b2\u2217satisfy the following\nrequirements:\nd0\n=\nO\n\u0012\n\u03c4 \u2212\nB\nr\nn\nlog m\n\u0013\n,\nwhere \u03c4 \u2212\nB \u2264\n1\n\u03c4 1/2\nB\n+ 2C6r1/2\nm,m\n,\n(4.8)\nand\nn\n\u2265\n2000dK4\n\u03b42\nlog\n\u001260em\nd\u03b4\n\u0013\n, where\n(4.9)\nd\n=\n2d0 + 2d0amax\n16K2(2d0, 3k0, A1/2)(3k0)2(3k0 + 1)\n\u03b42\n. (4.10)\nLet b\u03c4B be as de\ufb01ned in de\ufb01ned in (1.5). Let b\u03b2 be an optimal solution to the Conic\nprogramming estimator as in (1.8) with input (b\u03b3, b\u0393) as de\ufb01ned in (1.6). Suppose\n\u03c9\n\u224d\nD0M\u03f5\u03c1n\nand\n\u00b5 \u224dD\u2032\n0e\u03c4 1/2\nB K\u03c1n,\n(4.11)\nwhere e\u03c4 1/2\nB\n:= b\u03c4 1/2\nB\n+ C6r1/2\nm,m.\nThen with probability at least 1 \u2212c\u2032\u2032\nm2 \u22122 exp(\u2212\u03b42n/2000K4),\nfor\n2 \u2265q \u22651,\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\nq \u2264C\u2032D\u2032\n0K2d1/q\n0\nr\nlog m\nn\n\u0012\n\u03c4 \u2020/2\nB\n\u2225\u03b2\u2217\u22252 + M\u03f5\nK\n\u0013\n;(4.12)\nUnder the same assumptions, the predictive risk admits the following bound\n1\nn\n\r\r\rX(b\u03b2 \u2212\u03b2\u2217)\n\r\r\r\n2\n2 \u2264C\u2032\u2032(\u2225B\u22252 + amax)K2d0\nlog m\nn\n\u0010\n\u03c4 \u2021\nBK2 \u2225\u03b2\u2217\u22252\n2 + M 2\n\u03f5\n\u0011\n,\nwith the same probability as above, where c\u2032\u2032, C\u2032, C\u2032\u2032 > 0 are some absolute constants.\nWe give an outline for the proof of Theorem 7 in Section 6.5, and show the actual proof\nin Section 13.\n4.1. Oracle results on the Lasso-type estimator\nWe now discuss the improvement being made in Theorem 6 and Theorem 7.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n25\nThe Signal-to-noise ratio. Let us rede\ufb01ne the Signal-to-noise ratio by\nS/M\n:=\nK2 \u2225\u03b2\u2217\u22252\n2\n\u03c4 +\nB K2 \u2225\u03b2\u2217\u22252\n2 + M 2\u03f5\n,\nwhere\nS\n:=\nK2 \u2225\u03b2\u2217\u22252\n2 and\nM := M 2\n\u03f5 + \u03c4 +\nB K2 \u2225\u03b2\u2217\u22252\n2 .\nWhen either the noise level M\u03f5 or the measurement error strength in terms of \u03c4 +/2\nB\nK \u2225\u03b2\u2217\u22252\nincreases, we need to increase the penalty parameter \u03bb correspondingly; moreover,\nwhen d \u224d\n1\nM 2\nA\nn\nlog m, we have\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2\n\u2225\u03b2\u2217\u22252\n= OP\n \nD\u2032\n0K2\nr\nM\nS\n1\n\u03d6(s0 + 1)\n!\n,\nwhich eventually becomes a vacuous bound when M \u226bS.\nFinally, suppose B = \u03c32\nwI, we have \u2225B\u22251/2\n2\n= \u03c3w and \u03c4B = \u03c32\nw. In this setting,\nwe recover essentially the same \u21132 error bound as that in Corollary 1 of [30] in case\n\u2225\u03b2\u2217\u22252 \u224d1, as we have on event A0 \u2229B0,\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2 \u2264C(\u03c3w + a1/2\nmax)\n\u03bbmin(A)\nq\n\u03c32\u03f5 + \u03c32w \u2225\u03b2\u2217\u22252\n2\nr\nd log m\nn\n(4.13)\nwhere \u03c32\n\u03f5 \u224dM 2\n\u03f5 and K2 \u224d1. However, when \u2225\u03b2\u2217\u22252 = \u2126(1), our statistical precision\nappears to be sharper as we allow the term \u2225\u03b2\u2217\u22252 to be removed entirely from the RHS\nwhen \u03c3w \u21920 and hence recover the regular Lasso rate of convergence.\nThe penalization parameter. We focus now on the penalization parameter \u03bb in (1.7).\nThe effective rank condition in (3.2) implies that for n = O(m log m)\n\u2225B\u22252 \u2264\n\u03c4B\n16c\u2032K4\nlog m\nlog(3eM 3\nA/2) + log(m log m) \u2212log n \u2264CB\u03c4B log m\n(4.14)\nwhere CB =\n1\n16c\u2032K4 log(3eM 3\nA/2) given that log(m log m) \u2212log n > 0. This bound is\nvery crude given that in practice, we focus on cases where n \u226am log m. Note that\nunder (A1) (A2) and (A3), we have for n = O(m log m),\n\u03c4 +\nB\n\u224d\n\u03c4B + \u2225A\u22252 + \u2225B\u22252\nm\n\u2264\n\u03c4B + 1\nm(\u03ba(A)\u03bbmin(A) + CB\u03c4B log m) \u224d\u03c4B + O\n\u0012\u03bbmin(A)\n\u221am\n\u0013\n.\nWithout knowing \u03c4B, we will use b\u03c4B as de\ufb01ned in (1.5). Notice that we know neither\nD\u2032\n0 nor Doracle in the de\ufb01nition of \u03bb, where D2\noracle \u224dD2; Indeed,\n2D2 \u2264D2\noracle \u22644D2.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n26\nHowever, assuming that we normalize the column norms of the design matrix X to be\nroughly at the same scale, we have for \u03c4B = O(1) and m suf\ufb01ciently large,\nD\u2032\n0 \u224d1\nwhile\nDoracle/\u221am = o(1) in case \u2225A\u22252 , \u2225B\u22252 \u2264M\nfor some large enough constant M. In summary, compared to Theorem 3, in \u03c8, we\nreplace D2 = 2(\u2225A\u22252 + \u2225B\u22252) with D\u2032\n0 := \u2225B\u22252\n1/2 + a1/2\nmax so that the dependency\non \u2225A\u22252 becomes much weaker. As mentioned in Section 3.2, we may use the plug-\nin estimate \u2225bB\u22252 in D\u2032\n0, where bB is the corrected gram matrix\n1\nmXXT \u2212tr(A)\nm Im.\nFinally, the concentration of measure bound for the estimator b\u03c4B as in (1.5) is stated in\nLemma 5, which ensures that b\u03c4B is indeed a good proxy for \u03c4B (cf. Lemma 23).\nThe sparsity parameter. The condition on d (and D\u03c6) for the Lasso estimator as\nde\ufb01ned in (4.3) suggests that as \u03c4B \u21920, and thus \u03c4 +\nB \u21920, the constraint on the\nsparsity parameter d becomes slightly more stringent when K2M 2\n\u03f5 /b2\n0 \u224d1 and much\nmore restrictive when K2M 2\n\u03f5 /b2\n0 = o(1). Moreover, suppose we require\nM 2\n\u03f5 = \u2126(\u03c4 +\nB K2 \u2225\u03b2\u2217\u22252\n2),\nthat is, the stochastic error \u03f5 in the response variable y as in (1.1a) does not converge\nto 0 as quickly as the measurement error W in (1.1b) does, then the sparsity constraint\nbecomes essentially unchanged as \u03c4 +\nB \u21920 as we show now.\nCase 1. Suppose \u03c4B \u21920 and M\u03f5 = \u2126(\u03c4 +/2\nB\nK \u2225\u03b2\u2217\u22252). In this case, essentially, we\nrequire that\nd \u2264c0\u03bb2\nmin(A)\n\u03d62(s0 + 1)\nn\nlog m\n\u001ac\u2032c\u2032\u2032K2M 2\n\u03f5\nb2\n0\n\u22271\n\u001b\n(4.15)\nwhere D\u03c6 \u224dK2M 2\n\u03f5\nb2\n0\ngiven that\n\u03c4 +\nB K4\u03c6 \u2264\u03c4 +\nB K4 \u2225\u03b2\u2217\u22252\n2\nb2\n0\n\u226aK2M 2\n\u03f5\nb2\n0\nwhere c0, c\u2032 are absolute constants and c\u2032\u2032 := \u2225B\u22252+amax\n\u03d62(s0+1) \u224d1 where \u03d6(s0+1) =\n\u03c1max(s0 + 1, A) + \u03c4B. In this case, the sparsity constraint becomes essentially\nunchanged as \u03c4 +\nB \u21920.\nCase 2. Analogous to (3.4), when M 2\n\u03f5 \u2264\u03c4 +\nB \u03c6K2b2\n0, we could represent the condition on\nd as follows:\nd\n\u2264\nCAc\u2032c\u2032\u2032\u03c4 +\nB K4\u03c6\nn\nlog m \u2264CAc\u2032c\u2032\u2032D\u03c6\nn\nlog m\nwhich is suf\ufb01cient for (4.3) to hold for \u03c4B \u21920; Indeed, by assumption that\nc\u2032K4 \u22641 and M 2\n\u03f5 \u2264\u03c4 +\nB \u03c6K2b2\n0, we have\n8 > 2c\u2032K4\u03c4 +\nB \u03c6 \u2265c\u2032D\u03c6 \u224dc\u2032\u03c4 +\nB K4\u03c6.\nHence, for c\u2032\u03c4 +\nB K4 \u22641, we have\nd\n\u2264\nCA(c\u2032c\u2032\u2032\u03c4 +\nB K4\u03c6 \u22278)\nn\nlog m \u224dCAc\u2032\u2032(c\u2032\u03c4 +\nB K4\u03c6 \u22278)\nn\nlog m\n\u2264\nCAc\u2032\u2032c\u2032\u03c4 +\nB K4\u03c6\nn\nlog m \u224dCAc\u2032\u2032c\u2032D\u03c6\nn\nlog m\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n27\nThis condition, however, seems to be unnecessarily strong, when \u03c4B \u21920 (and\nM\u03f5 \u21920 simultaneously). We focus on the following Case 2 in the present work.\nFor both cases, it is clear that sample size needs to satisfy\nn = e\u2126\n\u0012\nd log m(\u03c1max(s0 + 1, A) + \u03c4B)4\n\u03bbmin(A)2(\u2225B\u22252 + amax)\n\u0013\n,\n(4.16)\nwhere e\u2126(\u00b7) notation hides parameters K, M\u03f5, \u03c6 and b0, which we treat as absolute\nconstants that do not change as \u03c4B \u21920. These tradeoffs are somehow different from\nthe behavior of the Conic programming estimator (cf (4.17)). We will provide a more\ndetailed analysis in Sections 6.1 and 6.3.\n4.2. Oracle results on the Conic programming estimator\nIn order to exploit the oracle bound as stated in Theorem 12 regarding\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e,\nwe need to know the noise level \u03c4B := tr(B)/n in W and then we can set\n\u00b5\n\u224d\nD\u2032\n0(\u03c4 1/2\nB\n+ Doracle\n\u221am )K\u03c1n\nwhile retaining \u03c9 \u224dD0M\u03f5\u03c1n,\nwhere recall\n\u03c1n = C0K\nr\nlog m\nn\nand\nD0 = \u221a\u03c4B + \u221aamax.\nThis will in turn lead to improved bounds in Theorems 6 and 7.\nThe penalization parameter. Without knowing the parameter \u03c4B, we rely on the es-\ntimate from b\u03c4B as in (1.5), as discussed in Section 3. For a chosen parameter C6 \u224d\nDoracle, we use b\u03c4 1/2\nB\n+ C6r1/2\nm,m to replace \u03c4 +/2\nB\n:= \u03c4 1/2\nB\n+ Doracle/\u221am and set\n\u00b5\n\u224d\nC0D\u2032\n0K2(b\u03c4 1/2\nB\n+ Doracler1/2\nm,m)\nr\nlog m\nn\nin view of Corollary 14, where an improved error bound over\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221eis stated.\nWithout knowing Doracle, we could replace it with an upper bound; for example, as-\nsuming that D2\noracle \u224d\u2225A\u22252 + \u2225B\u22252 = O\n\u0010q\nn\nlog m\n\u0011\n, we could set\n\u00b5 \u224dC0D\u2032\n0K2(b\u03c4 1/2\nB\n+ O(m\u22121/4))\nr\nlog m\nn\n.\nThe sparsity parameter. Roughly speaking, for the Conic programming estimator (1.8),\none can think of d0 as being bounded:\nd0\n=\nO\n\u0012\n\u03c4 \u2212\nB\nr\nn\nlog m\n^\nn\nlog(m/d0)\n\u0013\nwhere \u03c4 \u2212\nB \u224d\u03c4 \u22121/2\nB\n(4.17)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n28\nThat is, when \u03c4B decreases, we allow larger values of d0; however, when \u03c4B \u21920,\nthe sparsity level of d = O (n/ log(m/d)) starts to dominate, which enables the Conic\nprogramming estimator to achieve results similar to the Dantzig Selector when the\ndesign matrix X0 is a subgaussian random matrix satisfying the Restricted Eigenvalue\nconditions; See for example [6, 4, 38].\nIn particular, when \u03c4B \u21920, Theorem 7 allows us to recover a rate close to that of the\nDantzig selector with an exact recovery if \u03c4B = 0 is known a priori; see Section 16.\nMoreover the constraint (3.8) on the sparsity parameter d0 appearing in Theorem 4 can\nnow be relaxed as in (4.8). In summary, our results in Theorem 7 are stronger than\nthose in [3] (cf. Corollary 1) as their rates as stated therein are at the same order as ours\nin Theorem 4. We illustrate this dependency on \u03c4B in Section 8 with numerical exam-\nples, where we clearly show an advantage by taking the noise level into consideration\nwhen choosing the penalty parameters for both the Lasso and the Conic programming\nestimators.\n5. Optimization error on the gradient descent algorithm\nWe now present our computational convergence bounds. First we present Lemma 8\nregarding the RSC and RSM conditions on the loss function (1.7). Lemma 8 follows\nfrom Lemma 15 immediately.\nLemma 8. Suppose all conditions as stated in Theorem 3 hold. Suppose event A0\nholds. Then (2.8) and (2.9) hold with \u03b1\u2113= 5\n8\u03bbmin(A), \u03b1u = 11\n8 \u03bbmax(A) and\n\u03c4\u2113(Ln) = \u03c4u(Ln) = \u03c40\nlog m\nn\n,\nwhere\n\u03c40 \u224d400C2\u03d6(s0 + 1)2\n\u03bbmin(A)\n.\n(5.1)\nTheorem 9. Suppose all conditions in Theorem 6 hold and let \u03c8 be de\ufb01ned therein.\nLet g(\u03b2) = 1\n\u03bb\u03c1\u03bb(\u03b2) where \u03c1\u03bb(\u03b2) = \u03bb \u2225\u03b2\u22251. Consider the optimization program (1.10)\nfor a radius R such that \u03b2\u2217is feasible and a regularization parameter chosen such that\n\u03bb\n\u2265\n\u001216R\u03be\n1 \u2212\u03ba\n\u0013 _  \n12\u03c8\nr\nlog m\nn\n!\n.\n(5.2)\nSuppose that the step size parameter \u03b6 \u2265\u03b1u \u224d3\n2\u03bbmax(A). Suppose that the sparsity\nparameter and sample size further satisfy the following relationship:\nd\n<\nn\n512\u03c40 log m\n\u0012 \u03bbmin(A)2\n12\u03bbmax(A)\n^ (\u03b1\u2113)2\n5\u03b6\n\u0013\n=: \u00afd.\n(5.3)\nThen on event A0 \u2229B0, the conclusions in Theorem 2 hold, where\nP (A0 \u2229B0) \u22651\u22124 exp\n\u0012\n\u2212\nc3n\nM 2\nA log m log\n\u0012Vm log m\nn\n\u0013\u0013\n\u22122 exp\n\u0012\n\u22124c2n\nM 2\nAK4\n\u0013\n\u221222/m3.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n29\nCorollary 10. Suppose all conditions as stated in Theorem 9 hold and event A0 \u2229B0\nde\ufb01ned therein holds. Consider for some constant M \u2264400\u03c40 and \u00af\u03b42 as de\ufb01ned in\nTheorem 2,\n\u03b42 \u224dc\u03b52\nstat\n1 \u2212\u03ba\nd log m\nn\n=: \u00af\u03b42\nand\n\u03b42 \u2264M \u00af\u03b42 \u2264400\u03c40\u00af\u03b42.\nThen for all t \u2265T \u2217(\u03b4) as in (2.17) and R = \u2126(b0\n\u221a\nd),\n\r\r\r\u03b2t \u2212b\u03b2\n\r\r\r\n2\n2\n\u2264\n3\n\u03b1\u2113\n\u03b42 + \u03b1\u2113\n4 \u03b52\nstat + O\n\u0012\u03b42\u03b52\nstat\nb2\n0\n\u0013\n.\n(5.4)\nFinally, suppose we \ufb01x for M+ = 32C\u03d6(s0+1)\n\u03bbmin(A)\n,\nR \u224d\np\n\u00afdb0 \u224d\nb0\n20M+\np\n6\u03ba(A)\nr\nn\nlog m,\nin view of the upper bound \u00afd (5.3). Then for all t \u2265T \u2217(\u03b4) as in (2.17),\n\r\r\r\u03b2t \u2212b\u03b2\n\r\r\r\n2\n2\n\u2264\n3\n\u03b1\u2113\n\u03b42 + \u03b1\u2113\n4 \u03b52\nstat + 2\n\u03b1\u2113\n\u03b44\nb2\n0 \u2225A\u22252\n.\n(5.5)\nWe prove Theorem 9 and Corollary 10 in Section 14.\n5.1. Discussions\nThroughout this section, we assume \u03c8 (4.2) is as de\ufb01ned in Theorem 6. Assume that\n\u03b6 \u2265\u03b1u \u2265\u00af\u03b1\u2113. In addition, suppose that the radius R \u224db0\n\u221a\nd as we set in (1.7). Let\n\u00afd0 \u2264\nn\n160M 2\n+ log m be as de\ufb01ned in (4.3), where recall that we require the following\ncondition on d:\nd\n\u2264\nCA {c\u2032C\u03c6 \u22278}\nn\nlog m =: \u00afd0,\nwhere CA =\n1\n160M 2\n+\n,\nC\u03c6\n=\n\u2225B\u22252 + amax\n\u03d6(s0 + 1)2 D\u03c6\nand\nb2\n0 \u2265\u2225\u03b2\u2217\u22252\n2 \u2265\u03c6b2\n0.\nThen by the proof of Lemma 18,\nb0\np \u00afd0\n\u2264\n5s0\n3\u03b1\nr\nlog m\nn\n\u03c8 =: \u03c8\n\u03c4\nr\nlog m\nn\n,\nwhere\n\u03c4 = 3\u03b1\n5s0\n.\n(5.6)\nIn contrast, under (5.3), the following upper bound holds on d, which is slightly more\nrestrictive in the sense that the maximum level of sparsity allowed on \u03b2\u2217has decreased\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n30\nby a factor proportional to \u03ba(A) compared to the upper bound \u00afd0 (4.3) in Theorem 6;\nNow we require that |supp(\u03b2\u2217)| \u2264\u00afd, where for CA =\n1\n160M 2\n+ ,\n\u00afd\n\u224d\nn\u03bbmin(A)2\n1024C2\u03d6(s0 + 1)2 log m\n1\n2400\u03ba(A)\n(5.7)\n\u2248\nCA\nn\nlog m\n\u0012 \u03bbmin(A)\n15\u03bbmax(A)\n\u0013\n\u224d\u00afd0\n1\n\u03ba(A).\nTo consider the general cases as stated in Theorem 6, we consider the ideal case when\nwe set\n\u03b6 = \u03b1u = 11\n8 \u03bbmax(A)\nsuch that\n\u03b6\n\u00af\u03b1\u2113\n\u2248\u03b1u/(59\n60\u03b1\u2113) \u224d\u03ba(A),\nwhere \u03b1\u2113= 5\n8\u03bbmin(A).\nFollowing the derivation in Remark 14.1, we have\n\u03be\n1 \u2212\u03ba \u22646\u03c4(Ln) + 80\u03b6\n\u00af\u03b1\u2113\n\u03c4(Ln) \u2248200\u03ba(A)\u03c4(Ln).\n(5.8)\nCombining (5.6) and (5.8), it is clear that one can set\n\u03bb\n=\n\u2126\n \n\u03ba(A)\u03c8\nr\nlog m\nn\n!\n(5.9)\nin order to satisfy the condition (5.2) on \u03bb in Theorem 2 when we set\nR \u224db0\np \u00afd0\n=\nO\n \n\u03c8\n\u03c4\nr\nlog m\nn\n!\n(5.10)\nand hence\nR\u03c4\u03ba(A)\n=\nO\n \n\u03ba(A)\u03c8\nr\nlog m\nn\n!\n.\nThis choice is potentially too conservative because we are setting R in (5.10) with re-\nspect to the upper sparsity level \u00afd0 chosen to guarantee statistical convergence, leading\nto a larger than necessary penalty parameter as in (5.9). Similarly, when we choose\nstep size parameter \u03b6 to be too large, we need to increase the penalty parameter \u03bb\ncorrespondingly given the following lower bound: \u03bb = \u2126\n\u0010\nR\u03be\n1\u2212\u03ba\n\u0011\nwhere\nR\u03be\n1 \u2212\u03ba\n=\nR\n \n2\u03c4(Ln)\n \u00af\u03b1\u2113\n4\u03b6 + 2\u03f1\n\u00af\u03b1\u2113\n4\u03b6 \u22122\u03f1 +\n5\n\u00af\u03b1\u2113\n4\u03b6 \u22122\u03f1\n!!\n\u2265\n40R\u03c4(Ln) \u03b6\n\u00af\u03b1\u2113\n+ 2R\u03c4(Ln) \u224dR\u03c4(Ln)\u03ba(A).\nSuppose we set \u03b6 = 3\n2\u03bbmax(A) and\n\u03b6\n\u00af\u03b1\u2113\u22483\u03ba(A) as in Theorem 9. It turns out that the\nless conservative choice of \u03bb as in (5.11)\n\u03bb\n\u224d\n\u0010\nb0\np\n\u03ba(A)\u03d6(s0)\n_\n\u03c8\n\u0011 r\nlog m\nn\n(5.11)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n31\nis suf\ufb01cient, for example when \u03c4B = \u2126(1), for which we now set\nR \u224db0\n\u221a\nd \u224d\nb0\n20M+\n1\np\n6\u03ba(A)\nr\nn\nlog m\nas in Corollary 10. We will discuss the two scenarios as considered in Section 4. See\nthe detailed discussions in Section 14.\n6. Proof of theorems\nIn Section 6.1, we develop in Theorem 12 the crucial large deviation bound on\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r.\nThis entity appears in the constraint set in the Conic programming estimator (1.8), and\nis directly related to the choice of \u03bb for the corrected Lasso estimator in view of The-\norem 16. Its corollaries are stated in Corollary 13 and Corollary 14. In section 6.2,\nwe provide an outline and additional Lemmas 15 and 17 to prove Theorem 3. The full\nproof of Theorem 3 appears in Section 10. In Section 6.3, we give an outline illustrating\nthe improvement for the Lasso error bounds as stated in Theorem 6. We emphasize the\nimpact of this improvement over sparsity parameter d, which we restate in Lemma 18.\nIn Section 6.4, we provide an outline as well as technical results for Theorem 4. In\nSection 6.5, we give an outline illuminating the improvement in error bounds for the\nConic programming estimator as stated in Theorem 7.\n6.1. Stochastic error terms\nIn this section, we \ufb01rst develop stochastic error bounds in Lemma 11, where we also\nde\ufb01ne some events B4, B5, B10. Recall that B6 was de\ufb01ned in Lemma 5. Putting the\nbounds in Lemma 11 together with that in Lemma 5 yields Theorem 12.\nLemma 11. Assume that the stable rank of B, \u2225B\u22252\nF / \u2225B\u22252\n2 \u2265log m. Let Z, X0 and\nW as de\ufb01ned in Theorem 3. Let Z0, Z1 and Z2 be independent copies of Z. Let \u03f5T \u223c\nY M\u03f5/K where Y := eT\n1 ZT\n0 . Denote by B4 the event such that for \u03c1n := C0K\nq\nlog m\nn\n,\n1\nn\n\r\r\rA\n1\n2 ZT\n1 \u03f5\n\r\r\r\n\u221e\n\u2264\n\u03c1nM\u03f5a1/2\nmax\nand\n1\nn\n\r\r\rZT\n2 B\n1\n2 \u03f5\n\r\r\r\n\u221e\n\u2264\n\u03c1nM\u03f5\n\u221a\u03c4B\nwhere\n\u03c4B = tr(B)\nn\n.\nThen P (B4) \u22651 \u22124/m3. Moreover, denote by B5 the event such that\n1\nn\n\r\r(ZT BZ \u2212tr(B)Im)\u03b2\u2217\r\r\n\u221e\n\u2264\n\u03c1nK \u2225\u03b2\u2217\u22252\n\u2225B\u2225F\n\u221an\nand\n1\nn\n\r\rXT\n0 W\u03b2\u2217\r\r\n\u221e\n\u2264\n\u03c1nK \u2225\u03b2\u2217\u22252\n\u221a\u03c4Ba1/2\nmax.\nThen P (B5) \u22651 \u22124/m3.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n32\nFinally, denote by B10 the event such that\n1\nn\n\r\r(ZT BZ \u2212tr(B)Im)\n\r\r\nmax\n\u2264\n\u03c1nK \u2225B\u2225F\n\u221an\nand\n1\nn\n\r\rXT\n0 W\n\r\r\nmax\n\u2264\n\u03c1nK\u221a\u03c4Ba1/2\nmax.\nThen P (B10) \u22651 \u22124/m2.\nWe prove Lemma 11 in Section C.2. Denote by B0 := B4 \u2229B5 \u2229B6, which we use\nthroughout this paper.\nTheorem 12. Suppose (A1) holds. Let \u03c1n = C0K\nq\nlog m\nn\n. Suppose that\n\u2225B\u22252\nF / \u2225B\u22252\n2 \u2265log m\nwhere m \u226516.\nLet b\u0393 and b\u03b3 be as in (1.6). Let D0 = \u221a\u03c4B + \u221aamax and D\u2032\n0 be as de\ufb01ned in (2.23).\nLet D1 = \u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221an . On event B0, for which P (B0) \u22651 \u221216/m3,\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\n\u0012\nD\u2032\n0K\u03c4 1/2\nB\n\u2225\u03b2\u2217\u22252 + 2D1K\n\u221am \u2225\u03b2\u2217\u2225\u221e+ D0M\u03f5\n\u0013\n\u03c1n.\n(6.1)\nWe next state the \ufb01rst Corollary 13 of Theorem 12, which we use in proving Theo-\nrems 3 and 4. Here we state a somewhat simpli\ufb01ed bound on\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221efor the sake\nof reducing the number of unknown parameters involved with a slight worsening of the\nstatistical error bounds when \u03c4B \u224d1. On the other hand, the bound in (6.1) provides a\nsigni\ufb01cant improvement over the error bound in Corollary 13 in case \u03c4B = o(1).\nCorollary 13. Suppose all conditions in Theorem 12 hold. Let b\u0393 and b\u03b3 be as in (1.6).\nOn event B0, we have for D2 = 2(\u2225A\u22252 + \u2225B\u22252) and some absolute constant C0\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\u2264\u03c8\nr\nlog m\nn\n,\nwhere \u03c8 = C0D2K (K \u2225\u03b2\u2217\u22252 + M\u03f5)\nis as de\ufb01ned in Theorem 3.\nIn particular, Corollary 13 ensures that for the corrected Lasso estimator, (6.7) holds\nwith high probability for \u03bb chosen as in (3.6). We prove Corollary 13 in Section D.\nWhat happens when \u03c4B \u21920? Recall D0 = \u221a\u03c4B +a1/2\nmax and D\u2032\n0 :=\np\n\u2225B\u22252 +a1/2\nmax.\nWhen \u03c4B \u21920, we have by Theorem 12\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n=\nO\n\u0012\nD1K 1\n\u221am \u2225\u03b2\u2217\u2225\u221e+ D0KM\u03f5\n\u0013\nK\nr\nlog m\nn\nwhere D0 \u2192a1/2\nmax and D1 =\n\u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221an\n\u2192\u2225A\u22251/2\n2\nunder (A1), given that\n\u2225B\u2225F /\u221an \u2264\u03c4 1/2\nB\n\u2225B\u22251/2\n2\n\u21920. In this case, the error term involving \u2225\u03b2\u2217\u22252 in (4.2)\nvanishes, and we only need to set (cf. Theorem 16)\n\u03bb \u22652\u03c8\nr\nlog m\nn\nfor\n\u03c8 \u224da1/2\nmaxKM\u03f5 + \u2225A\u22251/2\n2\nK2 \u2225\u03b2\u2217\u2225\u221e/m1/2, (6.2)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n33\nwhere the second term in \u03c8 de\ufb01ned immediately above comes from the estimation error\nin Lemma 5; this term vanishes if we were to assume that (1) tr(B) is also known or\n(2) \u2225\u03b2\u2217\u2225\u221e= o(M\u03f5m1/2/K). For both cases, by setting \u03bb \u224d4a1/2\nmaxKM\u03f5\nq\nlog m\nn\n, we\ncan recover the regular Lasso rate of\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\nq = Op(\u03bbd1/q),\nfor\nq = 1, 2,\nwhen the design matrix X is almost free of measurement errors.\nFinally, we state a second Corollary 14 of Theorem 12. Corollary 14 is essentially a\nrestatement of the bound in (6.1).\nCorollary 14. Suppose all conditions in Theorem 12 hold. Let D0, D\u2032\n0, Doracle, and\n\u03c4 +/2\nB\n:= \u03c4 1/2\nB\n+ Doracle\n\u221am\nbe as de\ufb01ned in (2.23) and (4.1). On event B0,\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\n\u03c8\nr\nlog m\nn\n,\nwhere\n\u03c8 := C0K\n\u0010\nD\u2032\n0\u03c4 +/2\nB\nK \u2225\u03b2\u2217\u22252 + D0M\u03f5\n\u0011\n.\n(6.3)\nThen P (B0) \u22651 \u221216/m3.\nWe mention in passing that Corollaries 13 and 14 are crucial in proving Theorems 3, 4, 6\nand 7.\n6.2. Outline for proof of Theorem 3\nIn this section, we state Theorem 16, and two Lemmas 15 and 17. Theorem 3 fol-\nlows from Theorem 16 in view of Corollary 13, Lemmas 15 and 17. In more details,\nLemma 15 checks the Lower and the Upper RE conditions on the modi\ufb01ed gram ma-\ntrix,\nb\u0393A := 1\nn(XT X \u2212btr(B)Im),\n(6.4)\nwhile Lemma 17 checks condition (6.6) as stated in Theorem 16 for curvature \u03b1 and\ntolerance \u03c4 regarding the lower RE condition as derived in Lemma 15.\nFirst, we replace (A3) with (A3\u2019) which reveals some additional information regarding\nthe constant hidden inside the O(\u00b7) notation.\n(A3\u2019) Suppose (A3) holds; moreover, mn \u22654096C2\n0D2\n2K4 log m/\u03bb2\nmin(A) for D2 =\n2(\u2225A\u22252 + \u2225B\u22252), or equivalently,\n\u03bbmin(A)\n\u2225A\u22252 + \u2225B\u22252\n> CK\nr\nlog m\nmn for some large enough contant CK.\nLemma 15. (Lower and Upper-RE conditions) Suppose (A1), (A2) and (A3\u2019) hold.\nDenote by V := 3eM 3\nA/2, where MA is as de\ufb01ned in (2.7). Let s0 \u226532 be as de\ufb01ned\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n34\nin (2.6). Recall that we denote by A0 the event that the modi\ufb01ed gram matrix b\u0393 as\nde\ufb01ned in (1.6) satis\ufb01es the Lower as well as Upper RE conditions with\ncurvature\n\u03b1 = 5\n8\u03bbmin(A), smoothness e\u03b1 = 11\n8 \u03bbmax(A)\nand tolerance\n384C2\u03d6(s0)2\n\u03bbmin(A)\nlog m\nn\n\u2264\u03c4 := \u03bbmin(A) \u2212\u03b1\ns0\n\u2264396C2\u03d62(s0 + 1)\n\u03bbmin(A)\nlog m\nn\nfor \u03b1, e\u03b1 and \u03c4 as de\ufb01ned in De\ufb01nitions 2.2 and 2.3, and C, s0, \u03d6(s0) in (2.6). Suppose\nthat for some c\u2032 > 0 and c\u2032K4 < 1,\ntr(B)\n\u2225B\u22252\n\u2265\nc\u2032K4 s0\n\u03b52 log\n\u00123em\ns0\u03b5\n\u0013\nwhere \u03b5 =\n1\n2MA\n.\n(6.5)\nThen P (A0) \u22651 \u22124 exp\n\u0010\n\u2212\nc3n\nM 2\nA log m log\n\u0010\nVm log m\nn\n\u0011\u0011\n\u22122 exp\n\u0010\n\u22124c2n\nM 2\nAK4\n\u0011\n\u22126/m3.\nThe main focus of the current section is then to apply Theorem 16 to show Theorem 3.\nTheorem 16 follows from Theorem 1 by [30].\nTheorem 16. Consider the regression model in (1.1a) and (1.1b). Let d \u2264n/2. Let b\u03b3, b\u0393\nbe as constructed in (1.6). Suppose that the matrix b\u0393 satis\ufb01es the Lower-RE condition\nwith curvature \u03b1 > 0 and tolerance \u03c4 > 0,\n\u221a\nd\u03c4 \u2264min\n\u001a\n\u03b1\n32\n\u221a\nd\n, \u03bb\n4b0\n\u001b\n,\n(6.6)\nwhere d, b0 and \u03bb are as de\ufb01ned in (1.7). Then for any d-sparse vectors \u03b2\u2217\u2208Rm, such\nthat \u2225\u03b2\u2217\u22252 \u2264b0 and\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\u22641\n2\u03bb,\n(6.7)\nthe following bounds hold:\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2 \u226420\n\u03b1 \u03bb\n\u221a\nd\nand\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n1 \u226480\n\u03b1 \u03bbd,\n(6.8)\nwhere b\u03b2 is an optimal solution to the corrected Lasso estimator as in (1.7).\nWe include the proof of Theorem 16 for the sake of self-containment and defer it to\nSection G for clarity of presentation.\nLemma 17. Let c\u2032, \u03c6, b0, M\u03f5, M+ and K be as de\ufb01ned in Theorem 3, where we assume\nthat b2\n0 \u2265\u2225\u03b2\u2217\u22252\n2 \u2265\u03c6b2\n0 for some 0 < \u03c6 \u22641. Suppose all conditions in Lemma 15\nhold. Suppose that s0 \u226532 and\nd := |supp(\u03b2\u2217)| \u2264CA\nn\nlog m {c\u2032D\u03c6 \u22272}\nwhere CA :=\n1\n40M 2\n+\n(6.9)\nand\nD\u03c6 = K4\n\u0012 M 2\n\u03f5\nK2b2\n0\n+ \u03c6\n\u0013\n\u2265K4\u03c6 \u2265\u03c6.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n35\nThen the following condition holds\nd \u2264\n\u03b1\n32\u03c4\n^ 1\n\u03c4 2\nlog m\nn\n\u0012 \u03c8\nb0\n\u00132\n,\n(6.10)\nwhere \u03c8 = C0D2K(K \u2225\u03b2\u2217\u22252 + M\u03f5) is as de\ufb01ned in (3.6), \u03b1 = 5\u03bbmin(A)/8, and \u03c4 is\nas de\ufb01ned in Lemma 15.\nWe prove Lemmas 15 and 17 in Sections F and H.1 respectively. Lemma 15 follows\nimmediately from Corollary 25. We prove Lemmas 15 and Corollary 25 in Sections F\nand L respectively.\nRemark 6.1. Clearly for d, b0, \u03c6 as bounded in Theorem 3, we have by assump-\ntion (3.3) the following upper and lower bound on D\u03c6:\n2K4\u03c6 \u2265D\u03c6 :=\n\u0012M 2\n\u03f5 K2\nb2\n0\n+ K4\u03c6\n\u0013\n\u2265K4\u03c6.\nIn this regime, the conditions on d as in (6.9) can be conveniently expressed as that in\n(3.4) instead.\n6.3. Improved bounds for the corrected Lasso estimator\nThe proof of Theorem 6 follows exactly the same line of arguments as in Theorem 3,\nexcept that we now use the improved bound on the error term\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221egiven\nin Corollary 14, instead of that in Corollary 13. Moreover, we replace Lemma 17 with\nLemma 18, the proof of which follows from Lemma 17 with d now being bounded as in\n(4.3) and \u03c8 being rede\ufb01ned as in (6.3). The proof of Lemma 18 appears in Section H.2.\nSee Section 12 for the proof of Theorem 6.\nLemma 18. Let c\u2032, \u03c6, b0, M\u03f5, M+ and K be as de\ufb01ned in Theorem 3. Suppose all\nconditions in Lemma 15 hold. Suppose that (4.3) holds:\nd := |supp(\u03b2\u2217)| \u2264CA\nn\nlog m {c\u2032c\u2032\u2032D\u03c6 \u22278} ,\nwhere CA :=\n1\n160M 2\n+\n, (6.11)\nc\u2032\u2032 := \u2225B\u22252 + amax\n\u03d6(s0 + 1)2\n\u2264\n\u0012\nD\u2032\n0\n\u03d6(s0 + 1)\n\u00132\nand D\u03c6 = K2M 2\n\u03f5\nb2\n0\n+ \u03c4 +\nB K4\u03c6.\nThen (6.10) holds with \u03c8 as de\ufb01ned in Theorem 6 and \u03b1 = 5\n8\u03bbmin(A).\n6.4. Outline for proof of Theorem 4\nWe provide an outline and state the technical lemmas needed for proving Theorem 4.\nOur \ufb01rst goal is to show that the following holds with high probability,\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n=\n\r\r 1\nnXT (y \u2212X\u03b2\u2217) + 1\nn btr(B)\u03b2\u2217\r\r\n\u221e\u2264\u00b5 \u2225\u03b2\u2217\u22252 + \u03c9,\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n36\nwhere \u00b5, \u03c9 are chosen as in (6.12). This forms the basis for proving the \u2113q convergence,\nwhere q \u2208[1, 2], for the Conic programming estimator (1.8). This follows immediately\nfrom Theorem 12 and Corollary 13. More explicitly, we will state it in Lemma 19.\nLemma 19. Let D0 = \u221a\u03c4B +\u221aamax and D2 = 2(\u2225A\u22252 +\u2225B\u22252) be as in Theorem 4.\nSuppose all conditions in Theorem 12 hold. Then on event B0 as de\ufb01ned therein, the\npair (\u03b2, t) = (\u03b2\u2217, \u2225\u03b2\u2217\u22252) belongs to the feasible set of the minimization problem (1.8)\nwith\n\u00b5 \u224d2D2K\u03c1n\nand\n\u03c9 \u224dD0M\u03f5\u03c1n,\nwhere\n\u03c1n := C0K\nr\nlog m\nn\n.\n(6.12)\nBefore we proceed, we \ufb01rst need to introduce some notation and de\ufb01nitions. Let X0 =\nZ1A1/2 be de\ufb01ned as in (1.4). Let k0 = 1+\u03bb. First we need to de\ufb01ne the \u2113q-sensitivity\nparameter for \u03a8 := 1\nnXT\n0 X0 following [3]:\n\u03baq(d0, k0)\n=\nmin\nJ:|J|\u2264d0\nmin\n\u2206\u2208ConeJ(k0)\n\u2225\u03a8\u2206\u2225\u221e\n\u2225\u2206\u2225q\n,\nwhere\n(6.13)\nConeJ(k0)\n=\n{x \u2208Rm | s.t. \u2225xJc\u22251 \u2264k0 \u2225xJ\u22251} .\n(6.14)\nSee also [21]. Let (b\u03b2, bt) be the optimal solution to (1.8) and denote by v = b\u03b2 \u2212\u03b2\u2217. We\nwill state the following auxiliary lemmas, the \ufb01rst of which is deterministic in nature.\nThe two lemmas re\ufb02ect the two geometrical constraints on the optimal solution to (1.8).\nThe optimal solution b\u03b2 satis\ufb01es:\n1. The vector v obeys the following cone constraint: \u2225vSc\u22251 \u2264k0 \u2225vS\u22251, and bt \u2264\n1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252.\n2. \u2225\u03a8v\u2225\u221eis upper bounded by a quantity at the order of O (\u00b5(\u2225\u03b2\u2217\u22252 + \u2225v\u22251) + \u03c9).\nLemma 20. Let \u00b5, \u03c9 > 0 be set. Suppose that the pair (\u03b2, t) = (\u03b2\u2217, \u2225\u03b2\u2217\u22252) belongs\nto the feasible set of the minimization problem (1.8), for which (b\u03b2, bt) is an optimal\nsolution. Denote by v = b\u03b2 \u2212\u03b2\u2217. Then\n\u2225vSc\u22251\n\u2264\n(1 + \u03bb) \u2225vS\u22251 and bt \u22641\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252 .\nLemma 21. On event B0 \u2229B10,\n\u2225\u03a8v\u2225\u221e\u2264\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52 \u2225v\u22251 + \u03c9\u2032,\nwhere \u00b51 = 2\u00b5, \u00b52 = \u00b5( 1\n\u03bb + 1) and \u03c9\u2032 = 2\u03c9 for \u00b5, \u03c9 as de\ufb01ned in (6.12).\nNow combining Lemma 6 of [3] and an earlier result of the two authors (cf. Theo-\nrem 27 [38]), we can show that the RE(2d0, 3(1 + \u03bb), A1/2) condition and the sample\nrequirement as in (4.9) are enough to ensure that the \u2113q-sensitivity parameter satis\ufb01es\nthe following lower bound for all 1 \u2264q \u22642: for some contant c,\n\u03baq(d0, k0)\n\u2265\ncd\u22121/q\n0\n,\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n37\nwhich ensures that for v = b\u03b2 \u2212\u03b2\u2217and \u03a8 = 1\nnXT\n0 X0,\n\u2225\u03a8v\u2225\u221e\n\u2265\n\u03baq(d0, k0)\u2225v\u2225q \u2265cd\u22121/q\n0\n\u2225v\u2225q .\n(6.15)\nCombining (6.15) with Lemmas 19, 20 and 21 gives us both the lower and upper\nbounds on \u2225\u03a8v\u2225\u221e, with the lower bound being \u03baq(d0, k0) \u2225v\u2225q and the upper bound as\nspeci\ufb01ed in Lemma 21. Following some algebraic manipulation, this yields the bound\non the \u2225v\u2225q for all 1 \u2264q \u22642. We prove Theorem 4 in Section 11 and Lemmas 19, 20\nand 21 in Section I. The proof of Lemma 20 follows the same line of arguments in [3]\nin view of Lemma 19.\n6.5. Improved bounds for the DS-type estimator\nLemma 22 follows directly from Corollary 14.\nLemma 22. Suppose all conditions in Corollary 14 hold. Let D0 = \u221a\u03c4B+\u221aamax \u224d1\nunder (A1). Then on event B0, the pair (\u03b2, t) = (\u03b2\u2217, \u2225\u03b2\u2217\u22252) belongs to the feasible set\n\u03a5 of the minimization problem (1.8) with\n\u00b5 \u2265D\u2032\n0\u03c4 +/2\nB\nK\u03c1n\nand\n\u03c9 \u2265D0M\u03f5\u03c1n,\n(6.16)\nwhere \u03c4 +/2\nB\n:= \u03c4 1/2\nB\n+ Doracle\n\u221am\nis as de\ufb01ned in (4.1).\nLemma 23. On event B6 and (A1), the choice of e\u03c4 1/2\nB\n:= b\u03c4 1/2\nB\n+ C6r1/2\nm,m as in (4.11),\nwhere recall rm,m = 2C0K2\nq\nlog m\nmn , satis\ufb01es for m \u226516 and C0 \u22651,\n\u03c4 +/2\nB\n\u2264\ne\u03c4 1/2\nB\n\u2264\u03c4 1/2\nB\n+ 3\n2C6r1/2\nm,m =: \u03c4 \u2020/2\nB ,\n(6.17)\ne\u03c4B\n\u2264\n2\u03c4B + 3C2\n6rm,m \u224d\u03c4 \u2021\nB, and moreover\ne\u03c4 1/2\nB \u03c4 \u2212\nB \u22641.\n(6.18)\nWe next state an updated result in Lemma 24.\nLemma 24. On event B0 \u2229B10, the solution b\u03b2 to (1.8) with \u00b5, \u03c9 as in (4.11) satis\ufb01es\nfor v := b\u03b2 \u2212\u03b2\u2217\n\r\r 1\nnXT\n0 X0v\n\r\r\n\u221e\u2264\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52 \u2225v\u22251 + \u03c9\u2032,\nwhere \u00b51 = 2\u00b5, \u00b52 = 2\u00b5(1 +\n1\n2\u03bb) and \u03c9\u2032 = 2\u03c9.\n7. Lower and Upper RE conditions\nThe goal of this section is to show that for \u2206de\ufb01ned in (7.4), the presumption in\nLemmas 37 and 39 as restated in (7.1) holds with high probability (cf Theorem 26).\nWe \ufb01rst state a deterministic result showing that the Lower and Upper RE conditions\nhold for b\u0393A under condition (7.1) in Corollary 25. This allows us to prove Lemma 15 in\nSection F. See Sections K and L, where we show that Corollary 25 follows immediately\nfrom the geometric analysis result as stated in Lemma 39.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n38\nCorollary 25. Let 1/8 > \u03b4 > 0. Let 1 \u2264k < m/2. Let Am\u00d7m be a symmetric\npositive semide\ufb01nite covariance matrice. Let b\u0393A be an m \u00d7 m symmetric matrix and\n\u2206= b\u0393A \u2212A. Let E = \u222a|J|\u2264kEJ, where EJ = span{ej : j \u2208J}. Suppose that\n\u2200u, v \u2208E \u2229Sm\u22121\n\f\fuT \u2206v\n\f\f \u2264\u03b4 \u22643\n32\u03bbmin(A).\n(7.1)\nThen the Lower and Upper RE conditions hold: for all \u03c5 \u2208Rm,\n\u03c5T b\u0393A\u03c5\n\u2265\n5\n8\u03bbmin(A) \u2225\u03c5\u22252\n2 \u22123\u03bbmin(A)\n8k\n\u2225\u03c5\u22252\n1\n(7.2)\nand\n\u03c5T b\u0393A\u03c5\n\u2264\n11\n8 \u03bbmax(A) \u2225\u03c5\u22252\n2 + 3\u03bbmin(A)\n8k\n\u2225\u03c5\u22252\n1 .\n(7.3)\nTheorem 26. Let Am\u00d7m, Bn\u00d7n be symmetric positive de\ufb01nite covariance matrices.\nLet E = \u222a|J|\u2264kEJ for 1 \u2264k < m/2. Let Z, X be n \u00d7 m random matrices de\ufb01ned as\nin Theorem 3. Let b\u03c4B be de\ufb01ned as in (1.5). Let\n\u2206:= b\u0393A \u2212A := 1\nnXT X \u2212b\u03c4BIm \u2212A.\n(7.4)\nSuppose that for some absolute constant c\u2032 > 0 and 0 < \u03b5 \u22641\nC ,\ntr(B)\n\u2225B\u22252\n\u2265\n\u0012\nc\u2032K4 k\n\u03b52 log\n\u00123em\nk\u03b5\n\u0013\u0013 _\nlog m,\n(7.5)\nwhere C = C0/\n\u221a\nc\u2032 for C0 as chosen to satisfy (H.6).\nThen with probability at least 1\u22124 exp\n\u0010\n\u2212c2\u03b52\ntr(B)\nK4\u2225B\u22252\n\u0011\n\u22122 exp\n\u0000\u2212c2\u03b52 n\nK4\n\u0001\n\u22126/m3\nfor c2 \u22652, we have for all u, v \u2208E \u2229Sm\u22121,\n\f\fuT \u2206v\n\f\f \u22648C\u03d6(k)\u03b5 + 4C0D1K2\nr\nlog m\nmn ,\nwhere \u03d6(k) = \u03c4B + \u03c1max(k, A), and D1 \u2264\u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221an ,\nWe prove Theorem 26 in Section M.\n8. Numerical results\nIn this section, we present results from numerical simulations designed to validate the\ntheoretical predictions as presented in previous sections. We implemented the compos-\nite gradient descent algorithm as described in [1, 30, 31] for solving the corrected Lasso\nobjective function (1.7) with (b\u0393, b\u03b3) as de\ufb01ned in (1.6). For the Conic programming es-\ntimator, we use the implementation provided by the authors [3] with the same input\n(b\u0393, b\u03b3) (1.6). Throughout our experiments, A is a correlation matrix with amax = 1. We\nset the following as our default parameters: D\u2032\n0 = \u2225B\u22251/2\n2\n+ 1, D0 = \u221a\u03c4B + 1 and\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n39\nR = \u2225\u03b2\u2217\u22252\n\u221a\nd, where d is the sparsity parameter, the number of non-zero entries in\n\u03b2\u2217. In one set of simulations, we also vary R.\nIn our simulations, we look at three different models from which A and B will be\nchosen. Let \u2126= A\u22121 = (\u03c9ij) and \u03a0 = B\u22121 = (\u03c0ij). Let E denote edges in \u2126, and\nF denote edges in \u03a0. We choose A from one of these two models:\n\u2022 AR(1) model. In this model, the covariance matrix is of the form A = {\u03c1|i\u2212j|}i,j.\nThe graph corresponding to the precision matrix A\u22121 is a chain.\n\u2022 Star-Block model. In this model the covariance matrix is block-diagonal with\nequal-sized blocks whose inverses correspond to star structured graphs, where\nAii = 1, for all i. We have 32 subgraphs, where in each subgraph, 16 nodes\nare connected to a central hub node with no other connections. The rest of the\nnodes in the graph are singletons. The covariance matrix for each block S in A\nis generated by setting Sij = \u03c1A if (i, j) \u2208E, and Sij = \u03c12\nA otherwise.\nWe choose B from one of the following models. Recall that \u03c4B = tr(B)/n.\n\u2022 For B and B\u2217= B/\u03c4B = \u03c1(B), we consider the AR(1) model with two param-\neters. First we choose the AR(1) parameter \u03c1B\u2217\u2208{0.3, 0.7} for the correlation\nmatrix B\u2217. We then set B = \u03c4BB\u2217, where \u03c4B \u2208{0.3, 0.7, 0.9} depending on\nthe experimental setup.\n\u2022 We also consider a second model based on \u03a0 = B\u22121, where we use the random\nconcentration matrix model in [57]. The graph is generated according to a type\nof Erd\u02ddos\u2013R\u00b4enyi random graph model. Initially, we set \u03a0 = cIn\u00d7n, and c is\na constant. Then we randomly select n log n edges and update \u03a0 as follows:\nfor each new edge (i, j), a weight w > 0 is chosen uniformly at random from\n[wmin, wmax] where wmax > wmin > 0; we subtract w from \u03c0ij and \u03c0ji, and\nincrease \u03c0ii and \u03c0jj by w. This keeps \u03a0 positive de\ufb01nite. We then rescale B to\nhave a certain desired trace parameter \u03c4B.\nFor a given \u03b2\u2217, we \ufb01rst generate matrices A and B, where A is m \u00d7 m and B is n \u00d7 n.\nFor the given covariance matrices A and B, we repeat the following steps to estimate\n\u03b2\u2217in the errors-in-variables model as in (1.1a) and (1.1b),\n1. We \ufb01rst generate random matrices X0 \u223cNf,m(0, A\u2297I) and W \u223cNf,m(0, I \u2297\nB) independently from the matrix variate normal distribution as follows. Let Z \u2208\nRn\u00d7m be a Gaussian random ensemble with independent entries Zij satisfying\nEZij = 0, EZ2\nij = 1. Let Z1, Z2 be independent copies of Z. Let X0 = Z1A1/2\nand W = B1/2Z2, where A1/2 and B1/2 are the unique square root of the\npositive de\ufb01nite matrix A and B = \u03c4BB\u2217respectively.\n2. We then generate X = X0 + W and y = X0\u03b2\u2217+ \u03f5, where \u03f5i i.i.d. \u223cN(0, 1).\nWe compute b\u03c4B, b\u03b3 and b\u0393 according to (1.5) and (1.6) using X, y, where by (1.5),\nb\u03c4B := 1\nn btr(B) =\n1\nmn\n\u0000\u2225X\u22252\nF \u2212ntr(A)\n\u0001\n+.\n3. Finally, we feed X and y to the Composite Gradient Descent algorithm as de-\nscribed in [1, 30] to solve the Lasso program (1.7) to recover \u03b2\u2217, where we set\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n40\nthe step size parameter to be \u03b6. The output of this step is denoted by b\u03b2, the esti-\nmated \u03b2\u2217. We then compute the relative error of b\u03b2: \u2225b\u03b2 \u2212\u03b2\u2217\u2225/\u2225\u03b2\u2217\u2225, where \u2225\u00b7\u2225\ndenotes either the \u21131 or the \u21132 norm.\nThe \ufb01nal relative error is the average of 100 runs for each set of tuning and step-size\nparameters; for the Conic programming estimator, we solve (1.8) instead of (1.7) to\nrecover \u03b2\u2217.\n8.1. Relative error\nIn the \ufb01rst experiment, A and B are generated using the AR(1) model with parameters\n\u03c1A, \u03c1B\u2217\u2208{0.3, 0.7} and trace parameter \u03c4B \u2208{0.3, 0.7, 0.9}. We see in Figures 1\nand 2 that a larger sample size is required when \u03c1A, \u03c1B\u2217or \u03c4B increases. To explain\nthese results, we \ufb01rst recall the following de\ufb01nition of the Signal-to-noise ratio, where\nwe take K = M\u03b5 \u224d1\nS/M\n\u224d\n\u2225\u03b2\u2217\u22252\n2\n\u03c4B \u2225\u03b2\u2217\u22252\n2 + 1\n=\n1\n\u03c4B + (1/ \u2225\u03b2\u2217\u22252\n2)\n,\nwhere\nS\n:=\n\u2225\u03b2\u2217\u22252\n2 and\nM := 1 + \u03c4B \u2225\u03b2\u2217\u22252\n2 ,\nwhich clearly increases as \u2225\u03b2\u2217\u22252\n2 increases or as the measurement error metric \u03c4B de-\ncreases. We keep \u2225\u03b2\u2217\u22252 = 5 throughout our simulations. The corrected Lasso recovery\nproblem thus becomes more dif\ufb01cult as \u03c4B increases. Indeed, we observe that a larger\nsample size n is needed when \u03c4B increases from 0.3 to 0.9 in order to control the rela-\ntive \u21132 error to stay at the same level. Moreover, in view of Theorem 6, we can express\nthe relative error as follows: for \u03b1 \u224d\u03bbmin(A) and K \u224d1,\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2\n\u2225\u03b2\u2217\u22252\n= OP\n \n(\u2225B\u22251/2\n2\n+ 1)\n\u03bbmin(A)\nr\nM\nS\nr\nd log m\nn\n!\n.\n(8.1)\nNote that when \u2225\u03b2\u2217\u22252 is large enough and \u03c4B = \u2126(1), the factor preceding\nq\nd log m\nn\non the RHS of (8.1) is proportional to (\u2225B\u22251/2\n2\n+1)\u221a\u03c4B\n\u03bbmin(A)\n.\nWhen we plot the relative \u21132 error \u2225b\u03b2 \u2212\u03b2\u2217\u22252/\u2225\u03b2\u2217\u22252 versus the rescaled sample size\nn\nd log m under the same S/M ratio, the two sets of curves corresponding to \u03c1A = 0.3\nand \u03c1A = 0.7 indeed line up in Figure 1(b), as predicted by (8.1). We observe in\nFigure 1(b), the rescaled curves overlap well for different values of (m, d) for each\n\u03c1A when we keep (\u03c1B\u2217, \u03c4B) and the length \u2225\u03b2\u2217\u22252 = 5 invariant. Moreover, the upper\nbound on the relative \u21132 error (8.1) characterizes the relative positions of these two\nsets of curves in that the ratio between the \u21132 error corresponding to \u03c1A = 0.7 and\nthat for \u03c1A = 0.3 along the y-axis roughly falls within the interval (2, 3) for each n,\nwhile \u03bbmin(AR(1), 0.3)/\u03bbmin(AR(1), 0.7) = 3. These results are consistent with the\ntheoretical predictions in Theorems 3 and 6.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n41\n\u03c1B*=0.3 \u03c4B=0.3\nn\nrelative l2 error\n\u03c1A=0.3 m = 256\n\u03c1A=0.3 m = 512\n\u03c1A=0.3 m = 1024\n\u03c1A=0.7 m = 256\n\u03c1A=0.7 m = 512\n\u03c1A=0.7 m = 1024\n500\n1000\n1500\n2000\n2500\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n\u03c1B*=0.3 \u03c4B=0.3\nn / d log(m)\nrelative l2 error\n\u03c1A=0.3 m = 256\n\u03c1A=0.3 m = 512\n\u03c1A=0.3 m = 1024\n\u03c1A=0.7 m = 256\n\u03c1A=0.7 m = 512\n\u03c1A=0.7 m = 1024\n2\n4\n6\n8\n10\n12\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(a)\n(b)\n\u03c1A=0.3 \u03c1B* = 0.3\nn / d log(m)\nrelative l2 error\n\u03c4B=0.3 m = 256\n\u03c4B=0.3 m = 512\n\u03c4B=0.3 m = 1024\n\u03c4B=0.7 m = 256\n\u03c4B=0.7 m = 512\n\u03c4B=0.7 m = 1024\n\u03c4B=0.9 m = 256\n\u03c4B=0.9 m = 512\n\u03c4B=0.9 m = 1024\n500\n1000\n1500\n2000\n2500\n0.2\n0.4\n0.6\n0.8\n\u03c1A=0.3 \u03c1B* = 0.3\nn / d log(m)\nrelative l2 error\n\u03c4B=0.3 m = 256\n\u03c4B=0.3 m = 512\n\u03c4B=0.3 m = 1024\n\u03c4B=0.7 m = 256\n\u03c4B=0.7 m = 512\n\u03c4B=0.7 m = 1024\n\u03c4B=0.9 m = 256\n\u03c4B=0.9 m = 512\n\u03c4B=0.9 m = 1024\n2\n4\n6\n8\n10\n12\n0.2\n0.4\n0.6\n0.8\n(c)\n(d)\nFIG 1. Plots of the relative \u21132 error after running composite gradient descent algorithm on recovering \u03b2\u2217\nusing the corrected Lasso objective function with sparsity parameter d = \u230a\u221am\u230b, where we vary m \u2208\n{256, 512, 1024}. We generate A and B using the AR(1) model with \u03c1A, \u03c1B\u2217\u2208{0.3, 0.7} and \u03c4B =\n{0.3, 0.7, 0.9}. In the left and right column, we plot the relative \u21132 error with respect to sample size n as\nwell as the rescaled sample size n/(d log m). As n increases, we see that the statistical error decreases. In\nthe top row, we vary the AR(1) parameter \u03c1A \u2208{0.3, 0.7}, while holding (\u03c4B, \u03c1\u2217\nB) and \u2225\u03b2\u2217\u22252 invariant.\nPlot (a) shows the relative \u21132 error versus n for m = 256, 512, 1024. In Plots (c) and (d), we vary the trace\nparameter \u03c4B \u2208{0.3, 0.7, 0.9}, while \ufb01xing the AR(1) parameters \u03c1A, \u03c1\u2217\nB = 0.3. Plot (b) and (d) show\nthe relative \u21132 error versus the rescaled sample size n/(d log m). The curves now align for different values\nof m in the rescaled plots, consistent with the theoretical prediction in Theorem 6.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n42\n\u03c1A = 0.3\nn\nrelative l2 error\n\u03c4B= 0.3\n\u03c4B= 0.7\nm=256\nm=512\nm=1024\nm=256\nm=512\nm=1024\n\u03c1B*=0.3 \n\u03c1B*=0.3 \n\u03c1B*=0.3 \n\u03c1B*=0.7 \n\u03c1B*=0.7 \n\u03c1B*=0.7 \n\u03c1B*=0.3 \n\u03c1B*=0.3 \n\u03c1B*=0.3 \n\u03c1B*=0.7 \n\u03c1B*=0.7 \n\u03c1B*=0.7 \n500\n1000\n1500\n2000\n2500\n0.2\n0.4\n0.6\n0.8\n\u03c1A = 0.3\nn / d log(m)\nrelative l2 error\n\u03c4B= 0.3\n\u03c4B= 0.7\nm=256\nm=512\nm=1024\nm=256\nm=512\nm=1024\n\u03c1B*=0.3 \n\u03c1B*=0.3 \n\u03c1B*=0.3 \n\u03c1B*=0.7 \n\u03c1B*=0.7 \n\u03c1B*=0.7 \n\u03c1B*=0.3 \n\u03c1B*=0.3 \n\u03c1B*=0.3 \n\u03c1B*=0.7 \n\u03c1B*=0.7 \n\u03c1B*=0.7 \n2\n4\n6\n8\n10\n12\n0.2\n0.4\n0.6\n0.8\n(a)\n(b)\nFIG 2. Plots of the relative \u21132 error after running composite gradient descent algorithm on recovering \u03b2\u2217\nusing the corrected Lasso objective function with sparsity parameter d = \u230a\u221am\u230b, where we vary m \u2208\n{256, 512, 1024}. We generate A and B using the AR(1) model with \u03c1A = 0.3 and \u03c1\u2217\nB \u2208{0.3, 0.7}.\nWe set B = \u03c4BB\u2217and vary the trace parameter \u03c4B \u2208{0.3, 0.7} for each value of \u03c1B\u2217. The parameters\n\u03c4B and \u03c1\u2217\nB affect the rate of convergence through D\u2032\n0 = \u2225B\u22251/2\n2\n+ a1/2\nmax and \u03c4 1/2\nB\n. Plot (b) shows the\nrelative \u21132 error versus the rescaled sample size n/(d log m). We observe that as \u03c4B increases from 0.3 to\n0.7, the two sets of curves corresponding to \u03c1B\u2217= 0.3, 0.7 become visibly more separated. As n increases,\nall curves converge to 0.\nIn Figure 1(c) and (d), we also show the effect of \u03c4B when \u03c4B is chosen from {0.3, 0.7, 0.9},\nwhile \ufb01xing the AR(1) parameters \u03c1A = 0.3 and \u03c1B\u2217= 0.3. As predicted by our the-\nory, as the measurement error magnitude \u03c4B increases, M increases, resulting in a larger\nrelative \u21132 error for a \ufb01xed sample size n.\nWhile the effect of \u03c1A as shown in (8.1) through the minimal eigenvalue of A is directly\nvisible in Figure 1(b), the effect of \u03c1B\u2217is more subtle, as it is modulated by \u03c4B as\nshown in Figure 2(a) and (b). When \u03c4B is \ufb01xed, our theory predicts that \u2225B\u22252 plays\na role in determining the \u2113p error, p = 1, 2, through the penalty parameter \u03bb in view\nof (8.1). The effect of \u03c1B\u2217, which goes into the parameter D\u2032\n0 = \u2225B\u22251/2\n2\n+a1/2\nmax \u224d1, is\nnot changing the sample requirement or the rate of convergence as signi\ufb01cantly as that\nof \u03c1A when \u03c4B = 0.3. This is shown in the bottom set of curves in Figure 2(a) and (b).\nOn the other hand, the trace parameter \u03c4B plays a dominating role in determining the\nsample size as well as the \u2113p error for p = 1, 2, especially when the length of the signal\n\u03b2\u2217is large: \u2225\u03b2\u2217\u22252 = \u2126(1). In particular, the separation between the two sets of curves\nin Figure 2(b), which correspond to the two choices of \u03c1B\u2217, is clearly modulated by\n\u03c4B and becomes more visible when \u03c4B = 0.7.\nThese \ufb01ndings are also consistent with our theoretical prediction that in order to guar-\nantee statistical and computational convergence, the sample size needs to grow accord-\ning to the following relationship to be speci\ufb01ed in (8.2). We will show in the proof of\nTheorem 9 that the condition on sparsity d as stated in (5.3) implies that as \u03c1A, or \u03c4B,\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n43\nor the step size parameter \u03b6 increases, we need to increase the sample size in order\nto guarantee computational convergence for the composite gradient descent algorithm\ngiven the following lower bound:\nn\n=\n\u2126\n\u0012\nd\u03c40 log m\n\u001a \u03bbmax(A)\n\u03bbmin(A)2\n\u001b _ \u001a\n\u03b6\n(\u00af\u03b1\u2113)2\n\u001b\u0013\n,\nwhere\n(8.2)\n\u03c40\n\u224d\n(\u03c1max(s0, A) + \u03c4B)2\n\u03bbmin(A)\n.\nWe illustrate the effect of the penalty and step size parameters in Section 8.2.\n8.2. Corrected Lasso via GD versus Conic programming estimator\nIn the second experiment, both A and B are generated using the AR(1) model with\nparameters \u03c1A = 0.3, \u03c1B\u2217= 0.3, and \u03c4B \u2208{0.3, 0.7}. We set m = 1024, d = 10 and\n\u2225\u03b2\u2217\u22252 = 5. We then compare the performance of the corrected Lasso estimator (1.7)\nusing the composite gradient descent algorithmic with the Conic programming estima-\ntor, which is a convex program designed and implemented by authors of [3].\nWe consider three choices for the step size parameter for the composite gradient descent\nalgorithm: \u03b61 = \u03bbmax(A) + 1\n2\u03bbmin(A), \u03b62 =\n3\n2\u03bbmax(A) and \u03b63 = 2\u03bbmax(A). We\nobserve that the gradient descent algorithm consistently produces an output such that\nits statistical error in \u21132 norm is lower than the best solution produced by the Conic\nprogramming estimator, when both methods are subject to optimal tuning after we \ufb01x\nupon the radius R =\n\u221a\nd \u2225\u03b2\u2217\u22252 for (1.10) and (\u03c9, \u03bb) in (1.8) as follows. As illustrated\nin our theory, one can think of the parameter \u03bb in (1.7) and parameters \u00b5, \u03c9 in (1.8)\nsatisfying\n\u03bb \u224d\u00b5 \u2225\u03b2\u2217\u22252 + \u03c9,\nwhere we set \u03c9 = 0.1D0\nq\nlog m\nn\n, where the factor 0.1 is chosen without loss of gen-\nerality, as we will sweep over f \u2208(0, 0.8] to run through a suf\ufb01ciently large range of\nvalues of the tuning parameters:\n\u2022 For the corrected Lasso estimator, we set \u03bb = fD\u2032\n0b\u03c4 1/2\nB\n\u2225\u03b2\u2217\u22252\nq\nlog m\nn\n+ \u03c9;\n\u2022 For the Conic programming estimator, we use \u00b5 = fD\u2032\n0b\u03c4 1/2\nB\nq\nlog m\nn\n. We set\n\u03bb = 1 in (1.8), which is independent of the Lasso penalty.\nThe factor f is chosen to re\ufb02ect the fact that in practice, we do not know the exact value\nof \u2225\u03b2\u2217\u22252 or \u2225\u03b2\u2217\u22251, D0 or D\u2032\n0, or other parameters related to the spectrum properties of\nA, B; moreover, in practice, we wish to understand the whole-path behavior for both\nestimators.\nIn Figures 3 and 4, we plot the relative error in \u21131 and \u21132 norm as n increases from\n100 to 2500, while sweeping over penalty factor f \u2208[0.05, 0.8] for \u03c4B = 0.3 and\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n44\n\u03c4B = 0.7 respectively. For both estimators, the relative \u21132 and \u21131 error versus the scaled\nsample size n/(d log m) are also plotted. In these \ufb01gures, green dashed lines are for\nthe corrected Lasso estimator via gradient descent algorithm, and blue dotted lines are\nfor the Conic programming estimator. These plots allow us to observe the behaviors of\nthe two estimators across a set of tuning parameters. Overall, we see that both methods\nare able to achieve low relative error \u2113p, p = 1, 2 norm when \u03bb and \u00b5 are chosen from\na suitable range.\nFor the corrected Lasso estimator, we display results where the step size parameter \u03b6 is\nset to \u03b62 = 3\n2\u03bbmax(A) and \u03b63 = 2\u03bbmax(A) in the left and right column respectively. We\nmention in passing that the algorithm starts to converge even when we set \u03b6 = \u03b61 =\n\u03bbmax(A) + 1\n2\u03bbmin(A) as we observe quantitively similar behavior as the displayed\ncases. For both estimators, we observe that we need a larger sample size n in case\n\u03c4B = 0.7 in order to control the error at the same level as in case \u03c4B = 0.3.\nIn Figure 5, we plot the \u21132 and \u21131 error versus the penalty factor f \u2208[0.05, 0.8] for\nsample size n \u2208{300, 600, 1200}. We plot results for \u03c4B = 0.3 and \u03c4B = 0.7\nin the left and right column respectively. For these plots, we focus on cases when\nn > d\u03ba(A) log m, by choosing n \u2208{300, 600, 1200}; Otherwise, the gradient descent\nalgorithm does not yet reach the sample requirement (8.2) that guarantees computa-\ntional convergence. In Figure 5, we observe that the Conic programming estimator is\nrelatively stable over the choices of \u00b5 once f \u22650.2. The composite gradient algorithm\nfavors smaller penalties such as f \u2208[0.05, 0.2], leading to smaller relative error in the\n\u21131 and \u21132 norm, consistent with our theoretical predictions. These results also con\ufb01rm\nour theoretical prediction that the Lasso and Conic programming penalty parameters \u03bb\nand \u00b5 need to be adaptively chosen based on the noise level \u03c4B, because a larger than\nnecessary amount of penalty will cause larger relative error in both \u21131 and \u21132 norm.\n8.3. Sensitivity to tuning parameters\nIn the third experiment, we change the \u21131-ball radius R \u2208{R\u2217, 5R\u2217, 9R\u2217} in (1.10),\nwhere R\u2217= \u2225\u03b2\u2217\u22252\n\u221a\nd, while running through different penalties for the composite\ngradient descent algorithm. In the left column in Figure 6, A and B are generated\nusing the AR(1) model with \u03c1A = 0.3, \u03c1B\u2217= 0.3 and \u03c4B = 0.7. In the right column,\nwe set \u03c4B = 0.3, while keeping other parameters invariant.\nAs predicted by our theory, a larger radius demands correspondingly larger penalty\nto ensure consistent estimation using the composite gradient descent algorithm; this\nin turn will increase the relative error when R is too large, for example, when R =\ne\u2126(\nq\nn\nlog m), where the e\u2126(\u00b7) notation hides parameters involving \u03c4B and \u03ba(A). This is\nobserved in Figure 6. When n is suf\ufb01ciently large relative to \u03c4B and \u03ba(A), the optimal\n\u21131 and \u21132 error become less sensitive with regard to the choice of R, so long as R =\neO(\nq\nn\nlog m), where eO(\u00b7) hides parameters involving \u03c4B and \u03ba(A), as shown in Figure 6.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n45\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.3 \u03b6 = \u03b62 \nn / d log(m)\nrelative L2 error\nf = 0.05\nf = 0.10\nf = 0.20\nf = 0.30\nf = 0.40\nf = 0.50\nf = 0.60\nf = 0.80\n2\n4\n6\n8\n10\n12\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.3 \u03b6 = \u03b63 \nn / d log(m)\nrelative L2 error\nf = 0.05\nf = 0.10\nf = 0.20\nf = 0.30\nf = 0.40\nf = 0.50\nf = 0.60\nf = 0.80\n2\n4\n6\n8\n10\n12\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.3 \u03b6 = \u03b62 \nn / d log(m)\nrelative L1 error\nf = 0.05\nf = 0.10\nf = 0.20\nf = 0.30\nf = 0.40\nf = 0.50\nf = 0.60\nf = 0.80\n2\n4\n6\n8\n10\n12\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.3 \u03b6 = \u03b63 \nn / d log(m)\nrelative L1 error\nf = 0.05\nf = 0.10\nf = 0.20\nf = 0.30\nf = 0.40\nf = 0.50\nf = 0.60\nf = 0.80\n2\n4\n6\n8\n10\n12\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nFIG 3. Plots of the relative \u21131 and \u21132 error\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r / \u2225\u03b2\u2217\u2225for the Conic programming estimator and the\ncorrected Lasso estimator obtained via running the composite gradient descent algorithm on (approximately)\nrecovering \u03b2\u2217. Set parameters d = 10 and m = 1024 while varying n. Generate A and B using the AR(1)\nmodel with parameters \u03c1A = 0.3, \u03c1B\u2217= 0.3 and \u03c4B = 0.3. Set \u03b6 \u2208{\u03b61, \u03b62, \u03b63}. We compare the\nperformance of the corrected Lasso and the Conic programming estimators over choices of \u03bb and \u00b5 while\nsweeping through f \u2208(0, 0.8]. In the top row, we plot the relative \u21132 error for the Conic programming\nestimator (blue dotted lines) and the corrected Lasso (green dashed lines) via the composite gradient descent\nalgorithm with step size parameter set to be \u03b62 = 3\n2 \u03bbmax(A) and \u03b63 = 2\u03bbmax(A); in the bottom row,\nwe plot the relative \u21131 error under the same settings. We note that the composite gradient descent algorithm\nstarts to converge even when we set the step size parameter to be \u03b61 = \u03bbmax(A) + 1\n2 \u03bbmin(A).\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n46\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.7 \u03b6 = \u03b62 \nn / d log(m)\nrelative L2 error\nf = 0.05\nf = 0.10\nf = 0.20\nf = 0.30\nf = 0.40\nf = 0.50\nf = 0.60\nf = 0.80\n2\n4\n6\n8\n10\n12\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.7 \u03b6 = \u03b63 \nn / d log(m)\nrelative L2 error\nf = 0.05\nf = 0.10\nf = 0.20\nf = 0.30\nf = 0.40\nf = 0.50\nf = 0.60\nf = 0.80\n2\n4\n6\n8\n10\n12\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.7 \u03b6 = \u03b62 \nn / d log(m)\nrelative L1 error\nf = 0.05\nf = 0.10\nf = 0.20\nf = 0.30\nf = 0.40\nf = 0.50\nf = 0.60\nf = 0.80\n2\n4\n6\n8\n10\n12\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.7 \u03b6 = \u03b63 \nn / d log(m)\nrelative L1 error\nf = 0.05\nf = 0.10\nf = 0.20\nf = 0.30\nf = 0.40\nf = 0.50\nf = 0.60\nf = 0.80\n2\n4\n6\n8\n10\n12\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nFIG 4. Plots of the relative \u21131 and \u21132 error\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r / \u2225\u03b2\u2217\u2225after running the Conic programming estima-\ntor and composite gradient descent algorithm on recovering \u03b2\u2217using the corrected Lasso objective function\nwith sparsity parameter d = 10 and m = 1024 while varying n. Both A and B are generated using the\nAR(1) model with parameters \u03c1A = 0.3, \u03c1B\u2217= 0.3 and \u03c4B = 0.7. We compare the performance of\nthe corrected Lasso (green dashed lines) and the Conic programming estimators (blue dotted lines) over\nchoices of \u03bb and \u00b5 while sweeping through f \u2208(0, 0.8]. For the composite gradient descent algorithm, we\nchoose \u03b6 from {\u03b61, \u03b62, \u03b63}. In the top row, we plot the \u21132 error for the Conic and the corrected Lasso with\n\u03b62 = 3\n2 \u03bbmax(A) and \u03b63 = 2\u03bbmax(A), while in the bottom row, we plot the \u21131 error corresponding to the\ntwo step size parameters.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n47\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.3 n = 300 \nf\nrelative error\nL2 GD\nL2 Conic\nL1 GD\nL1 Conic\n0.0\n0.2\n0.4\n0.6\n0.8\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.7 n = 300 \nf\nrelative error\nL2 GD\nL2 Conic\nL1 GD\nL1 Conic\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.3 n = 600 \nf\nrelative error\nL2 GD\nL2 Conic\nL1 GD\nL1 Conic\n0.0\n0.2\n0.4\n0.6\n0.8\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.7 n = 600 \nf\nrelative error\nL2 GD\nL2 Conic\nL1 GD\nL1 Conic\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.3 n = 1200 \nf\nrelative error\nL2 GD\nL2 Conic\nL1 GD\nL1 Conic\n0.0\n0.2\n0.4\n0.6\n0.8\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n\u03c1A = 0.3 \u03c1B* = 0.3 \u03c4B = 0.7 n = 1200 \nf\nrelative error\nL2 GD\nL2 Conic\nL1 GD\nL1 Conic\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFIG 5. Plot of the relative error in \u21132 and \u21131 norm versus the penalty factor f \u2208(0, 0.8] as we change\nthe sample size n. Set m = 1024 and d = 10. Both A and B are generated using the AR(1) model\nwith parameters \u03c1A = 0.3 and \u03c1B\u2217= 0.3. We plot the relative error in \u21131 and \u21132 norm versus the penalty\nparameter factor f \u2208(0, 0.8] for n = 300, 600, 1200 when \u03b6 = 3\n2 \u03bbmax(A). In the left column, \u03c4B = 0.3.\nIn the right column, we set \u03c4B = 0.7.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n48\n A = 0.3  B * = 0.3  B = 0.7 n = 600\nf\nrelative error\nR* L2\n5R* L2\n9R* L2\nR* L1\n5R* L1\n9R* L1\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\n\u03c1A = 0.3 \u03c1B * = 0.3 \u03c4B = 0.3 n = 600\nf\nrelative error\nR* L2\n5R* L2\n9R* L2\nR* L1\n5R* L1\n9R* L1\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\n\u03c1A = 0.3 \u03c1B * = 0.3 \u03c4B = 0.7 n = 1200\nf\nrelative error\nR* L2\n5R* L2\n9R* L2\nR* L1\n5R* L1\n9R* L1\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\n\u03c1A = 0.3 \u03c1B * = 0.3 \u03c4B = 0.3 n = 1200\nf\nrelative error\nR* L2\n5R* L2\n9R* L2\nR* L1\n5R* L1\n9R* L1\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\n\u03c1A = 0.3 \u03c1B * = 0.3 \u03c4B = 0.7 n = 2500\nf\nrelative error\nR* L2\n5R* L2\n9R* L2\nR* L1\n5R* L1\n9R* L1\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\n\u03c1A = 0.3 \u03c1B * = 0.3 \u03c4B = 0.3 n = 2500\nf\nrelative error\nR* L2\n5R* L2\n9R* L2\nR* L1\n5R* L1\n9R* L1\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nFIG 6. Plot of the relative error in \u21132 and \u21131 norm versus the penalty factor f \u2208(0, 0.8] as we change\nthe radius R. Set m = 1024, d = 10 and n \u2208{600, 1200, 2500}. We change the \u21131-ball radius R \u2208\n{R\u2217, 5R\u2217, 9R\u2217}, where R\u2217= \u2225\u03b2\u2217\u22252\n\u221a\nd, while running through different penalties for the composite\ngradient descent algorithm. In the left column, A and B are generated using the AR(1) model with \u03c1A =\n0.3, \u03c1B\u2217= 0.3 and \u03c4B = 0.7. In the right column, we set \u03c4B = 0.3, while keeping other parameters\ninvariant.imsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n49\n\u03c1A = 0.3 \u03c1B * = 0.3 \u03c4B = 0.3 f = 0.2\nIteration count\nlog(L2 error)\n1\n2\n3\n6\n12\n25\n0\n20\n40\n60\n80\n100\n\u221215\n\u221210\n\u22125\n0\n\u03c1A = 0.3 \u03c1B * = 0.3 \u03c4B = 0.7 f = 0.2\nIteration count\nlog(L2 error)\n1\n2\n3\n6\n12\n25\n0\n20\n40\n60\n80\n100\n\u221215\n\u221210\n\u22125\n0\n(a)\n(b)\nFIG 7. Plots of the statistical error log(\u2225\u03b2t \u2212\u03b2\u2217\u22252), and the optimization error log(\u2225\u03b2t \u2212b\u03b2\u22252) versus\niteration number t, generated by running the composite gradient descent algorithm on the corrected Lasso\nobjective function. Each curve represents an average over 10 random trials, each with a different initializa-\ntion point of \u03b20. In Plots (a) and (b), B is generated using the AR(1) model with \u03c1B\u2217= 0.3 and A is\ngenerated using the AR(1) model with \u03c1A = 0.3. We set \u03c4B = 0.3, 0.7 in Plot (a) and (b) respectively. We\nset n = \u2308\u03c1d log m\u2309, where we vary \u03c1 \u2208{1, 2, 3, 6, 12, 25}.\n8.4. Statistical and optimization error in Gradient Descent\nIn the last set of experiments, we study the statistical error and optimization error for\neach iteration within the composite gradient descent algorithm. We observe a geometric\nconvergence of the optimization error \u2225\u03b2t \u2212b\u03b2\u22252.\nFor each experiment, we repeat the following procedure 10 times: we start with a ran-\ndom initialization point \u03b20 and apply the composite gradient descent algorithm to com-\npute an estimate b\u03b2; we compute the optimization error log(\u2225\u03b2t \u2212b\u03b2\u22252), which records\nthe difference between \u03b2t and b\u03b2, where b\u03b2 is the \ufb01nal solution. In all simulations, we\nplot the log error log(\u2225\u03b2t \u2212b\u03b2\u22252) between the iterate \u03b2t at time t versus the \ufb01nal solu-\ntion b\u03b2, as well as the statistical error log(\u2225\u03b2t \u2212\u03b2\u2217\u22252), which is the difference between\n\u03b2t and \u03b2\u2217at time t. Each curve plots the results averaged over ten random instances.\nIn the \ufb01rst experiment, both A and B are generated using the AR(1) model with pa-\nrameters \u03c1A = 0.3 and \u03c1B\u2217= 0.3. We set m = 1024, d = 10 and \u03c4B \u2208{0.3, 0.7}.\nThese results are shown in Figure 7. Within each plot, the red curves show the statisti-\ncal error and the blue curves show the optimization error. We can see the optimization\nerror \u2225\u03b2t \u2212b\u03b2\u22252 decreases exponentially for each iteration, obeying a geometric con-\nvergence. To illuminate the dependence of convergence rate on the sample size n, we\nstudy the optimization error log(\u2225\u03b2t \u2212b\u03b2\u22252) when n = \u2308\u03c1d log m\u2309, where we vary\n\u03c1 \u2208{1, 2, 3, 6, 12, 25}. When n = d log m, the composite gradient algorithm fails to\nconverge since the sample size is too small for the RSC/RSM conditions to hold, result-\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n50\ning in the oscillatory behavior of the algorithm for a constant step size. As the factor\n\u03c1 increases, the lower and upper RE curvature \u03b1 and smoothness parameter e\u03b1 become\nmore concentrated around \u03bbmin(A) and \u03bbmax(A) respectively, and the tolerance pa-\nrameter \u03c4 decreases at the rate of log m\nn\n. Hence we observe faster rates of convergence\nfor \u03c1 = 25, 12, 6 compared to \u03c1 = 2, 3. This is well aligned with our theoretical\nprediction that once n = \u2126(\u03ba(A)\n\u03c40\n\u03bbmin(A)d log m) (cf. (8.2)), we expect to observe a\ngeometric convergence of the computational error \u2225\u03b2t \u2212b\u03b2\u22252.\nFor the statistical error, we \ufb01rst observe the geometric contraction, and then the curves\n\ufb02atten out after a certain number of iterations, con\ufb01rming the claim that \u03b2t converges\nto \u03b2\u2217only up to a neighborhood of radius de\ufb01ned through the statistical error bound\n\u03b52\nstat; that is, the geometric convergence is not guaranteed to an arbitrary precision, but\nonly to an accuracy related to statistical precision of the problem measured by \u21132 error:\n\u2225b\u03b2 \u2212\u03b2\u2217\u22252\n2 =: \u03b52\nstat between the global optimizer b\u03b2 and the true parameter \u03b2\u2217.\nIn the second experiment, A is generated from the Star-Block model, where we have\n32 subgraphs and each subgraph has 16 edges; B is generated using the random graph\nmodel with n log n edges and adjusted to have \u03c4B = 0.3. We set m = 1024, n =\n2500 and d = 10. We then choose \u03c1A \u2208{0.3, 0.5, 0.7, 0.9}. The results are shown\nin Figure 8(b). As we increase \u03c1A, we need larger sample size to control the statistical\nerror. Hence for a \ufb01xed n, the statistical error is bigger for \u03c1A = 0.7, compared to cases\nwhere \u03c1A = 0.5 or \u03c1A = 0.3, for which we have \u03ba(A) = 42.06 and \u03ba(A) = 10.2 (for\n\u03c1A = 0.3) respectively; Moreover, the rates of convergence are faster for the latter two\ncompared to \u03c1A = 0.7, where \u03ba(A) = 169.4. When \u03c1A = 0.9, the composite gradient\ndescent algorithm fails to converge as \u03c1(A) is too large (hence not plotted here) with\nrespect to the sample size we \ufb01x upon. In Figure 8(a), we show results of A being\ngenerated using the AR(1) model with four choices of \u03c1A \u2208{0.3, 0.5, 0.7, 0.9} and B\nbeing generated using the AR(1) model with \u03c1B\u2217= 0.7 and \u03c4B = 0.3. We observe\nquantitively similar behavior as in Figure 8(b).\n9. Proof of Lemma 1\nProof of Lemma 1.\nPart I: Suppose that the Lower-RE condition holds for \u0393 :=\nAT A. Let x \u2208Cone(s0, k0). Then\n\u2225x\u22251 \u2264(1 + k0) \u2225xT0\u22251 \u2264(1 + k0)\u221as0 \u2225xT0\u22252 .\nThus for x \u2208Cone(s0, k0) \u2229Sp\u22121 and \u03c4(1 + k0)2s0 \u2264\u03b1/2, we have\n\u2225Ax\u22252 = (xT AT Ax)1/2\n\u2265\n\u0010\n\u03b1 \u2225x\u22252\n2 \u2212\u03c4 \u2225x\u22252\n1\n\u00111/2\n\u2265\n\u0010\n\u03b1 \u2225x\u22252\n2 \u2212\u03c4(1 + k0)2s0 \u2225xT0\u22252\n2\n\u00111/2\n\u2265\n\u0000\u03b1 \u2212\u03c4(1 + k0)2s0\n\u00011/2 \u2265\nr\u03b1\n2 .\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n51\n \u03c1B * = 0.7 \u03c4B = 0.3  n =  2500\nIteration count\nlog(L2 error)\n\u03c1A= 0.3\n\u03c1A= 0.5\n\u03c1A= 0.7\n\u03c1A= 0.9\n0\n20\n40\n60\n80\n100\n\u221215\n\u221210\n\u22125\n0\n A: Star\u2212Block; B: Random \u03c4B = 0.3  n =  2500\nIteration count\nlog(L2 error)\n\u03c1A= 0.3\n\u03c1A= 0.5\n\u03c1A= 0.7\n\u03c1A= 0.9\n0\n20\n40\n60\n80\n100\n\u221215\n\u221210\n\u22125\n0\n(a)\n(b)\nFIG 8. Plots of the statistical error log(\u2225\u03b2t \u2212\u03b2\u2217\u22252), and the optimization error when we change the\ntopology. In the last experiment, we have m = 1024, d = 10 and n = 2500. In (a), A is generated using\nthe AR(1) model with four choices of \u03c1A \u2208{0.3, 0.5, 0.7, 0.9} and B is generated using AR(1) model\nwith \u03c1B\u2217= 0.7 and \u03c4B = 0.3. In (b), A follows the Star-Block model and B follows the random graph\nmodel. We show four choices of \u03c1A \u2208{0.3, 0.5, 0.7, 0.9}\nThus the RE(s0, k0, A) condition holds with\n1\nK(s0, k0, A)\n:=\nmin\nx\u2208Cone(s0,k0)\n\u2225Ax\u22252\n\u2225xT0\u22252\n\u2265\nr\u03b1\n2\nwhere we use the fact that for any J \u2208{1, . . . , p} such that |J| \u2264s0, \u2225xJ\u22252 \u2264\u2225xT0\u22252.\nWe now show the other direction.\nPart II. Assume that RE(4R2, 2R \u22121, A) holds for some integer R > 1. Assume that\nfor some R > 1\n\u2225x\u22251 \u2264R \u2225x\u22252 .\nLet (x\u2217\ni )p\ni=1 be non-increasing arrangement of (|xi|)p\ni=1. Then\n\u2225x\u22251\n\u2264\nR\n\uf8eb\n\uf8ed\ns\nX\nj=1\n(x\u2217\nj)2 +\n\u221e\nX\nj=s+1\n\u0012\u2225x\u22251\nj\n\u00132\n\uf8f6\n\uf8f8\n1/2\n\u2264\nR\n\u0012\n\u2225x\u2217\nJ\u22252\n2 + \u2225x\u22252\n1\n1\ns\n\u00131/2\n\u2264R\n\u0012\n\u2225x\u2217\nJ\u22252 + \u2225x\u22251\n1\n\u221as\n\u0013\nwhere J := {1, . . . , s}. Choose s = 4R2. Then\n\u2225x\u22251 \u2264R \u2225x\u2217\nJ\u22252 + 1\n2 \u2225x\u22251 .\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n52\nThus we have\n\u2225x\u22251\n\u2264\n2R \u2225x\u2217\nJ\u22252 \u22642R \u2225x\u2217\nJ\u22251\nand hence\n(9.1)\n\u2225x\u2217\nJc\u22251\n\u2264\n(2R \u22121) \u2225x\u2217\nJ\u22251 .\n(9.2)\nThen x \u2208Cone(4R2, 2R \u22121). Then for all x \u2208Sp\u22121 such that \u2225x\u22251 \u2264R \u2225x\u22252, we\nhave for k0 = 2R \u22121 and s0 := 4R2,\nxT \u0393x \u2265\n\u2225xT0\u22252\n2\nK2(s0, k0, A) \u2265\n\u2225x\u22252\n2\n\u221as0K2(s0, k0, A) =: \u03b1 \u2225x\u22252\n2\nwhere we use the fact that (1 + k0) \u2225xT0\u22252\n2 \u2265\u2225x\u22252\n2 by Lemma 33 with xT0 as de\ufb01ned\ntherein. Otherwise, suppose that \u2225x\u22251 \u2265R \u2225x\u22252. Then for a given \u03c4 > 0,\n\u03b1 \u2225x\u22252\n2 \u2212\u03c4 \u2225x\u22252\n1 \u2264(\n1\n\u221as0K2(s0, k0, A) \u2212\u03c4R2) \u2225x\u22252\n2 .\n(9.3)\nThus we have by the choice of \u03c4 as in (2.2) and (9.3)\nxT \u0393x \u2265\u03bbmin(\u0393) \u2225x\u22252\n2\n\u2265\n(\n1\n\u221as0K2(s0, k0, A) \u2212\u03c4R2) \u2225x\u22252\n2\n\u2265\n\u03b1 \u2225x\u22252\n2 \u2212\u03c4 \u2225x\u22252\n1 .\nThe Lemma thus holds.\n\u25a1\n10. Proof of Theorem 3\nThroughout this proof, we assume that A0 \u2229B0 holds. First we note that it is suf-\n\ufb01cient to have (3.2) in order for (6.5) to hold. Condition (3.2) guarantees that for\nV = 3eM 3\nA/2,\nr(B) := tr(B)\n\u2225B\u22252\n\u2265\n16c\u2032K4\nn\nlog m log Vm log m\nn\n\u2265\n16c\u2032K4\nn\nlog m log\n\u00123emM 3\nA log m\n2n\n\u0013\n=\nc\u2032K4 1\n\u03b52\n4\nM 2\nA\nn\nlog m log\n \n6emMA\n4\nM 2\nA (n/ log m)\n!\n\u2265\nc\u2032K4 1\n\u03b52 s0 log\n\u00126emMA\ns0\n\u0013\n= c\u2032K4 s0\n\u03b52 log\n\u00123em\ns0\u03b5\n\u0013\n(10.1)\nwhere \u03b5 =\n1\n2MA \u2264\n1\n128C , and the last inequality holds given that k log(cm/k) on the\nRHS of (10.1) is a monotonically increasing function of k,\ns0\n\u2264\n4n\nM 2\nA log m and MA = 64C(\u03c1max(s0, A) + \u03c4B)\n\u03bbmin(A)\n\u226564C.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n53\nNext we check that the choice of d as in (3.4) ensures that (6.9) holds for D\u03c6 de\ufb01ned\nthere. Indeed, for c\u2032K4 \u22641, we have\nd\n\u2264\nCA(c\u2032K4 \u22271) \u03c6n\nlog m \u2264CA (c\u2032D\u03c6 \u22271)\nn\nlog m.\nBy Lemma 15, we have on event A0, the modi\ufb01ed gram matrix b\u0393A :=\n1\nn(XT X \u2212\nbtr(B)Im) satis\ufb01es the Lower RE conditions with \u03b1 and \u03c4 as in (10.2). Theorem 3\nfollows from Theorem 16, so long as we can show that condition (6.6) holds for \u03bb \u2265\n4\u03c8\nq\nlog m\nn\n, where the parameter \u03c8 is as de\ufb01ned (3.6),\ncurvature \u03b1 = 5\n8\u03bbmin(A) and tolerance\n\u03c4 = \u03bbmin(A) \u2212\u03b1\ns0\n= 3\u03b1\n5s0\n.\n(10.2)\nCombining (10.2) and (6.6), we need to show (6.10) holds. This is precisely the content\nof Lemma 17. This is the end of the proof for Theorem 3\n\u25a1\n11. Proof of Theorem 4\nFor the set ConeJ(k0) as in (F.3),\n\u03baRE(d0, k0)\n:=\nmin\nJ:|J|\u2264d0\nmin\n\u2206\u2208ConeJ(k0)\n\f\f\u2206T \u03a8\u2206\n\f\f\n\u2225\u2206J\u22252\n2\n=\n\u0012\n1\nK(d0, k0, (1/\u221an)Z1A1/2)\n\u00132\n.\nRecall the following Theorem 27 from [38].\nTheorem 27. ([38]) Set 0 < \u03b4 < 1, k0 > 0, and 0 < d0 < p. Let A1/2 be an m \u00d7 m\nmatrix satisfying RE(d0, 3k0, A1/2) condition as in De\ufb01nition 2.1. Set\nd\n=\nd0 + d0 max\nj\n\r\r\rA1/2ej\n\r\r\r\n2\n2\n16K2(d0, 3k0, A1/2)(3k0)2(3k0 + 1)\n\u03b42\n.\nLet \u03a8 be an n \u00d7 m matrix whose rows are independent isotropic \u03c82 random vectors in\nRm with constant \u03b1. Suppose the sample size satis\ufb01es\nn \u22652000d\u03b14\n\u03b42\nlog\n\u001260em\nd\u03b4\n\u0013\n.\n(11.1)\nThen with probability at least 1 \u22122 exp(\u2212\u03b42n/2000\u03b14), RE(d0, k0, (1/\u221an)\u03a8A1/2)\ncondition holds for matrix (1/\u221an)\u03a8A with\n0 < K(d0, k0, (1/\u221an)\u03a8A1/2) \u2264K(d0, k0, A1/2)\n1 \u2212\u03b4\n.\n(11.2)\nProof of Theorem 4.\nSuppose RE(2d0, 3k0, A1/2) holds. Then for d as de\ufb01ned\nin (3.10) and n = \u2126(dK4 log(m/d)), we have with probability at least 1\u22122 exp(\u03b42n/2000K4),\nRE(2d0, k0,\n1\n\u221anZ1A1/2) condition holds with\n\u03baRE(2d0, k0)\n=\n\u0012\n1\nK(2d0, k0, (1/\u221an)Z1A1/2)\n\u00132\n\u2265\n\u0012\n1\n2K(2d0, k0, A1/2)\n\u00132\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n54\nby Theorem 27.\nThe rest of the proof follows from [3] Theorem 1 and thus we only provide a sketch. In\nmore details, in view of the lemmas shown in Section 6, we need\n\u03baq(d0, k0) \u2265cd\u22121/q\n0\nto hold for some constant c for \u03a8 := 1\nnXT\n0 X0. It is shown in Appendix C in [3] that\nunder the RE(2d0, k0,\n1\n\u221anZ1A1/2) condition, for any d0 \u2264m/2 and 1 \u2264q \u22642,\n\u03ba1(d0, k0)\n\u2265\ncd\u22121\n0 \u03baRE(d0, k0)\nand\n\u03baq(d0, k0)\n\u2265\nc(q)d\u22121/q\n0\n\u03baRE(2d0, k0),\n(11.3)\nwhere c(q) > 0 depends on k0 and q. The theorem is thus proved following exactly the\nsame line of arguments as in the proof of Theorem 1 in [3] in view of the \u2113q sensitivity\ncondition derived immediately above, in view of Lemmas 19, 20 and 21. Indeed, for\nv := b\u03b2 \u2212\u03b2\u2217, we have by de\ufb01nition of \u2113q sensitivity as in (6.13),\nc(q)d\u22121/q\n0\n\u03baRE(2d0, k0) \u2225v\u2225q\n\u2264\n\u03baq(d0, k0) \u2225v\u2225q \u2264\n\r\r 1\nnXT\n0 X0v\n\r\r\n\u221e\n\u2264\n\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52 \u2225v\u22251 + \u03c9\n\u2264\n\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52(2 + \u03bb) \u2225vS\u22251 + \u03c9\n\u2264\n\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52(2 + \u03bb)d1\u22121/q\n0\n\u2225vS\u2225q + \u03c9\n\u2264\n\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52(2 + \u03bb)d1\u22121/q\n0\n\u2225v\u2225q + \u03c9.(11.4)\nThus we have for d0 = c0\np\nn/ log m, where c0 is suf\ufb01ciently small,\nd\u22121/q\n0\n(c(q)\u03baRE(2d0, k0) \u2212\u00b52(2 + \u03bb)d0) \u2225v\u2225q \u2264\u00b51 \u2225\u03b2\u2217\u22252 + \u03c9\nand hence\n\u2225v\u2225q \u2264C(4D2\u03c1nK \u2225\u03b2\u2217\u22252 + 2D0M\u03f5\u03c1n)d1/q\n0\n\u22644CD2\u03c1n(K \u2225\u03b2\u2217\u22252 + M\u03f5)d1/q\n0\nfor some constant C = 1/ (c(q)\u03baRE(2d0, k0) \u2212\u00b52(2 + \u03bb)d0) \u22651/ (2c(q)\u03baRE(2d0, k0)),\nwhere\n\u00b52(2 + \u03bb)d0\n=\n2D2K\u03c1n( 1\n\u03bb + 1)(2 + \u03bb)c0\np\nn/ log m\n=\n2c0C0D2K2(2 + \u03bb)( 1\n\u03bb + 1)\nis suf\ufb01ciently small and thus (3.11) holds. The prediction error bound follows exactly\nthe same line of arguments as in [3] which we omit here. See proof of Theorem 7 in\nSection 6.5 for details.\n\u25a1\n12. Proof of Theorem 6\nThroughout this proof, we assume that A0 \u2229B0 holds. The proof is also identical to\nthe proof of Theorem 3 up till (10.2), except that we replace the condition on d as in\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n55\nthe theorem statement by (4.3). Theorem 6 follows from Theorem 16, so long as we\ncan show that condition (6.6) holds for \u03b1 and \u03c4 = \u03bbmin(A)\u2212\u03b1\ns0\nas de\ufb01ned in (10.2), and\n\u03bb \u22652\u03c8\nq\nlog m\nn\n, where the parameter \u03c8 is as de\ufb01ned (6.3). Combining (10.2) and (6.6),\nwe need to show (6.10) holds. This is precisely the content of Lemma 18. This is the\nend of the proof for Theorem 6.\n\u25a1\n13. Proof of Theorem 7\nThroughout this proof, we assume that B0 \u2229B10 holds. The rest of the proof follows\nthat of Theorem 4, except for the last part. Let \u00b51, \u00b52, \u03c9 be as de\ufb01ned in Lemma 21.\nWe have for \u00b52 := 2\u00b5(1 +\n1\n2\u03bb), where \u00b5 = D\u2032\n0K\u03c1ne\u03c4 1/2\nB\nand d0 = c0\u03c4 \u2212\nB\np\nn/ log m,\n\u00b52(2 + \u03bb)d0\n=\n2C0D\u2032\n0K2e\u03c4 1/2\nB ( 1\n2\u03bb + 1)(2 + \u03bb)c0\u03c4 \u2212\nB\n(13.1)\n\u2264\n2c0C0D\u2032\n0K2(2 + \u03bb)( 1\n2\u03bb + 1) \u22641\n2c(q)\u03baRE(2d0, k0),\nwhich holds when c0 is suf\ufb01ciently small, where \u03c4 \u2212\nB e\u03c4 1/2\nB\n\u22641 by (6.18). Hence\n\u00b52d0 \u2264c(q)\u03baRE(2d0, k0)\n2(2 + \u03bb)\n.\nThus for c0 suf\ufb01ciently small, \u00b51 = 2\u00b5, we have by (11.3), (13.1), (11.4) and (6.17),\nd\u22121/q\n0\n1\n2(c(q)\u03baRE(2d0, k0)) \u2225v\u2225q\n=\nd\u22121/q\n0\n(c(q)\u03baRE(2d0, k0) \u2212\u00b52(2 + \u03bb)d0) \u2225v\u2225q\n\u2264\n(\u03baq(d0, k0) \u2212\u00b52(2 + \u03bb)d1\u22121/q\n0\n) \u2225v\u2225q \u2264\u00b51 \u2225\u03b2\u2217\u22252 + \u03c9\n\u2264\n2D\u2032\n0\u03c1nK2((\u03c4 1/2\nB\n+ (3/2)C6r1/2\nm,m) \u2225\u03b2\u2217\u22252 + M\u03f5/K)\n(13.2)\nand thus (4.12) holds, following the proof in Theorem 4. The prediction error bound\nfollows exactly the same line of arguments as in [3], which we now include for the sake\ncompleteness. Following (4.12), we have by (13.2),\n\u2225v\u22251\n\u2264\nC11d0(\u00b51 \u2225\u03b2\u2217\u22252 + \u03c9), where C11 = 2/ (c(q)\u03baRE(2d0, k0)) ,\nand hence \u00b52 \u2225v\u22251\n\u2264\nC11\u00b52d0(\u00b51 \u2225\u03b2\u2217\u22252 + \u03c9)\n\u2264\nC11\n1\n2(2 + \u03bb) (c(q)\u03baRE(2d0, k0)) (\u00b51 \u2225\u03b2\u2217\u22252 + \u03c9)\n=\n1\n2 + \u03bb(\u00b51 \u2225\u03b2\u2217\u22252 + \u03c9).\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n56\nThus we have by (13.2), (6.18) and the bounds immediately above,\n1\nn\n\r\r\rX(b\u03b2 \u2212\u03b2\u2217)\n\r\r\r\n2\n2\n\u2264\n\u2225v\u22251\n\r\r 1\nnXT\n0 X0v\n\r\r\n\u221e\n\u2264\nC11d0(\u00b51 \u2225\u03b2\u2217\u22252 + \u03c9) (\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52 \u2225v\u22251 + 2\u03c9)\n\u2264\nC11d0(\u00b51 \u2225\u03b2\u2217\u22252 + \u03c9)(1 +\n1\n2 + \u03bb) (\u00b51 \u2225\u03b2\u2217\u22252 + 2\u03c9)\n=\nC\u2032(D\u2032\n0)2K4d0\nlog m\nn\n\u0012\ne\u03c4 1/2\nB\n\u2225\u03b2\u2217\u22252 + M\u03f5\nK\n\u00132\n\u2264\nC\u2032\u2032(\u2225B\u22252 + amax)K2d0\nlog m\nn\n\u0010\n(2\u03c4B + 3C2\n6rm,m)K2 \u2225\u03b2\u2217\u22252\n2 + M 2\n\u03f5\n\u0011\n,\nwhere (D\u2032\n0)2 \u22642 \u2225B\u22252 + 2amax. The theorem is thus proved.\n\u25a1\n14. Proof of Theorem 9\nSuppose that event A0 \u2229B0 holds. The condition on d in (5.3) implies that\nn\n>\n512d\u03c40 log m\n\u001a12\u03bbmax(A)\n\u03bbmin(A)2\n\u001b _ \u001a 4\u03b6\n(\u00af\u03b1\u2113)2\n\u001b\n, where\n(14.1)\n\u03c40\n\u224d\n400C2\u03d6(s0 + 1)2\n\u03bbmin(A)\n.\n(14.2)\nTo see this, note that the following holds by the \ufb01rst bound in (5.3):\n\u03bd\u2113= 64d\u03c40 log m\nn\n\u226464d\u03c40 log m\n\u03bbmin(A)\n256d\u03c40 log m \u221724\u03ba(A) = \u03bbmin(A)\n96\u03ba(A) \u2264\u03b1\u2113\n60, (14.3)\nwhere \u03b1\u2113= 5\n8\u03bbmin(A) by Lemma 8, and hence \u00af\u03b1\u2113\u226559\u03b1\u2113\n60 . Thus we have\n\u03b12\n\u2113\n5\u03b6 \u2264\u00af\u03b12\n\u2113\n4\u03b6 ,\nwhere\n\u03b6 \u2265\u03b1u > \u03bbmax(A) \u2265\u03ba(A)\u00af\u03b1\u2113.\nNow, by de\ufb01nition of \u03bd(d, m, n) and the second bound on n in (14.1),\n2\u03bd(d, m, n) = 128d\u03c4u(Ln)\n:=\n128d\u03c40 log m\nn\n\u2264(\u00af\u03b1\u2113)2\n16\u03b6\nThen\n2\u03f1\n:=\n4\u03bd(d, m, n)\n\u00af\u03b1\u2113\n= 256d\u03c4u(Ln)\n\u00af\u03b1\u2113\n\u2264\u00af\u03b1\u2113\n8\u03b6 .\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n57\nThat is, we actually need to have for 2\u03f1 \u2264\u00af\u03b1\u2113\n8\u03b6\n\u03be\n1 \u2212\u03ba\n=\n1\n1 \u2212\u03f12\u03c4(Ln)\n\u0012 \u00af\u03b1\u2113\n4\u03b6 + 2\u03f1 + 5\n\u0013\n1 \u2212\u03f1\n\u00af\u03b1\u2113\n4\u03b6 \u22122\u03f1\n=\n2\u03c4(Ln)\n\u0012 \u00af\u03b1\u2113\n4\u03b6 + 2\u03f1 + 5\n\u0013\n1\n\u00af\u03b1\u2113\n4\u03b6 \u22122\u03f1\n=\n2\u03c4(Ln)\n \u00af\u03b1\u2113\n4\u03b6 + 2\u03f1\n\u00af\u03b1\u2113\n4\u03b6 \u22122\u03f1 + 40\u03b6\n\u00af\u03b1\u2113\n!\n\u22642\u03c4(Ln)\n\u0012\n3 + 40\u03b6\n\u00af\u03b1\u2113\n\u0013\n\u2264\n6\u03c4(Ln) + 80\u03b6\n\u00af\u03b1\u2113\n\u03c4(Ln),\nwhere we use the second bound in (14.1), and hence\n\u00af\u03b1\u2113\n4\u03b6 + 2\u03f1\n\u2264\n3\n2\n\u00af\u03b1\u2113\n4\u03b6\nand\n(1 \u2212\u03ba)(1 \u2212\u03f1) = \u00af\u03b1\u2113\n4\u03b6 \u22122\u03f1\n\u2265\n1\n2\n\u00af\u03b1\u2113\n4\u03b6 .\nFinally, putting all bounds in (2.11), we have 0 < \u03ba < 1. Thus the conclusion of\nTheorem 2 hold.\n\u25a1\n14.1. Proof of Corollary 10\nSuppose that event A0 \u2229B0 holds. We \ufb01rst show that\n4\u03bd\u03b52\nstat + 4\u03c4(Ln)\u03f52 \u224d64\u03c4(Ln)\n\u0012\n4d\u03b52\nstat + \u03b44\n\u03bb2\n\u0013\nin case\n\u03b42 \u2264M \u00af\u03b42.\nRecall that \u03be \u226510\u03c4\u2113(Ln) by de\ufb01nition of \u03be in (2.12). The condition (5.2) on \u03bb as stated\nin Theorem 2 indicates that\n\u03bb \u2265160b0\n\u221a\nd\u03c4\u2113(Ln)\n1 \u2212\u03ba\nwhere\nR \u224db0\n\u221a\nd.\n(14.4)\nWe \ufb01rst show that for the choice of \u03bb and R as in (14.4),\n\r\r\rb\u2206t\r\r\r\n2\n2\n\u2264\n2\n\u00af\u03b1\u2113\n\u0012\n\u03b42 + 64\u03c4\n\u0012\n4d\u03b52\nstat + \u03b44\n\u03bb2\n\u0013\u0013\n\u2264\n3\n\u03b1\u2113\n\u03b42 + \u03b1\u2113\u03b52\nstat\n4\n+ 2\n\u03b1\u2113\nO\n\u0012\u03b42\n\u03c40\nM\u03b52\nstat\n400b2\n0\n\u0013\n.\nThen (5.4) holds.\nFor the second term on the RHS of (2.16), we have by (14.1),\nn\n\u2265\n256d\u03c40 log m\n\u00af\u03b1\u2113\n8\u03b6\n\u00af\u03b1\u2113\n,\nwhere\n\u03c40 \u224d400C2\u03d6(s0 + 1)2\n\u03bbmin(A)\n.\n(14.5)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n58\nThus\n4\u03bd\u2113\n=\n256d\u03c40\nlog m\nn\n\u2264\u00af\u03b1\u2113\n\u00af\u03b1\u2113\n8\u03b6\nand\n2\n\u00af\u03b1\u2113\n4\u03bd\u03b52\nstat \u2264\u00af\u03b1\u2113\n4\u03b6 \u03b52\nstat \u2264\u03b1\u2113\u03b52\nstat\n4\u03b6\n.\nConsider the choice of \u00af\u03b7 = \u03b42, where M \u00af\u03b42 \u2265\u00af\u03b7 = \u03b42 \u2265c\u03b52\nstat\n1\u2212\u03ba\nd log m\nn\n=: \u00af\u03b42.\nThus we have for (14.4),\n2\u03b42\n\u03bb\n\u2264\nMc\u03b52\nstat\n1 \u2212\u03ba\nd log m\nn\n1 \u2212\u03ba\n160b0\n\u221a\nd\u03c4\u2113(Ln)\n= Mc\u03b52\nstat\n160b0\n\u221a\nd\n\u03c40\n< R\nand hence \u03f5 = 4\u03b42\n\u03bb .\nThen for the last term on the RHS of (2.16), we have for \u03c4\u2113(Ln) \u224d\u03c4,\n4\u03c4\u2113(Ln)\u03f52\n=\n16\u03c4\u2113(Ln) min\n\u00122\u03b42\n\u03bb , R\n\u00132\n=\n64\u03c4 \u03b44\n\u03bb2 \u2264\u03b44(1 \u2212\u03ba)2\n400b2\n0\u03c40\nn\nd log m\n\u2264\n\u03b42 cM\u03b52\nstat\n1 \u2212\u03ba\n(1 \u2212\u03ba)2\n400b2\n0\u03c40\n=\nc\u03b42\n\u03c40\nM\u03b52\nstat(1 \u2212\u03ba)\n400b2\n0\n=\nO\n\u0012c\u03b42\n\u03c40\nM\u03b52\nstat\n400b2\n0\n\u0013\nwhere \u03b42 \u2264Mc\u03b52\nstat\n1\u2212\u03ba\nd log m\nn\n.\nFinally, suppose we \ufb01x\nR \u224d\nb0\n20M+\np\n6\u03ba(A)\nr\nn\nlog m\nin view of the upper bound \u00afd (5.7). Then in order for\n\u03bb \u226516R\n\u03be\n1 \u2212\u03ba\nto hold, we need to set\n\u03bb\n\u2265\n640R\u03c4(Ln)\u03ba(A),\nbecause of the following lower bound\n\u03be\n1\u2212\u03ba \u226540\u03c4(Ln)\u03ba(A) as shown in (2.22).\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n59\nThen (5.5) holds given that the last term on the RHS of (2.16) is now bounded by\n2\n\u00af\u03b1\u2113\n4\u03c4\u2113(Ln)\u03f52\n=\n2\n\u00af\u03b1\u2113\n16\u03c4\u2113(Ln) min\n\u00122\u03b42\n\u03bb , R\n\u00132\n\u226460\n59\n2\n\u03b1\u2113\n64\u03b44\n6402R2\u03ba(A)2\u03c4\u2113(Ln)\n\u2264\n60\n59\n2\n\u03b1\u2113\n\u03b44\n6400\u03ba(A)\u03c40\n \n20M+\n\u221a\n6\nb0\n!2\n=\n60\n59\n12\n\u03b1\u2113\n\u03b44M 2\n+\n16b2\n0\u03ba(A)\u03c40\n\u226460\n59\n3\u03b44\n4\u03b1\u2113\n1024\n400b2\n0 \u2225A\u22252\n\u2264\n2\u03b44\nb2\n0\u03b1\u2113\u2225A\u22252\n.\n\u25a1\nRemark 14.1. First we obtain an upper bound on\n\u03be\n1\u2212\u03ba for \u03b6 = \u03b1u \u224d3\u03bbmax(A)\n2\nand\n59\n60\n5\n8\u03bbmin(A) \u2264\u00af\u03b1\u2113\n\u03be\n1 \u2212\u03ba\n\u2264\n6\u03c4(Ln) + 80\u03b6\n\u00af\u03b1\u2113\n\u03c4(Ln)\n\u2264\n6\u03c4(Ln) +\n80\u03b6\n59\n60\n5\n8\u03bbmin(A)\u03c4(Ln)\n\u2248\n200\u03c4(Ln)\u03ba(A).\nNow we obtain an upper bound using R \u2264b0\n\u221a\nd for d \u2264\u00afd as in (5.7),\nR\n\u03be\n1 \u2212\u03ba\n\u2264\n200\u03ba(A)\u03c4b0\n\u221a\nd \u2264200\u03ba(A)\u03c40\nr\nlog m\nn\nb0\n20M+\np\n6\u03ba(A)\n=\n200\u03ba(A)\u03c40\n\u03bbmin(A)\n\u03d6(s0 + 1)\nr\nlog m\nn\nb0\n640C\np\n6\u03ba(A)\n\u2264\n10b0\n\u03c40\nM+\n\u221a\n6\np\n\u03ba(A)\nr\nlog m\nn\n=\n125b0\n\u221a\n6 C\u03d6(s0 + 1)\np\n\u03ba(A)\nr\nlog m\nn\n,\nwhere we use (5.3) and the fact that\n\u03c40\nM+ = 12.5C\u03d6(s0 + 1). We now discuss the\nimplications of this bound on the choice of \u03bb in Section 5.1. We consider two cases.\n\u2022 When \u03c4B = \u2126(1). It is suf\ufb01cient to have for \u2225\u03b2\u2217\u22252 \u2264b0 and \u03c4B \u224d1,\n\u03bb\n\u2265\n16Cb0\n\u0012\n50\np\n\u03ba(A)\u03d6(s0 + 1)\n_ \u0012\nD\u2032\n0K(K\u03c4 1/2\nB\n+ M\u03f5\nb0\n)\n\u0013\u0013 r\nlog m\nn\nfollowing the discussions in Section 4, where the \ufb01rst and the second term on\nthe RHS are at the same order except that the new lower bound involves the\ncondition number \u03ba(A), while the original bound in Theorem 6 involves only\nD\u2032\n0 = \u2225B\u22251/2\n2\n+ a1/2\nmax.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n60\n\u2022 When \u03c4B = o(1) and M\u03f5 = \u2126(\u03c4 +/2\nB\nK \u2225\u03b2\u2217\u22252). Now d satis\ufb01es (4.15) and hence\nb0\n\u221a\nd \u2264\n1\n4\n\u221a\n5M+\nr\nn\nlog m\n(\u221a\nc\u2032D\u2032\n0KM\u03f5\n\u03d6(s0 + 1) \u2227b0\n)\n.\nNow combining this with the condition on d as in (5.3) implies that it is suf\ufb01cient\nto set R such that\nR\n\u03be\n1 \u2212\u03ba\n\u224d\n\u03ba(A)\u03c4\nM+\n \nb0\np\n\u03ba(A)\n^ D\u2032\n0KM\u03f5\n\u03d6(s0 + 1)\n! r\nn\nlog m\n=\n\u03ba(A)\u03d6(s0 + 1)\n \nb0\np\n\u03ba(A)\n^ D\u2032\n0KM\u03f5\n\u03d6(s0 + 1)\n! r\nlog m\nn\n\u2248\n\u0010\nb0\u03d6(s0 + 1)\np\n\u03ba(A) \u2227\u03ba(A)D\u2032\n0KM\u03f5\n\u0011 r\nlog m\nn\n=: \u00afU.\nHence it is suf\ufb01cient to have for \u03c8 \u224dD\u2032\n0K\n\u0010\nM\u03f5 + K\u03c4 +/2\nB\n\u2225\u03b2\u2217\u22252\n\u0011\nas in (4.2),\n\u03bb\n\u2265\n\u0000 \u00afU \u2228\u03c8\n\u0001 r\nlog m\nn\n.\n15. Proof of Theorem 12\nClearly the condition on the stable rank of B guarantees that\nn \u2265r(B) = tr(B)\n\u2225B\u22252\n= tr(B) \u2225B\u22252\n\u2225B\u22252\n2\n\u2265\u2225B\u22252\nF / \u2225B\u22252\n2 \u2265log m.\nThus the conditions in Lemmas 11 and 5 hold. First notice that\nb\u03b3\n=\n1\nn\n\u0000XT\n0 X0\u03b2\u2217+ W T X0\u03b2\u2217+ XT\n0 \u03f5 + W T \u03f5\n\u0001\n( 1\nnXT X \u2212\nbtr(B)\nn\nIm)\u03b2\u2217\n=\n1\nn(XT\n0 X0 + W T X0 + XT\n0 W + W T W \u2212\nbtr(B)\nn\nIm)\u03b2\u2217.\nThus\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\n\r\rb\u03b3 \u22121\nn\n\u0000XT X \u2212btr(B)Im\n\u0001\n\u03b2\u2217\r\r\n\u221e\n=\n1\nn\n\r\rXT\n0 \u03f5 + W T \u03f5 \u2212\n\u0000W T W + XT\n0 W \u2212btr(B)Im\n\u0001\n\u03b2\u2217\r\r\n\u221e\n\u2264\n1\nn\n\r\rXT\n0 \u03f5 + W T \u03f5\n\r\r\n\u221e+ 1\nn\n\r\r(W T W \u2212btr(B)Im)\u03b2\u2217\r\r\n\u221e+\n\r\r 1\nnXT\n0 W\u03b2\u2217\r\r\n\u221e\n\u2264\n1\nn\n\r\rXT\n0 \u03f5 + W T \u03f5\n\r\r\n\u221e+ 1\nn(\n\r\r(ZT BZ \u2212tr(B)Im)\u03b2\u2217\r\r\n\u221e) + 1\nn\n\r\rXT\n0 W\u03b2\u2217\r\r\n\u221e\n+ 1\nn\n\f\fbtr(B) \u2212tr(B)\n\f\f \u2225\u03b2\u2217\u2225\u221e=: U1 + U2 + U3 + U4.\nBy Lemma 11 we have on B4 for D0 := \u221a\u03c4B + a1/2\nmax,\nU1 = 1\nn\n\r\rXT\n0 \u03f5 + W T \u03f5\n\r\r\n\u221e= 1\nn\n\r\r\rA\n1\n2 ZT\n1 \u03f5 + ZT\n2 B\n1\n2 \u03f5\n\r\r\r\n\u221e\u2264\u03c1nM\u03f5D0,\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n61\nand on event B5 for D\u2032\n0 :=\np\n\u2225B\u22252 + a1/2\nmax,\nU2 + U3 = 1\nn\n\r\r(ZT BZ \u2212tr(B)Im)\u03b2\u2217\r\r\n\u221e+ 1\nn\n\r\rXT\n0 W\u03b2\u2217\r\r\n\u221e\n\u2264\n\u03c1nK \u2225\u03b2\u2217\u22252\n\u0012\u2225B\u2225F\n\u221an\n+ \u221a\u03c4Ba1/2\nmax\n\u0013\n\u2264K\u03c1n \u2225\u03b2\u2217\u22252 \u03c4 1/2\nB D\u2032\n0,\nwhere recall \u2225B\u2225F \u2264\np\ntr(B) \u2225B\u22251/2\n2\n. Denote by B0 := B4 \u2229B5 \u2229B6. We have on\nB0 and under (A1), by Lemmas 11 and 5 and D1 de\ufb01ned therein,\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\nU1 + U2 + U3 + U4\n\u2264\n\u03c1nM\u03f5D0 + D\u2032\n0\u03c4 1/2\nB K\u03c1n \u2225\u03b2\u2217\u22252 + 1\nn\n\f\fbtr(B) \u2212tr(B)\n\f\f \u2225\u03b2\u2217\u2225\u221e\n\u2264\nD0M\u03f5\u03c1n + D\u2032\n0K\u03c4 1/2\nB\n\u2225\u03b2\u2217\u22252 \u03c1n + D1 \u2225\u03b2\u2217\u2225\u221erm,m\n\u2264\nD0M\u03f5\u03c1n + D\u2032\n0K\u03c4 1/2\nB\n\u2225\u03b2\u2217\u22252 \u03c1n + 2D1K 1\n\u221am\u03c1n.\nFinally, we have by the union bound, P (B0) \u22651 \u221216/m3. This is the end of the proof\nof Theorem 12.\n\u25a1\n16. Conclusion\nIn this paper, we provide a uni\ufb01ed analysis on the rates of convergence for both the cor-\nrected Lasso estimator (1.7) and the Conic programming estimator (1.8). As n increases\nor as the measurement error metric \u03c4B decreases, we see performance gains over the\nentire paths for both \u21131 and \u21132 error for both estimators as expected. When we focus\non the lowest \u21132 error along the paths as we vary the penalty factor f \u2208[0.05, 0.8], the\ncorrected Lasso via the composite gradient descent algorithm performs slightly better\nthan the Conic programming estimator as shown in Figure 5.\nFor the Lasso estimator, when we require that the stochastic error \u03f5 in the response\nvariable y as in (1.1a) does not approach 0 as quickly as the measurement error W in\n(1.1b) does, then the sparsity constraint becomes essentially unchanged as \u03c4B \u21920.\nThese tradeoffs are somehow different from the behavior of the Conic programming\nestimator versus the Lasso estimator; however, we believe the differences are minor.\nEventually, as \u03c4B \u21920, the relaxation on d as in (4.17) enables the Conic programming\nestimator to achieve bounds which are essentially identical to the Dantzig Selector\nwhen the design matrix X0 is a subgaussian random matrix satisfying the Restricted\nEigenvalue conditions; See for example [6, 4, 38].\nWhen \u03c4B \u21920 and M\u03f5 = \u2126(\u03c4 +\nB K \u2225\u03b2\u2217\u22252), we set\n\u03bb \u22652\u03c8\nr\nlog m\nn\n,\nwhere\n\u03c8 := 4C0D\u2032\n0KM\u03f5,\n(16.1)\nso as to recover the regular lasso bounds in \u2113q loss for q = 1, 2 in (4.5) in Theorem 6.\nMoreover, suppose that tr(B) is given, then one can drop the second term in \u03c8 as in\n(4.2) involving \u2225\u03b2\u2217\u22252 entirely and hence recover the lasso bound as well.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n62\nFinally, we note that the bounds corresponding to the Upper RE condition as stated in\nCorollary 25, Theorem 26 and Lemma 15 are not needed for Theorems 3 and 6. They\nare useful to ensure algorithmic convergence and to bound the optimization error for\nthe gradient descent-type of algorithms as considered in [1, 30], when one is interested\nin approximately solving the nonconvex optimization function (1.7). Our Theorem 9\nillustrates this result. Our theory in Theorem 9 predicts the dependencies of the compu-\ntational and statistical rates of convergence for the corrected Lasso via gradient descent\nalgorithm on the condition number \u03ba(A), the trace parameter \u03c4B and the radius R as\n\u03bb \u224d\nR\u03be\n1 \u2212\u03ba \u224d\u03c40\u03ba(A)R log m\nn\n,\nwhere\n\u03c40 \u224d(\u03c1max(s0, A) + \u03c4B)2\n\u03bbmin(A)\ndepends on \u03c4B, sparse and minimal eigenvalues of A. Therefore, we need to increase\nthe penalty when we increase the \u21131-ball radius R in (1.10) in order to ensure algorith-\nmic and statistical convergence as predicted in Theorem 9. This is well-aligned with the\nobservation in Figure 6. Our numerical results validate such algorithmic and statistical\nconvergence properties.\nAcknowledgements\nThe authors thank the Editor, the Associate editor and two referees for their construc-\ntive comments. We thank Rob Kass, Po-Ling Loh, Seyoung Park, Kerby Shedden and\nMartin Wainwright for helpful discussions. We thank Professor Alexander Belloni for\nproviding us the code implementing the Conic programming estimator.\nAppendix A: Outline\nWe prove Theorem 2 in Section B. In Sections C, we present variations of the Hanson-\nWright inequality as recently derived in [37] (cf. Lemma 32). We prove Lemma 11\nin Section C.2. In Sections H and I, we prove the technical lemmas for Theorems 3\nand 4 respectively. In Section J, we prove the Lemmas needed for Proof of Theorem 7.\nIn order to prove Corollary 25, we need to \ufb01rst state some geometric analysis results\nSection K. We prove Corollary 25 in Section L and Theorem 26 in Section M.\nAppendix B: Proof of Theorem 2\nLet us \ufb01rst de\ufb01ne the following shorthand notation\nb\u2206t\n=\n\u03b2t \u2212b\u03b2\nand\n\u03b4t = \u03c6(\u03b2t) \u2212\u03c6(b\u03b2).\nThe proof of the theorem requires two technical Lemmas 28 and 30. Both are stated\nunder assumption (B.1), which is stated in terms of a given tolerance \u00af\u03b7 > 0 and integer\nT > 0 such that\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2) \u2264\u00af\u03b7, \u2200t \u2265T,\n(B.1)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n63\nwhere the distance between \u03b2t and the global optimizer b\u03b2 is measured in terms of the\nobjective function \u03c6, namely, \u03b4t = \u03c6(\u03b2t) \u2212\u03c6(b\u03b2).\nWe \ufb01rst show Lemma 28, which ensures that the vector b\u2206t := \u03b2t \u2212b\u03b2 satis\ufb01es a certain\ncone-type condition. The proof is omitted, as it is a shortened proof of Lemma 1 of [31].\nLemma 28. (Iterated Cone Bound) Under the conditions of Theorem 2, suppose there\nexists a pair (\u00af\u03b7, T) such that (B.1) holds. Then for any iteration t \u2265T, we have\n\r\r\r\u03b2t \u2212b\u03b2\n\r\r\r\n1 \u22644\n\u221a\nd\n\r\r\r\u03b2t \u2212b\u03b2\n\r\r\r\n2 + 8\n\u221a\nd\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2 + 2 \u00b7 min\n\u00122\u00af\u03b7\n\u03bb , R\n\u0013\n.\nWe next state the following auxiliary result on the loss function. We use Lemma 29 in\nthe proof of Lemma 28 and Corollary 10.\nLemma 29. Denote by \u03c4\u2113(Ln) := \u03c40\nlog m\nn\nand \u03bd\u2113= 64d\u03c4\u2113(Ln). Let \u00af\u03f5stat = 8\n\u221a\nd\u03b5stat,\nwhere \u03b5stat =\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2 and \u03f5 = 2\u00b7min\n\u0000 2\u00af\u03b7\n\u03bb , R\n\u0001\n. Under the assumptions of Lemma 28,\nwe have for b\u2206t := \u03b2t \u2212b\u03b2 and t > T,\nT (b\u03b2, \u03b2t)\n\u2265\n\u03b1\u2113\u2212\u03bd\u2113\n2\n\r\r\rb\u2206t\r\r\r\n2\n2 \u22122\u03c4\u2113(Ln)(\u00af\u03f5stat + \u03f5)2\nand\n(B.2)\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2)\n\u2265\nT (\u03b2t, b\u03b2) \u2265\u03b1\u2113\u2212\u03bd\u2113\n2\n\r\r\rb\u2206t\r\r\r\n2\n2 \u22122\u03c4\u2113(Ln)(\u00af\u03f5stat + \u03f5)2.\n(B.3)\nLemma 30. (Lemma 3 of Loh-Wainwright (2015)) Suppose the RSC and RSM con-\nditions as stated in (2.8) and (2.9) hold with parameters (\u03b1\u2113, \u03c4\u2113(Ln)) and (\u03b1u, \u03c4u(Ln))\nrespectively. Under the conditions of Theorem 2, suppose there exists a pair (\u00af\u03b7, T) such\nthat (B.1) holds. Then for any iteration t \u2265T, we have for 0 < \u03ba < 1,\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2)\n\u2264\n\u03bat\u2212T (\u03c6(\u03b2T ) \u2212\u03c6(b\u03b2)) +\n\u03be\n1 \u2212\u03ba(\u00af\u03f52\nstat + \u03f52)\nfor\n\u00af\u03f5stat\n:=\n8\n\u221a\nd\u03b5stat\nand\n\u03f5 = 2 \u00b7 min\n\u00122\u00af\u03b7\n\u03bb , R\n\u0013\n,\nwhere the quantities \u03ba and \u03c8 are as de\ufb01ned in Theorem 2 (cf. (2.11) and (2.12)).\nProof of Theorem 2.\nWe are now ready to put together the \ufb01nal argument for\nthe theorem. First notice that (2.16) follows from (2.15) directly in view of (B.3) and\nLemma 28, where we set \u00af\u03b7 = \u03b42, \u00af\u03f5stat = 8\n\u221a\nd\u03b5stat and \u03f5 = 2 min\n\u0010\n2\u03b42\n\u03bb , R\n\u0011\n.\nFollowing (B.3), we have for \u03bd\u2113= 64d\u03c4\u2113(Ln),\n\u03b1\u2113\u2212\u03bd\u2113\n2\n\r\r\rb\u2206t\r\r\r\n2\n2\n\u2264\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2) + 2\u03c4\u2113(Ln)(\u00af\u03f5stat + \u03f5)2,\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n64\nand thus\n\r\r\rb\u2206t\r\r\r\n2\n2\n\u2264\n2\n\u00af\u03b1\u2113\n(\u03c6(\u03b2t) \u2212\u03c6(b\u03b2) + 4\n\u00af\u03b1\u2113\n\u03c4\u2113(Ln)(\u00af\u03f5stat + \u03f5)2\n\u2264\n2\n\u00af\u03b1\u2113\n\u0000\u03b42 + 2\u03c4\u2113(Ln)(2\u00af\u03f52\nstat + 2\u03f52)\n\u0001\n\u2264\n2\n\u00af\u03b1\u2113\n\u0000\u03b42 + 2\u03c4\u2113(Ln)(128d\u03b52\nstat + 2\u03f52)\n\u0001\n\u2264\n2\n\u00af\u03b1\u2113\n\u0000\u03b42 + 4\u03bd\u2113\u03b52\nstat + 4\u03c4\u2113(Ln)\u03f52\u0001\n.\n(B.4)\nThe remainder of the proof follows an argument in [1]. We \ufb01rst prove the following\ninequality:\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2) \u2264\u03b42,\n\u2200t \u2265T \u2217(\u03b4).\nWe divide the iterations t \u22650 into a series of epochs [T\u2113, T\u2113+1] and defend the toler-\nances \u00af\u03b70 > \u00af\u03b71 > . . . such that\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2) \u2264\u00af\u03b7\u2113,\n\u2200t \u2265T\u2113.\nIn the \ufb01rst iteration, we apply Lemma 30 with \u00af\u03b70 := \u03c6(\u03b20) \u2212\u03c6(b\u03b2) to obtain\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2) \u2264\u03bat(\u03c6(\u03b20) \u2212\u03c6(b\u03b2)) +\n\u03be\n1 \u2212\u03ba(\u00af\u03f52\nstat + 4R2)\nfor any iteration t \u22650.\nSet\n\u00af\u03b71 :=\n2\u03be\n1 \u2212\u03ba(\u00af\u03f52\nstat + 4R2)\nand\nT1 :=\n\u0018log(2\u00af\u03b70/\u00af\u03b71)\nlog(1/\u03ba)\n\u0019\n.\nThen we have for any iteration t \u2265T1\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2) \u2264\u00af\u03b71 :=\n4\u03be\n1 \u2212\u03ba max\n\b\n\u00af\u03f52\nstat, 4R2\t\n.\nThe same argument can be now be applied in a recursive manner. Suppose that for\nsome \u2113\u22651, we are given a pair (\u00af\u03b7\u2113, T\u2113) such that\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2) \u2264\u00af\u03b7\u2113, \u2200t \u2265T\u2113.\n(B.5)\nWe now de\ufb01ne\n\u00af\u03b7\u2113+1 :=\n2\u03be\n1 \u2212\u03ba(\u00af\u03f52\nstat + \u03f52\n\u2113)\nand\nT\u2113+1 :=\n\u0018log(2\u00af\u03b7\u2113/\u00af\u03b7\u2113+1)\nlog(1/\u03ba)\n\u0019\n+ T\u2113.\nWe can apply Lemma 30 to obtain for any iteration t \u2265T\u2113and \u03b5\u2113:= 2 min{ \u00af\u03b7\u2113\n\u03bb , R},\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2) \u2264\u03bat\u2212T\u2113(\u03c6(\u03b2T\u2113) \u2212\u03c6(b\u03b2)) +\n\u03be\n1 \u2212\u03ba(\u00af\u03f52\nstat + \u03b52\n\u2113),\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n65\nwhich implies that for all t \u2265T\u2113+1,\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2) \u2264\u00af\u03b7\u2113+1 \u2264\n4\u03be\n1 \u2212\u03ba max{\u00af\u03f52\nstat, \u03b52\n\u2113}\nby our choice of {\u03b7\u2113, T\u2113}\u2113\u22651. Finally, we use the recursion\n\u00af\u03b7\u2113+1 \u2264\n4\u03be\n1 \u2212\u03ba max(\u00af\u03f52\nstat, \u03b52\n\u2113)\nand\nT\u2113\u2264\u2113+ log(2\u2113\u00af\u03b70/\u00af\u03b7\u2113)\nlog(1/\u03ba)\n(B.6)\nto establish the recursion that\n\u00af\u03b7\u2113+1 \u2264\n\u00af\u03b7\u2113\n42\u2113\u22121\nand\n\u03b5\u2113+1 := \u00af\u03b7\u2113+1\n\u03bb\n\u2264R\n42\u2113\u2200\u2113= 1, 2, . . . .\n(B.7)\nTaking these statements as given, we need to have\n\u00af\u03b7\u2113\u2264\u03b42.\nIt is suf\ufb01cient to establish that\n\u03bbR\n42\u2113\u22121 \u2264\u03b42.\nThus we \ufb01nd that the error drops below \u03b42 after at most\n\u2113\u03b4 \u2265log\n\u0000log(R\u03bb/\u03b42)/ log(4)\n\u0001\n/ log 2 + 1 = log log(R\u03bb/\u03b42)\nepochs. Combining the above bound on \u2113\u03b4 with the recursion (B.6)\nT\u2113\u2264\u2113+ log(2\u2113\u00af\u03b70/\u00af\u03b7\u2113)\nlog(1/\u03ba)\n,\nwe conclude that\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2) \u2264\u03b42\nis guaranteed to hold for all iterations\nt > \u2113\u03b4\n\u0012\n1 +\nlog 2\nlog(1/\u03ba)\n\u0013\n+ log(\u00af\u03b70/\u03b42)\nlog(1/\u03ba) .\nTo establish (B.7), we start with \u2113= 0 and establish that for \u00af\u03f5stat = 8\n\u221a\nd\u03f5stat =\no(\n\u221a\nd) = o(R)\n\u00af\u03b71\n\u03bb\n:=\n4\u03be\n(1 \u2212\u03ba)\u03bb max(\u00af\u03f52\nstat, 4R2) =\n16R\u03be\n(1 \u2212\u03ba)\u03bbR \u2264R\n4\n(B.8)\nand thus\n\u03b51\n:=\n2 min{ \u00af\u03b71\n\u03bb , R} = R/2 \u2264\u03b50 = R.\n(B.9)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n66\nAssume that \u00af\u03f5stat \u2264\u03b51 (otherwise, we are done at the \ufb01rst iteration). First, we obtain\nfor \u2113= 1,\n\u00af\u03b72\n\u2264\n4\u03be\n1 \u2212\u03ba max(\u00af\u03f52\nstat, \u03b52\n1) =\n4\u03be\n1 \u2212\u03ba\u03b52\n1 =\n4\u03be\n1 \u2212\u03ba\n\u00122\u00af\u03b71\n\u03bb\n\u00132\n\u2264\n16\u03be\n1 \u2212\u03ba\n\u00af\u03b72\n1\n\u03bb2 \u226416\u03beR\n1 \u2212\u03ba\n\u00af\u03b71\n4\u03bb \u2264\u00af\u03b71\n4 ,\nand\n\u00af\u03b72\n\u03bb\n\u2264\n\u00af\u03b71\n4\u03bb \u2264R\n16,\nwhere in the last three steps, we use the fact that \u03bb \u2265\n16R\u03be\n(1\u2212\u03ba) and (B.8). Thus (B.6) holds\nfor \u2113= 1.\nNow assume that (B.7) holds for d \u2264\u2113. In the induction step, we again use the assump-\ntion that \u03b5\u2113:= 2 \u00af\u03b7\u2113\n\u03bb \u2265\u00af\u03f5stat and (B.6) to obtain\n\u00af\u03b7\u2113+1\n\u2264\n4\u03be\n1 \u2212\u03ba max(\u00af\u03f52\nstat, \u03b52\n\u2113) =\n16\u03be\n1 \u2212\u03ba\n\u00af\u03b72\n\u2113\n\u03bb2\n\u2264\n16\u03be\n1 \u2212\u03ba\nR\n42(\u2113\u22121)\n\u00af\u03b7\u2113\n\u03bb = 16R\u03be\n1 \u2212\u03ba\n1\n\u03bb\n\u00af\u03b7\u2113\n42(\u2113\u22121)\n\u2264\n\u00af\u03b7\u2113\n42(\u2113\u22121) .\nFinally, by the induction assumption\n\u00af\u03b7\u2113\n\u03bb \u2264\nR\n42\u2113\u22121 ,\nwe use the bound immediately above to obtain\n\u00af\u03b7\u2113+1\n\u03bb\n\u2264\n\u00af\u03b7\u2113\n42(\u2113\u22121)\n1\n\u03bb \u2264\nR\n42\u2113\u22121\n1\n42(\u2113\u22121) \u2264R\n42\u2113.\nThe rest of the proof follows from that of Corollary 10. This is the end of the proof for\nTheorem 2.\n\u25a1\nIt remains to prove Lemma 29.\nProof of Lemma 29.\nUsing the RSC condition, we have for \u03c4\u2113(Ln) := \u03c40\nlog m\nn\nand \u03bd\u2113= 64d\u03c4\u2113(Ln) \u2264\u03b1\u2113\n48 ,\nT (b\u03b2, \u03b2t)\n\u2265\n\u03b1\u2113\n2\n\r\r\rb\u2206t\r\r\r\n2\n2 \u2212\u03c4\u2113(Ln)\n\r\r\rb\u2206t\r\r\r\n2\n1\n(B.10)\n\u2265\n\u03b1\u2113\n2\n\r\r\rb\u2206t\r\r\r\n2\n2 \u2212\u03c4\u2113(Ln)\n\u0012\n2 \u221716d\n\r\r\rb\u2206t\r\r\r\n2\n2 + 2(\u00af\u03f5stat + \u03f5)2\n\u0013\n\u2265\n1\n2 \u00af\u03b1\u2113\n\r\r\rb\u2206t\r\r\r\n2\n2 \u22122\u03c4\u2113(Ln)(\u00af\u03f5stat + \u03f5)2\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n67\nand by Lemma 28, for any iteration t \u2265T,\n\r\r\rb\u2206t \u2212b\u03b2\n\r\r\r\n1\n\u2264\n4\n\u221a\nd\n\r\r\r\u03b2t \u2212b\u03b2\n\r\r\r\n2 + 8\n\u221a\nd\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2 + 2 \u00b7 min\n\u00122\u00af\u03b7\n\u03bb , R\n\u0013\n\u2264\n4\n\u221a\nd\n\r\r\rb\u2206t\r\r\r\n2 + (\u00af\u03f5stat + \u03f5).\nBy convexity of function g, we have\ng(\u03b2t) \u2212g(b\u03b2) \u2212\u27e8\u2207g(b\u03b2), \u03b2t \u2212b\u03b2 \u27e9\u22650.\n(B.11)\nThus\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2) \u2212\u27e8\u2207\u03c6(b\u03b2), \u03b2t \u2212b\u03b2 \u27e9\n=\nLn(\u03b2t) \u2212Ln(b\u03b2) \u2212\u27e8\u2207Ln(b\u03b2), \u03b2t \u2212b\u03b2 \u27e9+ \u03bb(g(\u03b2t) \u2212g(b\u03b2) \u2212\u27e8\u2207g(b\u03b2), \u03b2t \u2212b\u03b2 \u27e9).\nMoreover, by the \ufb01rst order optimality condition for b\u03b2, we have for all feasible \u03b2t \u2208\u2126\n\u27e8\u2207\u03c6(b\u03b2), \u03b2t \u2212b\u03b2 \u27e9\u22650,\nand thus\n\u03c6(\u03b2t) \u2212\u03c6(b\u03b2)\n\u2265\nLn(\u03b2t) \u2212Ln(b\u03b2) \u2212\u27e8\u2207Ln(b\u03b2), \u03b2t \u2212b\u03b2 \u27e9= T (\u03b2t, b\u03b2),\nwhere similar to (B.10), we have\nT (\u03b2t, b\u03b2)\n\u2265\n\u03b11\n\r\r\rb\u2206t\r\r\r\n2\n2 \u2212\u03c4\u2113(Ln)\n\r\r\rb\u2206t\r\r\r\n2\n1\n\u2265\n(\u03b11 \u221232d\u03c4\u2113(Ln))\n\r\r\rb\u2206t\r\r\r\n2\n2 \u22122\u03c4\u2113(Ln)(\u00af\u03f5stat + \u03f5)2\n=\n1\n2 \u00af\u03b1\u2113\n\r\r\rb\u2206t\r\r\r\n2\n2 \u22122\u03c4\u2113(Ln)(\u00af\u03f5stat + \u03f5)2,\nand by Lemma 28,\n\r\r\rb\u2206t\r\r\r\n2\n1\n\u2264\n32d\n\r\r\rb\u2206t\r\r\r\n2\n2 + 2\n\u0012\n8\n\u221a\nd\u03b5stat + 2 \u00b7 min\n\u00122\u00af\u03b7\n\u03bb , R\n\u0013\u00132\n\u2264\n32d\n\r\r\rb\u2206t\r\r\r\n2\n2 + 2(\u00af\u03f5stat + \u03f5)2.\n\u25a1\nAppendix C: Some auxiliary results\nWe \ufb01rst need to state the following form of the Hanson-Wright inequality as recently\nderived in Rudelson and Vershynin [37], and an auxiliary result in Lemma 32 which\nmay be of independent interests.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n68\nTheorem 31. Let X = (X1, . . . , Xm) \u2208Rm be a random vector with independent\ncomponents Xi which satisfy E (Xi) = 0 and \u2225Xi\u2225\u03c82 \u2264K. Let A be an m \u00d7 m\nmatrix. Then, for every t > 0,\nP\n\u0000\f\fXT AX \u2212E\n\u0000XT AX\n\u0001\f\f > t\n\u0001\n\u22642 exp\n\"\n\u2212c min\n \nt2\nK4 \u2225A\u22252\nF\n,\nt\nK2 \u2225A\u22252\n!#\n.\nWe note that following the proof of Theorem 31, it is clear that the following holds: Let\nX = (X1, . . . , Xm) \u2208Rm be a random vector as de\ufb01ned in Theorem 31. Let Y, Y \u2032 be\nindependent copies of X. Let A be an m \u00d7 m matrix. Then, for every t > 0,\nP\n\u0000\f\fY T AY \u2032\f\f > t\n\u0001\n\u22642 exp\n\"\n\u2212c min\n \nt2\nK4 \u2225A\u22252\nF\n,\nt\nK2 \u2225A\u22252\n!#\n.\n(C.1)\nWe next need to state Lemma 32, which we prove in Section N.\nLemma 32. Let u, w \u2208Sn\u22121. Let A \u227b0 be an m \u00d7 m symmetric positive de\ufb01nite\nmatrix. Let Z be an n \u00d7 m random matrix with independent entries Zij satisfying\nEZij = 0 and \u2225Zij\u2225\u03c82 \u2264K. Let Z1, Z2 be independent copies of Z. Then for every\nt > 0,\nP\n\u0010\f\f\fuT Z1A1/2ZT\n2 w\n\f\f\f > t\n\u0011\n\u2264\n2 exp\n \n\u2212c min\n \nt2\nK4tr(A),\nt\nK2 \u2225A\u22251/2\n2\n!!\n,\nP\n\u0000\f\fuT ZAZT w \u2212EuT ZAZT w\n\f\f > t\n\u0001\n\u2264\n2 exp\n \n\u2212c min\n \nt2\nK4 \u2225A\u22252\nF\n,\nt\nK2 \u2225A\u22252\n!!\n,\nwhere c is the same constant as de\ufb01ned in Theorem 31.\nC.1. Proof of Lemma 5\nFirst we write\nXXT \u2212tr(A)In\n=\n\u0000Z1A1/2 + B1/2Z2)\n\u0000Z1A1/2 + B1/2Z2\n\u0001T \u2212tr(A)In\n=\n\u0000Z1A1/2 + B1/2Z2)\n\u0000ZT\n2 B1/2 + A1/2ZT\n1\n\u0001\n\u2212tr(A)In\n=\nZ1A1/2ZT\n2 B1/2 + B1/2Z2ZT\n2 B1/2\n+B1/2Z2A1/2ZT\n1 + Z1AZT\n1 \u2212tr(A)In.\nThus we have for \u02c7tr(B) := 1\nm\n\u0000\u2225X\u22252\nF \u2212ntr(A)\n\u0001\n,\n1\nn(\u02c7tr(B) \u2212tr(B)) :=\n1\nmn\n\u0000\u2225X\u22252\nF \u2212ntr(A) \u2212mtr(B)\n\u0001\n=\n1\nmn(tr(XXT ) \u2212ntr(A) \u2212mtr(B))\n=\n2\nmntr(Z1A1/2ZT\n2 B1/2) +\n\u0012tr(B1/2Z2ZT\n2 B1/2)\nmn\n\u2212tr(B)\nn\n\u0013\n+tr(Z1AZT\n1 )\nmn\n\u2212tr(A)\nm\n.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n69\nBy constructing a new matrix An = In \u2297A, which is block diagonal with n identical\nsubmatrices A along its diagonal, we prove the following large deviation bound: for\nt1 = C0K2 \u2225A\u2225F\n\u221an log m and n > log m,\nP\n\u0000\f\ftr(Z1AZT\n1 ) \u2212ntr(A)\n\f\f \u2265t1\n\u0001\n= P\n\u0010\f\f\fvec { Z1 }T (I \u2297A)vec { Z1 } \u2212ntr(A)\n\f\f\f \u2265t1\n\u0011\n\u2264\nexp\n \n\u2212c min\n \nt2\n1\nK4 \u2225An\u22252\nF\n,\nt1\nK2 \u2225An\u22252\n!!\n\u2264\n2 exp\n \n\u2212c min\n \n(C0K2\u221an log m \u2225A\u2225F )2\nnK4 \u2225A\u22252\nF\n, C0K2\u221an log m \u2225A\u2225F\nK2 \u2225A\u22252\n!!\n\u2264\n2 exp (\u22124 log m) ,\nwhere the \ufb01rst inequality holds by Theorem 31 and the second inequality holds given\nthat \u2225An\u22252\nF = n \u2225A\u22252\nF and \u2225An\u22252 = \u2225A\u22252.\nSimilarly, by constructing a new matrix Bm = Im \u2297B, which is block diagonal with\nm identical submatrices B along its diagonal, we prove the following large deviation\nbound: for t2 = C0K2 \u2225B\u2225F\n\u221am log m and m \u22652,\nP\n\u0000\f\ftr(ZT\n2 BZ2) \u2212mtr(B)\n\f\f \u2265t2\n\u0001\n= P\n\u0010\f\f\fvec { Z2 }T (Im \u2297B)vec { Z2 } \u2212mtr(B)\n\f\f\f \u2265t2\n\u0011\n\u2264\nexp\n \n\u2212c min\n \nt2\n2\nK4m \u2225B\u22252\nF\n,\nt2\nK2 \u2225B\u22252\n!!\n\u2264\n2 exp\n \n\u2212c min\n \n(C0K2\u221am log m \u2225B\u2225F )2\nK4m \u2225B\u22252\nF\n, C0K2\u221am log m \u2225B\u2225F\nK2 \u2225B\u22252\n!!\n\u2264\n2 exp (\u22124 log m) .\nFinally, we have by (C.1) for t0 = C0K2p\ntr(A)tr(B) log m,\nP\n\u0010\f\f\fvec { Z1 }T B1/2 \u2297A1/2vec { Z2 }\n\f\f\f > t0\n\u0011\n\u2264\n2 exp\n \n\u2212c min\n \nt2\n0\nK4 \r\rB1/2 \u2297A1/2\r\r2\nF\n,\nt0\nK2 \r\rB1/2 \u2297A1/2\r\r\n2\n!!\n=\n2 exp\n \n\u2212c min\n \n(C0\np\ntr(A)tr(B) log m)2\ntr(A)tr(B)\n, C0\np\ntr(A)tr(B) log m\n\u2225B\u22251/2\n2\n\u2225A\u22251/2\n2\n!!\n\u2264\n2 exp(\u22124 log m),\nwhere we use the fact that r(A)r(B) \u2265log m,\n\r\rB1/2 \u2297A1/2\r\r\n2 = \u2225B\u22251/2\n2\n\u2225A\u22251/2\n2\nand\n\r\r\rB1/2 \u2297A1/2\r\r\r\n2\nF\n=\ntr((B1/2 \u2297A1/2)(B1/2 \u2297A1/2)) = tr(B \u2297A) = tr(A)tr(B).\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n70\nThus we have with probability 1 \u22126/m4,\n1\nn\n\f\f\u02c7tr(B) \u2212tr(B)\n\f\f =\n1\nmn\n\f\ftr(XXT ) \u2212ftr(A) \u2212mtr(B)\n\f\f\n\u2264\n2\nmn\n\f\f\fvec { Z1 }T (B1/2 \u2297A1/2)vec { Z2 }\n\f\f\f\n+\n\f\f\f\f\ntr(ZT\n2 BZ2)\nmn\n\u2212tr(B)\nn\n\f\f\f\f +\n\f\f\f\f\ntr(Z1AZT\n1 )\nmn\n\u2212tr(A)\nm\n\f\f\f\f\n\u2264\n1\nmn(2t0 + t1 + t2) =\n\u221alog m\n\u221amn C0K2\n\u0012\u2225A\u2225F\n\u221am + 2\u221a\u03c4A\u03c4B + \u2225B\u2225F\n\u221an\n\u0013\n\u2264\n2C0\n\u221alog m\n\u221amn K2D1 =: D1rm,m,\nwhere recall rm,m = 2C0K2\n\u221alog m\n\u221amn , D1 = \u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221an , and\n2\u221a\u03c4A\u03c4B \u2264\u03c4A + \u03c4B \u2264\u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221an .\nTo see this, recall\nm\u03c4A\n=\nm\nX\ni=1\n\u03bbi(A) \u2264\u221am(\nm\nX\ni=1\n\u03bb2\ni (A))1/2 = \u221am \u2225A\u2225F\nand\n(C.2)\nn\u03c4B\n=\nn\nX\ni=1\n\u03bbi(B) \u2264\u221an(\nn\nX\ni=1\n\u03bb2\ni (B))1/2 = \u221an \u2225B\u2225F\nwhere \u03bbi(A), i = 1, . . . , m and \u03bbi(B), i = 1, . . . , n denote the eigenvalues of positive\nsemide\ufb01nite covariance matrices A and B respectively.\nDenote by B6 the following event\n\b 1\nn\n\f\f\u02c7tr(B) \u2212tr(B)\n\f\f \u2264D1rm,m\n\t\n.\nClearly btr(B) := (\u02c7tr(B))+ by de\ufb01nition (1.5). As a consequence, on B6, btr(B) =\n\u02c7tr(B) > 0 when \u03c4B > D1rm,m; hence\n1\nn\n\f\fbtr(B) \u2212tr(B)\n\f\f = 1\nn\n\f\f\u02c7tr(B) \u2212tr(B)\n\f\f \u2264D1rm,m.\nOtherwise, it is possible that \u02c7tr(B) < 0. However, suppose we set\nb\u03c4B := 1\nn btr(B) := 1\nn(\u02c7tr(B) \u22280),\nthen we can also guarantee that\n|b\u03c4B \u2212\u03c4B| = |\u03c4B| \u2264D1rm,m\nin case\n\u03c4B \u2264D1rm,m.\nThe lemma is thus proved.\n\u25a1\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n71\nC.2. Proof of Lemma 11\nFollowing Lemma 32, we have for all t > 0, B \u227b0 being an n \u00d7 n symmetric positive\nde\ufb01nite matrix, and v, w \u2208Rm\nP\n\u0010\f\f\fvT ZT\n1 B1/2Z2w\n\f\f\f > t\n\u0011\n\u22642 exp\n\"\n\u2212c min\n \nt2\nK4tr(B),\nt\nK2 \u2225B\u22251/2\n2\n!#\n(C.3)\nand\nP\n\u0000\f\fvT ZT BZw \u2212EvT ZT BZw\n\f\f > t\n\u0001\n\u22642 exp\n \n\u2212c min\n \nt2\nK4 \u2225B\u22252\nF\n,\nt\nK2 \u2225B\u22252\n!!\n.(C.4)\nProof of Lemma 11.\nLet e1, . . . , em \u2208Rm be the canonical basis spanning\nRm. Let x1, . . . , xm, x\u2032\n1, . . . , x\u2032\nm \u2208Rn be the column vectors Z1, Z2 respectively. Let\nY \u223ceT\n1 ZT\n0 . Let wi =\nA1/2ei\n\u2225A1/2ei\u22252\nfor all i. Clearly the condition on the stable rank of\nB guarantees that\nn \u2265r(B) = tr(B)\n\u2225B\u22252\n= tr(B) \u2225B\u22252\n\u2225B\u22252\n2\n\u2265\u2225B\u22252\nF / \u2225B\u22252\n2 \u2265log m.\nBy (C.1), we obtain for t\u2032 = C0M\u03f5K\np\ntr(B) log m\nP\n\u0010\n\u2203j,\n\f\f\f\u03f5T B1/2Z2ej\n\f\f\f > t\u2032\u0011\n=\nP\n\u0012\n\u2203j, M\u03f5\nK\n\f\f\feT\n1 ZT\n0 B1/2Z2ej\n\f\f\f > C0M\u03f5K\np\nlog mtr(B)\n1\n2\n\u0013\n\u2264\nexp(log m)P\n\u0010\f\f\fY T B1/2x\u2032\nj\n\f\f\f > C0K2p\nlog mtr(B)\n1\n2\n\u0011\n\u22642/m3\nwhere the last inequality holds by the union bound, given that tr(B)\n\u2225B\u22252 \u2265log m; Similarly,\nfor all j and t = C0K2\u221alog mtr(B)1/2,\nP\n\u0010\f\f\fY T B1/2x\u2032\nj\n\f\f\f > t\n\u0011\n\u2264\n2 exp\n \n\u2212c min\n \nt2\nK4tr(B),\nt\nK2 \u2225B\u22251/2\n2\n!!\n,\n\u2264\n2 exp\n \n\u2212c min\n \nC2\n0 log m, C0 log1/2 m\np\ntr(B)\n\u2225B\u22251/2\n2\n!!\n\u2264\n2 exp\n\u0000\u2212c min(C2\n0, C0) log m\n\u0001\n\u22642 exp (\u22124 log m) .\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n72\nLet v, w \u2208Sm\u22121. Thus we have by Lemma 32, for t0 = C0M\u03f5K\u221an log m, \u03c4 =\nC0K2\u221an log m, wj =\nA1/2ej\n\u2225A1/2ej\u22252\nand n \u2265log m,\nP\n\u0000\u2203j,\n\f\f\u03f5T Z1wj\n\f\f > t0\n\u0001\n\u2264P\n\u0012\n\u2203j, M\u03f5\nK\n\f\fY T Z1wj\n\f\f > C0M\u03f5K\np\nn log m\n\u0013\n\u2264\nmP\n\u0010\f\fY T Z1wj\n\f\f > C0K2p\nn log m\n\u0011\n=\nexp(log m)P\n\u0000\f\feT\n1 ZT\n0 Z1wj\n\f\f > \u03c4\n\u0001\n\u22642 exp\n\u0012\n\u2212c min\n\u0012 \u03c4 2\nnK4 , \u03c4\nK2\n\u0013\u0013\n=: V\nwhere\nV\n\u2264\n2 exp\n\u0012\n\u2212c min\n\u0012(C0K2\u221an log m)2\nnK4\nC0K2\u221an log m\nK2\n\u0013\n+ log m\n\u0013\n\u2264\n2m exp\n\u0010\n\u2212c min\n\u0010\nC2\n0 log m, C0 log1/2 m\u221an\n\u0011\u0011\n\u2264\n2m exp\n\u0000\u2212c min(C2\n0, C0) log m\n\u0001\n\u22642 exp (\u22123 log m) .\nTherefore we have with probability at least 1 \u22124/m3,\n\r\r\rZT\n2 B\n1\n2 \u03f5\n\r\r\r\n\u221e\n:=\nmax\nj=1,...,m \u27e8\u03f5T B1/2Z2, ej \u27e9\u2264t\u2032 = C0M\u03f5K\np\ntr(B) log m\n\r\r\rA\n1\n2 ZT\n1 \u03f5\n\r\r\r\n\u221e\n:=\nmax\nj=1,...,m \u27e8A1/2ej, ZT\n1 \u03f5 \u27e9\n\u2264\nmax\nj=1,...,m\n\r\r\rA1/2ej\n\r\r\r\n2\nmax\nj=1,...,m \u27e8wj, ZT\n1 \u03f5 \u27e9\n\u2264\na1/2\nmaxt0 = a1/2\nmaxC0M\u03f5K\np\nn log m.\nThe \u201cmoreover\u201d part follows exactly the same arguments as above. Denote by \u00af\u03b2\u2217:=\n\u03b2\u2217/ \u2225\u03b2\u2217\u22252 \u2208E \u2229Sm\u22121 and wi := A1/2ei/\n\r\rA1/2ei\n\r\r\n2. By (C.3)\nP\n\u0010\n\u2203i, \u27e8wi, ZT\n1 B1/2Z2 \u00af\u03b2\u2217\u27e9\u2265C0K2p\nlog mtr(B)1/2\u0011\n\u2264\nm\nX\ni=1\nP\n\u0010\n\u27e8wi, ZT\n1 B1/2Z2 \u00af\u03b2\u2217\u27e9\u2265C0K2p\nlog mtr(B)\n\u0011\n\u2264\n2 exp\n\u0000\u2212c min\n\u0000C2\n0 log m, C0 log m\n\u0001\n+ log m\n\u0001\n\u22642/m3.\nNow for t = C0K2\u221alog m \u2225B\u2225F and \u2225B\u2225F / \u2225B\u22252 \u2265\u221alog m,\nP\n\u0010\n\u2203ei : \u27e8ei, (ZT BZ \u2212tr(B)Im)\u00af\u03b2\u2217\u27e9\u2265C0K2p\nlog m \u2225B\u2225F\n\u0011\n\u2264\n2m exp\n\"\n\u2212c min\n \nt2\nK4 \u2225B\u22252\nF\n,\nt\nK2 \u2225B\u22252\n!#\n\u22642/m3.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n73\nBy the two inequalities immediately above, we have with probability at least 1\u22124/m3,\n\r\rXT\n0 W\u03b2\u2217\r\r\n\u221e=\n\r\r\rA1/2ZT\n1 B1/2Z2\u03b2\u2217\r\r\r\n\u221e\n\u2264\n\u2225\u03b2\u2217\u22252 max\nei\n\r\r\rA1/2ei\n\r\r\r\n2\n\u0012\nsup\nwi\n\u27e8wi, ZT\n1 B1/2Z2 \u00af\u03b2\u2217\u27e9\n\u0013\n\u2264\nC0K2 \u2225\u03b2\u2217\u22252\np\nlog ma1/2\nmax\np\ntr(B)\nand\n\r\r(ZT BZ \u2212tr(B)Im)\u03b2\u2217\r\r\n\u221e=\n\r\r(ZT BZ \u2212tr(B)Im)\u00af\u03b2\u2217\r\r\n\u221e\u2225\u03b2\u2217\u22252\n=\n\u2225\u03b2\u2217\u22252\n\u0012\nsup\nei\n\u27e8ei, (ZT BZ \u2212tr(B)Im)\u00af\u03b2\u2217\u27e9\n\u0013\n\u2264\nC0K2 \u2225\u03b2\u2217\u22252\np\nlog m \u2225B\u2225F .\nThe last two bounds follow exactly the same arguments as above, except that we replace\n\u03b2\u2217with ej, j = 1, . . . , m and apply the union bounds to m2 instead of m events, and\nthus P (B10) \u22651 \u22124/m2.\n\u25a1\nAppendix D: Proof of Corollary 13\nNow following (6.1), we have on event B0,\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\n\u03c1n\n\u0012\u00123\n4D2 + D2\n1\n\u221am\n\u0013\nK \u2225\u03b2\u2217\u22252 + D0M\u03f5\n\u0013\nwhere 2D1 \u22642 \u2225A\u22252 + 2 \u2225B\u22252 = D2, and for (D\u2032\n0)2 \u22642 \u2225B\u22252 + 2amax,\nD0 \u2264D\u2032\n0\n\u2264\nq\n2(\u2225B\u22252 + amax) \u22642(amax + \u2225B\u22252) = D2,\nand\nD\u2032\n0\u03c4 1/2\nB\n\u2264\n(\u2225B\u22251/2\n2\n+ a1/2\nmax)\u03c4 1/2\nB\n\u2264\u03c4B + 1\n2(\u2225B\u22252 + amax) \u22643\n4D2\ngiven that under (A1) : \u03c4A = 1, \u2225A\u22252 \u2265amax \u2265a1/2\nmax \u22651. Hence the lemma holds\nfor m \u226516 and \u03c8 = C0D2K (K \u2225\u03b2\u2217\u22252 + M\u03f5).\n\u25a1\nAppendix E: Proof of Corollary 14\nSuppose that event B0 holds. Recall D\u2032\n0 = \u2225B\u22251/2\n2\n+ a1/2\nmax. Denote by \u03c1n :=\nC0K\nq\nlog m\nn\n. By (6.1) and the fact that 2D1 := 2( \u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221an ) \u22642(\u2225A\u22251/2\n2\n+\n\u2225B\u22251/2\n2\n)(\u221a\u03c4A + \u221a\u03c4B) \u2264DoracleD\u2032\n0,\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\nD\u2032\n0K\u03c4 1/2\nB\n\u2225\u03b2\u2217\u22252 \u03c1n + 2D1K 1\n\u221am \u2225\u03b2\u2217\u2225\u221e\u03c1n + D0M\u03f5\u03c1n\n\u2264\nD\u2032\n0K \u2225\u03b2\u2217\u22252 \u03c1n\n\u0012\n\u03c4 1/2\nB\n+ Doracle\n\u221am\n\u0013\n+ D0M\u03f5\u03c1n\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n74\nThe corollary is thus proved.\n\u25a1\nAppendix F: Proof of Lemma 15\nIn view of Remark F.1, Condition (6.5) implies that (7.5) in Theorem 26 holds\nfor k = s0 and \u03b5 =\n1\n2MA . Now, by Theorem 26, we have \u2200u, v \u2208E \u2229Sm\u22121,\nunder (A1) and (A3), condition (7.1) holds under event A0, and so long as mn \u2265\n4096C2\n0D2\n2K4 log m/\u03bbmin(A)2,\n\f\fuT \u2206v\n\f\f\n\u2264\n8C\u03d6(s0)\u03b5 + 2C0D2K2\nr\nlog m\nmn\n=: \u03b4 with\n\u03b4\n\u2264\n\u03bbmin(A)\n16\n+ \u03bbmin(A)\n32\n= 3\n32\u03bbmin(A) \u22641\n8,\nwhich holds for all\n\u03b5 \u22641\n2\n\u03bbmin(A)\n64C\u03d6(s0) :=\n1\n2MA\n\u2264\n1\n128C\nwith P (A0) \u22651 \u22124 exp\n\u0010\n\u2212c2\u03b52\ntr(B)\nK4\u2225B\u22252\n\u0011\n\u22122 exp\n\u0000\u2212c2\u03b52 n\nK4\n\u0001\n\u22126/m3.\nHence, by Corollary 25, \u2200\u03b8 \u2208Rm,\n\u03b8T b\u0393A\u03b8 \u2265\u03b1 \u2225\u03b8\u22252\n2 \u2212\u03c4 \u2225\u03b8\u22252\n1\nand\n\u03b8T b\u0393A\u03b8 \u2264e\u03b1 \u2225\u03b8\u22252\n2 + \u03c4 \u2225\u03b8\u22252\n1 ,\nwhere \u03b1 = 5\n8\u03bbmin(A) and e\u03b1 = 11\n8 \u03bbmax(A) and \u03c4 = 3\n8\n\u03bbmin(A)\ns0\n.\nNow for s0 \u226532 as de\ufb01ned in (2.6), we have\ns0\n\u2264\nn\nlog m\n\u03bb2\nmin(A)\n1024C2\u03d6(s0)2\n(F.1)\nand\ns0 + 1\n\u2265\nn\nlog m\n\u03bb2\nmin(A)\n1024C2\u03d62(s0 + 1)\n(F.2)\ngiven that \u03c4B + \u03c1max(s0 + 1, A) = O(\u03bbmax(A)) in view of (2.5) and (A3). Thus\n384C2\u03d6(s0)2\n\u03bbmin(A)\nlog m\nn\n\u2264\u03c4\n=\n3\n8\n\u03bbmin(A)\ns0\n\u2264\n33\n32(s0 + 1)\n3\n8\n\u03bbmin(A)\ns0\n\u2264\n396C2\u03d62(s0 + 1)\n\u03bbmin(A)\nlog m\nn\n.\nThe lemma is thus proved in view of Remark F.1.\n\u25a1\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n75\nRemark F.1. Clearly the condition on tr(B)/ \u2225B\u22252 as stated in Lemma 15 ensures\nthat we have for \u03b5 =\n1\n2MA and s0 \u224d\n4n\nM 2\nA log m,\n\u03b52\ntr(B)\nK4 \u2225B\u22252\n\u2265\n\u03b52\nK4 c\u2032K4 s0\n\u03b52 log\n\u00123em\ns0\u03b5\n\u0013\n\u2265\nc\u2032s0 log\n\u00126emMA\ns0\n\u0013\n,\nand hence\nexp\n\u0012\n\u2212c2\u03b52\ntr(B)\nK4 \u2225B\u22252\n\u0013\n\u2264\nexp\n\u0012\n\u2212c\u2032c2s0 log\n\u00126emMA\ns0\n\u0013\u0013\n\u224d\nexp\n\u0012\n\u2212c3\n4n\nM 2\nA log m log\n\u00123eM 3\nAm log m\n2n\n\u0013\u0013\n.\nF.1. Comparing the two type of RE conditions in Theorems 3 and 4\nWe de\ufb01ne Cone(d0, k0), where 0 < d0 < m and k0 is a positive number, as the set of\nvectors in Rm which satisfy the following cone constraint:\nCone(d0, k0) = {x \u2208Rm | \u2203I \u2208{1, . . . , m}, |I| = d0 s.t. \u2225xIc\u22251 \u2264k0 \u2225xI\u22251} .\nFor each vector x \u2208Rm, let T0 denote the locations of the d0 largest coef\ufb01cients of x\nin absolute values. The following elementary estimate [38] will be used in conjunction\nwith the RE condition.\nLemma 33. For each vector x \u2208Cone(d0, k0), let T0 denotes the locations of the d0\nlargest coef\ufb01cients of x in absolute values. Then\n\u2225xT0\u22252 \u2265\n\u2225x\u22252\n\u221a1 + k0\n.\n(F.3)\nLemma 34. Suppose all conditions in Lemma 15 hold. Let k0 := 1 + \u03bb. Suppose that\nd0 = o\n\u0000s0/64(1 + 3\u03bb/4)2\u0001\n. Now suppose that\n\u03c4(1 + 3k0)22d0 = 2\u03c4(4 + 3\u03bb)2d0 \u2264\u03b1/2.\nThen on event A0, we have RE2(2d0, 3k0, b\u0393A) condition holds on b\u0393A in the sense that\nmin\nx\u2208Cone(2d0,3k0)\nxT b\u0393Ax\n\u2225xT0\u22252\n2\n\u2265\u03b1\n2 .\n(F.4)\nUnder (A2) and (A3), we could set d0 such that for some large enough constant CA,\nd0 \u2264\nn\nCA\u03ba(A)2 log m = O\n\u0012 \u03bb2\nmin(A)\n\u03d62(s0 + 1)\nn\nlog m\n\u0013\n(F.5)\nwhere \u03ba(A) := \u03bbmax(A)\n\u03bbmin(A) , so that d0 = O(s0) and (F.4) holds.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n76\nProof. Now following the proof Lemma 1, Part I. We have on A0, the Lower-RE con-\ndition holds for \u0393A. Thus for x \u2208Cone(2d0, 3k0)\u2229Sm\u22121 and \u03c4(1+3k0)22d0 \u2264\u03b1/2,\n\u2225x\u22252\n1 \u2264(1 + 3k0)2 \u2225xT0\u22252\n1 \u2264(1 + 3k0)22d0 \u2225xT0\u22252\n2 .\nThus\nxT b\u0393Ax\n\u2265\n\u0010\n\u03b1 \u2225x\u22252\n2 \u2212\u03c4 \u2225x\u22252\n1\n\u0011\n\u2265\n\u0010\n\u03b1 \u2225x\u22252\n2 \u2212\u03c4(1 + 3k0)22d0 \u2225xT0\u22252\n2\n\u0011\n\u2265\n\u0000\u03b1 \u2212\u03c4(1 + 3k0)22d0\n\u0001\n\u2225xT0\u22252\n2 \u2265\u03b1\n2 \u2225xT0\u22252\n2 .\nThus (F.4) holds. Now (F.5) follows from (F.1), which holds by de\ufb01nition of s0 as in\n(2.6), where s0 is tightly bounded in the sense that both (F.1) and (F.2) need to hold.\n\u25a1\nRemark F.2. We note that (F.4) can be understood to be the RE(2d0, 3k0) condition\non b\u0393A. In view of Lemma 15, it is clear that for d0 \u224d\np\nn/ log m, it holds that\n4d0(4 + 3\u03bb)2 = o(s0)\ngiven that \u03c4s0 = O(\u03b1) on event A0; indeed, we have by Lemma 15 the Lower-RE\ncondition holds for b\u0393A := AT A \u2212btr(B)Im, with \u03b1, \u03c4 > 0 such that\ncurvature \u03b1 = 5\n8\u03bbmin(A) and tolerance\n\u03c4 := 3\n8\n\u03bbmin(A)\ns0\n,\nwhere recall s0 \u226532 is as de\ufb01ned in (2.6); moreover, we replaced the parameter\nMA \u224d\u03c1max(s0,A)+\u03c4B\n\u03bbmin(A)\nwith \u03ba(A) in view of (2.5) and (A3).\nAppendix G: Proof of Theorem 16\nDenote by \u03b2 = \u03b2\u2217. Let S := supp \u03b2, d = |S| and\n\u03c5 = b\u03b2 \u2212\u03b2,\nwhere b\u03b2 is as de\ufb01ned in (1.7).\nWe \ufb01rst show Lemma 35, followed by the proof of Theorem 16.\nLemma 35. [4, 30] Suppose that (6.7) holds. Suppose that there exists a parameter \u03c8\nsuch that\n\u221a\nd\u03c4 \u2264\u03c8\nb0\nr\nlog m\nn\nand\n\u03bb \u22654\u03c8\nr\nlog m\nn\n,\nwhere b0, \u03bb are as de\ufb01ned in (1.7). Then\n\u2225\u03c5Sc\u22251 \u22643 \u2225\u03c5S\u22251 .\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n77\nProof. By the optimality of b\u03b2, we have\n\u03bb \u2225\u03b2\u22251 \u2212\u03bb\n\r\r\rb\u03b2\n\r\r\r\n1\n\u2265\n1\n2\nb\u03b2b\u0393b\u03b2 \u22121\n2\u03b2b\u0393\u03b2 \u2212\u27e8b\u03b3, v \u27e9\n=\n1\n2\u03c5b\u0393\u03c5 + \u27e8\u03c5, b\u0393\u03b2 \u27e9\u2212\u27e8\u03c5, b\u03b3 \u27e9\n=\n1\n2\u03c5b\u0393\u03c5 \u2212\u27e8\u03c5, b\u03b3 \u2212b\u0393\u03b2 \u27e9.\nHence, we have for \u03bb \u22654\u03c8\nq\nlog m\nn\n,\n1\n2\u03c5b\u0393\u03c5\n\u2264\n\u27e8\u03c5, b\u03b3 \u2212b\u0393\u03b2 \u27e9+ \u03bb\n\u0010\n\u2225\u03b2\u22251 \u2212\n\r\r\rb\u03b2\n\r\r\r\n1\n\u0011\n(G.1)\n\u2264\n\u03bb\n\u0010\n\u2225\u03b2\u22251 \u2212\n\r\r\rb\u03b2\n\r\r\r\n1\n\u0011\n+\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\n\r\r\r\n\u221e\u2225\u03c5\u22251 .\nHence\n\u03c5b\u0393\u03c5\n\u2264\n\u03bb\n\u0010\n2 \u2225\u03b2\u22251 \u22122\n\r\r\rb\u03b2\n\r\r\r\n1\n\u0011\n+ 2\u03c8\nr\nlog m\nn\n\u2225\u03c5\u22251\n(G.2)\n\u2264\n\u03bb\n\u0012\n2 \u2225\u03b2\u22251 \u22122\n\r\r\rb\u03b2\n\r\r\r\n1 + 1\n2 \u2225\u03c5\u22251\n\u0013\n\u2264\n\u03bb1\n2 (5 \u2225\u03c5S\u22251 \u22123 \u2225\u03c5Sc\u22251) ,\n(G.3)\nwhere by the triangle inequality, and \u03b2Sc = 0, we have\n2 \u2225\u03b2\u22251 \u22122\n\r\r\rb\u03b2\n\r\r\r\n1 + 1\n2 \u2225\u03c5\u22251\n=\n2 \u2225\u03b2S\u22251 \u22122\n\r\r\rb\u03b2S\n\r\r\r\n1 \u22122 \u2225\u03c5Sc\u22251 + 1\n2 \u2225\u03c5S\u22251 + 1\n2 \u2225\u03c5Sc\u22251\n\u2264\n2 \u2225\u03c5S\u22251 \u22122 \u2225\u03c5Sc\u22251 + 1\n2 \u2225\u03c5S\u22251 + 1\n2 \u2225\u03c5Sc\u22251\n\u2264\n1\n2 (5 \u2225\u03c5S\u22251 \u22123 \u2225\u03c5Sc\u22251) .\n(G.4)\nWe now give a lower bound on the LHS of (G.1), applying the lower-RE condition as\nin De\ufb01nition 2.2,\n\u03c5T b\u0393\u03c5\n\u2265\n\u03b1 \u2225\u03c5\u22252\n2 \u2212\u03c4 \u2225\u03c5\u22252\n1 \u2265\u2212\u03c4 \u2225\u03c5\u22252\n1\nand hence \u2212\u03c5T b\u0393\u03c5\n\u2264\n\u2225\u03c5\u22252\n1 \u03c4 \u2264\u2225\u03c5\u22251 2b0\n\u221a\nd\u03c4\n\u2264\n\u2225\u03c5\u22251 2b0\n\u03c8\nb0\nr\nlog m\nn\n= \u2225\u03c5\u22251 2\u03c8\nr\nlog m\nn\n\u2264\n1\n2\u03bb(\u2225\u03c5S\u22251 + \u2225\u03c5Sc\u22251),\n(G.5)\nwhere we use the assumption that\n\u221a\nd\u03c4 \u2264\u03c8\nb0\nr\nlog m\nn\nand \u2225\u03c5\u22251 \u2264\n\r\r\rb\u03b2\n\r\r\r\n1 + \u2225\u03b2\u22251 \u22642b0\n\u221a\nd,\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n78\nwhich holds by the triangle inequality and the fact that both b\u03b2 and \u03b2 have \u21131 norm\nbeing bounded by b0\n\u221a\nd. Hence by (G.3) and (G.5)\n0\n\u2264\n\u2212\u03c5b\u0393\u03c5 + 5\n2\u03bb \u2225\u03c5S\u22251 \u22123\n2\u03bb \u2225\u03c5Sc\u22251\n(G.6)\n\u2264\n1\n2\u03bb \u2225\u03c5S\u22251 + 1\n2\u03bb \u2225\u03c5Sc\u22251 + 5\n2\u03bb \u2225\u03c5S\u22251 \u22123\n2\u03bb \u2225\u03c5Sc\u22251\n\u2264\n3\u03bb \u2225\u03c5S\u22251 \u2212\u03bb \u2225\u03c5Sc\u22251 .\n(G.7)\nThus we have\n\u2225\u03c5Sc\u22251 \u22643 \u2225\u03c5S\u22251 ,\nand the lemma holds.\n\u25a1\nProof of Theorem 16.\nFollowing the conclusion of Lemma 35, we have\n\u2225\u03c5\u22251 \u22644 \u2225\u03c5S\u22251 \u22644\n\u221a\nd \u2225\u03c5\u22252 .\n(G.8)\nMoreover, we have by the lower-RE condition as in De\ufb01nition 2.2\n\u03c5T b\u0393\u03c5\n\u2265\n\u03b1 \u2225\u03c5\u22252\n2 \u2212\u03c4 \u2225\u03c5\u22252\n1 \u2265(\u03b1 \u221216d\u03c4) \u2225\u03c5\u22252\n2 \u22651\n2\u03b1 \u2225\u03c5\u22252\n2 ,\n(G.9)\nwhere the last inequality follows from the assumption that 16d\u03c4 \u2264\u03b1/2.\nCombining the bounds in (G.9), (G.8) and (G.2), we have\n1\n2\u03b1 \u2225\u03c5\u22252\n2\n\u2264\n\u03c5T b\u0393\u03c5 \u2264\u03bb\n\u0010\n2 \u2225\u03b2\u22251 \u22122\n\r\r\rb\u03b2\n\r\r\r\n1\n\u0011\n+ 2\u03c8\nr\nlog m\nn\n\u2225\u03c5\u22251\n\u2264\n5\n2\u03bb \u2225\u03c5S\u22251 \u226410\u03bb\n\u221a\nd \u2225\u03c5\u22252 .\nAnd thus we have \u2225\u03c5\u22252 \u226420\u03bb\n\u221a\nd. The theorem is thus proved.\n\u25a1\nAppendix H: Proofs for the Lasso-type estimator\nLet\nM+\n=\n32C\u03d6(s0 + 1)\n\u03bbmin(A)\nand\n\u03d6(s0 + 1) = \u03c1max(s0 + 1, A) + \u03c4B =: D.\nBy de\ufb01nition of s0, we have s0M 2\nA \u2264\n4n\nlog m and\n(s0 + 1) \u2265\nn\nM 2\n+ log m\ngiven that\n\u221a\ns0 + 1\u03d6(s0 + 1)\n\u2265\n\u03bbmin(A)\n32C\nr\nn\nlog m.\n(H.1)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n79\nTo prove the \ufb01rst inequality in (6.6) and (6.10), we need to show that\nd \u2264\n\u03b1\n32\u03c4 = \u03b1\n32\ns0\n\u03bbmin(A) \u2212\u03b1 = 5s0\n96 .\nThe \ufb01rst inequality in (6.6) holds so long as\nd\n\u2264\n1\n20\n1\nM 2\n+\nn\nlog m \u2264s0 + 1\n20\n\u22645(s0 + 1)\n100\n\u22645s0\n96 ,\n(H.2)\nwhere the last inequality holds so long as s0 \u226524. To prove the second inequality\nin (6.10), we need to show that\nd \u22641\n\u03c4 2\nlog m\nn\n\u0012 \u03c8\nb0\n\u00132\n,\nwhere\n\u03c4 = 3\n5\n\u03b1\ns0\nfor\n\u03b1 = 5\n8\u03bbmin(A),\nwhich in turn ensures that the second inequality in (6.6) holds for \u03bb \u22654\u03c8, for \u03c8\nappropriately chosen. We use the following inequality in the proof of Lemma 17 and\nLemma 18:\ns0 + 1\n\u03b12\n\u2265\n64\n25\u03bbmin(A)2\n1\nM 2\n+\nn\nlog m \u2265\n\u00128\n5\n1\n32C\u03d6(s0 + 1)\n\u00132\nn\nlog m\n=\n\u0012\n1\n20CD\n\u00132\nn\nlog m \u2265\n\u0012\n1\n10CD2\n\u00132\nn\nlog m,\n(H.3)\nwhere we use the fact that D = \u03d6(s0+1) = \u03c1max(s0+1, A)+\u03c4B \u2264\u2225A\u22252+\u2225B\u22252 :=\nD2/2.\nH.1. Proof of Lemma 17\nLet CA =\n1\n40M 2\n+ . The \ufb01rst inequality in (6.10) holds in view of (H.2). Recall that\nb2\n0 \u2265\u2225\u03b2\u2217\u22252\n2 \u2265\u03c6b2\n0 by de\ufb01nition of 0 < \u03c6 \u22641. Let C = C0/\n\u221a\nc\u2032. By (6.9) and (H.3),\nd\n\u2264\nCAc\u2032D\u03c6\nn\nlog m \u2264\n1\n40M 2\n+\n\u0012C0D2\nCD2\n\u00132\nD\u03c6\nn\nlog m\n\u2264\n25\n9\n32\n33\n32\n33\nn\nM 2\n+ log m\n\u0012\n1\n10CD2\n\u00132\nC2\n0D2\n2D\u03c6\n\u2264\n25\n9\n32\n33\n32(s0 + 1)\n33\n(s0 + 1)\n\u03b12\nlog m\nn\n\u0012 \u03c8\nb0\n\u00132\n\u2264\n25\n9\n(s0)2\n\u03b12\nlog m\nn\n\u0012 \u03c8\nb0\n\u00132\n,\nwhere\nC2\n0D2\n2D\u03c6\n=\nC2\n0D2\n2\n\u0012K2M 2\n\u03f5\nb2\n0\n+ K4\u03c6\n\u0013\n\u2264\nC2\n0D2\n2\nK2\nb2\n0\n(M\u03f5 + K \u2225\u03b2\u2217\u22252)2 =\n\u0012 \u03c8\nb0\n\u00132\n,\n(H.4)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n80\nfor \u03c8 = C0D2K(K \u2225\u03b2\u2217\u22252+M\u03f5) as de\ufb01ned in (3.6). We have shown that (6.10) indeed\nholds, and the lemma is thus proved.\n\u25a1\nH.2. Proof of Lemma 18\nLet CA =\n1\n160M 2\n+ . The proof for d \u2264\n\u03b1\n32\u03c4 = 5s0\n96 follows from (H.2). In order to\nshow the second inequality, we follow the same line of arguments except that we need\nto replace one inequality (H.4) with (H.5). By de\ufb01nition of D\u2032\n0, we have \u2225B\u22252+amax \u2264\n(D\u2032\n0)2 \u22642(\u2225B\u22252 + amax). Let D = \u03d6(s0 + 1).\nBy (6.11), (H.1) and (H.3), we have for c\u2032\u2032 \u2264\n\u0010\nD\u2032\n0\nD\n\u00112\n,\nd\n\u2264\nCAc\u2032c\u2032\u2032D\u03c6\nn\nlog m \u2264\n1\n160M 2\n+\nn\nlog m\n\u0012C0D\u2032\n0\nCD\n\u00132\nD\u03c6\n\u2264\n25\n9\n322\n332\n\u0012\n1\n20CD\n\u00132 \u0000C2\n0(D\u2032\n0)2D\u03c6\n\u0001\nn\nM 2\n+ log m\n\u2264\n25\n9\n322\n332\n(s0 + 1)2\n\u03b12\nlog m\nn\n\u0012 \u03c8\nb0\n\u00132\n\u226425\n9\n(s0)2\n\u03b12\nlog m\nn\n\u0012 \u03c8\nb0\n\u00132\n,\nwhere assuming that s0 \u226532, we have the following inequality by de\ufb01nition of s0 and\n\u03b1 = 5\n8\u03bbmin(A),\ns0 + 1\n\u03b12\nlog m\nn\n\u2265\n\u00128\n5\n1\n32C\u03d6(s0 + 1)\n\u00132\n\u2265\n\u0012\n1\n20CD\n\u00132\n.\nWe now replace (H.4) with\nC2\n0(D\u2032\n0)2D\u03c6\n=\nC2\n0(D\u2032\n0)2 K4\nb2\n0\n\u0012M 2\n\u03f5\nK2 + \u03c4 +\nB \u03c6b2\n0\n\u0013\n\u2264\nC2\n0(D\u2032\n0)2 K2\nb2\n0\n\u0010\nM\u03f5 + \u03c4 +/2\nB\nK \u2225\u03b2\u2217\u22252\n\u00112\n\u2264\n\u0012 \u03c8\nb0\n\u00132\n,\n(H.5)\nwhere\nD\u03c6\n:=\nK2M 2\n\u03f5\nb2\n0\n+ \u03c4 +\nB K4\u03c6 \u2264K4\nb2\n0\n\u0012M 2\n\u03f5\nK2 + \u03c4 +\nB \u2225\u03b2\u2217\u22252\n2\n\u0013\nand \u03c8 = C0D\u2032\n0K\n\u0010\nK\u03c4 +/2\nB\n\u2225\u03b2\u2217\u22252 + M\u03f5K\n\u0011\nis now as de\ufb01ned in (4.2). The lemma is\nthus proved.\n\u25a1\nRemark H.1. Throughout this paper, we assume that C0 is a large enough constant\nsuch that for c as de\ufb01ned in Theorem 31,\nc min{C2\n0, C0} \u22654.\n(H.6)\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n81\nBy de\ufb01nition of s0, we have for \u03d62(s0) \u22651,\ns0\u03d62(s0)\n\u2264\nc\u2032\u03bb2\nmin(A)\n1024C2\n0\nn\nlog m,\nand hence\ns0\n\u2264\nc\u2032\u03bb2\nmin(A)\n1024C2\n0\nn\nlog m \u2264\u03bb2\nmin(A)\n1024C2\n0\nn\nlog m =: \u02c7s0.\nRemark H.2. The proof shows that one can take C = C0/\n\u221a\nc\u2032, and take\nV = 3eM 3\nA/2 = 3e643C3\u03d63(s0)\n2\u03bb3\nmin(A)\n\u22643e643C3\n0\u03d63(\u02c7s0)\n2(c\u2032)3/2\u03bb3\nmin(A).\nHence a suf\ufb01cient condition on r(B) is:\nr(B) \u226516c\u2032K4\nn\nlog m\n\u0012\n3 log 64C0\u03d6(\u02c7s0)\n\u221a\nc\u2032\u03bbmin(A)\n+ log 3em log m\n2n\n\u0013\n.\n(H.7)\nAppendix I: Proofs for the Conic Programming estimator\nWe next provide proof for Lemmas 19 to 21 in this section.\nI.1. Proof of Lemma 19\nSuppose event B0 holds. Then by the proof of Corollary 13,\n\r\r 1\nnXT (y \u2212X\u03b2\u2217) + 1\nn btr(B)\u03b2\u2217\r\r\n\u221e\n=\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\n2C0D2K2 \u2225\u03b2\u2217\u22252\nr\nlog m\nn\n+ C0D0KM\u03f5\nr\nlog m\nn\n=:\n\u00b5 \u2225\u03b2\u2217\u22252 + \u03c9.\nThe lemma follows immediately for the chosen \u00b5, \u03c9 as in (6.12) given that (\u03b2\u2217, \u2225\u03b2\u2217\u22252) \u2208\n\u03a5.\n\u25a1\nI.2. Proof of Lemma 20\nBy optimality of (b\u03b2, bt), we have\n\r\r\rb\u03b2\n\r\r\r\n1 + \u03bb\n\r\r\rb\u03b2\n\r\r\r\n2 \u2264\n\r\r\rb\u03b2\n\r\r\r\n1 + \u03bbbt \u2264\u2225\u03b2\u2217\u22251 + \u03bb \u2225\u03b2\u2217\u22252 .\nThus we have for S := supp(\u03b2\u2217),\n\r\r\rb\u03b2\n\r\r\r\n1 =\n\r\r\rb\u03b2Sc\n\r\r\r\n1 +\n\r\r\rb\u03b2S\n\r\r\r\n1\n\u2264\n\u2225\u03b2\u2217\u22251 + \u03bb(\u2225\u03b2\u2217\u22252 \u2212\n\r\r\rb\u03b2\n\r\r\r\n2).\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n82\nNow by the triangle inequality,\n\r\r\rb\u03b2Sc\n\r\r\r\n1 = \u2225vSc\u22251\n\u2264\n\u2225\u03b2\u2217\nS\u22251 \u2212\n\r\r\rb\u03b2S\n\r\r\r\n1 + \u03bb(\u2225\u03b2\u2217\u22252 \u2212\n\r\r\rb\u03b2\n\r\r\r\n2)\n\u2264\n\u2225vS\u22251 + \u03bb(\u2225\u03b2\u2217\u22252 \u2212\n\r\r\rb\u03b2\n\r\r\r\n2)\n\u2264\n\u2225vS\u22251 + \u03bb(\u2225\u03b2\u2217\u22252 \u2212\n\r\r\rb\u03b2S\n\r\r\r\n2)\n=\n\u2225vS\u22251 + \u03bb \u2225vS\u22252 \u2264(1 + \u03bb) \u2225vS\u22251 .\nThe lemma thus holds given\nbt\n\u2264\n1\n\u03bb(\u2225\u03b2\u2217\u22251 \u2212\n\r\r\rb\u03b2\n\r\r\r\n1) + \u2225\u03b2\u2217\u22252 \u22641\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252 .\n\u25a1\nI.3. Proof of Lemma 21\nRecall the following shorthand notation:\nD0\n=\n(\u221a\u03c4B + \u221aamax)\nand D2 = 2(\u2225A\u22252 + \u2225B\u22252).\nFirst we rewrite an upper bound for v = b\u03b2 \u2212\u03b2\u2217, D = tr(B) and bD = btr(B),\n\r\rXT\n0 X0v\n\r\r\n\u221e\n=\n\r\r\r(X \u2212W)T X0(b\u03b2 \u2212\u03b2\u2217)\n\r\r\r\n\u221e\u2264\n\r\r\rXT X0(b\u03b2 \u2212\u03b2\u2217)\n\r\r\r\n\u221e+\n\r\rW T X0v\n\r\r\n\u221e\n\u2264\n\r\r\rXT (X b\u03b2 \u2212y) \u2212bDb\u03b2\n\r\r\r\n\u221e+\n\r\rXT \u03f5\n\r\r\n\u221e+\n\r\r\r(XT W \u2212D)b\u03b2\n\r\r\r\n\u221e\n+\n\r\r\r( bD \u2212D)b\u03b2\n\r\r\r\n\u221e+\n\r\rW T X0v\n\r\r\n\u221e,\nwhere\n\r\r\rXT X0(b\u03b2 \u2212\u03b2\u2217)\n\r\r\r\n\u221e\n\u2264\n\r\r\rXT (X0 b\u03b2 \u2212y + \u03f5)\n\r\r\r\n\u221e\n=\n\r\r\rXT ((X \u2212W)b\u03b2 \u2212y)\n\r\r\r\n\u221e+\n\r\rXT \u03f5\n\r\r\n\u221e\n\u2264\n\r\r\rXT (X b\u03b2 \u2212y) \u2212bDb\u03b2\n\r\r\r\n\u221e+\n\r\rXT \u03f5\n\r\r\n\u221e\n+\n\r\r\r(XT W \u2212D)b\u03b2\n\r\r\r\n\u221e+\n\r\r\r( bD \u2212D)b\u03b2\n\r\r\r\n\u221e.\nOn event B0, we have by Lemma 20 and the fact that b\u03b2 \u2208\u03a5,\nI :=\n\r\r\rb\u03b3 \u2212b\u0393b\u03b2\n\r\r\r\n\u221e\n=\n\r\r\r 1\nnXT (y \u2212X b\u03b2) + 1\nn bDb\u03b2\n\r\r\r\n\u221e\u2264\u00b5bt + \u03c9\n\u2264\n\u00b5( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + \u03c9\n=\n2D2K\u03c1n( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + D0\u03c1nM\u03f5;\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n83\nand on event B4,\nII\n:=\n1\nn\n\r\rXT \u03f5\n\r\r\n\u221e\u22641\nn(\n\r\rXT\n0 \u03f5\n\r\r\n\u221e+\n\r\rW T \u03f5\n\r\r\n\u221e)\n\u2264\n\u03c1nM\u03f5(a1/2\nmax + \u221a\u03c4B) = D0\u03c1nM\u03f5.\nThus on event B0, we have\nI + II \u22642D2K\u03c1n( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + 2D0\u03c1nM\u03f5 = \u00b5( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + 2\u03c9.\nNow on event B6, we have for 2D1 \u2264D2\nIV :=\n\r\r\r( bD \u2212D)b\u03b2\n\r\r\r\n\u221e\n\u2264\n\f\f\f bD \u2212D\n\f\f\f\n\r\r\rb\u03b2\n\r\r\r\n\u221e\u22642D1K 1\n\u221am\u03c1n(\u2225\u03b2\u2217\u2225\u221e+ \u2225v\u2225\u221e)\n\u2264\nD2K 1\n\u221am\u03c1n(\u2225\u03b2\u2217\u22252 + \u2225v\u22251).\nOn event B5 \u2229B10, we have\nIII := 1\nn\n\r\r\r(XT W \u2212D)b\u03b2\n\r\r\r\n\u221e\n\u2264\n1\nn\n\r\r(XT W \u2212D)\u03b2\u2217\r\r\n\u221e+ 1\nn\n\r\r(XT W \u2212D)v\n\r\r\n\u221e\n\u2264\n1\nn\n\r\rXT\n0 W\u03b2\u2217\r\r\n\u221e+ 1\nn\n\r\r(W T W \u2212D)\u03b2\u2217\r\r\n\u221e\n+\n1\nn\n\u0000\r\r(ZT BZ \u2212tr(B)Im)\n\r\r\nmax +\n\r\rXT\n0 W\n\r\r\nmax\n\u0001\n\u2225v\u22251\n\u2264\n\u03c1nK\n\u0012\u2225B\u2225F\n\u221an\n+ \u221a\u03c4Ba1/2\nmax\n\u0013\n(\u2225v\u22251 + \u2225\u03b2\u2217\u22252),\nand V = 1\nn\n\r\rW T X0v\n\r\r\n\u221e\n\u2264\n1\nn\n\r\rW T X0\n\r\r\nmax \u2225v\u22251 \u2264\u03c1nK\u221a\u03c4Ba1/2\nmax \u2225v\u22251 .\nThus we have on B0 \u2229B10,\nIII + IV + V\n\u2264\n\u03c1nK\n\u0012\n\u2225B\u22252 + \u03c4B + amax +\n2\n\u221am(\u2225A\u22252 + \u2225B\u22252)\n\u0013\n(\u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n\u2264\n\u03c1nK (4 \u2225B\u22252 + 3 \u2225A\u22252) (\u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n\u2264\n2D2K\u03c1n(\u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n\u2264\n\u00b5(\u2225v\u22251 + \u2225\u03b2\u2217\u22252),\nwhere D0 \u2264D2 and \u03c4A = 1, and\n\r\r 1\nnXT\n0 X0v\n\r\r\n\u221e\n\u2264\nI + II + III + IV + V\n\u2264\n\u00b5( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + 2D0M\u03f5\u03c1n + \u00b5(\u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n\u2264\n2\u00b5 \u2225\u03b2\u2217\u22252 + \u00b5( 1\n\u03bb + 1) \u2225v\u22251 + 2\u03c9.\nThe lemma thus holds.\n\u25a1\nAppendix J: Proof for Theorem 7\nWe prove Lemmas 22 to 24 in this section.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n84\nJ.1. Proof of Lemma 22\nSuppose event B0 holds. Then by the proof of Corollary 14, we have for D\u2032\n0 =\n\u2225B\u22251/2\n2\n+ a1/2\nmax,\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\nD\u2032\n0\u03c4 +/2\nB\nK\u03c1n \u2225\u03b2\u2217\u22252 + D0M\u03f5\u03c1n,\nwhere \u03c4 +/2\nB\n= \u221a\u03c4B + Doracle\n\u221am\nand Doracle = 2(\u2225B\u22251/2\n2\n+\u2225A\u22251/2\n2\n). The lemma follows\nimmediately for \u00b5, \u03c9 as chosen in (6.16).\n\u25a1\nJ.2. Proof of Lemma 23\nSuppose event B6 holds. We \ufb01rst show (6.17) and (6.18). Recall rm,m := 2C0K2\nq\nlog m\nmn \u2265\n2C0K2 log1/2 m\nm\n. By Lemma 5, we have on event B6,\n|b\u03c4B \u2212\u03c4B|\n\u2264\nD1rm,m.\nMoreover, we have under (A1),\n1 = \u03c4A \u2264D1 := \u2225A\u2225F\nm1/2 + \u2225B\u2225F\nn1/2 \u2264\u2225A\u22252 + \u2225B\u22252 \u2264(Doracle\n2\n)2,\nin view of (C.2). Hence\np\nD1 \u2264Doracle\n2\n= \u2225B\u22251/2\n2\n+ \u2225A\u22251/2\n2\n.\nBy de\ufb01nition and construction, we have \u03c4B, b\u03c4B \u22650,\n\f\f\fb\u03c4 1/2\nB\n\u2212\u03c4 1/2\nB\n\f\f\f\n\u2264\nb\u03c4 1/2\nB\n+ \u03c4 1/2\nB ,\nand\n\f\f\fb\u03c4 1/2\nB\n\u2212\u03c4 1/2\nB\n\f\f\f\n2\n\u2264\n\f\f\f(b\u03c4 1/2\nB\n+ \u03c4 1/2\nB )(b\u03c4 1/2\nB\n\u2212\u03c4 1/2\nB )\n\f\f\f = |b\u03c4B \u2212\u03c4B| .\nThus,\n\f\f\fb\u03c4 1/2\nB\n\u2212\u03c4 1/2\nB\n\f\f\f\n\u2264\np\n|b\u03c4B \u2212\u03c4B| \u2264\np\nD1r1/2\nm,m \u2264Doracle\n2\nr1/2\nm,m\nand for C6 \u2265Doracle \u22652\u221aD1 and Doracle = 2(\u2225A\u22251/2\n2\n+ \u2225B\u22251/2\n2\n),\nb\u03c4 1/2\nB\n\u2212Doracle\n2\nr1/2\nm,m \u2264\u03c4 1/2\nB\n\u2264b\u03c4 1/2\nB\n+ Doracle\n2\nr1/2\nm,m.\n(J.1)\nThus we have for \u03c4 +/2\nB\nas de\ufb01ned in (4.1), (J.1) and the fact that\nr1/2\nm,m \u2265\np\n2C0K (log m)1/4\n\u221am\n\u22652/\u221am for m \u226516 and C0 \u22651,\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n85\nthe following inequalities hold: for K \u22651,\n\u03c4 +/2\nB\n:=\n\u03c4 1/2\nB\n+ Doraclem\u22121/2\n(J.2)\n\u2264\nb\u03c4 1/2\nB\n+ Doracle\n2\nr1/2\nm,m + Doracle\n2\nr1/2\nm,m\n\u2264\nb\u03c4 1/2\nB\n+ Doracler1/2\nm,m \u2264e\u03c4 1/2\nB ,\nwhere the last inequality holds by the choice of e\u03c4 1/2\nB\n\u2265b\u03c4 1/2\nB\n+Doracler1/2\nm,m as in (4.11).\nMoreover, by (J.1),\ne\u03c4 1/2\nB\n:=\nb\u03c4 1/2\nB\n+ C6r1/2\nm,m \u2264\u03c4 1/2\nB\n+ Doracle\n2\nr1/2\nm,m + C6r1/2\nm,m\n\u2264\n\u03c4 1/2\nB\n+ 3\n2C6r1/2\nm,m,\nand\ne\u03c4B\n:=\n(b\u03c4 1/2\nB\n+ C6r1/2\nm,m)2 \u22642b\u03c4B + 2C2\n6rm,m\n\u2264\n2\u03c4B + 2D1rm,m + 2C2\n6rm,m\n\u2264\n2\u03c4B + D2\noracle\n2\nrm,m + 2C2\n6rm,m \u22642\u03c4B + 3C2\n6rm,m.\nThus (6.17) and (6.18) hold given that 2D1 \u2264D2\noracle/2 \u2264C2\n6/2.\nFinally, we have for \u03c4 \u2212\nB as de\ufb01ned in (4.8),\ne\u03c4 1/2\nB \u03c4 \u2212\nB \u2264(\u03c4 1/2\nB\n+ 3\n2C6r1/2\nm,m)\u03c4 \u2212\nB \u2264\u03c4 1/2\nB\n+ 3\n2C6r1/2\nm,m\n\u03c4 1/2\nB\n+ 2C6r1/2\nm,m\n\u22641.\n\u25a1\nRemark J.1. The set \u03a5 in our setting is equivalent to the following: for \u00b5, \u03c9 as de\ufb01ned\nin (4.11) and \u03b2 \u2208Rm,\n\u03a5 =\n\b\n(\u03b2, t) :\n\r\r 1\nnXT (y \u2212X\u03b2) + 1\nn btr(B)\u03b2\n\r\r\n\u221e\u2264\u00b5t + \u03c9, \u2225\u03b2\u22252 \u2264t\n\t\n.\n(J.3)\nJ.3. Proof of Lemma 24\nFor the rest of the proof, we will follow the notation in the proof for Lemma 21.\nNotice that the bounds as stated in Lemma 20 remain true with \u03c9, \u00b5 chosen as in (6.16),\nso long as (\u03b2\u2217, \u2225\u03b2\u2217\u22252) \u2208\u03a5. This indeed holds by Lemma 22: for \u03c9 and \u00b5 (4.11) as\nchosen in Theorem 7, we have by (J.2),\n\u00b5 \u224dD\u2032\n0e\u03c4 1/2\nB K\u03c1n \u2265D\u2032\n0K\u03c1n\u03c4 +/2\nB\n,\nwhere\n\u03c4 +/2\nB\n= (\u221a\u03c4B + Doracle\n\u221am ),\nwhich ensures that (\u03b2\u2217, \u2225\u03b2\u2217\u22252) \u2208\u03a5 by Lemma 22.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n86\nOn event B0, we have by Lemma 20 and the fact that b\u03b2 \u2208\u03a5 as in (J.3)\nI + II\n:=\n\r\r\rb\u03b3 \u2212b\u0393b\u03b2\n\r\r\r\n\u221e+ 1\nn\n\r\rXT \u03f5\n\r\r\n\u221e\n\u2264\n\r\r\r 1\nnXT (y \u2212X b\u03b2) + 1\nn bDb\u03b2\n\r\r\r\n\u221e+ \u03c9 \u2264\u00b5bt + 2\u03c9\n\u2264\n\u00b5( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + 2\u03c9,\nfor \u03c9, \u00b5 as chosen in (4.11). Now on event B6, we have under (A1),\nIV :=\n\r\r\r( bD \u2212D)b\u03b2\n\r\r\r\n\u221e\n\u2264\n\f\f\f bD \u2212D\n\f\f\f\n\r\r\rb\u03b2\n\r\r\r\n\u221e\u22642D1K 1\n\u221am\u03c1n(\u2225\u03b2\u2217\u2225\u221e+ \u2225v\u2225\u221e)\n\u2264\nD\u2032\n0\nDoracle\n\u221am K\u03c1n(\u2225\u03b2\u2217\u22252 + \u2225v\u22251),\nwhere 2D1 \u2264DoracleD\u2032\n0 for 1 \u2264D\u2032\n0 := \u2225B\u22251/2\n2\n+ a1/2\nmax, for amax \u2265\u03c4A = 1 and\nDoracle = 2\n\u0010\n\u2225B\u22251/2\n2\n+ \u2225A\u22251/2\n2\n\u0011\n. Hence\nIII + IV + V \u2264\u03c1nK\u221a\u03c4B\n\u0010\n\u2225B\u22251/2\n2\n+ a1/2\nmax\n\u0011\n(\u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n+2D1K 1\n\u221am\u03c1n(\u2225\u03b2\u2217\u22252 + \u2225v\u22251) + \u03c1nK\u221a\u03c4Ba1/2\nmax \u2225v\u22251\n\u2264\nD\u2032\n0K\u03c1n(\u2225v\u22251 + \u2225\u03b2\u2217\u22252)(\u221a\u03c4B + Doracle\n\u221am ) + \u03c1nK\u221a\u03c4Ba1/2\nmax \u2225v\u22251\n\u2264\nD\u2032\n0K\u03c1n\u03c4 +/2\nB\n(\u2225v\u22251 + \u2225\u03b2\u2217\u22252) + D\u2032\n0K\u03c1n\n\u221a\u03c4B \u2225v\u22251\n\u2264\nC0D\u2032\n0K2\nr\nlog m\nn\n(\u03c4 1/2\nB\n+ Doracle\n\u221am )(2 \u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n\u2264\n\u00b5(2 \u2225v\u22251 + \u2225\u03b2\u2217\u22252),\nfor \u00b5 as de\ufb01ned in (4.11) in view of (J.2).\nThus we have\nI + II + III + IV + V\n\u2264\n\u00b5( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + 2\u03c9 + \u00b5(2 \u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n=\n2\u00b5((1 + 1\n2\u03bb) \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + 2\u03c9,\nand the improved bound as stated in the Lemma thus holds.\n\u25a1\nAppendix K: Some geometric analysis results\nLet us de\ufb01ne the following set of vectors in Rm:\nCone(s) := {\u03c5 : \u2225\u03c5\u22251 \u2264\u221as \u2225\u03c5\u22252}\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n87\nFor each vector x \u2208Rm, let T0 denote the locations of the s largest coef\ufb01cients of x in\nabsolute values. Any vector x \u2208Sm\u22121 satis\ufb01es:\n\r\rxT c\n0\n\r\r\n\u221e\u2264\u2225xT0\u22251 /s\n\u2264\n\u2225xT0\u22252\n\u221as\n.\n(K.1)\nWe need to state the following result from [33]. Let Sm\u22121 be the unit sphere in Rm,\nfor 1 \u2264s \u2264m,\nUs := {x \u2208Rm : | supp(x)| \u2264s}.\n(K.2)\nThe sets Us is an union of the s-sparse vectors. The following three lemmas are well-\nknown and mostly standard; See [33] and [30].\nLemma 36. For every 1 \u2264s \u2264m and every I \u2282{1, . . . , m} with |I| \u2264s,\np\n|I|Bm\n1 \u2229Sm\u22121 \u22822 conv(Us \u2229Sm\u22121) =: 2 conv\n\uf8eb\n\uf8ed[\n|J|\u2264s\nEJ \u2229Sm\u22121\n\uf8f6\n\uf8f8\nand moreover, for \u03c1 \u2208(0, 1],\np\n|I|Bm\n1 \u2229\u03c1Bm\n2 \u2282(1 + \u03c1) conv(Us \u2229Bm\n2 ) =: (1 + \u03c1) conv\n\uf8eb\n\uf8ed[\n|J|\u2264s\nEJ \u2229Sm\u22121\n\uf8f6\n\uf8f8.\nProof. Fix x \u2208Rm. Let xT0 denote the subvector of x con\ufb01ned to the locations of its\ns largest coef\ufb01cients in absolute values; moreover, we use it to represent its 0-extended\nversion x\u2032 \u2208Rm such that x\u2032\nT c = 0 and x\u2032\nT0 = xT0. Throughout this proof, T0 is\nunderstood to be the locations of the s largest coef\ufb01cients in absolute values in x.\nMoreover, let (x\u2217\ni )m\ni=1 be non-increasing rearrangement of (|xi|)m\ni=1. Denote by\nL\n=\n\u221asBm\n1 \u2229\u03c1Bm\n2\nand\nR\n=\n2 conv\n\uf8eb\n\uf8ed[\n|J|\u2264s\nEJ \u2229Bm\n2\n\uf8f6\n\uf8f8= 2 conv\n\u0000E \u2229Bm\n2\n\u0001\n.\nAny vector x \u2208Rm satis\ufb01es:\n\r\rxT c\n0\n\r\r\n\u221e\u2264\u2225xT0\u22251 /s\n\u2264\n\u2225xT0\u22252\n\u221as\n.\n(K.3)\nIt follows that for any \u03c1 > 0, s \u22651 and for all z \u2208L, we have the ith largest coordinate\nin absolute value in z is at most \u221as/i, and\nsup\nz\u2208L\n\u27e8x, z \u27e9\n\u2264\nmax\n\u2225z\u22252\u2264\u03c1 \u27e8xT0, z \u27e9+\nmax\n\u2225z\u22251\u2264\u221as \u27e8xT c\n0 , z \u27e9\n\u2264\n\u03c1 \u2225xT0\u22252 +\n\r\rxT c\n0\n\r\r\n\u221e\n\u221as\n\u2264\n\u2225xT0\u22252 (\u03c1 + 1) ,\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n88\nwhere clearly max\u2225z\u22252\u2264\u03c1 \u27e8xT0, z \u27e9= \u03c1 Ps\ni=1(x\u22172\ni )1/2. And denote by SJ := Sm\u22121\u2229\nEJ,\nsup\nz\u2208R\n\u27e8x, z \u27e9\n=\n(1 + \u03c1) max\nJ:|J|\u2264s max\nz\u2208SJ \u27e8x, z \u27e9\n=\n(1 + \u03c1) \u2225xT0\u22252 ,\ngiven that for a convex function \u27e8x, z \u27e9, the maximum happens at an extreme point;\nand in this case, it happens for z such that z is supported on T0, such that zT0 =\nxT0\n\u2225xT0\u22252\nand zT c\n0 = 0.\n\u25a1\nLemma 37. Let 1/5 > \u03b4 > 0. Let E = \u222a|J|\u2264sEJ for 0 < s < m/2 and k0 > 0. Let\n\u2206be a m \u00d7 m matrix such that\n\f\fuT \u2206v\n\f\f \u2264\u03b4,\n\u2200u, v \u2208E \u2229Sm\u22121\n(K.4)\nThen for all v \u2208\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001\n,\n\f\f\u03c5T \u2206\u03c5\n\f\f\n\u2264\n4\u03b4.\n(K.5)\nProof. First notice that\nmax\n\u03c5\u2208\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001\n\f\f\u03c5T \u2206\u03c5\n\f\f\n\u2264\nmax\nw,u\u2208\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001\n\f\fwT \u2206u\n\f\f .\n(K.6)\nNow that we have decoupled u and w on the RHS of (K.6), we \ufb01rst \ufb01x u.\nThen for any \ufb01xed u \u2208Sm\u22121 and matrix \u2206\u2208Rm\u00d7m, f(w) =\n\f\fwT \u2206u\n\f\f is a convex\nfunction of w, and hence for w \u2208\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001\n\u22822 conv\n\u0010S\n|J|\u2264s EJ \u2229Sm\u22121\u0011\n,\nmax\nw\u2208\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001\n\f\fwT \u2206u\n\f\f\n\u2264\n2\nmax\nw\u2208conv(E\u2229Sm\u22121)\n\f\fwT \u2206u\n\f\f\n=\n2\nmax\nw\u2208E\u2229Sm\u22121\n\f\fwT \u2206u\n\f\f ,\nwhere the maximum occurs at an extreme point of the set conv(E \u2229Sm\u22121) because of\nthe convexity of the function f(w).\nClearly the RHS of (K.6) is bounded by\nmax\nu,w\u2208\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001\n\f\fwT \u2206u\n\f\f\n=\nmax\nu\u2208\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001\nmax\nw\u2208\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001\n\f\fwT \u2206u\n\f\f\n\u2264\n2\nmax\nu\u2208\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001\nmax\nw\u2208\n\u0000E\u2229Sm\u22121\u0001\n\f\fwT \u2206u\n\f\f\n=\n2\nmax\nu\u2208\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001 g(u),\nwhere the function g of u \u2208\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001\nis de\ufb01ned as\ng(u) =\nmax\nw\u2208\n\u0000E\u2229Sm\u22121\u0001\n\f\fwT \u2206u\n\f\f ;\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n89\ng(u) is convex since it is the maximum of a function fw(u) :=\n\f\fwT \u2206u\n\f\f which is\nconvex in u for each w \u2208(E \u2229Sm\u22121).\nThus we have for u \u2208(\u221asBm\n1 \u2229Bm\n2 ) \u22822 conv\n\u0010S\n|J|\u2264s EJ \u2229Sm\u22121\u0011\n=: 2 conv\n\u0000E \u2229Sm\u22121\u0001\n,\nmax\nu\u2208\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001 g(u)\n\u2264\n2\nmax\nu\u2208conv(E\u2229Sm\u22121) g(u)\n=\n2\nmax\nu\u2208E\u2229Sm\u22121 g(u)\n(K.7)\n=\n2\nmax\nu\u2208E\u2229Sm\u22121\nmax\nw\u2208E\u2229Sm\u22121\n\f\fwT \u2206u\n\f\f \u22644\u03b4,\n(K.8)\nwhere (K.7) holds given that the maximum occurs at an extreme point of the set\nconv(E \u2229Bm\n2 ), because of the convexity of the function g(u).\n\u25a1\nCorollary 38. Suppose all conditions in Lemma 37 hold. Then \u2200\u03c5 \u2208Cone(s),\n\f\f\u03c5T \u2206\u03c5\n\f\f\n\u2264\n4\u03b4 \u2225\u03c5\u22252\n2 .\n(K.9)\nProof. It is suf\ufb01cient to show that \u2200\u03c5 \u2208Cone(s) \u2229Sm\u22121,\n\f\f\u03c5T \u2206\u03c5\n\f\f\n\u2264\n4\u03b4.\nDenote by Cone := Cone(s). Clearly this set of vectors satisfy:\nCone \u2229Sm\u22121 \u2282\n\u0000\u221asBm\n1 \u2229Bm\n2\n\u0001\n.\nThus (K.9) follows from (K.5).\n\u25a1\nRemark K.1. Suppose we relax the de\ufb01nition of Cone(s) to be:\nCone(s) := {\u03c5 : \u2225\u03c5\u22251 \u22642\u221as \u2225\u03c5\u22252}.\nClearly, Cone(s, 1) \u2282Cone(s). given that \u2200u \u2208Cone(s, 1), we have\n\u2225u\u22251 \u22642 \u2225uT0\u22251 \u22642\u221as \u2225uT0\u22252 \u22642\u221as \u2225u\u22252 .\nLemma 39. Suppose all conditions in Lemma 37 hold. Then for all \u03c5 \u2208Rm,\n\f\f\u03c5T \u2206\u03c5\n\f\f \u22644\u03b4(\u2225\u03c5\u22252\n2 + 1\ns \u2225\u03c5\u22252\n1).\n(K.10)\nProof. The lemma follows given that \u2200\u03c5 \u2208Rm, one of the following must hold:\nif \u03c5 \u2208Cone(s)\n\f\f\u03c5T \u2206\u03c5\n\f\f\n\u2264\n4\u03b4 \u2225\u03c5\u22252\n2 ;\n(K.11)\notherwise\n\f\f\u03c5T \u2206\u03c5\n\f\f\n\u2264\n4\u03b4\ns \u2225\u03c5\u22252\n1 ,\n(K.12)\nleading to the same conclusion in (K.10).\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n90\nWe have shown (K.11) in Lemma 37. Let Cone(s)c be the complement set of Cone(s)\nin Rm. That is, we focus now on the set of vectors such that\nCone(s)c := {\u03c5 : \u2225\u03c5\u22251 \u2265\u221as \u2225\u03c5\u22252}\nand show that for u = \u221as\nv\n\u2225v\u22251 ,\n\f\fvT \u2206v\n\f\f\n\u2225v\u22252\n1\n:=\n1\ns\n\f\fuT \u2206u\n\f\f \u22641\ns\u03b4.\nNow, the last inequality holds by Lemma 37 given that\nu \u2208(\u221asBm\n1 \u2229Bm\n2 ) \u22822 conv\n\uf8eb\n\uf8ed[\n|J|\u2264s\nEJ \u2229Bm\n2\n\uf8f6\n\uf8f8\nand thus\n\f\fvT \u2206v\n\f\f\n\u2225v\u22252\n1\n\u2264\n1\ns\nsup\nu\u2208\u221asBm\n1 \u2229Bm\n2\n\f\fuT \u2206u\n\f\f \u22641\ns4\u03b4.\n\u25a1\nAppendix L: Proof of Corollary 25\nFirst we show that for all \u03c5 \u2208Rm, (L.1) holds. It is suf\ufb01cient to check that the\ncondition (K.4) in Lemma 37 holds. Then, (L.1) follows from Lemma 39: for \u03c5 \u2208Rm,\n\f\f\u03c5T \u2206\u03c5\n\f\f \u22644\u03b4(\u2225\u03c5\u22252\n2 + 1\nk \u2225\u03c5\u22252\n1) \u22643\n8\u03bbmin(A)(\u2225\u03c5\u22252\n2 + 1\nk \u2225\u03c5\u22252\n1).\n(L.1)\nThe Lower and Upper RE conditions thus immediately follow. The Corollary is thus\nproved.\n\u25a1\nAppendix M: Proof of Theorem 26\nWe \ufb01rst state the following preliminary results in Lemmas 40 and 41; their proofs\nappear in Section O. Throughout this section, the choice of C = C0/\n\u221a\nc\u2032 satis\ufb01es the\nconditions on C in Lemmas 40 and 41, where recall min{C0, C2\n0} \u22654/c for c as\nde\ufb01ned in Theorem 31. For a set J \u2282{1, . . . , m}, denote FJ = A1/2EJ, where recall\nEJ = span{ej : j \u2208J}. Let Z be an n \u00d7 m random matrix with independent entries\nZij satisfying EZij = 0, 1 = EZ2\nij \u2264\u2225Zij\u2225\u03c82 \u2264K. Let Z1, Z2 be independent\ncopies of Z.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n91\nLemma 40. Suppose all conditions in Theorem 26 hold. Let\nE =\n[\n|J|=k\nEJ \u2229Sm\u22121.\nSuppose that for some c\u2032 > 0 and \u03b5 \u22641\nC , where C = C0/\n\u221a\nc\u2032,\nr(B) := tr(B)\n\u2225B\u22252\n\u2265\nc\u2032kK4 log(3em/k\u03b5)\n\u03b52\n.\n(M.1)\nThen for all vectors u, v \u2208E\u2229Sm\u22121, on event B1, where P (B1) \u22651\u22122 exp\n\u0010\n\u2212c2\u03b52\ntr(B)\nK4\u2225B\u22252\n\u0011\nfor c2 \u22652,\n\f\fuT ZT BZv \u2212EuT ZT BZv\n\f\f\n\u2264\n4C\u03b5tr(B).\nLemma 41. Suppose that \u03b5 \u22641/C, where C is as de\ufb01ned in Lemma 40. Suppose that\n(M.1) holds. Let\nE =\n[\n|J|=k\nEJ\nand\nF =\n[\n|J|=k\nFJ.\n(M.2)\nThen on event B2, where P (B2) \u22651 \u22122 exp\n\u0010\n\u2212c2\u03b52\ntr(B)\nK4\u2225B\u22252\n\u0011\nfor c2 \u22652, we have for\nall vectors u \u2208E \u2229Sm\u22121 and w \u2208F \u2229Sm\u22121,\n\f\f\fwT ZT\n1 B1/2Z2u\n\f\f\f\n\u2264\nC\u03b5tr(B)\n(1 \u2212\u03b5)2 \u2225B\u22251/2\n2\n\u22644C\u03b5tr(B)/\u2225B\u22251/2\n2\n.\nIn fact, the same conclusion holds for all y, w \u2208F \u2229Sm\u22121; and in particular, for\nB = I, we have the following.\nCorollary 42. Suppose all conditions in Lemma 40 hold. Suppose that F = A1/2E\nfor E as de\ufb01ned in Lemma 40. Let\nn\n\u2265\nc\u2032kK4 log(3em/k\u03b5)\n\u03b52\n.\n(M.3)\nThen on event B3, where P (B3) \u22651 \u22122 exp\n\u0000\u2212c2\u03b52n 1\nK4\n\u0001\n, we have for all vectors\nw, y \u2208F \u2229Sm\u22121 and \u03b5 \u22641/C for C is as de\ufb01ned in Lemma 40,\n\f\fyT ( 1\nnZT Z \u2212I)w\n\f\f\n\u2264\n4C\u03b5.\n(M.4)\nWe prove Lemmas 40 and 41 and Corollary 42 in Section O. We are now ready to prove\nTheorem 26.\nProof of Theorem 26.\nLet\n\u2206:= b\u0393A \u2212A := 1\nnXT X \u22121\nn btr(B)Im \u2212A\n=\n( 1\nnXT\n0 X0 \u2212A) + 1\nn\n\u0000W T X0 + XT\n0 W\n\u0001\n+ 1\nn\n\u0000W T W \u2212btr(B)Im\n\u0001\n,\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n92\nwhere recall X0 = Z1A1/2. Notice that\n\f\f\fuT (b\u0393A \u2212A)\u03c5\n\f\f\f =\n\f\fuT (XT X \u2212btr(B)Im \u2212A)\u03c5\n\f\f\n\u2264\n\f\fuT ( 1\nnXT\n0 X0 \u2212A)\u03c5\n\f\f +\n\f\fuT 1\nn(W T X0 + XT\n0 W)\u03c5\n\f\f +\n\f\f\fuT ( 1\nnW T W \u2212\nbtr(B)\nf\nIm)\u03c5\n\f\f\f\n\u2264\n\f\f\fuT A1/2 1\nnZT\n1 Z1A1/2\u03c5 \u2212uT A\u03c5\n\f\f\f +\n\f\fuT 1\nn(W T X0 + XT\n0 W)\u03c5\n\f\f\n+\n\f\fuT ( 1\nnZT\n2 BZ2 \u2212\u03c4BIm)\u03c5\n\f\f + 1\nn\n\f\fbtr(B) \u2212tr(B)\n\f\f \f\fuT \u03c5\n\f\f =: I + II + III + IV.\nFor u \u2208E \u2229Sm\u22121, de\ufb01ne h(u) :=\nA1/2u\n\u2225A1/2u\u22252\n. The conditions in (M.1) and (M.3) hold\nfor k.\nWe \ufb01rst bound the middle term as follows. Fix u, \u03c5 \u2208E \u2229Sm\u22121. Then on event B2,\nfor \u03a5 = ZT\n1 B1/2Z2,\n\f\fuT (W T X0 + XT\n0 W)\u03c5\n\f\f\n=\n\f\f\fuT ZT\n2 B1/2Z1A1/2\u03c5 + uT A1/2ZT\n1 B1/2Z2\u03c5\n\f\f\f\n\u2264\n\f\fuT \u03a5T h(v)\n\f\f\n\r\r\rA1/2v\n\r\r\r\n2 +\n\f\fh(u)T \u03a5\u03c5\n\f\f\n\r\r\rA1/2u\n\r\r\r\n2\n\u2264\n2\nmax\nw\u2208F \u2229Sm\u22121,\u03c5\u2208E\u2229Sm\u22121\n\f\fwT \u03a5\u03c5\n\f\f \u03c11/2\nmax(k, A)\n\u2264\n8C\u03b5tr(B)\n\u0012\u03c1max(k, A)\n\u2225B\u22252\n\u00131/2\n.\nWe now use Lemma 40 to bound both I and III. We have for C as de\ufb01ned in Lemma 40,\non event B1 \u2229B3,\n\f\fuT (ZT\n2 BZ2 \u2212tr(B)Im)\u03c5\n\f\f\n\u22644C\u03b5tr(B).\nMoreover, by Corollary 42, we have on event B3, for all u, v \u2208E \u2229Sm\u22121,\n\f\fuT ( 1\nnXT\n0 X0 \u2212A)\u03c5\n\f\f\n=\n\f\f\fuT A1/2ZT ZA1/2\u03c5 \u2212uT A\u03c5\n\f\f\f\n=\n\f\fh(u)T ( 1\nnZT Z \u2212I)h(\u03c5)\n\f\f\n\r\r\rA1/2u\n\r\r\r\n2\n\r\r\rA1/2\u03c5\n\r\r\r\n2\n\u2264\n1\nn maxw,y\u2208F \u2229Sm\u22121\n\f\fwT (ZT Z \u2212I)y\n\f\f \u03c1max(k, A)\n\u2264\n4C\u03b5\u03c1max(k, A).\nThus we have on event B1 \u2229B2 \u2229B3 and for \u03c4B := tr(B)/n,\nI + II + III\n\u2264\n4C\u03b5\n \n\u03c1max(k, A) + 2\u03c4B\n\u0012\u03c1max(k, A)\n\u2225B\u22252\n\u00131/2\n+ \u03c4B\n!\n\u2264\n8C\u03b5 (\u03c4B + \u03c1max(k, A)) .\nOn event B6, we have for D1 as de\ufb01ned in Lemma 5,\nIV \u2264|b\u03c4B \u2212\u03c4B| \u22642C0D1K2\nr\nlog m\nmn .\nThe theorem thus holds by the union bound.\n\u25a1\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n93\nAppendix N: Proof of Lemma 32\nLemma 43 is a well-known fact.\nLemma 43. Let Auw := (u \u2297w) \u2297A, where u, w \u2208Sm\u22121 for m \u22652. Then\n\u2225Auw\u22252 \u2264\u2225A\u22252 and \u2225Auw\u2225F \u2264\u2225A\u2225F .\nProof of Lemma 32.\nLet z1, . . . , zn, z\u2032\n1, . . . , z\u2032\nn \u2208Rm be the row vectors Z1, Z2\nrespectively. Notice that we can write the quadratic form as follows:\nuT Z1A1/2ZT\n2 w\n=\nX\ni,j=1,m\nuiwjziA1/2z\u2032\nj\n=\nvec\n\b\nZT\n1\n\tT \u0000(u \u2297w) \u2297A1/2\u0001\nvec\n\b\nZT\n2\n\t\n=:\nvec\n\b\nZT\n1\n\tT A1/2\nuw vec\n\b\nZT\n2\n\t\n,\nuT ZAZT w\n=\nvec\n\b\nZT \tT \u0000(u \u2297w) \u2297A\n\u0001\nvec\n\b\nZT \t\n=:\nvec\n\b\nZT \tT Auwvec\n\b\nZT \t\nwhere clearly by independence of Z1, Z2,\nEvec\n\b\nZT\n1\n\tT \u0000(u \u2297w) \u2297A1/2\u0001\nvec\n\b\nZT\n2\n\t\n=\n0,\nand\nEvec { Z }T \u0000(u \u2297u) \u2297A\n\u0001\nvec { Z }\n=\ntr\n\u0000(u \u2297u) \u2297A\n\u0001\n= tr(A).\nThus we invoke (C.1) and Lemma 43 to show the concentration bounds on event\n{\n\f\fuT Z1A1/2ZT\n2 w\n\f\f > t}:\nP\n\u0010\f\f\fuT Z1A1/2ZT\n2 w\n\f\f\f > t\n\u0011\n\u2264\n2 exp\n\uf8eb\n\uf8ec\n\uf8ed\u2212min\n\uf8eb\n\uf8ec\n\uf8ed\nt2\nK4\n\r\r\rA1/2\nuw\n\r\r\r\n2\nF\n,\nt\nK2\n\r\r\rA1/2\nuw\n\r\r\r\n2\n\uf8f6\n\uf8f7\n\uf8f8\n\uf8f6\n\uf8f7\n\uf8f8\n\u2264\n2 exp\n \n\u2212min\n \nt2\nK4tr(A),\nt\nK2 \r\rA1/2\r\r\n2\n!!\n.\nSimilarly, we have by Theorem 31 and Lemma 43,\nP\n\u0000\f\fuT ZAZT w \u2212EuT ZAZT w\n\f\f > t\n\u0001\n\u2264\n2 exp\n \n\u2212c min\n \nt2\nK4 \u2225Auw\u22252\nF\n,\nt\nK2 \u2225Auw\u22252\n!!\n\u2264\n2 exp\n \n\u2212c min\n \nt2\nK4 \u2225A\u22252\nF\n,\nt\nK2 \u2225A\u22252\n!!\n.\nThe Lemma thus holds.\n\u25a1\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n94\nAppendix O: Proof of Lemmas 40 and 41 and Corollary 42\nThroughout the following proof, we denote by r(B) = tr(B)\n\u2225B\u22252 . Let \u03b5 \u2264\n1\nC where C is\nlarge enough so that cc\u2032C2 \u22654, and hence the choice of C = C0/\n\u221a\nc\u2032 satis\ufb01es our\nneed.\nProof of Lemma 40.\nFirst we prove concentration bounds for all pairs of u, v \u2208\n\u03a0\u2032, where \u03a0\u2032 \u2282Sm\u22121 is an \u03b5-net of E. Let t = CK2\u03b5tr(B). We have by Lemma 32,\nand the union bound,\nP\n\u0000\u2203u, v \u2208\u03a0\u2032,\n\f\fuT ZT BZv \u2212EuT ZT BZv\n\f\f > t\n\u0001\n\u2264\n2 |\u03a0\u2032|2 exp\n\"\n\u2212c min\n \nt2\nK4 \u2225B\u22252\nF\n,\nt\nK2 \u2225B\u22252\n!#\n\u2264\n2 |\u03a0\u2032|2 exp\n\u0014\n\u2212c min\n\u0012\nC2, CK2\n\u03b5\n\u0013 \u03b52r(B)\nK4\n\u0015\n\u2264\n2 exp\n\u0000\u2212c2\u03b52r(B)/K4\u0001\n,\nwhere we use the fact that \u2225B\u22252\nF \u2264\u2225B\u22252 tr(B) and\n|\u03a0\u2032| \u2264\n\u0012m\nk\n\u0013\n(3/\u03b5)k \u2264exp(k log(3em/k\u03b5)),\nwhile\nc min\n\u0012\nC2, CK2\n\u03b5\n\u0013\n\u03b52 r(B)\nK4\n=\ncC2\u03b52\ntr(B)\n\u2225B\u22252 K4\n\u2265\ncC2\n0k log\n\u00003em\nk\u03b5\n\u0001\n\u22654k log\n\u00003em\nk\u03b5\n\u0001\n.\nDenote by B2 the event such that for \u039b :=\n1\ntr(B)(ZT BZ \u2212I),\nsup\nu,v\u2208\u03a0\u2032\n\f\fvT \u039bu\n\f\f\n\u2264\nC\u03b5 =: r\u2032\nk,n\nholds. A standard approximation argument shows that under B2 and for \u03b5 \u22641/2,\nsup\nx,y\u2208Sm\u22121\u2229E\n\f\fyT \u039bx\n\f\f \u2264\nr\u2032\nk,n\n(1 \u2212\u03b5)2 \u22644C\u03b5.\n(O.1)\nThe lemma is thus proved.\n\u25a1\nProof of Lemma 41.\nBy Lemma 32, we have for t = C\u03b5tr(B)/ \u2225B\u22251/2\n2\nfor\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n95\nC = C0/\n\u221a\nc\u2032,\nP\n\u0010\f\f\fwT ZT\n1 B1/2Z2u\n\f\f\f > t\n\u0011\n\u2264\nexp\n\uf8eb\n\uf8ed\u2212c min\n\uf8eb\n\uf8edC2 tr(B)2\n\u2225B\u22252 \u03b52\nK4tr(B) , C\u03b5tr(B)\nK2 \u2225B\u22252\n\uf8f6\n\uf8f8\n\uf8f6\n\uf8f8\n\u2264\n2 exp\n\u0012\n\u2212c min\n\u0012C2\u03b52rB\nK4\n, C\u03b5rB\nK2\n\u0013\u0013\n\u2264\n2 exp\n\u0012\n\u2212c min\n\u0012\nC2, CK2\n\u03b5\n\u0013\n\u03b52rB/K4\n\u0013\n.\nChoose an \u03b5-net \u03a0\u2032 \u2282Sm\u22121 such that\n\u03a0\u2032 =\n[\n|J|=k\n\u03a0\u2032\nJ\nwhere\n\u03a0\u2032\nJ \u2282EJ \u2229Sm\u22121\n(O.2)\nis an \u03b5-net for EJ \u2229Sm\u22121 and\n|\u03a0\u2032| \u2264\n\u0012m\nk\n\u0013\n(3/\u03b5)k \u2264exp(k log(3em/k\u03b5)).\nSimilarly, choose \u03b5-net \u03a0 of F \u2229Sm\u22121 of size at most exp(k log(3em/k\u03b5)). By the\nunion bound and Lemma 32, and for K2 \u22651,\nP\n\u0010\n\u2203w \u2208\u03a0, u \u2208\u03a0\u2032 s.t.\n\f\f\fwT ZT\n1 B1/2Z2u\n\f\f\f \u2265C\u03b5tr(B)/\u2225B\u22251/2\n2\n\u0011\n\u2264\n|\u03a0\u2032| |\u03a0| 2 exp\n\u0000\u2212c min\n\u0000CK2/\u03b5, C2\u0001\n\u03b52rB/K4\u0001\n\u2264\nexp (2k log(3em/k\u03b5)) 2 exp\n\u0000\u2212cC2\u03b52rB/K4\u0001\n\u2264\n2 exp\n\u0000\u2212c2\u03b52rB/K4\u0001\n,\nwhere C is large enough such that cc\u2032C2 := C\u2032 > 4 and for \u03b5 \u22641\nC ,\nc min\n\u0000CK2/\u03b5, C2\u0001\n\u03b52\ntr(B)\n\u2225B\u22252 K4 \u2265C\u2032k log(3em/k\u03b5) \u22654k log(3em/k\u03b5).\nDenote by \u03a5 := ZT\n1 B1/2Z2. A standard approximation argument shows that if\nsup\nw\u2208\u03a0,u\u2208\u03a0\u2032\n\f\fwT \u03a5u\n\f\f \u2264C\u03b5 tr(B)\n\u2225B\u22251/2\n2\n=: rk,n,\nan event which we denote by B2, then for all u \u2208E and w \u2208F,\n\f\f\fwT ZT\n1 B1/2Z2u\n\f\f\f \u2264\nrk,n\n(1 \u2212\u03b5)2 .\n(O.3)\nThe lemma thus holds for c2 \u2265C\u2032/2 \u22652.\n\u25a1\nProof of Corollary 42.\nClearly (M.4) implies that (M.1) holds for B = I. Clearly (M.3)\nholds following the analysis of Lemma 40 by setting B = I, while replacing event B1\nwith B3, which denotes an event such that\nsup\nu,v\u2208\u03a0\n1\nn\n\f\fvT (ZT Z \u2212I)u\n\f\f\n\u2264\nC\u03b5.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n96\nThe rest of the proof follows by replacing E with F everywhere. The corollary thus\nholds.\n\u25a1\nReferences\n[1] AGARWAL, A., NEGAHBAN, S. and WAINWRIGHT, M. (2012). Fast global con-\nvergence of gradient methods for high-dimensional statistical recovery. Annals of\nStatistics 40.\n[2] ALLEN, G. and TIBSHIRANI, R. (2010). Transposable regularized covariance\nmodels with an application to missing data imputation. Annals of Applied Statis-\ntics 4 764-790.\n[3] BELLONI, A., ROSENBAUM, M. and TSYBAKOV, A. (2014). Linear and\nConic Programming Estimators in High-Dimensional Errors-in-variables Mod-\nels. arXiv:1408.0241.\n[4] BICKEL, P., RITOV, Y. and TSYBAKOV, A. (2009). Simultaneous Analysis of\nLasso and Dantzig Selector. The Annals of Statistics 37 1705\u20131732.\n[5] BONILLA, E., CHAI, K. M. and WILLIAMS, C. (2008). Multi-task Gaussian\nprocess prediction. In In Advances in Neural Information Processing Systems 20\n(NIPS 2010).\n[6] CAND`ES, E. and TAO, T. (2007). The Dantzig selector: statistical estimation\nwhen p is much larger than n. Annals of Statistics 35 2313-2351.\n[7] CARROLL, R. J., GAIL, M. H. and LUBIN, J. H. (1993). Case-control studies\nwith errors in predictors. Journal of American Statistical Association 88 177 \u2013\n191.\n[8] CARROLL, R. J., GALLO, P. P. and GLESER, L. J. (1985). Comparison of least\nsquares and errors-in-variables regression with special reference to randomized\nanalysis of covariance. Journal of American Statistical Association 80 929 \u2013 932.\n[9] CARROLL, R. and WAND, M. (1991). Semiparametric estimation in logistic mea-\nsurement error models. J. R. Statist. Soc. B 53 573-585.\n[10] CARROLL, R. J., RUPPERT, D., STEFANSKI, L. A. and CRAINICEANU, C. M.\n(2006). Measurement Error in Nonlinear Models (Second Edition). Chapman &\nHall.\n[11] CHEN, Y. and CARAMANIS, C. (2013). Noisy and Missing Data Regression:\nDistribution-Oblivious Support Recovery. In Proceedings of The 30th Interna-\ntional Conference on Machine Learning ICML-13.\n[12] CHEN, S., DONOHO, D. and SAUNDERS, M. (1998). Atomic decomposition by\nbasis pursuit. SIAM Journal on Scienti\ufb01c and Statistical Computing 20 33\u201361.\n[13] COHEN, M. R. and KOHN, A. K. (2011). Measuring and interpreting neuronal\ncorrelations. Nature Neuroscience 14 809-811.\n[14] COOK, J. R. and STEFANSKI, L. A. (1994). Simulation-extrapolation estima-\ntion in parametric measurement error models. Journal of the American Statistical\nAssociation 89 1314\u20131328.\n[15] DAWID, A. P. (1981). Some Matrix-Variate Distribution Theory: Notational Con-\nsiderations and a Bayesian Application. Biometrika 68 265\u2013274.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n97\n[16] DEMPSTER, A., LAIRD, N. and RUBIN, D. (1977). Maximum likelihood from\nincomplete data via the EM algorithm. Journal of the Royal Statistical Society,\nSeries B 39 1-38.\n[17] DUTILLEUL, P. (1999). The MLE Algorithm for the matrix normal distribution.\nJournal of Statistical Computation and Simulation 64 105\u2013123.\n[18] EFRON, B. (2009). Are a set of microarrays independent of each other? Ann. App.\nStatist. 3 922\u2013942.\n[19] FAN, J. and LI, R. (2001). Variable selection via nonconcave penalized likelihood\nand its oracle properties. Journal of American Statistical Association 96 1348\u2013\n1360.\n[20] FULLER, W. A. (1987). Measurement error models. John Wiley and Sons.\n[21] GAUTIER, E. and TSYBAKOV, A. (2011). High-dimensional instrumental vari-\nables regression and con\ufb01dence sets. arXiv:1105.2454.\n[22] GUPTA, A. and VARGA, T. (1992). Characterization of Matrix Variate Normal\nDistributions. Journal of Multivariate Analysis 41 80-88.\n[23] HALL, P. and MA, Y. (2007). Semiparametric estimators of functional measure-\nment error models with unknown error. Journal of the Royal Statistical Society B\n69 429-446.\n[24] HWANG, J. T. (1986). Multiplicative Errors-in-Variables Models with Applica-\ntions to Recent Data Released by the U.S. Department of Energy. Journal of\nAmerican Statistical Association 81 680\u2013688.\n[25] ITURRIA, S. J., CARROLL, R. J. and FIRTH, D. (1999). Polynomial regression\nand estimating functions in the presence of multiplicative measurement error.\nJournal of the Royal Statistical Society, Series B, Methodological 61 547-561.\n[26] KALAITZIS, A., LAFFERTY, J., LAWRENCE, N. and ZHOU, S. (2013). The Bi-\ngraphical Lasso. In Proceedings of The 30th International Conference on Ma-\nchine Learning ICML-13 1229-1237.\n[27] KASS, R., VENTURA, V. and BROWN, E. (2005). Statistical Issues in the Anal-\nysis of Neuronal Data. J Neurophysiol 94 8\u201325.\n[28] LIANG, H., H \u00a8ARDLE, W. and CARROLL, R. J. (1999). Estimation in a semipara-\nmetric partially linear errors-in-variables model. Ann. Statist. 27 1519-1535.\n[29] LIANG, H. and LI, R. (2009). Variable selection for partially linear models with\nmeasurement Errors. Journal of the American Statistical Association 104 234-\n248.\n[30] LOH, P. and WAINWRIGHT, M. (2012). High-dimensional regression with noisy\nand missing data: Provable guarantees with nonconvexity. The Annals of Statistics\n40 1637\u20131664.\n[31] LOH, P. and WAINWRIGHT, M. (2015). Regularized M-estimators with noncon-\nvexity: Statistical and algorithmic theory for local optima. Journal of Machine\nLearning Research 16 559\u2013616.\n[32] MA, Y. and LI, R. (2010). Variable selection in measurement error models.\nBernoulli 16 274-300.\n[33] MENDELSON, S., PAJOR, A. and TOMCZAK-JAEGERMANN, N. (2008). Uni-\nform uncertainty principle for Bernoulli and subgaussian ensembles. Constructive\nApproximation 28 277\u2013289.\n[34] NESTEROV, Y. (2007). Gradient methods for minimizing composite objective\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n98\nfunction. CORE DISCUSSION PAPER 2007/76, Center for Operations Research\nand Econometrics (CORE), Catholic University of Louvain (UCL) 1-30.\n[35] ROSENBAUM, M. and TSYBAKOV, A. (2010). Sparse recovery under matrix un-\ncertainty. The Annals of Statistics 38 2620-2651.\n[36] ROSENBAUM, M. and TSYBAKOV, A. (2013). Improved matrix uncertainty se-\nlector. IMS Collections 9 276-290.\n[37] RUDELSON, M. and VERSHYNIN, R. (2013). Hanson-Wright inequality and sub-\ngaussian concentration. Electronic Communications in Probability 18 1\u20139.\n[38] RUDELSON, M. and ZHOU, S. (2013). Reconstruction from anisotropic random\nmeasurements. IEEE Transactions on Information Theory 59 3434-3447.\n[39] RUDELSON, M. and ZHOU, S. (2015). High dimensional errors-in-variables\nmodels with dependent measurements. arXiv:1502.02355.\n[40] RUFF, D. A. and COHEN, M. R. (2014). Attention can either increase or decrease\nspike count correlations in visual cortex. Nature Neuroscience 17 1591-7.\n[41] S\u00d8RESEN, \u00d8., FRIGENSSI, A. and THORESEN, M. (2014a). Measurement Error\nin Lasso: Impact and Likelihood Bias Correction. Statistical Sinica Preprint.\n[42] S\u00d8RESEN, \u00d8., FRIGENSSI, A. and THORESEN, M. (2014b). Covariate Selec-\ntion in High-Dimensional Generalized Linear Models with Measurement Error.\narXiv:1407.1070.\n[43] ST \u00a8ADLER, N., STEKHOVEN, D. J. and B \u00a8UHLMANN, P. (2014). Pattern Alternat-\ning Maximization Algorithm for Missing Data in High-Dimensional Problems.\nJournal of Machine Learning Research 15 1903-1928.\n[44] STEFANSKI, L. A. (1985). The effects of measurement error on parameter esti-\nmation. Biometrika 72 583\u2013592.\n[45] STEFANSKI, L. A. (1990). Rates of convergence of some estimators in a class of\ndeconvolution problems. Statistics and Probability Letters 9 229\u2013235.\n[46] STEFANSKI, L. A. and COOK, J. R. (1995). Simulation-extrapolation: The mea-\nsurement error jackknife. Journal of the American Statistical Association 90\n1247\u20131256.\n[47] STRIMMER, K. (2003). Modeling gene expression measurement error: a quasi-\nlikelihood approach. BMC Bioinformatics 4.\n[48] TIBSHIRANI, R. (1996). Regression shrinkage and selection via the Lasso. J. Roy.\nStatist. Soc. Ser. B 58 267-288.\n[49] TROPP, J. A. (2004). Greed is good: Algorithmic results for sparse approxima-\ntion. IEEE Trans. Inform. Theory 50 2231\u20132241.\n[50] TROPP, J. and GILBERT, A. (2007). Signal recovery from random measurements\nvia orthogonal matching pur- suit. IEEE Trans. Inform. Theory 53 4655-4666.\n[51] VIAL, J. P. (1982). Strong convexity of sets and functions. Journal of Mathemat-\nical Economics 9 187\u2013205.\n[52] WERNER, K., JANSSON, M. and STOICA, P. (2008). On Estimation of Covari-\nance Matrices With Kronecker Product Structure. IEEE Transactions on Signal\nProcessing 56 478 \u2013 491.\n[53] XU, Q. and YOU, J. (2007). Covariate Selection for Linear Errors-in-Variables\nRegression Models. Communications in Statistics \u2013 Theory and Methods 36.\n[54] YU, K., LAFFERTY, J., ZHU, S. and GONG, Y. (2009). Large-scale collaborative\nprediction using a nonparametric random effects model. Proceedings of the 26th\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\nRudelson and Zhou/Errors-in-variables\n99\nInternational Conference on Machine Learning.\n[55] ZHANG, C. H. (2010). Nearly unbiased variable selection under minimax con-\ncave penalty. Ann. Statist. 38 894-942.\n[56] ZHOU, S. (2014). Gemini: Graph estimation with matrix variate normal in-\nstances. Annals of Statistics 42 532\u2013562.\n[57] ZHOU, S., LAFFERTY, J. and WASSERMAN, L. (2010). Time Varying Undirected\nGraphs. Machine Learning 80 295-319.\nimsart-ejs ver. 2014/10/16 file: eiv-rz.tex date: October 16, 2018\n"}