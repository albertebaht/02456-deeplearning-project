{"text": "Estimation in linear errors-in-variables models\nwith unknown error distribution\nL. H. Nghiem \u22171, M. C. Byrd \u20201, and C. J. Potgiter \u20211,2\n1Department of Statistical Science, Southern Methodist University, Dallas, TX\n2Department of Statistics, University of Johannesburg, South Africa\nDecember 4, 2018\nAbstract\nParameter estimation in linear errors-in-variables models typically requires that the mea-\nsurement error distribution be known (or estimable from replicate data). A generalized method\nof moments approach can be used to estimate model parameters in the absence of knowledge of\nthe error distributions, but requires the existence of a large number of model moments. In this\npaper, parameter estimation based on the phase function, a normalized version of the character-\nistic function, is considered. This approach requires the model covariates to have asymmetric\ndistributions, while the error distributions are symmetric. Parameter estimation is then based\non minimizing a distance function between the empirical phase functions of the noisy covari-\nates and the outcome variable. No knowledge of the measurement error distribution is required\nto calculate this estimator. Both the asymptotic and \ufb01nite sample properties of the estimator\nare considered. The connection between the phase function approach and method of moments\nis also discussed. The estimation of standard errors is also considered and a modi\ufb01ed bootstrap\nalgorithm is proposed for fast computation. The newly proposed estimator is competitive when\ncompared to generalized method of moments, even while making fewer model assumptions on\nthe measurement error. Finally, the proposed method is applied to a real dataset concerning\nthe measurement of air pollution.\nKeywords: Asymmetric distribution; Empirical characteristic function; Measurement error;\nMethod of moments; Phase function.\n1\nIntroduction\nErrors-in-variables models arise when some covariates cannot be measured accurately. Sources of\nmeasurement error include the instruments used to measure the variables of interest and the inade-\nquacy of measurements taken over the short term being used as proxies for long-term variables. In\n\u2217Email: lnghiem@smu.edu\n\u2020Email: mcbyrd@smu.edu\n\u2021Email: cjpotgieter@smu.edu\n1\narXiv:1812.00492v1  [stat.ME]  3 Dec 2018\nthe classic measurement error framework, this results in observed covariates having larger variance\nthan the true predictors. Let X = (X(1), . . . , X(p))\u22a4\u2208Rp denote the true model covariates and let\nY \u2208R denote the outcome of interest. For \u03b21 \u2208Rp, the relationship between X and Y is assumed\nto be Y = \u03b20 + X\u22a4\u03b21 + \u03b5 with intercept \u03b20 \u2208R and error \u03b5 \u2208R. In an errors-in-variables model,\nX is not directly observed. Rather, W = (W (1), . . . , W (p))\u22a4\u2208Rp is observed with W = X + U\ndenoting the covariates contaminated by additive measurement error, and U \u2208Rp denoting the\nmeasurement error. This model represents the classic formulation of the errors-in-variables model\nand the estimation of \u03b2 = (\u03b20, \u03b2\u22a4\n1 )\u22a4is of interest.\nAbove, the model error \u03b5 is assumed to be symmetric about 0 with scale parameter \u03c32 and the\nmeasurement error U is assumed to be symmetric about 0 \u2208Rp with scale matrix \u03a3u. Gener-\nally, \u03c32 and \u03a3u represent, respectively, the variance of \u03b5 and covariance matrix of U when these\nquantities are well-de\ufb01ned. The covariates X, measurement error U and model error \u03b5 are further-\nmore assumed mutually independent. Given a sample (W1, Y1), . . . , (Wn, Yn), it is well known\nthat regression of the Yi on the Wi using traditional methods such as ordinary least squares leads\nto an inconsistent and biased estimate of \u03b2, see Carroll et al. (2006). Hence, adjusting for the\npresence of measurement error is important for accurately describing the relationship between the\ntrue covariates and the outcome of interest.\nThere is a vast literature on correcting for the effect of measurement error dating back to Wald\n(1940). An overview of more modern approaches to estimation in the presence of measurement\nerror can be found in Fuller (2009). Most of the methods proposed in the literature require strong\nassumptions on the distribution of the measurement error U. Speci\ufb01cally, it is generally assumed\nthat the covariance matrix \u03a3u is known. For example, in the simple linear model setting where\nY = \u03b20 + \u03b21X + \u03b5 and W = X + U, a consistent estimator of the slope \u03b21 is obtained as\n\u02c6\u03b21 = \u02c6\u03b2(W)\n1\n\u03c32\nX/(\u03c32\nX + \u03c32\nU), where \u02c6\u03b2(W)\n1\nis the estimated slope obtained by ordinary least squares\nregression of Y on W. Calculation of this estimator requires that the variance of measurement er-\nror \u03c32\nU be known or estimable. In the multiple predictor setting, there is generally no closed-form\nsolution for the corrected estimator. Instead, simulation-extrapolation , \ufb01rst proposed by Stefanski\n& Cook (1995), is frequently used. The simulation-extrapolation procedure evaluates the effects of\nmeasurement error on the estimator by increasing the level of measurement error through a simu-\nlation step, and then extrapolating to the setting of no measurement error. simulation-extrapolation\nalso requires that \u03a3u be known or estimable. In many commonly assumed scenarios, such as when\nboth the covariates X and measurement error U are assumed to be multivariate normal, \u03a3u can\nonly be estimated in the presence of auxiliary data, see Reiers\u00f8l (1950) for a discussion of the\nidenti\ufb01ability of errors-in-variables models. Another approach to correcting for measurement er-\nror is regression calibration, see Carroll & Stefanski (1990). Here, a regression of X on W is\nused to estimate X, say \u02c6X, and then the linear model parameters are estimated by regressing Y on\n\u02c6X. The regression of X on W is assumed to be available through an either validation data or an\ninstrumental variable T.\nWhen the distributions of X, U and \u03b5 are fully speci\ufb01ed, likelihood methods can also be used to\nestimate parameters, see Schafer & Purdy (1996) and Higdon & Schafer (2001). Implementation\nof these likelihood methods generally require the use of numerical methods such as Gaussian\nquadrature or Monte Carlo integration. The EM algorithm of Dempster et al. (1977) can also be\nused. An approach that does not require the distribution of X to be known is the conditional score\nmethod of Stefanski & Carroll (1987). However, this method does require that both parametric\n2\nmodels for the conditional distributions Y | X and W | X be speci\ufb01ed.\nThe method of moments approach to estimating linear errors-in-variables models dates back\nto the work of Reiers\u00f8l (1941), who estimates the slope of the simple errors-in-variables model\nthrough third-order moments. Gillard (2014) considers slope estimators based on third and fourth\nmoments, and \ufb01nds these to have large variances. More recently, methods based on the matching of\nhigher-order moments, or variants such as cumulants, have been explored with renewed interest.\nErickson & Whited (2002) express high-order residual moments as nonlinear functions of both\ncoef\ufb01cients and nuisance parameters, while Erickson et al. (2014) express the third and fourth\nresidual cumulants as a linear function of the coef\ufb01cients. The latter also establishes that the two\nmethods are asymptotically equivalent. The method of moments approach is nonparametric in the\nsense that it does not require parametric distributions to be speci\ufb01ed for any of the components.\nHowever, an implementation based on the \ufb01rst M sample moments generally requires 2M \ufb01nite\npopulation moments.\nThis paper proposes a method of estimation that is fully nonparametric, in that implementation\ndoes not require parametric speci\ufb01cations of any model components, nor does it require the exis-\ntence of model moments. Furthermore, the method does not require that the measurement error\nvariance be known, if it exists, and replication data is not needed. The estimator makes use of\nthe empirical phase function, a normalized version of the empirical characteristic function. The\nempirical phase function was considered in the context of density deconvolution by Delaigle &\nHall (2016) and Nghiem & Potgieter (2018). The method has two assumptions: the measurement\nerror U is symmetric around 0 with strictly positive characteristic function, and the distribution of\nX is asymmetric. These assumptions are fundamental for the identi\ufb01ability of the phase function\nof X, which forms the basis of the estimation procedure. The assumptions are discussed in greater\ndetail in Section 2.1; see also Delaigle & Hall (2016) for an in-depth discussion.\nThe remainder of this article is organized as below. In Section 2, we introduce the phase\nfunction-based estimator, develop its asymptotic properties, and establishes a connection to the\nmethod of moments approach. Section 3 considers some computational aspects relating to the\nestimator, including estimating standard errors in practice. Section 4 presents a simulation study\nto illustrate the performance of the phase function estimator and compare it with existing meth-\nods. Section 5 applies the phase function estimator to a real dataset, and Section 6 contains some\nconcluding remarks.\n2\nPhase Function Minimum Distance Estimation\n2.1\nPhase Function-Based Estimation\nConsider the simple linear errors-in-variables model with observed sample (Wi, Yi), i = 1, . . . , n\nwhere\nYi = \u03b20 + \u03b21Xi + \u03b5i\nand\nWi = Xi + Ui.\n(1)\nHere, the Xi \u2208R are independent and identically distributed with asymmetric density function\nfX, the Ui \u2208R and \u03b5i \u2208R are independent and identically distributed with respective density\nfunctions fU and f\u03b5, both symmetric about 0 and having strictly positive characteristic functions.\nFurthermore,the Xi, Ui and \u03b5i are assumed mutually independent. It should be noted that the\nmethod developed here can also be used in the more general setting where each error term Ui and\n3\n\u03b5i has a unique density function, say fU,i and f\u03b5,i, as long as these are all independent, symmetric\nabout 0, and have strictly positive characteristic functions. However, for simplicity of exposition\nthe scenario with common error densities fU and f\u03b5 is presented. As to the assumed positivity\nof the characteristic functions, we note that many commonly used continuous distributions in the\napplication of regression and measurement error satisfy this condition. This includes the Gaussian,\nLaplace, and Student\u2019s t distributions. In general, the only symmetric distributions excluded are\nthose de\ufb01ned on bounded intervals, such as the uniform. In the context of density deconvolution,\nDelaigle & Hall (2016) assumed that the random variable X does not have a symmetric component,\ni.e. there is no symmetric random variable S for which X can be decomposed as X = X0 + S for\narbitrary random variable X0. In the present setting, this strict assumption is not required. More\nspeci\ufb01cally, we only require that the covariate X not be symmetric.\nNow, let \u03c6X(t) = E [exp (itX)] denote the characteristic function of a random variable X. The\nphase function of X is then de\ufb01ned as the normalized characteristic function,\n\u03c1X(t) = \u03c6X(t)\n|\u03c6X(t)|,\n(2)\nwhere |z| = (z\u00afz)1/2 is the complex norm with \u00afz denoting the complex conjugate of z. We now\npresent our \ufb01rst result that establishes a relationship between the phase functions of W and Y .\nLemma 1 Consider univariate random variables W = X + U and Y = \u03b20 + \u03b21X + \u03b5. Assume\nthat X asymmetric with phase function \u03c1X(t), and that U and \u03b5 are symmetric about 0 with strictly\npositive characteristic functions. The phase function for Y is then given by\n\u03c1Y (t) = exp (it\u03b20) \u03c1X(\u03b21t) = exp (it\u03b20) \u03c1W(\u03b21t).\nHence, the phase function of Y can be fully speci\ufb01ed in terms of \u03c1W(t), the phase function of W,\nand parameters (\u03b20, \u03b21).\nThe above lemma follows follows simply by noting that W and Y have characteristic functions\n\u03c6W(t) = \u03c6X(t)\u03c6U(t) and \u03c6Y (t) = exp (it\u03b20) \u03c6X(\u03b21t)\u03c6\u03b5(t). When evaluating the respective phase\nfunctions, the error components \u03c6U(t) and \u03c6\u03b5(t) are canceled out in due to their assumed positivity.\nA full derivation of the relationship is given in Section A.1 of the Supplemental Material.\nEmpirical estimates of the phase functions of W and Y can be obtained from a random sample\n(Wj, Yj), j = 1, . . . , n. De\ufb01ne\n\u02c6\u03c1W(t) =\n\u02c6\u03c6W(t)\n|\u02c6\u03c6W(t)|\n=\nPn\nj=1 exp(itWj)\nhPn\nj=1\nPn\nk=1 exp {it (Wj \u2212Wk)}\ni1/2,\nwith a similar de\ufb01nition holding for \u02c6\u03c1Y (t). The empirical phase functions can now be used to\nconstruct minimum distance estimators of the model parameters (\u03b20, \u03b21). De\ufb01ne statistic\nD(b0, b1) =\nZ \u221e\n\u2212\u221e\n|\u02c6\u03c1Y (t) \u2212exp (itb0)\u02c6\u03c1W(b1t)|2 w(t)dt,\n(3)\nwhere the weight function w(t) is chosen to ensure that the integral is well-de\ufb01ned. The estimator\n(\u02c6\u03b20, \u02c6\u03b21) is then computed as the global minimizer of the function D(b0, b1).\n4\nThe above idea can be easily extended to the case of multivariate regression with both error-\nprone and error-free covariates. Consider the model Y = \u03b20+X\u22a4\u03b21+Z\u22a4\u03b22+\u03b5 where X, \u03b21 \u2208Rp1\nand Z, \u03b22 \u2208Rp2. Here, Z represents the covariates measured without error. As before, let W =\nX + U denote the contaminated version of X where U is p1-dimensional symmetric measurement\nerror. Let V = \u03b20 + X\u22a4\u03b21 + Z\u22a4\u03b22 so that Y = V + \u03b5. It then follows that \u03c1Y (t) = \u03c1V (t).\nSimilarly, consider the linear combination in terms of the contaminated W, say\neV = \u03b20 + W \u22a4\u03b21 + Z\u22a4\u03b22 = V + U \u22a4\u03b21 = V + eU\nwith eU = U \u22a4\u03b21 \u2208R having distribution symmetric about zero with strictly positive characteristic\nfunction. It then also follows that \u03c1eV (t) = \u03c1V (t). Hence, the variables Y , V and eV all have the\nsame phase function. To estimate \u03b2 = (\u03b20, \u03b2\u22a4\n1 , \u03b2\u22a4\n2 )\u22a4, it is possible to construct a distance metric\nequivalent to (3),\nD(b0, b1, b2) =\nZ \u221e\n\u2212\u221e\n\f\f\u02c6\u03c1Y (t) \u2212exp (itb0)\u02c6\u03c1eV (t|b1, b2)\n\f\f2 w(t)dt\n(4)\nwhere, given n random observations (Wj, Zj, Yj), the empirical phase function corresponding to\neV is\n\u02c6\u03c1eV (t|\u03b21, \u03b22) =\nPn\nj=1 exp\n\b\nit\n\u0000W \u22a4\nj \u03b21 + Z\u22a4\nj \u03b22\n\u0001\t\n\u0010Pn\nj=1\nPn\nk=1 exp\nh\nit\nn\n(Wj \u2212Wk)\u22a4\u03b21 + (Zj \u2212Zk)\u22a4\u03b22\noi\u00111/2.\n(5)\nNote that the statistic (4) does not treat the variables measured with and without error any differ-\nently. As such, the phase function approach could be implemented without knowledge of which\nvariables are subject to measurement error. The estimate of \u03b2 = (\u03b20, \u03b2\u22a4\n1 , \u03b2\u22a4\n2 )\u22a4is found by mini-\nmizing D(b0, b1, b2).\n2.2\nAsymptotic Properties of Phase Function Estimators\nIn this section, we verify that the estimators obtained by minimizing statistic D in (3) satisfy the\nconditions required of M-estimators, and are therefore asymptotically normal. To this end, we \ufb01rst\nestablish the almost sure convergence of D to an appropriate limit. Note that, while the asymptotic\nproperties of the phase function-based estimator are considered in the context of a simple linear\nerrors-in-variables model, the results easily extend to the multivariate case.\nLemma 2 Assume that independent pairs (W1, Y1), . . . , (Wn, Yn) are observed with Wi = Xi+Ui\nand Yi = \u03b20 + \u03b21Xi + \u03b5i, with the distribution of Xi asymmetric, and with Ui and \u03b5i having\ndistributions symmetric about 0 and with strictly positive characteristic functions. Furthermore,\nlet w(t) be a non-negative weight function with bounded support, taken without loss of generality\nto be [\u2212c, c].\nFor this choice of weight function, the statistic D(b0, b1) de\ufb01ned in (3) converges almost surely\nto a limit Dtrue(b0, b1) with\nDtrue(b0, b1) =\nZ \u221e\n\u2212\u221e\n|\u03c1Y (t) \u2212exp(itb0)\u03c1W(b1t)|2 w(t)dt.\nThe limit has unique global minimum Dtrue(\u03b20, \u03b21) = 0.\n5\nThe proof of this lemma follows upon noting the empirical characteristic functions \u02c6\u03c6W(t) is\nan unbiased estimator of the true characteristic function \u03c6W(t) and converges almost surely to\n\u03c6W(t) on any bounded interval [\u2212c, c], see Theorem 2.1 of Feuerverger et al. (1977). Applying\nthe continuous mapping theorem, the empirical phase function \u02c6\u03c1W(t) also converges almost surely\nto the true phase function \u03c1W(t) on [\u2212c, c], and is an asymptotically unbiased estimator thereof.\nThe convergence of D to Dtrue follows from this. Next, noting that a phase function is uniquely\nidenti\ufb01ed by the asymmetric part of the corresponding distribution, the function Dtrue has a global\nminimum of 0 at the true parameter values (\u03b20, \u03b21).\nTheorem 1 Assume that conditions (i) and (ii) from Lemma 1 hold. Let \u02c6\u03b2 = ( \u02c6\u03b20, \u02c6\u03b21)\u22a4denote the\nminimizer of D in (3). This estimator is consistent for the true \u03b2 = (\u03b20, \u03b21)\u22a4, and is asymptotically\nnormal,\n\u221an\n\u0010\n\u02c6\u03b2 \u2212\u03b2\n\u0011\n\u2192N\n\u00000, B\u22121AB\u22121\u0001\n(6)\nwhere\nA = E\n\u0000\u03bb\u03bb\u22a4\u0001\nand B = E\n\u0012 \u2202\u03bb\n\u2202\u03b2\u22a4\n\u0013\n(7)\nwith \u03bb = \u2202D\n\u2202\u03b2 .\nThe consistency of \u02c6\u03b2 follows from Lemma 1 above and Theorem 5.7 in van der Vaart (2000).\nHaving established consistency, and noting that D has in\ufb01nitely many bounded and continuous\nderivatives, asymptotic normality follows from Theorem 5.21 in van der Vaart (2000).\n2.3\nConnection to Method of Moments Estimation\nDelaigle & Hall (2016) show that for any random variable X with in\ufb01nite number of moments, the\nphase function of X can be expressed as\n\u03c1X(t) = exp\n( \u221e\nX\nj=1\n(\u22121)j+1t2j\u22121\u03baX\n2j\u22121\n(2j \u22121)!\n)\n,\nwhere \u03baX\nj denotes the jth cumulant of X. In other words, if the in\ufb01nite series above converges, the\nphase function is determined uniquely by the odd-order cumulants of X. In this context, consider\nthe model (1). If X, U, and \u03b5 have an in\ufb01nite number of \ufb01nite moments, the same holds true for\nW and Y . Speci\ufb01cally, for (W, Y ) following the linear errors-in-variables model, it follows that\nexp\n(\ni\n\u221e\nX\nj=1\n(\u22121)j+1t2j\u22121\u03baY\n2j\u22121\n(2j \u22121)!\n)\n= exp\n\"\ni\n(\nt\u03b20 +\n\u221e\nX\nj=1\n(\u22121)j+1(\u03b21t)2j\u22121\u03baW\n2j\u22121\n(2j \u22121)!\n)#\n.\n(8)\nOne can use (8) and match the coef\ufb01cients of t2j\u22121 to determine the relationship between the jth\nodd cumulants of W and Y . For example, considering the coef\ufb01cients of t and t3 gives \u03baY\n1 =\n\u03b20 + \u03b21\u03baW\n1 and \u03baY\n3 = \u03b23\n1\u03baW\n3 .\n6\nNow, using properties of the complex norm, it follows that\n1\n4 |\u03c1Y (t) \u2212exp(it\u03b20)\u03c1W(\u03b21t)|2 = sin2\n( \u221e\nX\nj=1\n(\u22121)j+1t2j\u22121(\u03baY\n2j\u22121 \u2212\u03b22j\u22121\n1\n\u03baW\n2j\u22121)\n(2j \u22121)!\n\u2212t\u03b20\n)\n.\nWhen inference is based on the sample phase functions, the population cumulants above are re-\nplaced by their sample counterparts, and minimizing (3) is equivalent to choosing the parameters\n\u03b20 and \u03b21 such that a function of the difference of all odd cumulants is minimized. As such, when\nthe underlying distributions have an in\ufb01nite number of moments, the phase function approach can\nbe thought of as a method of moments-type approach that makes use of all odd cumulants of the\nvariables of interest.\n3\nComputational Considerations\n3.1\nComputing the Estimators\nDirect minimization of statistics (3) and (4) is generally computationally expensive. In this section,\na computational method is proposed that leads to faster calculation of the estimators. The idea is\npresented for the univariate errors-in-variables model, but can easily be extended to the multivariate\nmodel setting.\nLemma 3 Consider the statistics (3) with weight function\nw(t) = Kt\u2217(t)\n\" n\nX\nj=1\ncos(tYj)\nn\nX\nj=1\ncos {t (b0 + b1Wj)}\n#2\nwhere Kt\u2217(t) = K(t/t\u2217) and K(t) is a non-negative kernel function with bounded support on\nsome interval [\u22121, 1]. Minimization of (3) is then equivalent to minimization of\nD(b0, b1) =\nZ t\u2217\n0\n\" n\nX\ni=1\nn\nX\nj=1\nsin {t(Yi \u2212b0 \u2212b1Wj)}\n#2\nKt\u2217(t)dt\n(9)\nThe derivation of this lemma is given in the Section A.2 of the Supplementary Material. It\nis based on an application of Euler\u2019s formula, and noting that the ratio of the imaginary and real\ncomponents of the phase function are equal to the same ratio calculated from the characteristic\nfunction. Formula (9) has computational complexity O(n2). However, by some algebra it can be\nre-expressed as\nD(b0, b1) =\nZ t\u2217\n0\n\"\nSy\nn\nX\nj=1\ncos {t(b0 + b1Wj)} \u2212Cy\nn\nX\nj=1\nsin {t(b0 + b1Wj)}\n#2\nKt\u2217(t)dt,\n(10)\nwith Cy = Pn\nj=1 cos(tYj) and Sy = Pn\nj=1 sin(tYj). Evaluating (10) has computational complexity\nO(n). The particular choice of weight function avoids instabilities that can occur in (3) as a result\n7\nof dividing by numbers close to 0. With regards to choosing an appropriate constant t\u2217, we follow\nthe suggestion in Delaigle & Hall (2016) who let t\u2217be the smallest t > 0 such that |\u02c6\u03c6Y (t)| \u2264n\u22121/4.\nWhen considering simpli\ufb01cation of statistic (9), It is also possible to eliminate the integral in\nthe equation. To this end, let \u03c6K,h(\u03b1) =\nR h\n\u2212h cos(\u03b1t)Kh(t)dt. It then follows that\nD(b0, b1) \u221d\nX\ni,j,k,l\nh\n\u03c6K,t\u2217{Yi \u2212Yk \u2212b1(Wj \u2212Wl)} \u2212\u03c6K,t\u2217{Yi + Yk + 2b0 + b1(Wj + Wl)}\ni\n.\n(11)\nNote that while expression (11) eliminates the need to numerically evaluate an integral as in (10),\nwe generally found that the form in (10) was much faster to compute than the expression involving\nthe quadruple sum in (11).\nNow, considering again the recommended computational form in (10). By an application of the\nLeibniz rule, the \ufb01rst partial derivatives of D with respect to b0 and b1, denoted here \u03bb(b0, b1) =\n{\u03bb0(b0, b1), \u03bb1(b0, b1)}\u22a4, are\n\u03bb0 = \u2202D\n\u2202b0\n= \u22122\nZ t\u2217\n0\n\"X\ni,j\nsin {t(Yi \u2212b0 \u2212b1Wj)}\n# \"X\ni,j\ncos {t(Yi \u2212b0 \u2212b1Wj)}\n#\nKt\u2217(t)dt\n(12)\nand\n\u03bb1 = \u2202D\n\u2202b1\n= \u22122\nZ t\u2217\n0\n\"X\ni,j\nsin {t(Yi \u2212b0 \u2212b1Wj)}\n# \"X\ni,j\nWj cos {t(Yi \u2212b0 \u2212b1Wj)}\n#\ntKt\u2217(t)dt.\n(13)\nThe expressions for \u03bb0 and \u03bb1 can be used as estimating equations to solve for ( \u02c6\u03b20, \u02c6\u03b21). These\nexpressions will also be useful in the next section when considering the estimation of standard\nerrors for the estimators.\n3.2\nStandard Error Estimation\nWe now consider estimation of the covariance matrix of \u02c6\u03b2. Using the asymptotic variance as given\nin Theorem 1 would be reasonable, but direct evaluation of matrices A and B in (7) is not possible\nas this requires knowledge of the distributions of X, U and \u03b5. If these distributions were known, a\nlikelihood approach could be used for parameter estimation rather than the proposed phase function\napproach.\nThe bootstrap is a A popular method for estimating the covariance matrix of estimated param-\neters in a nonparametric setting such as this is the bootstrap. This requires repeated calculation\nof bootstrap estimators \u02c6\u03b2\u2217\nb based on bootstrap samples (W \u2217\nb,i, Y \u2217\nb,i), i = 1, . . . , n for b = 1, . . . , B\ndrawn with replacement from the observed sample. The estimated covariance matrix is then taken\nto be the sample covariance matrix of the bootstrap replicates \u02c6\u03b2\u2217\nb. The procedure can be slow due\nto the repeated evaluation of a computationally expensive loss function for each bootstrap sample.\nImplementation is described in Algorithm 1.\nWe propose here a modi\ufb01ed bootstrap algorithm for estimating the standard errors that com-\nbines bootstrap methodology with approximation of matrices A and B in (7). To this end, note\nthat matrix A is the covariance matrix of \u03bb, the \ufb01rst partial derivatives of (9) given by (12) and\n8\n(13) in the univariate setting. As such, bootstrap methodology can be used to estimate matrix A,\nwhile B can estimated by evaluating the second derivatives of D at the parameter estimates \u02c6\u03b20 and\n\u02c6\u03b21. Expressions for these second derivatives are unwieldy, but are easily evaluated numerically;\nsee Section A.3 in the supplemental material. We refer to this approach as the plug-in bootstrap\napproach and outline implementation in Algorithm 2. Note that the plug-in covariance matrix is\norders of magnitude faster to compute that the boostrap estimator as it does not require repeated\nminimization of a statistic involving numerical integration.\nAlgorithm 1. Full Bootstrap Algorithm\n\u2022 For b = 1, . . . , B\n\u2013 Sample n pairs with replacement from the observed data to obtain bootstrap sample\n(W \u2217\ni,b, Y \u2217\ni,b), i = 1, . . . , n.\n\u2013 Calculate the bootstrap estimators \u02c6\u03b2\u2217\nb by minimizing (9) using the bootstrap sample.\n\u2022 Calculate the empirical covariance matrix of the bootstrap statistics \u02c6\u03b2\u2217\n1, . . . , \u02c6\u03b2\u2217\nB,\n\u02c6\u03a3boot = 1\nB\nB\nX\nb=1\n\u0010\n\u02c6\u03b2\u2217\nb \u2212\u00af\u03b2\u2217\u0011 \u0010\n\u02c6\u03b2\u2217\nb \u2212\u00af\u03b2\u2217\u0011\u22a4\n(14)\nwhere \u00af\u03b2\u2217= B\u22121 P\nb \u02c6\u03b2\u2217\nb is the mean of the bootstrap replicates.\nAlgorithm 2. Plug-in Bootstrap Algorithm\n\u2022 For b = 1, . . . , B\n\u2013 Sample n pairs with replacement from the observed data to obtain bootstrap sample\n(W \u2217\ni,b, Y \u2217\ni,b), i = 1, . . . , n.\n\u2013 Calculate \u03bb\u2217\nb = \u03bb\u2217\nb(\u02c6\u03b20, \u02c6\u03b21) as in (12) and (13) using the bth bootstrap sample.\n\u2022 Calculate\n\u02c6Aboot = 1\nB\nB\nX\nb=1\n\u03bb\u2217\nb\u03bb\u2217\u22a4\nb\nand\n\u02c6B =\n\u0014\n\u2202\u03bb\n\u2202[b0, b1]\u22a4\n\u0015\n(b0,b1)=(\u02c6\u03b20,\u02c6\u03b21)\n.\n\u2022 Calculate plug-in covariance matrix \u02c6\u03a3plug = \u02c6B\u22121 \u02c6Aboot \u02c6B\u22121.\n9\n4\nSimulation Study\nAn extensive simulation study was conducted to evaluate the performance of the phase function-\nbased estimators for various underlying distributions. In this section, we report and discuss a\nrepresentative selection of these simulation results.\nFirst, parameter estimation was explored in the univariate setting. Data were generated ac-\ncording to the model Yi = \u03b20 + \u03b21Xi + \u03b5i and Wi = Xi + Ui, i = 1, . . . , n with true param-\neters (\u03b20, \u03b21) = (1, 3). Three asymmetric distributions were used to simulate X, namely (1) a\nhalf-normal distribution, X \u223c|N(0, 1)|, (2) an exponential distribution, X \u223cexp(1), and (3) a bi-\nmodal mixture distribution, X \u223c0.5N(5, 12)+0.5N(2.5, 0.62). Three different distributions were\nconsidered for error components U and \u03b5, namely the normal, t-distribution with 2.5 degrees of\nfreedom, and the Cauchy distribution. For the Normal and t2.5 distributions, the error components\nwere simulated to have mean 0 and respective variances \u03c32\nU and \u03c32\n\u03b5. For the Cauchy distribution,\nthe error components were simulated to be symmetric about 0 and have respective interquartile\nrange (IQR) \u03c3U and \u03c3\u03b5. The variance and IQR parameters were chosen to achieve speci\ufb01c noise-\nto-signal ratios, pW = \u03c32\nU/\u03c32\nX and pY = \u03c32\n\u03b5/(\u03b21\u03c3X)2. The noise-to-signal ratios pairs reported\nhere are (pW, pY ) = (0.25, 0.40). Results are reported for sample sizes n \u2208{500, 1000}. Simula-\ntion with other noise-to-signal ratios were carried out, and these results are reported in the Section\nC.2 of the Supplement Material. For each possible con\ufb01guration of simulation speci\ufb01cations,\nN = 2000 samples were generated.\nFor the Normal and t2.5 error cases, four different estimators were calculated for each simu-\nlated dataset. First, the naive estimators ignoring measurement error were obtained by regressing\nthe contaminated W on Y . Second, the generalized method of moments estimators using M = 3\nmoments were computed. Three different choices of weight function were considered for the phase\nfunction estimator. Table C.1 in the supplemental material compares the resulting estimators. As\nthe weight function K(t) = (1 \u2212|t|)2I(|t| \u22641) was found to have consistently good perfor-\nmance,the corresponding results are reported here. Finally, the disattenuated regression estimators\nwere also calculated. For disattenuation, the parameters (\u03c32\nU, \u03c32\nX) were treated as known quanti-\nties, and would not be computable in practice under the minimal model assumptions for the phase\nfunction method. For the Cauchy error case with in\ufb01nite variance, no analog for disattenuation\nis known. However, even though there is no theoretical justi\ufb01cation for doing so, the generalized\nmethod of moments estimators were computed to compare to the phase function estimators.\nNow, letting \u02c6\u03b2(method)\nm,j\ndenote the estimator of \u03b2j calculated for the mth sample with the super-\nscript \u201cmethod\u201d a placeholder for a speci\ufb01c method from those listed above, de\ufb01ne squared error\nSE(method)\nm,j\n= [\u02c6\u03b2(method)\nm,j\n\u2212\u03b2j]2. As both the generalized method of moments and phase function\nestimators are very prone to outliers in small samples, the median square errors is used rather than\nmean square error, as the former is more robust against these outliers. For the Normal and t2.5 error\ncase, we report in Table 1 the ratios of median square errors for the naive, generalized method of\nmoments, and phase function estimators relative to the disattenuated estimators. An entry in the\ntable larger than 1 indicates superior performance of the disattenuated estimators, while an entry\nsmaller than 1 indicates superior performance of the associated method. Entries can also be com-\npared across methods, with a larger entry indicating worse performance of a method for a given set\nof simulation speci\ufb01cations. The full simulation results, including the median squared error and a\nrobust estimate of standard error based in the interquartile range, are given in the Section C.2 of\nthe Supplement Material.\n10\nTable 1: Ratio of median square error of estimators relative to the disattenuated regression estima-\ntors in the univariate model simulation with model errors being Normal and t2.5 distributions. Note\nGMM stands for generalized method of moments.\nError type\nTrue X\nn\nNaive\nGMM\nPhase\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n\u03b20\n\u03b21\nNormal\n|N(0, 1)|\n500\n73.0\n98.7\n2.35\n2.81\n2.15\n2.42\n1000\n131.3\n189.8\n2.55\n3.16\n1.83\n2.32\nexp(1)\n500\n44.3\n67.5\n1.18\n1.15\n1.18\n1.59\n1000\n89.0\n129.4\n1.27\n1.24\n1.36\n1.77\nBimodal\n500\n100.7\n118.4\n10.1\n11.5\n5.71\n6.48\n1000\n200.2\n235.0\n9.74\n11.2\n4.02\n4.74\nt2.5\n|N(0, 1)|\n500\n5.89\n6.18\n0.30\n0.29\n0.16\n0.18\n1000\n8.75\n9.32\n0.29\n0.29\n0.09\n0.12\nexp(1)\n500\n6.64\n5.88\n0.17\n0.12\n0.14\n0.19\n1000\n9.09\n8.75\n0.16\n0.11\n0.10\n0.12\nBimodal\n500\n6.29\n6.10\n1.26\n1.21\n0.27\n0.24\n1000\n9.29\n9.12\n1.46\n1.42\n0.12\n0.11\nConsidering the results in Table 1, we note that the naive estimator performs the worst among\nall the considered estimators across all simulation con\ufb01gurations. This is to be expected due to\nthe known bias when not correcting for measurement error. For normally distributed errors, the\nphase function estimator performs better than generalized method of moments for both the cases\nX distributed as half-normal and as a bimodal mixture of normals. The improvement of the phase\nmethod is especially dramatic in the bimodal X case considered. On the other hand, for X hav-\ning an exponential distribution, generalized method of moments performs better than the phase\nfunction method.\nWe reach similar conclusions when considering the case of a t2.5 distribution for the error.\nOverall, the phase function method has superior performance for the cases X half-normal and X\nbimodal. In the case of X having an exponential distribution, generalized method of moments does\nbetter at estimating the slope \u03b21, while the phase function method does better at estimating the in-\ntercept \u03b20. We initially found the good performance of generalized method of moments surprising,\nas its implementation here makes use of the third sample moments, whereas third moments do not\nexist for the error distribution used. However, generalized method of moments downweights the\nsecond and third sample moments using the fourth through sixth sample moments. Intuitively, the\nlatter quantities will be really large, to some extent regulating the effect of using the former on es-\ntimating parameters. Still, the performance of the phase function method is generally far superior\nin this setting. In fact, noting that most of the median square error ratios are much smaller than 1,\nthe phase function method is seen to be superior to correcting for attenuation using known error\nvariances.\nTable 2 presents the simulation results for the generalized method of moments and the phase\nfunction estimators when the model errors follow a Cauchy distribution. In all the considered\nsettings, the phase function estimator has a much smaller median square error than the generalized\nmethod of moments. The poor performance of generalized method of moments is expected because\nno moments exists for the Cauchy distribution. The phase function method, however, still performs\n11\nTable 2: Median square error of the generalized method of moments estimators, denoted GMM in\nthe table, and the phase function estimators when the model errors are Cauchy.\nTrue X\nn\nGMM\nPhase\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n|N(0, 1)|\n500\n4.44\n8.99\n0.05\n0.10\n1000\n4.42\n9.00\n0.02\n0.04\nexp(1)\n500\n5.43\n8.99\n0.03\n0.05\n1000\n6.29\n9.00\n0.01\n0.03\nBimodal\n500\n52.39\n8.94\n2.32\n0.15\n1000\n53.60\n8.98\n1.85\n0.12\nwell as it does not rely on the existence of error moments.\nA second simulation study was done using two predictors, one measured with error and one\nwithout. Data were simulated according to the model Yi = \u03b20 + \u03b2XXi + \u03b2ZZi + \u03f5i, Wi = Xi + Ui,\ni = 1, . . . , n with parameters \u03b20 = 0, \u03b2X = 3, and \u03b2Z = 2. Here, X is the error-prone covariate\nwhile Z is error-free. Samples sizes n \u2208{1000, 2000} were considered. We include here results\nfor the two cases X half-normal and X having the bimodal normal mixture de\ufb01ned at the start\nof this section. The covariate Z was generated from the same distribution as X, and a normal\ncopula with \u03c1 = 0.5 was used to generate X and Z to be correlated. The error distributions were\ntaken to be normal with noise-to-signal ratios as in the univariate simulation. For each simulation\ncon\ufb01guration, 2000 replications were run. For each run, the phase function estimators and the naive\nestimators for both \u03b2X and \u03b2Z were computed. Furthermore, simulation-extrapolation of Stefanski\n& Cook (1995) was also implemented using the known measurement error variance. When the\nmeasurement error variance is unknown or not estimable, simulation-extrapolation cannot be used.\nIt is therefore included for comparative purposes. Table 3 reports again the ratio of median square\nerror for the naive and phase function methods relative to the simulation-extrapolation estimators.\nAs before, see Section C.3 of the supplement material for the full simulation results.\nTable 3: Ratio of median square error of estimators relative to the simulation-extrapolation regres-\nsion estimators in the bivariate model simulation.\nTrue X\nn\nNaive\nPhase\n\u03b2X\n\u03b2Z\n\u03b2X\n\u03b2Z\n|N(0, 1)|\n1000\n107.1\n35.4\n15.0\n40.2\n2000\n219.1\n81.2\n2.19\n6.47\nBimodal\n1000\n152.8\n51.9\n10.1\n24.5\n2000\n343.3\n104.7\n9.02\n19.9\nAgain, the poor performance of the naive method in Table 3 is not surprising. The phase func-\ntion method holds up well against simulation-extrapolation . It is clear that the method improves (in\na relative sense) as the sample size increases from 1000 to 2000. Furthermore, the phase function\napproach has large relative median squared errors when (pW, pY ) = (0.25, 0.4), corresponding\nto large measurement error contamination. However, these scenarios also improve, sometimes\ndramatically so, when the same size increases.\n12\nFinally, we performed a simulation study to examine the performance of the (full) bootstrap and\nplug-in bootstrap methods for estimating standard errors of the parameters. Data were simulated\nfrom the univariate model used to generate Table 1. For each simulated sample, both bootstrap\nmethods were used to estimate the standard errors of the model coef\ufb01cients. Reported here are\nthe results for X half-normal and X bimodal, and two sample sizes n \u2208{1000, 2000}. For each\nsimulation con\ufb01guration, 2000 samples were generated. For each, the phase function estimates\nwere computed. A total of B = 100 bootstrap samples were generated for each of the methods\ndescribed in Section 3.2 to estimate standard errors. The true standard error was also estimated\nusing the 1000 pairs \u02c6\u03b20, \u02c6\u03b21 estimated from the simulated data using the phase function methods.\nThe median of \u221an \u00d7 bse, with bse denoting estimated standard error, is reported in Table 4 for each\nmethod.\nWe note in Table 4 that the full bootstrap generally gives estimated standard errors very close\nto the true (Monte Carlo) values. The plug-in method has a tendency to over-estimate the stan-\ndard error, especially for sample size 1000. However, the plug-in method is superior in terms of\ncomputation speed. These computational time comparisons are based on running simulations on a\ndistributed computing system with 80 nodes consisting of 36 cores each with 256GB memory and\nwith an Intel Xeon E5-2695 v4 CPU. For sample size n = 1000, the average computation time for\nthe full bootstrap around 34 minutes, while the plug-in bootstrap had an average computation time\nof around 1 minute. Similarly, for sample size n = 2000 the full and plug-in average computation\ntimes were around 49 minutes and 2 minutes, respectively. In many instances, one might be will-\ning to use a method that over-estimates the size of the standard error for this type of speed-up in\ncomputation.\nTable 4: True standard error (Monte Carlo) and median of estimated standard error, scaled by the\nsquare root of the sample size, using two different bootstrap approaches.\nTrue X\nn\nMonte Carlo\nBootstrapfull\nBootstrapplug\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n|N(0, 1)|\n1000\n0.48\n0.56\n0.48\n0.55\n0.56\n0.71\n2000\n0.39\n0.46\n0.40\n0.46\n0.42\n0.53\nBimodal\n1000\n2.82\n0.75\n2.95\n0.79\n5.27\n1.36\n2000\n2.26\n0.60\n2.22\n0.59\n2.90\n0.75\n5\nAir Quality Data Examples\nHere, we consider a dataset analyzed by De Vito et al. (2008) considering the measurement of\ncarbon monoxide (CO) levels in present in an urban environment over time. The dataset is publicly\navailable online at the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets.html)\nand is labeled Air Quality. In the experiment reported, a low-cost gas multi-censor device, also\nknown as an electronic nose, was used to monitor atmospheric pollutants in an urban environment.\nCarbon monoxide was one of the pollutants being monitored and is of primary interest in our anal-\nysis. Measurements obtained by electronic noses use tin oxide as a proxy for carbon monoxide.\nThese devices are also subject to measurement error, especially when compared to networks of\n13\n0\n5\n10\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\n10\n15\n20\nFigure 1: Time series plots for carbon monoxide Yt (top) and the average sensor output Wt (bot-\ntom).\nspatially distributed \ufb01xed stations using industrial spectromoters. The latter are commonly used\nto monitor air pollution in urban environments, but use is restricted by cost and size considera-\ntions. The sources of measurement error for electronic noses range from known device stability\nissues to local atmospheric dynamics. Even so, it is desirable to consider the proper calibration\nof electronic noses for supplemental use in monitoring air pollution in urban areas. Speci\ufb01cally,\nwe consider estimating the true relationship between tin oxide (subject to measurement error) and\ncarbon monoxide.\nThe experiment, which lasted 13 months, was performed at a main road with heavy traf\ufb01c in\nan Italian city. During this period, hourly observations were collected from both an electronic nose\n(W data) and a distributed network of seven \ufb01xed stations (Y data). The measurements represent\nhourly averages of data collected at 8 second increments. The data are denoted (Wt, Yt), t =\n1, . . . , T, with T = 9357 hourly periods transpiring during the experiment. However, 2013 of these\nhave partially or completely missing data, leaving 7344 complete observations for the analysis.\nTime series plots of the measurements are shown in Figure 1. Note that the W measurements in\nthe \ufb01gure and throughout the analysis are equal to the original data divided by 100.\nTo account for time-of-day effects on pollution levels, the data were de-trended. To this end,\nlet Ik = {t : t = k + 24(j \u22121), j = 1, 2, . . . and t \u2264T} with k = 1, . . . , 24 denote the\ncollection of indices corresponding to measurements at hour k. De\ufb01ne observed hourly mean\n\u02c6\u00b5k = |Ik|\u22121 P\nt\u2208Ik Wt for k = 1, . . . , 24. The expression for \u02c6\u00b5k makes use of a slight abuse of\nnotation, as the sum is only taken over indices corresponding to non-missing observations. The\n14\nde-trended data are calculated as\n\u02dcWt = Wt \u2212\n24\nX\nk=1\nI{t \u2208Ik}\u02c6\u00b5k, t = 1, . . . , T.\nThe de-trended \u02dcYt are de\ufb01ned in an analogous manner, resulting in pairs ( \u02dcWt, \u02dcYt), t = 1, . . . , T.\nIt is now assumed that \u02dcYt = \u03b21Xt + \u03b5t and \u02dcWt = Xt + Ut with Xt denoting the true CO level at\ntime t. Note the lack of intercept term \u03b20 in the model. This is a result of de-trending the data. We\nassume that the error components Ut and \u03b5t are independent, and that these error components are\nindependent of stationary time series Xt. All variables are assumed to have \ufb01nite variance. The\nstationarity of Xt is important as this ensures that the empirical phase functions still a consistent\nestimate \u03c1X.\nThe generalized method of moments and phase function estimators of slope \u03b21 were calculated.\nTo account for the correlation structure in Xt, the block bootstrap with block length L = 192 was\nused to estimate the associated standard errors, see Kunsch (1989) for details on this technique.\nThe generalized method of moments estimator is \u02c6\u03b21\n(GMM) = 0.73 with estimated standard error\n0.07. The phase function estimator is \u02c6\u03b21\n(phase) = 0.71 with estimated standard error 0.02. The\nnaive estimator of slope is 0.52, indicating the strong effect of measurement error here. Comparing\nthe phase function and naive estimators of slope using the known attenuation relationship suggests\nthe proportion of error variance is 0.36. Moreover, the generalized method of moments and phase\nfunction estimates seemingly correct for the exogenous contamination present in the electronic\nnose measurements. While the two estimators are close to one another, the standard error of\ngeneralized method of moments is substantially larger than that of the phase function estimator.\n6\nConclusion\nThe proposed phase function methodology is a new solution to the linear errors-in-variables prob-\nlem where replicate data and/or prior knowledge of measurement error variance are not available.\nContamination of the observed features should not be ignored when making an inference, but\nstrong model requirements can make it dif\ufb01cult to appropriately correct the error and leave the\npractitioner with a biased model. To our knowledge, the only solution not making such strict as-\nsumptions is the generalized method of moments. Our proposed method is seen to be competitive\nwith generalized method of moments, and often has much smaller median squared error. Further-\nmore, the phase function-based method does not rely on the existence of an arbitrary number of\nmoments. Future work will consider combining the strengths of the generalized method of mo-\nments and phase function methods: generalized method of moments can be implemented when\nthe underlying variable has a symmetric distribution, whereas the phase function method requires\nasymmetry of the underlying distribution.\nReferences\nCARROLL, R. J., RUPPERT, D., STEFANSKI, L. A. & CRAINICEANU, C. M. (2006). Measure-\nment error in nonlinear models: a modern perspective. CRC press.\n15\nCARROLL, R. J. & STEFANSKI, L. A. (1990). Approximate quasi-likelihood estimation in models\nwith surrogate predictors. Journal of the American Statistical Association 85, 652\u2013663.\nDE VITO, S., MASSERA, E., PIGA, M., MARTINOTTO, L. & DI FRANCIA, G. (2008). On\n\ufb01eld calibration of an electronic nose for benzene estimation in an urban pollution monitoring\nscenario. Sensors and Actuators B: Chemical 129, 750\u2013757.\nDELAIGLE, A. & HALL, P. (2016). Methodology for non-parametric deconvolution when the\nerror distribution is unknown. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology) 78, 231\u2013252.\nDEMPSTER, A. P., LAIRD, N. M. & RUBIN, D. B. (1977). Maximum likelihood from incomplete\ndata via the em algorithm. Journal of the royal statistical society. Series B (methodological) ,\n1\u201338.\nERICKSON, T., JIANG, C. H. & WHITED, T. M. (2014). Minimum distance estimation of the\nerrors-in-variables model using linear cumulant equations. Journal of Econometrics 183, 211\u2013\n221.\nERICKSON, T. & WHITED, T. M. (2002). Two-step gmm estimation of the errors-in-variables\nmodel using high-order moments. Econometric Theory 18, 776\u2013799.\nFEUERVERGER, A., MUREIKA, R. A. et al. (1977). The empirical characteristic function and its\napplications. The annals of Statistics 5, 88\u201397.\nFULLER, W. A. (2009). Measurement error models, vol. 305. John Wiley & Sons.\nGILLARD, J. (2014).\nMethod of moments estimation in linear regression with errors in both\nvariables. Communications in Statistics-Theory and Methods 43, 3208\u20133222.\nHIGDON, R. & SCHAFER, D. W. (2001). Maximum likelihood computations for regression with\nmeasurement error. Computational statistics & data analysis 35, 283\u2013299.\nKUNSCH, H. R. (1989). The jackknife and the bootstrap for general stationary observations. The\nannals of Statistics , 1217\u20131241.\nLOMBARD, F. (2005). Nonparametric con\ufb01dence bands for a quantile comparison function. Tech-\nnometrics 47, 364\u2013371.\nNGHIEM, L. & POTGIETER, C. J. (2018). Density estimation in the presence of heteroscedastic\nmeasurement error of unknown type using phase function deconvolution. Statistics in medicine\n.\nREIERS\u00d8L, O. (1941). Con\ufb02uence analysis by means of lag moments and other methods of con-\n\ufb02uence analysis. Econometrica: Journal of the Econometric Society , 1\u201324.\nREIERS\u00d8L, O. (1950). Identi\ufb01ability of a linear relation between variables which are subject to\nerror. Econometrica: Journal of the Econometric Society , 375\u2013389.\n16\nSCHAFER, D. W. & PURDY, K. G. (1996). Likelihood analysis for errors-in-variables regression\nwith replicate measurements. Biometrika 83, 813\u2013824.\nSTEFANSKI, L. A. & CARROLL, R. J. (1987). Conditional scores and optimal scores for general-\nized linear measurement-error models. Biometrika 74, 703\u2013716.\nSTEFANSKI, L. A. & COOK, J. R. (1995). Simulation-extrapolation: the measurement error\njackknife. Journal of the American Statistical Association 90, 1247\u20131256.\nVAN DER VAART, A. W. (2000). Asymptotic statistics, vol. 3. Cambridge university press.\nWALD, A. (1940). The \ufb01tting of straight lines if both variables are subject to error. The Annals of\nMathematical Statistics 11, 284\u2013300.\nYANCEY, H., GEER, M. & PRICE, J. (1951). An investigation of the abrasiveness of coal and its\nassociated impurities. Mining Engineering 3, 262\u2013268.\n17\nSupplement Material\nA\nTechnical Results\nA.1\nProof of Lemma 1\nRecall that W = X + U with X is independent of U. By independence, the characteristic function\nof W is given by\n\u03c6W(t) = E(eitW) = E(eit(X+U)) = E(eitX)E(eitU) = \u03c6X(t)\u03c6U(t).\nBy assumption, the characteristic function of U satis\ufb01es \u03c6U(t) = |\u03c6U(t)| for t. Thus, the phase\nfunction for W is\n\u03c1W(t) = \u03c6W(t)\n|\u03c6W(t)| =\n\u03c6X(t)\u03c6U(t)\n|\u03c6X(t)||\u03c6U(t)| = \u03c6X(t)\n|\u03c6X(t)| = \u03c1X(t).\nSubsequently, the random variables W and X have the same phase function.\nSimilarly, random variable Y = \u03b20 + \u03b21X + \u03b5 has characteristic function\n\u03c6Y (t) = eit\u03b20\u03c6X(t\u03b21)\u03c6\u03f5(t).\nwith phase function given by\n\u03c1Y (t) =\neit\u03b20\u03c6X(t\u03b21)\u03c6\u03f5(t)\n|eit\u03b20||\u03c6X(t\u03b21)||\u03c6\u03f5(t)| = eit\u03b20\u03c1X(t\u03b21) = eit\u03b20\u03c1W(t\u03b21),\n(15)\nestablishing the relationship between the phase functions of W and Y .\nA.2\nProof of Lemma 3\nFor any complex number z, let R(z) = Im(z)/Re(z) denote the ratio of the imaginary and real\nparts of z. Now, consider the relationship that exists between the phase functions of Y and W as\ngiven in (15), and recall that any phase function has norm equal to 1 for all t. It follows that (15)\nis equivalent to\nR[\u03c1Y (t)] = R[exp(it\u03b20)]R[\u03c1W(\u03b21t)].\n(16)\nFurthermore, as the phase function is a scaled version of the characteristic function, R[\u03c1Y (t)] =\nR[\u03c6Y (t)] and (16) is equivalent to\nR[\u03c6Y (t)] = R[exp(it\u03b20)]R[\u03c6W(\u03b21t)].\nBy Euler\u2019s formula, this can be written as\nE [sin (tY )]\nE [cos (tY )] = E [sin (t (\u03b20 + \u03b21W))]\nE [cos (t (\u03b20 + \u03b21W))].\n(17)\nTherefore, minimizing the statistics D in equation (3) of the main paper is equivalent to minimizing\nD(b0, b1) =\nZ \u221e\n\u2212\u221e\n Pn\nj=1 sin(tYj)\nPn\nj=1 cos(tYj) \u2212\nPn\nj=1 sin (t (b0 + b1Wj))\nPn\nj=1 cos (t (b0 + b1Wj))\n!2\nw(t)dt.\n(18)\nIf choosing the weight function as stated in the lemma, the integrand is an even function with\nrespect to t. Then the result follows from simplifying the resulting trigonometric products.\n18\nA.3\nExpressions for the second derivative of D(b0, b1)\nIn Section 3.2 of the main paper, a plug-in bootstrap method is proposed for estimating the stan-\ndard errors of the phase function estimators. Evaluation there requires calculation of the second\nderivatives of the distance metric D evaluated at the estimators \u02c6\u03b20 and \u02c6\u03b21. These functions are\nreported here.\nSpeci\ufb01cally,\n\u22022D\n\u2202b2\n0\n=\nX\ni\nX\nj\nX\nk\nX\nl\nZ t\u2217\n0\n2t2Kt\u2217(t)\n\"\ncos {t(Yi \u2212b0 \u2212b1Wj)} cos {t(Yl \u2212b0 \u2212b1Wk)}\n\u2212sin {t(Yi \u2212b0 \u2212b1Wj)} sin {t(Yl \u2212b0 \u2212b1Wk)}\n#\ndt,\n\u22022D\n\u2202b0\u2202b1\n=\nX\ni\nX\nj\nX\nk\nX\nl\nZ t\u2217\n0\n2t2Kt\u2217(t)\n\"\nWj cos {t(Yi \u2212b0 \u2212b1Wj)} cos {t(Yl \u2212b0 \u2212b1Wk)}\n\u2212Wk sin {t(Yi \u2212b0 \u2212b1Wj)} sin {t(Yl \u2212b0 \u2212b1Wk)}\n#\ndt,\nand\n\u22022D\n\u2202b2\n1\n=\nX\ni\nX\nj\nX\nk\nX\nl\nZ t\u2217\n0\n2t2Kt\u2217(t)\n\"\nWjWk cos {t(Yi \u2212b0 \u2212b1Wj)} cos {t(Yl \u2212b0 \u2212b1Wk)}\n\u2212W 2\nk sin {t(Yi \u2212b0 \u2212b1Wj)} sin {t(Yl \u2212b0 \u2212b1Wk)}\n#\ndt.\nThe quadruple sums can be eliminated using some simple but tedious algebra, giving expres-\nsions that are computationally convenient,\n\u22022D\n\u2202b2\n0\n=\nZ t\u2217\n0\n2t2Kt\u2217(t)\n\"X\ni\nX\nj\ncos {t(Yi \u2212b0 \u2212b1Wj)}\n#2\ndt\n\u2212\nZ t\u2217\n0\n2t2Kt\u2217(t)\n\"X\ni\nX\nj\nsin {t(Yi \u2212b0 \u2212b1Wj)}\n#2\ndt,\n\u22022D\n\u2202b0\u2202b1\n=\nZ t\u2217\n0\n2t2Kt\u2217(t)\n\"X\ni\nX\nj\ncos {t(Yi \u2212b0 \u2212b1Wj)}\n# \"X\ni\nX\nj\nWj cos {t(Yi \u2212b0 \u2212b1Wj)}\n#\ndt\n\u2212\nZ t\u2217\n0\n2t2Kt\u2217(t)\n\"X\ni\nX\nj\nsin {t(Yi \u2212b0 \u2212b1Wj)}\n# \"X\ni\nX\nj\nWj sin {t(Yi \u2212b0 \u2212b1Wj)}\n#\ndt,\n19\nand\n\u22022D\n\u2202b2\n1\n=\nZ t\u2217\n0\n2t2Kt\u2217(t)\n\"X\ni\nX\nj\nWj cos {t(Yi \u2212b0 \u2212b1Wj)}\n#2\ndt\n\u2212\nZ t\u2217\n0\n2t2Kt\u2217(t)\n\"X\ni\nX\nj\nsin {t(Yi \u2212b0 \u2212b1Wj)}\n# \"X\ni\nX\nj\nW 2\nj sin {t(Yi \u2212b0 \u2212b1Wj)}\n#\ndt.\nThese expressions can be used to calculate the matrix \u02c6B required for the bootstrap plug-in method\nfor standard error estimation.\nB\nA brief review of the Generalized Method of Moments\nIn this section, we provide a brief overview of the generalized method of moments (GMM) ap-\nproach to linear errors-in-variables models. GMM is a popular approach to estimating the param-\neters of linear EIV models. Recall the model\nYi = \u03b20 + \u03b21Xi + \u03b5i\nand\nWi = Xi + Ui,\ni = 1, . . . , n. In this model, the parameters \u03b20 and \u03b21 are identi\ufb01able using moments of W and\nY up to order 3, provided E[(X \u2212\u00b5X)3] \u0338= 0. Similarly, the parameters are identi\ufb01able using\nmoments up to order 4 provided the distributions of X, U, and \u03b5 are not all Gaussian. We brie\ufb02y\nreview implementation of GMM here. Our approach is similar to that proposed by Erickson &\nWhited (2002). GMM is a viable nonparametric alternative to the phase function approach, in that\nno parametric model assumptions are required for implementation.\nFor GMM using sample moments up to order K, it is assumed that each of the variables X, U,\nand \u03b5 has at least 2K \ufb01nite moments. Furthermore, it is assumed that U and \u03b5 have distributions\nsymmetric about 0, E\n\u0002\nU 2k\u22121\u0003\n= E\n\u0002\n\u03b52k\u22121\u0003\n= 0 for k = 1, 2, . . . , K. Note that the use of the \ufb01rst\nK moments requires that the underlying distributions have 2K moments for the estimators derived\nhere to be asymptotically normally distributed with \ufb01nite variance.\nLet \u00b5X denote the mean of X, and let \u03c32\nX, \u03c32\nU, and \u03c32\n\u03b5 denote the respective variances of\nX, U, and \u03b5. Additionally, let \u00b5X,j = E [(X \u2212\u00b5X)j] denote the jth centered moment of X,\nj = 3, . . . , 2K, with equivalent notation holding for \u00b5U,j and \u00b5\u03b5,j. Finally, for the pair of random\nvariables (W, Y ), let \u03bdj,k denote the joint centered moments,\n\u03bdj,k = E\nh\n(W \u2212\u00b5X)j (Y \u2212\u03b20 \u2212\u03b21\u00b5X)ki\n.\n(19)\nDue to the independence of X, U, and \u03b5, the joint moment \u03bdj,k can be expressed in terms of the\nmarginal moments of X, U, and \u03b5 up to order j + k. Making a few special cases explicit, note that\n\u03bd2,0 = \u03c32\nX + \u03c32\nU, \u03bd1,1 = \u03b21\u03c32\nX, and \u03bd0,2 = \u03b22\n1\u03c32\nX + \u03c32\n\u03b5.\nNow, let \u03b8(1) = {\u00b5X, \u03b20, \u03b21} and \u03b8(2) = {\u03c32\nX, \u03c32\nU, \u03c32\n\u03b5}, and let \u03b8(2j\u22121) = {\u00b5X,2j\u22121} and\n\u03b8(2j) = {\u00b5X,2j, \u00b5U,2j, \u00b5\u03b5,2j}, j = 2, . . . , \u230aK/2\u230b, denote the higher-order moments. Finally, let\n20\n\u03b8K =\n\b\n\u03b8(1), . . . , \u03b8(K)\n\t\ndenote the collection of unknown parameters required to specify a model\nup to order K. The random variables\nAjk (\u03b8) = n\u22121/2\nn\nX\ni=1\nn\n(Wi \u2212\u00b5X)j (Yi \u2212\u03b20 \u2212\u03b21\u00b5X)k \u2212\u03bdjk\no\nhave E [Ajk] = 0 and Cov [Ajk, Aj\u2032,k\u2032] = \u03bdj+j\u2032,k+k\u2032 \u2212\u03bdjk\u03bdj\u2032,k\u2032 for j +j\u2032 +k +k\u2032 \u22642K. A such, the\nAj,k can be used to construct GMM estimators of the parameters. Speci\ufb01cally, let AK(\u03b8K) denote\nthe vector consisting of all terms Ajk with j, k = 0, . . . , K and 1 \u2264j + k \u2264K. Now, de\ufb01ne \u03a3K\nto be the covariance matrix corresponding to vector AK. This covariance matrix can be estimated\nempirically by de\ufb01ning joint sample moments\n\u02c6\u03bdj,k = 1\nn\nn\nX\ni=1\n\u0000Wi \u2212\u00afW\n\u0001j \u0000Yi \u2212\u00afY\n\u0001k\nand subsequently letting\nd\nCov [Ajk, Aj\u2032,k\u2032] = \u02c6\u03bdj+j\u2032,k+k\u2032 \u2212\u02c6\u03bdjk\u02c6\u03bdj\u2032,k\u2032,\nj + j\u2032 + k + k\u2032 \u22642K.\nLet \u02c6\u03a3K denote this estimated covariance matrix. The GMM parameter estimates are then found\nby minimizing the quadratic form\nGK(\u03b8K) = AK(\u03b8K)\u22a4\u02c6\u03a3\u22121\nK AK(\u03b8K).\n(20)\nNote that the implementation of the GMM approach requires the use of K \u22653, as the choices\nK = 1, 2 result in an overidenti\ufb01ed system in terms of the parameters in \u03b8K.\nC\nAdditional Simulation Results\nC.1\nThe effect of weighting function\nThe simulation study in Section 5.1 (univariate EIV model) of the main paper explore three dif-\nferent choice of weighting function in calculating the phase function estimator: K1(t) = (1 \u2212\n|t|)2I(|t| \u22641), K2(t) = (1 \u2212|t|)I(|t| \u22641), K3(t) = (1 \u2212t2)I(|t| \u22641). Table C.1 presents\nthe median SE of phase function estimates for these three weight function choices for a subset of\nsimulation settings with X \u223c|N(0, 1)| or X \u223cexp(1), and (pW, pY ) = (0.25, 0.40). The results\nfor simulation con\ufb01gurations not reported in Table C.1 follow the same general patterns.\n21\nTable C.1: Median square error and the corresponding interquartile range, scaled by the sample\nsize, for the phase function estimators with weighting functions K1(t), K2(t), and K3(t).\nTrue X\nn\nError\nK1(t)\nK2(t)\nK3(t)\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n|N(0, 1)|\n500\nNormal\n3.37\n4.41\n3.47\n4.79\n3.4\n4.74\n(0.14)\n(0.18)\n(0.14)\n(0.19)\n(0.14)\n(0.19)\nLaplace\n1.9\n3.19\n1.83\n3.07\n1.84\n3.04\n(0.09)\n(0.14)\n(0.08)\n(0.13)\n(0.08)\n(0.13)\n1000\nNormal\n3.21\n4.38\n3.31\n4.6\n3.26\n4.44\n(0.14)\n(0.18)\n(0.14)\n(0.2)\n(0.14)\n(0.19)\nLaplace\n1.65\n3.01\n1.62\n2.88\n1.62\n2.89\n(0.08)\n(0.13)\n(0.08)\n(0.12)\n(0.08)\n(0.12)\nexp(1)\n500\nNormal\n4.75\n4.28\n5.08\n5.07\n4.94\n4.72\n(0.21)\n(0.2)\n(0.23)\n(0.24)\n(0.23)\n(0.23)\nLaplace\n2.5\n3.63\n2.59\n3.89\n2.52\n3.63\n(0.11)\n(0.16)\n(0.12)\n(0.18)\n(0.11)\n(0.17)\n1000\nNormal\n5.55\n4.97\n5.92\n6.04\n5.86\n5.49\n(0.24)\n(0.23)\n(0.26)\n(0.27)\n(0.25)\n(0.25)\nLaplace\n2.62\n3.2\n2.56\n3.68\n2.56\n3.29\n(0.11)\n(0.15)\n(0.12)\n(0.17)\n(0.11)\n(0.15)\nAs can be seen in Table C.1, the choice of weights function does not have a major impact on\nthe quality of the estimators when using median square error as criterion. However, the choice\nof weight function K1(t) = (1 \u2212|t|)2I(|t| \u22641) most often results in the lowest median square\nerror for both \u03b20 and \u03b21. As such, the phase function-based estimators are compared to the other\nmethods of estimation for this choice of weight function.\nC.2\nFull Simulation Results for Univariate EIV model\nIn this section, we present the full results for the simulation studies in the simple EIV setting\nin Section 5.1 of the main paper.\nData were generated according to the model Yi = \u03b20 +\n\u03b21Xi + \u03b5i and Wi = Xi + Ui, i = 1, . . . , n with true parameters (\u03b20, \u03b21) = (1, 3). Three\nasymmetric distributions were used to simulate X, namely (1) a half-normal distribution, X \u223c\n|N(0, 1)|, (2) an exponential distribution, X \u223cexp(1), and (3) a bimodal mixture distribu-\ntion, X \u223c0.5N(5, 12) + 0.5N(2.5, 0.62). Three different distributions were considered for er-\nror components U and \u03b5, namely the normal, t-distribution with 2.5 degrees of freedom, and the\nCauchy distribution. For the Normal and t2.5 distributions, the error components were simulated\nto have mean 0 and respective variances \u03c32\nU and \u03c32\n\u03b5. For the Cauchy distribution, the error com-\nponents were simulated to be symmetric about 0 and have respective interquartile range (IQR)\n\u03c3U and \u03c3\u03b5. The variance and IQR parameters were chosen to achieve speci\ufb01c noise-to-signal ra-\ntios, pW = \u03c32\nU/\u03c32\nX and pY = \u03c32\n\u03b5/(\u03b21\u03c3X)2. The noise-to-signal ratios pairs reported here are\n(pW, pY ) \u2208{(0.075, 0.15), (0.25, 0.40)}. Results are reported for sample sizes n \u2208{500, 1000}.\nFor each con\ufb01guration, the median square error of each estimator is reported with the correspond-\ning interquartile range.\n22\nTable C.2: Median square errors of estimators and the corresponding interquartile range (in paren-\ntheses), scaled by the sample size, in the univariate regression simulation when the true distribution\nof X is half-normal.\nn\nError\n(pW, pY )\nNaive\nGMM\nDisattenuation\nPhase function\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n500\nNormal\n(0.075,0.15)\n14.16\n21.94\n1.17\n1.58\n0.5\n0.53\n0.95\n1.27\n(0.16)\n(0.21)\n(0.05)\n(0.07)\n(0.02)\n(0.02)\n(0.04)\n(0.05)\n(0.25,0.40)\n114.89\n180.01\n3.69\n5.12\n1.57\n1.82\n3.37\n4.41\n(0.73)\n(0.97)\n(0.18)\n(0.24)\n(0.07)\n(0.09)\n(0.14)\n(0.18)\nt2.5\n(0.075,0.15)\n8.45\n13.18\n0.78\n1.11\n1.45\n2.16\n0.59\n1.01\n(0.16)\n(0.22)\n(0.03)\n(0.05)\n(0.06)\n(0.07)\n(0.03)\n(0.04)\n(0.25,0.40)\n71.86\n112.03\n3.67\n5.34\n12.2\n18.13\n1.9\n3.19\n(0.95)\n(1.39)\n(0.2)\n(0.28)\n(0.38)\n(0.55)\n(0.09)\n(0.14)\n1000\nNormal\n(0.075,0.15)\n28.39\n44.16\n1.17\n1.62\n0.53\n0.56\n0.94\n1.21\n(0.24)\n(0.3)\n(0.06)\n(0.08)\n(0.02)\n(0.02)\n(0.04)\n(0.06)\n(0.25,0.40)\n229.6\n358.97\n4.45\n5.97\n1.75\n1.89\n3.21\n4.38\n(1.1)\n(1.39)\n(0.19)\n(0.24)\n(0.08)\n(0.08)\n(0.14)\n(0.18)\nt2.5\n(0.075,0.15)\n18.54\n29.12\n0.98\n1.31\n2.05\n3.08\n0.63\n0.99\n(0.27)\n(0.39)\n(0.05)\n(0.06)\n(0.08)\n(0.1)\n(0.03)\n(0.04)\n(0.25,0.40)\n154.67\n243.3\n5.04\n7.44\n17.67\n26.1\n1.65\n3.01\n(1.67)\n(2.52)\n(0.3)\n(0.47)\n(0.52)\n(0.79)\n(0.08)\n(0.13)\n23\nTable C.3: Median square errors of estimators and the corresponding interquartile range (in paren-\ntheses), scaled by the sample size, in the univariate regression simulation when the true distribution\nof X is exponential.\nn\nError\nNSR\nNaive\nGMM\nDisattenuation\nPhase function\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n500\nNormal\n(0.075,0.15)\n22.25\n22.4\n1.46\n0.95\n1.2\n0.67\n1.62\n1.69\n(0.31)\n(0.24)\n(0.07)\n(0.05)\n(0.05)\n(0.03)\n(0.07)\n(0.07)\n(0.25,0.40)\n178.85\n181.39\n4.77\n3.08\n4.04\n2.69\n4.75\n4.28\n(1.33)\n(1.17)\n(0.21)\n(0.15)\n(0.17)\n(0.11)\n(0.21)\n(0.2)\nt2.5\n(0.075,0.15)\n13.19\n13.39\n0.95\n0.58\n2.57\n2.3\n0.88\n1.07\n(0.29)\n(0.24)\n(0.04)\n(0.03)\n(0.1)\n(0.08)\n(0.04)\n(0.05)\n(0.25,0.40)\n117.47\n113.1\n2.97\n2.26\n17.69\n19.25\n2.5\n3.63\n(1.69)\n(1.59)\n(0.14)\n(0.11)\n(0.65)\n(0.58)\n(0.11)\n(0.16)\n1000\nNormal\n(0.075,0.15)\n43.83\n44.13\n1.55\n1.07\n1.16\n0.67\n1.74\n1.72\n(0.44)\n(0.34)\n(0.07)\n(0.05)\n(0.05)\n(0.03)\n(0.08)\n(0.07)\n(0.25,0.40)\n363.62\n362.91\n5.18\n3.48\n4.08\n2.8\n5.55\n4.97\n(1.99)\n(1.68)\n(0.23)\n(0.15)\n(0.19)\n(0.12)\n(0.24)\n(0.23)\nt2.5\n(0.075,0.15)\n29.74\n28.94\n1.05\n0.65\n3.28\n2.96\n0.92\n1.27\n(0.47)\n(0.41)\n(0.05)\n(0.04)\n(0.13)\n(0.11)\n(0.04)\n(0.06)\n(0.25,0.40)\n249.07\n242.99\n4.28\n3.06\n27.39\n27.79\n2.62\n3.2\n(2.89)\n(2.67)\n(0.19)\n(0.15)\n(0.93)\n(0.86)\n(0.11)\n(0.15)\n24\nTable C.4: Median square errors of estimators and the corresponding interquartile range (in paren-\ntheses), scaled by the sample size,in the univariate regression simulation when the true distribution\nof X is a mixture of normal distributions.\nn\nError\nNSR\nNaive\nGMM\nDisattenuation\nPhase function\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n500\nNormal\n(0.075,0.15)\n306.68\n21.62\n63.48\n4.74\n9.16\n0.53\n23.56\n1.61\n(3.31)\n(0.21)\n(2.52)\n(0.17)\n(0.39)\n(0.02)\n(1.14)\n(0.08)\n(0.25,0.40)\n2528.79\n178.52\n254.05\n17.4\n25.12\n1.51\n143.37\n9.76\n(13.87)\n(0.87)\n(11.5)\n(0.8)\n(1.14)\n(0.07)\n(6.73)\n(0.48)\nt2.5\n(0.075,0.15)\n192.04\n13.71\n43.9\n2.97\n28.71\n2.09\n15.48\n0.96\n(3.21)\n(0.22)\n(1.45)\n(0.1)\n(1.05)\n(0.08)\n(0.71)\n(0.05)\n(0.25,0.40)\n1572.13\n111.86\n314.14\n22.16\n249.82\n18.35\n67.16\n4.39\n(20.39)\n(1.37)\n(10.69)\n(0.74)\n(7.76)\n(0.55)\n(3.43)\n(0.23)\n1000\nNormal\n(0.075,0.15)\n616.32\n43.87\n71.47\n4.99\n9.1\n0.55\n20.96\n1.41\n(4.67)\n(0.3)\n(3.29)\n(0.23)\n(0.38)\n(0.02)\n(0.97)\n(0.06)\n(0.25,0.40)\n5047.12\n361.45\n245.51\n17.2\n25.21\n1.54\n101.42\n7.29\n(19.57)\n(1.24)\n(10.93)\n(0.75)\n(1.1)\n(0.07)\n(4.91)\n(0.35)\nt2.5\n(0.075,0.15)\n403.52\n28.6\n77.56\n5.38\n44.06\n3.11\n14.57\n0.94\n(5.49)\n(0.39)\n(2.61)\n(0.18)\n(1.57)\n(0.11)\n(0.64)\n(0.04)\n(0.25,0.40)\n3474.56\n246.26\n546.55\n38.33\n373.97\n27.02\n45.81\n3.05\n(35.62)\n(2.5)\n(19.73)\n(1.37)\n(11.8)\n(0.82)\n(2.52)\n(0.16)\n25\nTable C.5: Median square error and interquartile range of the GMM and phase function estimators\nin the univariate regression simulation when model errors are Cauchy\nTrue X\nn\n(pW, pY )\nGMM\nPhase function\n\u03b20\n\u03b21\n\u03b20\n\u03b21\n|N(0, 1)|\n500\n(0.075,0.15)\n4.04\n8.97\n0.02\n0.04\n(0.07)\n(0.07)\n(0.00)\n(0.00)\n(0.25,0.40)\n4.44\n8.99\n0.05\n0.1\n(0.07)\n(0.02)\n(0.00)\n(0.00)\n1000\n(0.075,0.15)\n4.5\n9\n0.01\n0.02\n(0.07)\n(0.03)\n(0.00)\n(0.00)\n(0.25,0.40)\n4.42\n9\n0.02\n0.04\n(0.07)\n(0.01)\n(0.00)\n(0.00)\nexp(1)\n500\n(0.075,0.15)\n4.09\n8.92\n0.01\n0.02\n(0.11)\n(0.12)\n(0.00)\n(0.00)\n(0.25,0.40)\n5.43\n8.99\n0.02\n0.05\n(0.12)\n(0.05)\n(0.00)\n(0.00)\n1000\n(0.075,0.15)\n5.2\n8.98\n0\n0.01\n(0.12)\n(0.08)\n(0.00)\n(0.00)\n(0.25,0.40)\n6.29\n9\n0.01\n0.02\n(0.12)\n(0.02)\n(0.00)\n(0.00)\nBimodal\n500\n(0.075,0.15)\n19.71\n8.75\n2.12\n0.13\n(1.8)\n(0.13)\n(0.09)\n(0.01)\n(0.25,0.40)\n52.39\n8.94\n2.32\n0.15\n(1.89)\n(0.07)\n(0.11)\n(0.01)\n1000\n(0.075,0.15)\n29.02\n8.93\n1.34\n0.08\n(1.89)\n(0.08)\n(0.06)\n(0.00)\n(0.25,0.40)\n53.6\n8.98\n1.85\n0.12\n(1.89)\n(0.02)\n(0.08)\n(0.01)\nC.3\nFull Simulation Results for Multiple Regression\nIn this section, we present the full results for the simulation study in the multiple EIV linear model\nsetting in the section 5 of the main paper. Data were simulated according to the model Yi =\n\u03b20 + \u03b2XXi + \u03b2ZZi + \u03f5i, Wi = Xi + Ui, i = 1, . . . , n with parameters \u03b20 = 0, \u03b2X = 3, and \u03b2Z = 2.\nHere, X is the error-prone covariate while Z is error-free. Samples sizes n \u2208{1000, 2000} were\nconsidered. We include here results for the two cases X half-normal and X having the bimodal\nnormal mixture de\ufb01ned in the simple EIV setting. The covariate Z was generated from the same\ndistribution as X, and a normal copula with \u03c1 = 0.5 was used to generate X and Z to be correlated.\nThe error distributions were taken to be normal and Laplace with noise-to-signal ratios as in the\nsimple EIV model. For each simulation con\ufb01guration, 2000 replications were run. Table C.6 and\nC.7 presents the median square error for the naive, phase function, and SIMEX estimator with their\ncorresponding interquartile ranges.\n26\nTable C.6: Median square error and interquartile range (in parentheses), scaled by the sample size\nfor the estimators in the multivariate regression simulation when X and Z are half-normal and\ncorrelated with correlation \u03c1 = .5.\nn\nError\n(pW, pY )\nNaive\nPhase function\nSIMEX\n\u03b2X\n\u03b2Z\n\u03b2X\n\u03b2Z\n\u03b2X\n\u03b2Z\n1000\nNormal\n(0.075,0.15)\n715.69\n167.7\n7.34\n9.96\n4.19\n2.78\n(1.92)\n(1.01)\n(0.32)\n(0.46)\n(0.18)\n(0.13)\n(0.25,0.40)\n2349.55\n540.65\n328.67\n613.13\n21.94\n15.26\n(4.31)\n(2.57)\n(148.75)\n(71.23)\n(0.97)\n(0.66)\nLaplace\n(0.075,0.15)\n705.18\n164.94\n7.75\n12.76\n5.79\n3.75\n(2.15)\n(1.05)\n(0.38)\n(0.58)\n(0.27)\n(0.16)\n(0.25,0.40)\n2329.48\n539.89\n331.13\n658.7\n32.09\n16.73\n(4.73)\n(2.55)\n(148.71)\n(71.53)\n(1.48)\n(0.74)\n2000\nNormal\n(0.075,0.15)\n1417.18\n326.47\n7.13\n9.26\n4.19\n3.1\n(2.73)\n(1.44)\n(0.3)\n(0.43)\n(0.19)\n(0.15)\n(0.25,0.40)\n4691.1\n1089.42\n46.83\n86.77\n21.41\n13.41\n(5.95)\n(3.82)\n(2.86)\n(5.69)\n(0.96)\n(0.61)\nLaplace\n(0.075,0.15)\n1419.04\n326.72\n8.34\n12.18\n6.12\n3.32\n(3.48)\n(1.55)\n(0.36)\n(0.57)\n(0.27)\n(0.16)\n(0.25,0.40)\n4679.33\n1073.46\n51.75\n109.01\n36.64\n17.17\n(7.36)\n(3.86)\n(3.22)\n(7.79)\n(1.51)\n(0.8)\nD\nAdditional Data Examples\nD.1\nAbrasiveness Index Data\nThe data analyzed here was originally considered by Lombard (2005) in the context of estimating\na quantile comparison function from paired data. Observations are pairs (Wj, Yj), j = 1, . . . , 98,\nwhere both Wj and Yj represent measures of the abrasiveness index (AI) of a batch of coal. The\nAI is considered a proxy for the quality of coal, and is used to determine the price of a batch of\ncoal. The Yj measurements were obtained using the YGP method, see Yancey et al. (1951). This\nmethod is widely used, but is costly to implement. The Wj measurements were obtained using a\nsimilar method that is less involved and cheaper to implement. Contracts are typically written in\nterms of the YGP measurements, and it is of interest to determine the relationship between the new\nmethod and the YGP method. Here, we treat both the Wj and Yj data as contaminated versions of\nthe true quality of a batch of coal, denoted Xj. Assume that the linear errors-in-variables structure\nholds, i.e. Wj = Xj + Uj and Yj = \u03b20 + \u03b21Xj + \u03b5j.\nIn Figure 2, we show kernel density estimates using normal reference plug-in bandwidths for\nboth W and Y . Bandwidths selected using unbiased cross-validation were also considered, but\ndid not alter the estimates in a visually discernible way. For the given data, the naive regression\nestimators, GMM estimators, and phase function-based estimators were calculated; the results are\nreported in Table D.1. Also reported are the estimated variance components based on the second\n27\nTable C.7: Median square error and interquartile range (in parentheses), scaled by the sample size,\nfor the estimators in the multivariate regression simulation when X and Z are mixtures of normal\ndistribution and correlated with correlation \u03c1 = .5.\nn\nError\n(pW, pY )\nNaive\nPhase function\nSIMEX\n\u03b2X\n\u03b2Z\n\u03b2X\n\u03b2Z\n\u03b2X\n\u03b2Z\n1000\nNormal\n(0.075,0.15)\n241.69\n56.84\n13.44\n18.65\n1.55\n1.29\n(0.83)\n(0.45)\n(0.56)\n(0.85)\n(0.07)\n(0.06)\n(0.25,0.40)\n1056.68\n249.27\n69.64\n117.72\n6.91\n4.81\n(2.22)\n(1.47)\n(5.55)\n(9.45)\n(0.29)\n(0.23)\nLaplace\n(0.075,0.15)\n239.77\n56.15\n14.74\n24.13\n2.04\n1.62\n(0.96)\n(0.47)\n(0.74)\n(1.23)\n(0.09)\n(0.07)\n(0.25,0.40)\n1056.6\n248.34\n149.4\n250.81\n9.49\n5.65\n(2.86)\n(1.45)\n(19.69)\n(65.43)\n(0.46)\n(0.26)\n2000\nNormal\n(0.075,0.15)\n483.2\n113.82\n10.74\n14.71\n1.77\n1.5\n(1.13)\n(0.63)\n(0.46)\n(0.71)\n(0.07)\n(0.06)\n(0.25,0.40)\n2127.88\n498.96\n55.95\n94.55\n6.2\n4.77\n(3.16)\n(2.02)\n(2.87)\n(4.63)\n(0.26)\n(0.22)\nLaplace\n(0.075,0.15)\n486.34\n113.48\n11.72\n21.03\n2.13\n1.59\n(1.4)\n(0.66)\n(0.62)\n(0.93)\n(0.1)\n(0.07)\n(0.25,0.40)\n2125.8\n499.99\n80.09\n144.17\n9.69\n5.44\n(4.18)\n(2.11)\n(4.47)\n(7.74)\n(0.45)\n(0.25)\nsample moments. Speci\ufb01cally, \u02c6\u03c32\nX = sWY /\u02c6\u03b21, \u02c6\u03c32\nU = max{0, s2\nW \u2212\u02c6\u03c32\nX}, and \u02c6\u03c32\n\u03b5 = max{0, s2\nY \u2212\n\u02c6\u03b22\n1\u02c6\u03c32\nX}, where sWY denotes the sample covariance, and s2\nW and s2\nY denote the sample variances.\nTable D.1: Naive, GMM, and phase function-based estimators of the linear errors-in-variables for\nthe abrasiveness index data.\nMethod\n\u02c6\u03b20\n\u02c6\u03b21\n\u02c6\u03c32\nX\n\u02c6\u03c32\nU\n\u02c6\u03c32\n\u03b5\nNaive\n94.959\n0.511\n709.478\n0\n242.123\nGMM\n14.619\n0.895\n398.862\n279.289\n0\nPhase\n-40.776\n1.157\n313.066\n396.411\n0\nThe results in Table D.1 are striking. As one would expect, the naive estimator of slope is\nshrunk towards 0 when compared to the GMM and phase function estimators of slope. Both GMM\nand the phase function approach suggest that, as seen by the estimates of \u03c32\nU, the new method\nintroduces a large amount of measurement error. On the other hand, the established YGP method\nhas estimated measurement error 0. Due to the small sample size, we are hesitant to conclude\nthat the YGP method is error free. However, the results do suggest that if the YGP method does\nintroduce measurement error, it is small relative to the measurement error introduced by the new\nmethod. Any company considering adoption of the new method for measuring the abrasiveness\nindex should whether the increased measurement error is worth the cost savings of the new method.\nTo assess the variability of the computed estimators, pairwise bootstrap resampling was used.\n28\n140\n160\n180\n200\n220\n240\n260\n280\nAI measurements\n0\n0.01\n0.02\nfW\ngY\nFigure 2: Kernel Density Estimators for W (new method) and Y (YGP method) data\nA total of B = 2000 bootstrap samples were taken. Both GMM and the phase function method is\nprone to outliers in small samples. Subsequently, the interquartile ranges (IQR) of the respective\nbootstrap distributions were used as robust measures of spread. For GMM, IQR\u2217(\u03b20) = 71.167\nand IQR\u2217(\u03b21) = 0.335. For the phase function method, IQR\u2217(\u03b20) = 56.031 and IQR\u2217(\u03b21) =\n0.271. While this suggests that the phase function method gives less variable results, we should\nnote that it is possible to choose a different measure of spread that contradicts this conclusion.\nSpeci\ufb01cally, the difference between the 10th and 90th percentiles of the bootstrap distributions\ngives estimated spread 0.450 and 0.624 for the slope estimators using GMM and the phase function\nmethod respectively. Ultimately, for the data at hand, it is not possible to conclude that one method\nis superior to the other.\nD.2\nAnalysis of OPEN study\nIn this section, the relationships between true dietary intakes and various measurements like biomark-\ners, diary, and self-report instruments are studied. In the National Cancer\u2019s Institute OPEN study,\ntwo indicators of dietary intakes of interest include protein intake and energy intake. For each\nindicator, each intake was measured by a food frequency questionnaire (FFQ), a 24-hour recall\ninterview, and a biomarker. Each measurement is replicated twice. The dataset is used to illustrate\nseveral examples of measurement error modeling in Carroll et al. (2006). The data made avail-\nable on the website of the cited monograph is not the actual data from the OPEN Study, but has\nbeen simulated to have similar properties to the true data. These are n = 223 observations in this\ndataset.\nFor each indicator, the \ufb01tted model is of the form Yi = \u03b20 + \u03b21X1i + \u03b22X2i + \u03b23X3i + \u03b5i,\nand Wjik = Xjik + Ujik, j = 1, 2, 3, i = 1, . . . , n, k = 1, 2 , where Yi is the true amount of\nthe indicator, X1i, X2i and X3i represent the (unobserved) amount of the indicator from biomarker\nmeasurement, FFQ, and interview of the ith subject respectively. If there is no measurement error\nexists, all the values X1i, X2i and X3i would be equal to the value of Yi. However, the observed\n29\ndata Wjik are all different from Yi, showing measurement error exists in all of the measurements.\nThe estimators that are computed include the naive estimator, the simulation-extrapolation\n(SIMEX) estimator, and the phase function estimator. All the estimators are computed based on Yi\nand Wji = 1\n2(Wji1 + Wji2). Note that the SIMEX estimator requires knowledge of the variance\nof the measurement errors, which is possible to estimate in this situation because replication data\nfor each measurement is available. The variance of measurement error associated with Wji was\ncomputed as\n\u02c6\u03c32\nj =\n1\n2n(n \u22121)\nn\nX\ni=1\n(Wji1 \u2212Wji2)2.\nThe phase function estimator was computed by minimizing the statistic\nD =\nZ t\u2217\n0\n n\nX\ni=1\nn\nX\nj=1\nsin [t (Yj \u2212W1i\u03b21i \u2212W2i\u03b22i \u2212W3i\u03b23i)]\n!2\nKt\u2217(t)dt.\nwith K(t) = (1\u2212|t|)2 and t\u2217being the smallest t > 0 such that |\u02c6\u03c6Y (t)| \u2264n\u22121/4. This minimization\nproblem is nonconvex, so the numerical algorithm was started at numerous points around the naive\nestimate. The estimates and its estimated standard error (in parentheses) for both protein and\nenergy intake were given in the Table D.2 and D.3 below. The standard error for the phase function\nand the SIMEX estimates was computed to be the interquartile range (IQR) of the corresponding\nestimates from B = 100 bootstrap samples, while the standard error for the naive was computed\nusing the traditional Fisher information matrix.\nTable D.2: Analysis of simulated OPEN data for protein intake\nMeasurement\nNaive\nSIMEX\nPhase Function\nFFQ\n0.041 (0.022)\n0.194 (0.068)\n-0.072 (0.306)\n24-hour recall\n0.041 (0.022)\n0.051 (0.036)\n0.127 (0.243)\nBiomarker\n0.587 (0.037)\n1.018 (0.138)\n0.836 (0.400)\nTable D.3: Analysis of simulated OPEN data for energy intake\nMeasurement\nNaive\nSIMEX\nPhase Function\nFFQ\n0.006 (0.008)\n0.003 (0.022)\n0.133 (0.148)\n24-hour recall\n0.006 (0.010)\n0.007 (0.037)\n0.226 (0.256)\nBiomarker\n0.932 (0.017)\n0.986 (0.028)\n0.859 (0.268)\nThe results from Table D.2 and D.3 show that for both protein and energy intake, only biomarker\nmeasurements have signi\ufb01cant effect on the true amount. In the case of protein intake, the naive es-\ntimates attentuates the effect of the biomarker considerably, while the SIMEX and phase function\nestimates are able to correct it. In the case of energy intake, the phase function estimate reduces the\nmagnitude of the relationship between biomarker measurement and the true amount. Compared\nto the SIMEX estimate, the phase function estimator has a much higher standard error. This is\nexpected because the SIMEX estimator uses knowledge of the measurement error variances, while\nthe phase function estimator does not.\n30\n"}