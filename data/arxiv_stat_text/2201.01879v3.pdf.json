{"text": "Biostatistics (2024), 0, 0, pp. 1\u2013??\ndoi:10.1093/biostatistics/main\nExponential family measurement error models\nfor single-cell CRISPR screens\nTIMOTHY BARRY\nDept. of Biostatistics, Harvard T.H. Chan School of Public Health, Boston MA\ntbarry@hsph.harvard.edu\nKATHRYN ROEDER\nDept. of Statistics and Data Science, Carnegie Mellon University, Pittsburgh, PA\nEUGENE KATSEVICH\nDept. of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA\nSummary\nCRISPR genome engineering and single-cell RNA sequencing have accelerated biological dis-\ncovery. Single-cell CRISPR screens unite these two technologies, linking genetic perturbations\nin individual cells to changes in gene expression and illuminating regulatory networks underly-\ning diseases. Despite their promise, single-cell CRISPR screens present considerable statistical\nchallenges. We demonstrate through theoretical and real data analyses that a standard method\nfor estimation and inference in single-cell CRISPR screens \u2013\u201cthresholded regression\u201d \u2013 exhibits\nattenuation bias and a bias-variance tradeoff as a function of an intrinsic, challenging-to-select\ntuning parameter. To overcome these difficulties, we introduce GLM-EIV (\u201cGLM-based errors-in-\nvariables\u201d), a new method for single-cell CRISPR screen analysis. GLM-EIV extends the classical\nerrors-in-variables model to responses and noisy predictors that are exponential family-distributed\nand potentially impacted by the same set of confounding variables. We develop a computational\n\u00a9 The Author 2024. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com\narXiv:2201.01879v3  [stat.ME]  12 Mar 2024\n2\nT. Barry, K. Roeder, and E. Katsevich\ninfrastructure to deploy GLM-EIV across hundreds of processors on clouds (e.g., Microsoft Azure)\nand high-performance clusters. Leveraging this infrastructure, we apply GLM-EIV to analyze two\nrecent, large-scale, single-cell CRISPR screen datasets, yielding several new insights.\nKey words: CRISPR, single-cell, GLM, mixture model, cloud computing\n1. Introduction\nCRISPR is a genome engineering tool that has enabled scientists to precisely edit human and\nnonhuman genomes, opening the door to new medical therapies (Musunuru and others, 2021) and\naccelerating biological discovery (Przybyla and Gilbert, 2022). Recently, scientists have paired\nCRISPR genome engineering with single-cell RNA sequencing (Datlinger and others, 2017). The\nresulting assays, known as \u201csingle-cell CRISPR screens,\u201d link genetic perturbations in individ-\nual cells to changes in gene expression. Single-cell CRISPR screens have enabled breakthrough\nprogress on longstanding challenges in genetics, such as causally mapping genome wide associa-\ntion study (GWAS) variants to target genes at genome-wide scale (Morris and others, 2023).\nDespite their promise, single-cell CRISPR screens present considerable statistical challenges.\nOne difficulty is that the \u201ctreatment\u201d \u2014 i.e., the presence or absence of a CRISPR perturbation\n\u2014 is assigned randomly to cells and is not directly observable. As a consequence, one cannot know\nwith certainty which cells were perturbed. Instead, one must leverage an indirect, quantitative\nproxy of perturbation presence or absence to \u201cguess\u201d which cells received a perturbation. This\nindirect proxy takes the form of a so-called guide RNA count, with higher counts indicating that\na cell is more likely to have been perturbed. A standard approach to single-cell CRISPR screen\nanalysis is to impute perturbation assignments onto the cells by simply thresholding the guide\nRNA counts; using these imputations, one can attempt to estimate the effect of the perturbation\non gene expression. We call this standard approach \u201cthresholded regression\u201d or the \u201cthresholding\nExponential family measurement error models for single-cell CRISPR screens\n3\nmethod.\u201d\nWe study estimation and inference in single-cell CRISPR screens from a statistical perspective,\nformulating the data generating mechanism using a new class of measurement error models. We\nassume that the response variable y is a GLM of an underlying predictor variable x\u2217and vector\nof confounders z. We do not observe x\u2217directly; rather, we observe a noisy version x of x\u2217that\nitself is a GLM of x\u2217and the same set of confounders z. The goal of the analysis is to estimate the\neffect of x\u2217on y using the observed data (x, y, z) only. In the context of the biological application,\nx\u2217, x, y, and z are CRISPR perturbations, guide RNA counts, gene expressions, and technical\nconfounders, respectively.\nOur work makes two main contributions. First, we conduct a detailed study of the thresholding\nmethod. Notably, we demonstrate on real data that the thresholding method exhibits attenuation\nbias and a bias-variance tradeoff as a function of the selected threshold, and we recover these\nphenomena in precise mathematical terms in a simplified Gaussian setting. Second, we intro-\nduce a new method, GLM-EIV (\u201cGLM-based errors-in-variables\u201d), for single-cell CRISPR screen\nanalysis. GLM-EIV extends the classical errors-in-variables model (Carroll and others, 2006) to\nresponses and noisy predictors that are exponential family-distributed and potentially impacted\nby the same set of confounding variables. GLM-EIV thereby implicitly estimates the probability\nthat each cell was perturbed, obviating the need to explicitly impute perturbation assignments\nvia thresholding. We implement several statistical accelerations to bring the cost of GLM-EIV\ndown to within about an order of magnitude of the thresholding method. We additionally de-\nvelop a Docker-containerized application to deploy GLM-EIV at-scale across tens or hundreds of\nprocessors on clouds (e.g., Microsoft Azure) and high-performance clusters.\nOur analyses indicate that single-cell CRISPR screens fall into two main problem settings:\nthe more challenging \u201chigh background contamination\u201d setting and the easier \u201clow background\ncontamination\u201d setting. GLM-EIV outperforms thresholded regression by a considerable margin\n4\nT. Barry, K. Roeder, and E. Katsevich\nin the high background contamination setting; in the low background contamination setting, by\ncontrast, GLM-EIV and thresholded regression perform similarly, provided that accurate guide\nRNA-to-cell assignments are used within the thresholded regression model. We show that a\nsimplified version of GLM-EIV can be used to obtain these guide RNA-to-cell assignments in the\nlow background contamination setting, thereby neutralizing a tuning parameter that until this\npoint has been challenging to select.\n2. Assay background\nThere are several classes of single-cell CRISPR screen assays, each suited to answer a different\nset of biological questions. In this work we mostly focus on high-multiplicity of infection (MOI)\nsingle-cell CRISPR screens, which we motivate and describe here. The human genome consists\nof genes, enhancers (segments of DNA that regulate the expression of one or more genes), and\nother genomic elements. GWAS have revealed that the majority (> 90%) of variants associated\nwith diseases lie outside genes and inside enhancers (Gallagher and Chen-Plotkin, 2018). These\nnoncoding variants are thought to contribute to disease by modulating the expression of one\nor more disease-relevant genes. Scientists do not know the gene (or genes) through which most\nnoncoding variants exert their effect, limiting the interpretability of GWAS results. A central\nopen challenge in genetics, therefore, is to link enhancers that harbor GWAS variants to the\ngenes that they target at genome-wide scale (Morris and others, 2023).\nHigh-MOI single-cell CRISPR screens are a promising emerging technology for resolving this\nchallenge (Morris and others, 2023; Mostafavi and others, 2023). High-MOI single-cell CRISPR\nscreens combine CRISPR interference (CRISPRi) \u2014 a version of CRISPR that represses a tar-\ngeted region of the genome \u2014 with single-cell sequencing. The experimental protocol is as follows.\nFirst, the scientist develops a library of several hundred to several thousand CRISPRi pertur-\nbations, each designed to target a candidate enhancer for repression. The scientist then cultures\nExponential family measurement error models for single-cell CRISPR screens\n5\ntens or hundreds of thousands of cells and delivers the CRISPRi perturbations to these cells. The\nperturbations assort into the cells randomly, with each cell receiving on average 10-40 distinct\nperturbations. Conversely, a given perturbation enters about 0.1-2% of cells (this work).\nAfter waiting several days for CRISPRi to take effect, the scientist profiles each cell\u2019s transcrip-\ntome (i.e., its gene expressions) and the set of perturbations that it received. Finally, the scientist\nconducts perturbation-to-gene association analyses. Figure 1a depicts this process schematically,\nwith colored bars (blue, red, and purple) representing distinct perturbations. For a given per-\nturbation (e.g., the perturbation represented in blue), the scientist partitions the cells into two\ngroups: those that received the perturbation (top) and those that did not (bottom). Next, for\na given gene, the scientist runs a differential expression analysis across the two groups of cells,\nproducing an estimate for the magnitude of the gene expression change in response to the pertur-\nbation. If the estimated change in expression is large, the scientist can conclude that the enhancer\ntargeted by the perturbation exerts a strong regulatory effect on the gene. This procedure is re-\npeated for a large set of preselected perturbation-gene pairs. The enhancer-by-enhancer approach\nis valid because the perturbations assort into cells approximately independently of one another.\nThe genomics literature has produced several methods for high-MOI single-cell CRISPR screen\nanalysis (Gasperini and others, 2019; Xie and others, 2019; Barry and others, 2021; Wang, 2021).\nFor example, Gasperini et al. applied negative binomial GLMs (as implemented in the Monocle\nsoftware; Trapnell and others (2014)) to carry out the differential expression analysis described\nabove. Moreover, Xie et al. applied chi-squared-like tests of independence for this purpose. Un-\nfortunately, both of these approaches have limitations: the former can break down when the gene\nexpression model is misspecified, and the latter does not adjust for the presence of technical con-\nfounders. In a prior work we introduced introduced SCEPTRE, a custom implementation of the\nconditional randomization test (Cand`es and others, 2018; Liu and others, 2022) tailored to single-\ncell CRISPR screen data. SCEPTRE simultaneously adjusts for confounder presence and ensures\n6\nT. Barry, K. Roeder, and E. Katsevich\nrobustness to expression model misspecification, thereby overcoming limitations of previous ap-\nproaches and demonstrating improved sensitivity and specificity on single-cell CRISPR screen\ndata. In this work we tackle a set of analysis challenges complimentary to those addressed by\nSCEPTRE. Most importantly, we seek to account for the fact that the perturbation is measured\nwith noise. Additionally, we seek to estimate (with confidence) the effect size of a perturbation on\ngene expression change, an objective that we did not consider in the original SCEPTRE study.\n3. Analysis challenges and proposed statistical model\nHigh-MOI single-cell CRISPR screens present several statistical challenges, four of which we\nhighlight here. Throughout, we consider a single perturbation-gene pair. First, the \u201ctreatment\u201d\nvariable \u2014 i.e., the presence or absence of a perturbation \u2014 cannot be directly observed. Instead,\nperturbed cells transcribe molecules called guide RNAs (or gRNAs) that serve as indirect proxies\nof perturbation presence. We must leverage these gRNAs to impute (explicitly or implicitly)\nperturbation assignments onto the cells (Figure 1b). Second, \u201ctechnical factors\u201d \u2014 sources of\nvariation that are experimental rather than biological in origin \u2014 impact the measurement of\nboth gene and gRNA expressions and therefore act as confounders (Figure 1b). Third, the gene\nand gRNA data are sparse, discrete counts. Consequently, classical statistical approaches that\nassume Gaussianity or homoscedasticity are not directly applicable. Finally, sequenced gRNAs\nsometimes map to cells that have not received a perturbation. This phenomenon, which we call\n\u201cbackground contamination,\u201d results from errors in the sequencing and alignment processes. The\nmarginal distribution of the gRNA counts is best conceptualized as a mixture model (Figure 1c;\nGaussian distributions used for illustration purposes only). Unperturbed and perturbed cells both\nexhibit nonzero gRNA count distributions, but this distribution is shifted upward for perturbed\ncells. Figure 1d shows example data on four (of possibly tens or hundreds of thousands of) cells.\nThe analysis objective is to leverage the gene expressions and gRNA counts to estimate the effect\nExponential family measurement error models for single-cell CRISPR screens\n7\nof the (latent) perturbation on gene expression, accounting for the technical factors.\nWe propose to model the single-cell CRISPR screen data-generating process using a pair of\nGLMs. Let n \u2208N be the number of cells assayed in the experiment. Consider a single perturbation\nand a single gene. For cell i \u2208{1, . . . , n}, let pi \u2208{0, 1} indicate perturbation presence or\nabsence; let mi \u2208N be the number of gene transcripts sequenced; let gi \u2208N be the number of\ngRNA transcripts sequenced; let dm\ni \u2208N be the number of gene transcripts sequenced across all\ngenes (i.e., the library size or sequencing depth); let dg\ni be the gRNA library size; and finally,\nlet zi \u2208Rd\u22122 be the cell-specific covariates, including sequencing batch, percent mitochondrial\nreads, etc. (We note that most single-cell CRISPR screens have been carried out on cell lines\nconsisting of a uniform cell type; however, if multiple cell types are present in the data, then cell\ntype could be included as a covariate in the model.) The letters \u201cm,\u201d \u201cg\u201d, and \u201cd\u201d stand for\n\u201cmRNA,\u201d \u201cgRNA,\u201d and \u201cdepth,\u201d respectively.\nBuilding on the work of several previous authors (Robinson and Smyth, 2008; Townes and oth-\ners, 2019; Hafemeister and Satija, 2019), Sarkar and Stephens (2021) proposed a simple strategy\nfor modeling single-cell gene expression data, which, in the framework of negative binomial GLMs,\nis equivalent to using the log-transformed library size as an offset term. Sarkar and Stephens\u2019\nframework enjoys strong theoretical and empirical support; therefore, we generalize their ap-\nproach to model both gene and gRNA modalities in single-cell CRISPR screen experiments. To\nthis end we assume that the gene expression counts are given by\nmi|(pi, zi, dm\ni ) \u223cNBsm(\u00b5m\ni );\nlog(\u00b5m\ni ) = \u03b2m\n0 + \u03b2m\n1 pi + \u03b3T\nmzi + log(dm\ni ),\n(3.1)\nwhere (i) NBsm(\u00b5m\ni ) is a negative binomial distribution with mean \u00b5m\ni and known size parameter\nsm; (ii) \u03b2m\n0 \u2208R, \u03b2m\n1 \u2208R, and \u03b3m \u2208Rd\u22122 are unknown parameters; and (iii) log(dm\ni ) is an offset\nterm. (We note that the \u201csize parameter\u201d is simply the inverse of the negative binomial dispersion\nparameter; \u201csize parameter\u201d does not refer to library size in this context.) Similarly, we model\n8\nT. Barry, K. Roeder, and E. Katsevich\nthe gRNA counts by\ngi|(pi, zi, dg\ni ) \u223cNBsg (\u00b5g\ni ) ;\nlog(\u00b5g\ni ) = \u03b2g\n0 + \u03b2g\n1pi + \u03b3T\ng zi + log(dg\ni ),\n(3.2)\nwhere \u00b5g\ni , sg, \u03b2g\n0, \u03b2g\n1, \u03b3g, and dg\ni are analogous. We use a negative binomial GLM to model the\ngRNA counts as well as the gene expressions because the gRNA transcripts are generated via the\nsame biological mechanism as the gene transcripts (Datlinger and others, 2017; Hill and others,\n2018). We model the marginal perturbation as pi \u223cBern(\u03c0), where pi is an unobserved binary\nvariable indicating presence (pi = 1) or absence (pi = 0) of the perturbation. We restrict \u03c0, the\nprobability of perturbation, to the interval (0, 1/2] to ensure that the model is identifiable; this\nrestriction is reasonable given that each perturbation infects only a small fraction of cells. The\ngRNA intercept term \u03b2g\n0 controls the ambient level of gRNA expression, i.e. the rate at which\ngRNA reads are generated in the absence of the perturbation. The perturbation coefficient \u03b2g\n1\ncontrols the extent to which perturbed and unperturbed cells differentially express the gRNA;\nthe target of inference \u03b2m\n1\nis challenging to estimate when \u03b2g\n1 is close to zero, as the gRNA\ndistributions of the perturbed and unperturbed cells are hard to differentiate in this region of\nthe problem space. Together, (3.1), (3.2), and the marginal distribution of pi define the negative\nbinomial GLM-EIV model.\nThe log-transformed sequencing depth log(dm\ni ) is included as an offset term in (3.1) so that\n\u03b2m\n0 + \u03b2m\n1 pi + \u03b3T\nmzi can be interpreted as a relative expression. Exponentiating both sides of (3.1)\nreveals that the mean gene expression \u00b5m\ni of the ith cell is exp\n\u0000\u03b2m\n0 + \u03b2m\n1 pi + \u03b3T\nmzi\n\u0001\ndm\ni . Because\ndm\ni\nis the sequencing depth, exp\n\u0000\u03b2m\n0 + \u03b2m\n1 pi + \u03b3T\nmzi\n\u0001\nis the fraction of all transcripts sequenced\nin the cell produced by the gene under consideration. The target of inference \u03b2m\n1 is the log fold\nchange in expression in response to the perturbation, controlling for the technical factors. Fold\nchange in this context is the ratio of the mean gene expression in perturbed cells to the mean\ngene expression in unperturbed cells. Hence, exp(\u03b2m\n1 ) = 1 (i.e., \u03b2m\n1 = 0) indicates no change in\nexpression, whereas exp(\u03b2m\n1 ) > 1 (i.e., \u03b2m\n1\n> 0) and exp(\u03b2m\n1 ) < 1 (i.e., \u03b2m\n1\n< 0) indicate an\nExponential family measurement error models for single-cell CRISPR screens\n9\nincrease and decrease in expression, respectively.\nIn this work we analyzed two large-scale, high-MOI, single-cell CRISPR screen datasets pub-\nlished by Gasperini and others (2019) and Xie and others (2019). Gasperini (resp., Xie) targeted\napproximately 6,000 (resp., 500) candidate enhancers in a population of approximately 200,000\n(resp., 100,000) cells. Gasperini additionally designed several hundred positive control, gene-\ntargeting perturbations and 50 non-targeting, negative control perturbations to assess method\nsensitivity and specificity.\n4. Analysis of the thresholding method\nWe studied thresholding from empirical and theoretical perspectives, highlighting several po-\ntential limitations of the approach. In the context of the negative bionomial GLM-EIV model\nintroduced above (3.1-3.2), the thresholding method leverages the gRNA counts (3.2) to im-\npute the latent perturbation indicator (3.2), thereby reducing the full data generating process\nto a single, gene expression model (3.1). We studied Gasperini et al.\u2019s variant of the threshold-\ning method (i.e., thresholded negative binomial regression), as this version of the thresholding\nmethod is standard and relates most closely to GLM-EIV. The method is defined as follows:\n1. For a given threshold c \u2208N, let the imputed perturbation assignment \u02c6pi \u2208{0, 1} be given\nby \u02c6pi = 0 if gi < c and \u02c6pi = 1 otherwise.\n2. Assume that mi is related to \u02c6pi, dm\ni , and zi through the following GLM:\nmi|(\u02c6pi, zi, dm\ni ) \u223cNBsm(\u00b5m\ni );\nlog(\u00b5m\ni ) = \u03b2m\n0 + \u03b2m\n1 \u02c6pi + \u03b3T\nmzi + log (dm\ni ) .\n(4.3)\nThe model (4.3) is equivalent to the model (3.2), but the latent perturbation indicator pi\nhas been replaced by the imputed perturbation indicator \u02c6pi.\n3. Fit a GLM to (4.3) to obtain an estimate and CI for the target of inference \u03b2m\n1 .\n10\nT. Barry, K. Roeder, and E. Katsevich\nTo shed light on empirical challenges of the thresholding method, we applied thresholded\nnegative binomial regression to analyze the set of positive control perturbation-gene pairs in\nthe Gasperini dataset. The positive control pairs consisted of perturbations that targeted gene\ntranscription start sites (TSSs) for inhibition. Repressing the TSS of a given gene decreases its\nexpression; therefore, the positive control pairs a priori are expected to exhibit a strong decrease\nin expression.\nTo investigate the sensitivity of the thresholding method to threshold choice, we deployed the\nmethod using three different choices for the threshold: 1, 5, and 20. We found that the chosen\nthreshold substantially impacted the results (Figure 2a-b): estimates for fold change produced by\nthreshold = 1 were smaller in magnitude (i.e., closer to the baseline of 1) than those produced by\nthreshold = 5 (Figure 2a). On the other hand, estimates produced by threshold = 5 and threshold\n= 20 were more concordant (Figure 2b).\nWe reasoned that thresholded regression systematically underestimated true effect sizes on the\npositive control pairs, especially for threshold = 1. For a given perturbation, the majority (> 98%)\nof cells are unperturbed. This imbalance leads to an asymmetry: misclassifying unperturbed cells\nas perturbed is intuitively \u201cworse\u201d than misclassifying perturbed cells as unperturbed. Misclassified\nunperturbed cells contaminate the set of truly perturbed cells, leading to attenuation bias; by\ncontrast, misclassified perturbed cells are swamped in number and \u201cneutralized\u201d by the truly\nunperturbed cells. Setting the threshold to a large number reduces the unperturbed-to-perturbed\nmisclassification rate, decreasing bias.\nWe hypothesized, however, that the reduction in bias obtained by selecting a large thresh-\nold causes the variance of the estimator to increase. To investigate, we compared p-values and\nconfidence intervals produced by threshold = 5 and threshold = 20 for the target of inference\n\u03b2m\n1 . We found that threshold = 5 yielded smaller (i.e., more significant) p-values and narrower\nconfidence intervals than did threshold = 20 (Figures 2c-d). We concluded that the threshold\nExponential family measurement error models for single-cell CRISPR screens\n11\ncontrols a bias-variance tradeoff: as the threshold increases, the bias of the estimator decreases\nand the variance increases.\nFinally, to determine whether there is an \u201cobvious\u201d location at which to draw the threshold,\nwe examined the empirical gRNA count distribution of a gRNA from the Gasperini (Figure 2e)\nand Xie (Figure 2f) dataset (counts of 0 omitted). The distributions peaked at 1 and then tapered\noff gradually; there did not exist a sharp boundary that cleanly separated the perturbed from the\nunperturbed cells. Overall, we concluded that the thresholding method faces several challenges:\n(i) the threshold is a tuning parameter that significantly impacts the results; (ii) the threshold\nmediates an intrinsic bias-variance tradeoff; and (iii) the gRNA count distributions may not imply\na clear threshold selection strategy.\nNext, we studied the thresholding method from a theoretical perspective, recovering in a\nsimplified Gaussian setting phenomena revealed in the empirical analysis. Due to space constraints\nwe relegate this analysis to Appendix A, but we briefly summarize the main results here. First,\nwe derived an exact expression for the asymptotic relative bias of the thresholding estimator\n\u02c6\u03b2m\n1 . Leveraging this exact expression, we showed that (i) the thresholding estimator strictly\nunderestimates (in absolute value) the true value of \u03b2m\n1 over all choices of the threshold and over\nall values of the regression coefficients (an example of attenuation bias; Stefanski (2000)); and (ii)\nthe magnitude of the bias decreases monotonically in \u03b2g\n1, comporting with the intuition that the\nproblem becomes easier as the gRNA mixture distribution becomes increasingly well-separated.\nSecond, we derived an asymptotically exact bias-variance decomposition for \u02c6\u03b2m, demonstrating\nthat as the threshold tends to infinity, the bias decreases and the variance increases.\n5. GLM-based errors-in-variables (GLM-EIV)\nWe introduce the general GLM-EIV model, which generalizes the negative binomial GLM-EIV\nmodel (3.1-3.2) to arbitrary exponential family response distributions and link functions, thereby\n12\nT. Barry, K. Roeder, and E. Katsevich\nproviding much greater modeling flexibility. We derive efficient methods for estimation and in-\nference in this model and develop a pipeline to deploy the model at-scale.\n5.1\nModel and model properties\nThe general GLM-EIV model uses an arbirary GLM to model the gene and gRNA modalities:\nmi|(pi, zi, om\ni ) \u223cfm(\u00b5m\ni );\nrm(\u00b5m\ni ) = \u03b2m\n0 + \u03b2m\n1 pi + \u03b3T\nmzi + om\ni ,\n(5.4)\ngi|(pi, zi, og\ni ) \u223cfg(\u00b5g\ni );\nrg(\u00b5g\ni ) = \u03b2g\n0 + \u03b2g\n1pi + \u03b3T\ng zi + og\ni .\n(5.5)\nHere, fm (resp., fg) is an exponential family distribution with mean \u00b5m\ni (resp., \u00b5g\ni ); rm and rg\nare the link function for the gene and gRNA models, respectively; and om\ni and og\ni are the (possibly\nzero) offset terms for the gene and gRNA models. In practice we typically set om\ni\nand og\ni to the\nlog-transformed library sizes (i.e., log(dm\ni ) and log(dg\ni )). Again, we assume that the unobserved\nperturbation indicator pi is drawn from a Bern(\u03c0) distribution. More model details are available\nin Appendix B.\nThe GLM-EIV model can be seen as a generalization of the simple errors-in-variables model\n(when the predictor is binary); the latter is defined as follows:\nyi = \u03b20 + \u03b21x\u2217\ni + \u03f5i;\nxi = x\u2217\ni + \u03c4i,\n(5.6)\nwhere, x\u2217\ni \u223cBern(\u03c0), \u03f5i, \u03c4i \u223cN(0, 1), and \u03f5i,\u03c4i, and x\u2217\ni are independent. GLM-EIV extends (5.6)\nin at least three directions: first, GLM-EIV allows yi and xi to follow exponential family (i.e,\nnot just Gaussian) distributions; second, GLM-EIV allows yi and xi to be related to x\u2217\ni through\narbitrary (i.e., not just linear) link functions; and finally, GLM-EIV allows confounders zi to\nimpact both xi and yi. Therefore, xi and yi can be conditionally dependent given x\u2217\ni , enabling\nGLM-EIV to capture more complex dependence relationships between xi and yi than is possible\nin (5.6) or other standard measurement error models.\nExponential family measurement error models for single-cell CRISPR screens\n13\n5.2\nEstimation and inference, and computational infrastructure\nWe derived an EM algorithm (Algorithm 1) to estimate the parameters of the GLM-EIV model.\nWe briefly introduce some notation. Let \u03b2m = [\u03b2m\n0 , \u03b2m\n1 , \u03b3m]T be the vector of unknown gene\nmodel parameters and \u03b2g = [\u03b2g\n0, \u03b2g\n1, \u03b3g]T the vector of unknown gRNA model parameters. Let\nm, g, om, and og be the vector of gene expressions, gRNA expressions, gene library sizes, and\ngRNA library sizes. Finally, let X be the observed design matrix; let \u02dcX be the augmented design\nmatrix that results from concatenating the column of (unobserved) pis to X; and let \u02dcX(0) (resp,\n\u02dcX(1)) be the matrix that results from setting all of the pis in \u02dcX to 0 (resp., 1).\nThe E step entails computing the membership probability (i.e., the probability of pertur-\nbation) in each cell. The membership probability Ti(1) of cell i \u2208{1, . . . , n} given the current\nparameter estimates (\u03b2(t)\nm , \u03b2(t)\ng , \u03c0(t)) and observed data (mi, gi) is Ti(1) = P(pi = 1|Mi = mi, Gi =\ngi, \u03b2(t)\nm , \u03b2(t)\ng , \u03c0(t)). We can calculate this quantity by applying (i) Bayes rule, (ii) the conditional\nindependence property of Mi and Gi, (iii) the density of Mi and Gi, and (iv) a log-sum-exp-type\ntrick to ensure numerical stability. Next, we produce updated estimates \u03c0(t+1), \u03b2(t+1)\ng\n, and \u03b2(t+1)\nm\nof the parameters by maximizing the M step objective function. It turns out that maximizing\nthis objective function is equivalent to setting \u03c0(t+1) to the mean of the current membership\nprobabilities and setting \u03b2(t+1)\ng\nand \u03b2(t+1)\nm\nto the fitted coefficients of a GLM weighted by the\ncurrent membership probabilities (Algorithm 1). We iterate through the E and M steps until the\nlog likelihood (B.1) converges (Appendix B). Our EM algorithm is reminiscent of (but distinct\nfrom) that of Ibrahim (1990), who also applied weighted GLM solvers to carry out an M step of\nan EM algorithm.\nAfter fitting the model, we perform inference on the estimated parameters. The easiest ap-\nproach, given the complexity of the log likelihood, would be to run a bootstrap. This strategy,\nhowever, is prohibitively slow, as the data are large and the EM algorithm is iterative. Therefore,\nwe derived an analytic formula for the asymptotic observed information matrix using Louis\u2019s The-\n14\nT. Barry, K. Roeder, and E. Katsevich\nAlgorithm 1 EM algorithm for GLM-EIV model.\nInput: Pilot estimates \u03b2curr\nm\n, \u03b2curr\ng\n, and \u03c0curr; data m, g, om, og, and X; gene expression distri-\nbution fm and link function rm; gRNA expression distribution fg and link function rg.\nwhile Not converged do\nfor i \u2208{1, . . . , n} do\n\u25b7E step\nTi(1) \u2190P\n\u0000pi = 1|Mi = mi, Gi = gi, \u03b2curr\nm\n, \u03b2curr\ng\n, \u03c0curr\u0001\nTi(0) \u21901 \u2212Ti(1)\nend for\n\u03c0curr \u2190(1/n) Pn\ni=1 Ti(1)\n\u25b7M step\nw \u2190[T1(0), T2(0), . . . , Tn(0), T1(1), T2(1), . . . , Tn(1)]T\nfor k \u2208{g, m} do\nFit a GLM GLMk with responses [k, k]T , offsets [ok, ok]T , weights w, design matrix\n[ \u02dcX(0)T , \u02dcX(1)T ]T , distribution fk, and link function rk.\nSet \u03b2curr\nk\nto the estimated coefficients of GLMk.\nend for\nCompute log likelihood using \u03b2curr\nm\n, \u03b2curr\ng\n, and \u03c0curr.\nend while\n\u02c6\u03b2m \u2190\u03b2curr\nm\n; \u02c6\u03b2g \u2190\u03b2curr\ng\n; \u02c6\u03c0 \u2190\u03c0curr.\nreturn (\u02c6\u03b2m, \u02c6\u03b2g, \u02c6\u03c0)\norem (Louis (1982); Appendix B). Leveraging this analytic formula, we can calculate standard\nerrors quickly, enabling us to perform inference in practice on real, large-scale data.\nA downside of the EM algorithm (Algorithm 1) is that it requires fitting many GLMs. As-\nsuming that we run the algorithm 15 times using randomly-generated pilot estimates (to improve\nchances of convergence to the global maximum), and assuming that the algorithm iterates through\nE and M steps about 10 times per run, we must fit approximately 300 GLMs. (These numbers are\nbased on exploratory applications of the method to real and simulated data.) We instead devised\na strategy to produce a highly accurate pilot estimate of the true parameters, enabling us to run\nthe algorithm once and converge upon the MLE within a few iterations. The strategy involves\nlayering several statistical \u201ctricks\u201d on top of one another. Briefly, we first obtain pilot estimates\nfor the nuisance parameters \u03b2m\n0 , \u03b3m, \u03b2g\n0, and \u03b3g by regressing the gene and gRNA expression\nvectors onto the observed design matrix X; the resulting estimates are close to the full GLM-EIV\nmodel maximum likelihood estimates because the probability of perturbation is small. Next, we\nobtain pilot estimates for \u03c0 and the perturbation effect parameters \u03b2m\n1 and \u03b2g\n1 by estimating a\nsimplified, \u201creduced\u201d GLM-EIV model; this second step does not require fitting any GLMs. (See\nExponential family measurement error models for single-cell CRISPR screens\n15\nAppendix C for additional details.) Overall, the statistical accelerations reduce the number of\nGLMs that must be fit to < 10 in most cases.\nNext, we developed a computational infrastructure to apply GLM-EIV to large-scale, single-\ncell CRISPR screen data. The infrastructure leverages Nextflow, a programming language that\nfacilitates building data-intensive pipelines, and ondisc, an R/C++ package that we developed (in\na separate project; preprint forthcoming) to facilitate large-scale computing on single-cell data.\nNextflow and ondisc together enable the construction of highly portable single-cell pipelines:\none can analyze data out-of-memory on a laptop or in a distributed fashion across hundreds of\nprocessors on a cloud (e.g., Microsoft Azure, Google Cloud) or high-performance cluster. Leverag-\ning these technologies, we built a Docker-containerized pipeline for deploying GLM-EIV at-scale.\nThe pipeline recycles computation when possible, saving a considerable amount of compute; see\nAppendix C.3 for details. Overall, the statistical accelerations and computational infrastructure\nmake the deployment of GLM-EIV to large-scale single-cell CRISPR screen quite feasible.\n5.3\nThe gRNA mixture assignment method\nThus far we have described two methods for estimating the effect of a perturbation on gene\nexpression: the simple thresholding method and the more complex GLM-EIV method. A third\napproach of intermediate complexity \u2014 which we call the \u201cgRNA mixture assignment\u201d approach\n\u2014 is to (i) fit a mixture model to the gRNA count distribution, (ii) use this fitted mixture model\nto impute perturbation identities onto cells, and then (iii) regress the gene expressions onto\nthe imputed perturbation indicators (as well as the remaining covariates). The gRNA mixture\nassignment approach enjoys at least two strengths relative to the simpler thresholding approach:\nthe former negates the threshold tuning parameter and can account for variation across cells due\nto covariates.\n16\nT. Barry, K. Roeder, and E. Katsevich\nReplogle and others (2020) proposed a simple gRNA mixture assignment strategy that involves\nfitting a Poisson-Gaussian mixture model to the log-transformed gRNA counts and then assigning\ngRNAs to cells using the posterior perturbation probabilities of the fitted model. (We call this\nmethod the Nat. Biotech. 2020 method, representing the journal and year in which the method\nappeared.) Unfortunately, this method poses several conceptual and practical difficulties. First,\nit is unclear how the method fits the Poisson component of the mixture distribution to the log-\ntransformed gRNA expressions, as the transformed expressions are not integer-valued. Second,\ndue to recent changes in the Python ecosystem, we and others have had difficulty with installing\nthe Python package upon which the Nat. Biotech. 2020 method relies. (See Appendix D for\nfurther discussion of the Nat. Biotech. 2020 method.)\nFollowing Replogle and others (2020), we devised an alternate gRNA mixture assignment\nstrategy that is tethered more closely to the data-generating mechanism. For a given gRNA, we\nregress the gRNA counts onto the (latent) perturbation indicator and covariates (while ignoring\nthe gene expressions; model 5.5). We assign perturbation identities to cells by thresholding the\nposterior perturbation probabilities of the fitted model at 1/2. The latent variable gRNA model is\na subset of the full GLM-EIV model (5.4-5.5). Thus, we used the GLM-EIV EM algorithm to fit\nthe latent variable gRNA model, enabling us to exploit the various techniques that we developed\nin the context of GLM-EIV for obtaining fast and numerically stable estimates.\n6. Simulation study\nWe conducted a comprehensive suite of six simulation studies to compare the empirical per-\nformance of GLM-EIV, the thresholding method, and the gRNA mixture assignment method.\n(We coupled the latter method to standard regression on the imputed perturbation assignments\nto estimate the perturbation effect size.) We describe one simulation study here and defer the\nremaining simulation studies to the Appendix G. We generated data on n = 50, 000 cells from\nExponential family measurement error models for single-cell CRISPR screens\n17\nthe GLM-EIV model, setting the target of inference \u03b2m\n1 to log(0.25) and the probability of per-\nturbation \u03c0 to 0.02. \u03b2m\n1\n= log(0.25) represents a decrease in gene expression by a factor of 4,\nwhich is a fairly large effect size on the order of what we might observe for a positive control\npair. We included \u201csequencing batch\u201d (modeled as a Bernoulli-distributed variable) as a covari-\nate and sequencing depth (modeled as a Poisson-distributed variable) as an offset. We varied the\nlog-fold change in gRNA expression, \u03b2g\n1, over a grid on the interval [log(1), log(4)]; \u03b2g\n1 controls\nproblem difficulty, with higher values corresponding to easier problem settings. We generated the\ngene expression count data from two response distributions: Poisson and negative binomial (size\nparameter fixed at s = 20 for the latter; see simulation study 3 for an exploration of different\nvalues of s). We generated the gRNA count data from a Poisson distribution. For each parameter\nsetting (defined by a \u03b2g\n1-distribution pair), we synthesized nsim = 500 i.i.d. datasets. Appendix G\ncompares the parameter values used in the simulation study to those estimated from real data.\nWe applied four methods to the simulated data: \u201cvanilla\u201d GLM-EIV, accelerated GLM-EIV,\nthresholded regression, and the gRNA mixture assignment method. We used the Bayes-optimal\ndecision boundary for classification as the threshold for the thresholding method (as derived in\nSection A.12). We ran all methods on the negative binomial data twice: once treating the size\nparameter s as a known constant and once treating s as unknown. In the latter case we used\nthe glm.nb function from the MASS package to estimate s before applying the methods (Ripley\nand others, 2013). We note that none of the methods accounts for the error in estimating s\nwhen computing coefficient standard errors. We display the results of the simulation study in\nFigure 3. Columns correspond to distributions (i.e., Poisson, NB with known s, and NB with\nunknown s), and rows correspond to performance metrics (i.e., bias, mean squared error, CI\ncoverage rate (nominal rate 95%), CI width, and method run time). The \u03b2g\n1 parameter is plotted\non the horizontal axis, and the methods are depicted in different colors. (GLM-EIV is masked by\naccelerated GLM-EIV in several panels).\n18\nT. Barry, K. Roeder, and E. Katsevich\nWe found that GLM-EIV outperformed the gRNA mixture method and that the gRNA mix-\nture method outperformed thresholded regression across the metrics of bias, mean squaured error,\nand confidence interval coverage. We reasoned that GLM-EIV outperformed the gRNA mixture\nmethod because (i) GLM-EIV leveraged information from both modalities (rather than the gRNA\nmodality alone) to assign perturbation identities to cells, and (ii) GLM-EIV produced soft rather\nthan hard assignments, capturing the inherent uncertainty in whether a perturbation occurred.\nWe additionally reasoned that the gRNA mixture method outperformed thresholded regression\nbecause the gRNA mixture method better accounted for heterogeneity across cells due to the\ncovariates. Notably, accelerated GLM-EIV performed as well as vanilla GLM-EIV on all sta-\ntistical metrics (rows 1-4) despite having substantially lower computational cost (bottom row).\nIn fact, the running time of accelerated GLM-EIV was almost within an order of magnitude of\nthat of the thresholding method. As expected, the confidence interval coverage of the methods\ndegraded somewhat in the negative binomial case under estimated s as opposed to known s, but\nthis difference was not substantial. Appendix G presents additional simulation studies in which\nwe generate data from a Gaussian model, vary \u03b2m\n1\nand s, and assess the performance of the\nmethods on data containing unmeasured covariates and outliers.\n7. Real data application I: estimating perturbation effects on high-MOI data\nLeveraging our computational infrastructure, we applied GLM-EIV and the thresholding method\nto analyze the entire Gasperini and Xie datasets. GLM-EIV ran in under two days on both\ndatasets, using no more than 250 processors and two gigabytes of memory per process. We report\nonly the most important aspects of the analysis and results in the main text; full details are\navailable in Appendix E. We set the threshold in the thresholding method to the approximate\nBayes-optimal decision boundary, as our theoretical analyses and simulation studies indicated\nthat the Bayes-optimal decision boundary is a good choice for the threshold when the gRNA\nExponential family measurement error models for single-cell CRISPR screens\n19\ncount distribution is well-separated. Operating under the assumption that the effect of the per-\nturbation on gRNA expression is similar across pairs, we leveraged the fitted GLM-EIV models\nto approximate the Bayes boundary in the following way: we (i) sampled several hundred gene-\nperturbation pairs, (ii) extracted the fitted values \u02c6\u03b2g and \u02c6\u03c0 from the GLM-EIV models fitted\nto these pairs, (iii) computed the median \u02c6\u03b2g and \u02c6\u03c0 across the \u02c6\u03b2gs and \u02c6\u03c0s, and (iv) used \u02c6\u03b2g and\n\u02c6\u03c0 to estimate a dataset-wide Bayes-optimal decision boundary (Section A.12). We repeated this\nprocedure on both datasets, yielding a threshold of 3 for Gasperini and 7 for Xie.\nWe compared GLM-EIV to thresholded regression on the real data, focusing specifically on\nthe negative control pairs (i.e., gene-perturbation pairs for which the ground truth fold change is\nknown to be 1; Appendix E). We found that GLM-EIV and the thresholding method produced\nsimilar results (Figure 4a-b): estimates, CI coverage rates, and CI widths were concordant. CI\ncoverage rates, which ranged from 87.7%-91.2%, were slightly below the nominal rate of 95%,\nlikely due to mild model misspecification. The estimated effect of the perturbation on gRNA\nexpression exp(\u02c6\u03b2g\n1) was unexpectedly large: the 95% CI for this parameter (averaged across pairs)\nwas [4306, 5186] and [300, 316] on the Gasperini and Xie data, respectively. We reasoned that\nthe datasets lay in a region of the parameter space in which thresholding is a tenable strategy\n(provided the threshold is selected well). However, this was not obvious a priori and may not\nbe the case for other datasets. We note that GLM-EIV produced outlier estimates (defined as\nestimated fold change < 0.75 or > 1.25) on a small (< 2.5% on Gasperini, < 0.05% on Xie)\nnumber of pairs consisting of a handful of genes, likely due to non-global EM convergence. These\noutliers are not plotted in Figures 4a-b but were used to compute the CI coverage reported in\nthe inset tables.\nTo evaluate performance of GLM-EIV versus thresholding in more challenging settings, we\nincreased the difficulty of the perturbation assignment problem by generating partially-synthetic\ndatasets. First, for a given pair, we sampled gRNA counts directly from the fitted GLM-EIV\n20\nT. Barry, K. Roeder, and E. Katsevich\nmodel. Next, to simulate elevated background contamination, we sampled gRNA counts from a\nslightly modified version of the fitted model in which we increased the mean gRNA expression\nof unperturbed cells while holding constant the mean gRNA expression of perturbed cells. We\ndefined a parameter called \u201cexcess background contamination\u201d (normed to take values in [0, 1]) to\nquantify the relative distance between the unperturbed and perturbed gRNA count distributions.\nWe held fixed the real-data gene expressions, library sizes, covariates, and fitted perturbation\nprobabilities in all settings.\nWe generated partially-synthetic data in the above manner for each of the 322 positive control\npairs in the Gasperini dataset, varying excess background contamination over the interval [0, 0.4].\nWe then applied GLM-EIV and the thresholding method to analyze the data. We present results\non two example pairs (the pair containing gene LRIF1 and the pair containing gene NDUFA2) in\nFigures 4c-d. We observed that the estimate produced by the methods on the raw data (depicted\nas a horizontal black line) coincided almost exactly with the estimate produced by the methods\non the partially-synthetic data generated by setting excess background contamination to zero\n(This result replicated across nearly all pairs; average relative difference 0.003.) We additionally\nobserved that as excess background contamination increased, the performance of thresholded\nregression degraded considerably while that of GLM-EIV remained stable.\nWe generalized the above analysis to the entire set of positive control pairs. First, for each\npair we computed the \u201crelative estimate change\u201d (REC) as a function of excess background\ncontamination, defined as the relative difference between the estimate at a given level of ex-\ncess contamination and zero excess contamination (Figure 4d). Next, we computed the median\nREC across all positive control pairs (Figure 4e; upper and lower bands indicate the pointwise\ninterquartile range of the REC). As excess background contamination increased, thresholded re-\ngression exhibited severe attenuation bias (as reflected by large median REC values); GLM-EIV,\nby contrast, remained mostly stable. Finally, letting \u02c6\u03b2m\n1\ndenote the estimate obtained on the\nExponential family measurement error models for single-cell CRISPR screens\n21\nraw data, we computed the CI coverage of \u02c6\u03b2m\n1 as a function of excess contamination. Under the\nassumption that \u02c6\u03b2m\n1 is close to the true parameter \u03b2m\n1 , the CI coverage of the former is similar\nto that of the latter. We computed the CI coverage of \u02c6\u03b2m\n1 by calculating each individual pair\u2019s\ncoverage of \u02c6\u03b2m\n1 (across the Monte Carlo replicates) and then averaging this quantity across all\npairs. GLM-EIV exhibited significantly higher CI coverage than thresholded regression as the\ndata became increasingly contaminated (Figure 4f; bands indicate 95% pointwise CIs). Coverage\nrates were slightly above the nominal level of 95% in some settings because we covered an esti-\nmate of \u03b2m\n1 rather than \u03b2m\n1 itself, leading to mild \u201coverfitting.\u201d Nonetheless, this experiment was\nmeaningful to assess the stability of both methods to elevated background contamination.\n8. Real data application II: assigning perturbations to cells on low-MOI data\nWe sought to explore whether the gRNA mixture assignment method that we proposed in Section\n5.3 \u2014 which is in effect a special case of GLM-EIV \u2014 might be an independently useful tool for\nassigning gRNAs to cells on real single-cell CRISPR screen data. We applied the gRNA mixture\nassignment method to assign gRNAs to cells on a low multiplicity-of-infection (or MOI) single-cell\nCRISPR screen of immune cells (Papalexi and others, 2021). (A low-MOI dataset, in contrast to a\nhigh-MOI dataset, is one in which the experimenter has aimed to insert exactly one perturbation\ninto each cell.) We elected to assess the performance of the gRNA mixture assignment method on\nlow-MOI data because the \u201cground truth\u201d gRNA-to-cell mapping is easier to ascertain in low MOI\nthan in high MOI. The majority of cells in a low-MOI screen contains a single perturbation, while\na fraction of cells contains zero or two or more perturbations. Thus, if a given gRNA constitutes\na large fraction (say, > 25%) of the gRNA reads in a given cell, we can confidently map that\ngRNA to that cell. Athough not foolproof, this strategy yields a reasonable approximation to\nthe ground truth in low MOI. (There is no analogous strategy for obtaining ground truth gRNA\nassignments in high MOI, as each cell in high MOI contains many gRNAs, and the number of\n22\nT. Barry, K. Roeder, and E. Katsevich\ngRNAs per cell is indeterminate and variable.)\nWe used our proposed gRNA mixture assignment method to obtain gRNA-to-cell assignments\nfor each gRNA in the low-MOI dataset (after restricting our attention to the 95% most highly\nexpressed gRNAs). We included the standard technical factors as covariates, including biolog-\nical replicate. We compared the mixture-model-based gRNA assignments to the ground truth\nassignments; the latter were obtained in the manner described above. Encouragingly, we found\nthat these two methods produced near-identical results. For example, the mixture model deter-\nmined that gRNA \u201cCUL3g2\u201d was present in 141 cells (and absent in the rest), while the ground\ntruth method indicated that \u201cCUL3g2\u201d was present in 137 cells (Figure 5a). Treating the ground\ntruth assignments as a reference, we constructed a confusion matrix to assess the classification\naccuracy of the mixture method assignments on CUL3g2 (Figure 5b). The sensitivity, specificity,\nand balanced accuracy of the mixture method assignments were high (1.000, 0.9998, and 0.9998,\nrespectively).\nWe replicated this analysis across the entire set of gRNAs, finding that the mixture method\nassignments exhibited consistently high concordance with the ground truth assignments as mea-\nsured by sensitivity, specificity, and balanced accuracy (although there were a few outliers; Figure\n5c). We concluded that the mixture assignment method was a statistically principled, fast, and\nnumerically stable strategy for the recapitulating the ground truth assignments with high fi-\ndelity. We sought to compare our gRNA mixture assignment method against the Nat. Biotech.\n2020 Poisson-Gaussian mixture method. Unfortunately, as discussed elsewhere (Section 5.3 and\nAppendix D), we were unable to get the Nat. Biotech. 2020 method (or approximations thereof\nwritten in R) working. We note that, in contrast to the Nat. Biotech. 2020 method, the proposed\nmethod allows for the inclusion of covariates (e.g., library size and batch) and models the gRNA\ncounts directly.\nExponential family measurement error models for single-cell CRISPR screens\n23\n9. Discussion\nIn this work we studied the problem of estimating the effect sizes of perturbations on changes\nin gene expression in high-MOI single-cell CRISPR screens, focusing specifically on the chal-\nlenge that the perturbation is unobserved. We showed through empirical, theoretical, and sim-\nulation analyses that the commonly-used thresholding method poses several difficulties: there\nexist settings (i.e., high background contamination settings) in which thresholding is not a ten-\nable strategy, and in settings in which thresholding is a tenable strategy (i.e., low background\ncontamination settings), selecting a good threshold is challenging and consequential. Next, we\ndeveloped GLM-EIV, a method that jointly models the gene and gRNA modalities to implicitly\nassign perturbation identities to cells and estimate perturbation effect sizes, thereby overcoming\nlimitations of the thresholding method. GLM-EIV demonstrated significantly improved perfor-\nmance relative to the thresholding method in high background contamination settings on both\nsynthetic and realistic semi-synthetic data.\nHowever, GLM-EIV and the thresholding method demonstrated roughly similar performance\non the two real high-MOI datasets that we examined, as the real data exhibited lower background\ncontamination than anticipated. We believe that this is an interesting finding in itself; moreover,\nfuture datasets may demonstrate higher levels of background contamination, in which case GLM-\nEIV could serve as an immediately applicable analytic tool. Finally, the gRNA mixture assignment\nmethod, which under the hood exploits the estimation machinery of GLM-EIV, is a statistically\nprincipled, numerically stable, fast, and accurate strategy for obtaining gRNA-to-cell assignments\non real data; these assignments can used as input to downstream methods (e.g., negative binomial\nregression or SCEPTRE; Figure 5d).\nWe anticipate that GLM-EIV could be applied to other types of multi-modal single-cell data,\nsuch as single-cell chromatin accessibility assays. A question of interest in such experiments is\nwhether chromatin state (i.e., closed or open) is associated with the expression of a gene or\n24\nT. Barry, K. Roeder, and E. Katsevich\nabundance of a protein (Mimitou and others, 2021). We do not directly observe the chromatin\nstate of a cell; instead, we observe tagged DNA fragments that serve as count-based proxies\nfor whether a given region of chromatin is open or closed. GLM-EIV might be applied in such\nexperiments to aid in the selection of thresholds or to analyze whole datasets. The full GLM-EIV\nmodel potentially could be applied to analyze low-MOI single-cell CRISPR screen data, but we\nanticipate that the relative ease of assigning gRNAs to cells in low MOI (as described in section\n8) may obviate the need for GLM-EIV in that setting.\nThe closest parallels to GLM-EIV in the statistical methodology literature are Gr\u00a8un and Leisch\n(2008) and Ibrahim (1990). Gr\u00a8un and Leisch derived a method for estimation and inference in\na k-component mixture of GLMs. While we prefer to view GLM-EIV as a generalized errors-in-\nvariables method, the GLM-EIV model is equivalent to a two-component mixture of products\nof GLM densities. Ibrahim proposed a procedure for fitting GLMs in the presence of missing-at-\nrandom covariates. Our method, by contrast, involves fitting two conditionally independent GLMs\nin the presence of a totally latent covariate. Thus, while Ibrahim and Gr\u00a8un & Leisch are helpful\nreferences, our estimation and inference tasks are more complex than theirs. Next, Aigner (1973)\nand Savoca (2000) proposed measurement error models that consist of unobserved binary rather\nthan continuous predictors; the latter are more commonly used in measurement error models.\nGLM-EIV likewise consists of a latent binary predictor, but unlike Aigner and Savoca, GLM-EIV\nhandles a much broader class of exponential family-generated data. Finally, GLM-EIV accounts\nfor a common source of measurement error between the predictor and response, a property not\nshared by classical measurement error models (Carroll and others, 2006). Additional related work\nis relayed in Appendix F.\nGLM-EIV might be applied to areas beyond genomics, such as psychology. Some psychological\nconstructs (e.g., presence or absence of a social media addiction) are latent and can be assessed\nonly through an imperfect proxy (e.g., the number of times one has checked social media). Re-\nExponential family measurement error models for single-cell CRISPR screens\n25\nsearchers might use GLM-EIV to regress an outcome variable (e.g., self-reported well-being) onto\nthe latent construct via the imperfect proxy, potentially resolving challenges related to attenua-\ntion bias and threshold selection. Applications to psychology and other areas are a topic of future\ninvestigation.\nSoftware, code, and results\nThe gRNA-only mixture assignment functionality of GLM-EIV is implemented in our sceptre\ntoolkit for single-cell CRISPR screen analysis (github.com/Katsevich-Lab/sceptre). The sceptre\nuser manual (timothy-barry.github.io/sceptre-book/sceptre.html) presents a detailed guide\non analyzing data using the sceptre software, including several sections on assigning gRNAs to\ncells using the mixture assignment method introduced in this work.\nResults are deposited at upenn.box.com/v/glmeiv-files-v1. Github repositories containing\nmanuscript replication code, the glmeiv R package, and the cloud/HPC-scale GLM-EIV pipeline\nare available at github.com/timothy-barry/glmeiv-manuscript, github.com/timothy-barry/\nglmeiv, and github.com/timothy-barry/glmeiv-pipeline, respectively. Detailed replication\ninstructions are available in the first repository.\nAcknowledgements\nWe thank Eric Tchetgen Tchetgen for helpful conversations, Xuran Wang for helping to pro-\ncess the Xie dataset, and Songcheng Dai for helping to deploy the GLM-EIV pipeline on Azure.\nWe additionally thank three anonymous reviewers whose comments considerably improved the\nmanuscript. This work used the Extreme Science and Engineering Discovery Environment (XSEDE;\nNSF grant ACI-1548562) and the Bridges-2 system (NSF grant ACI-1928147) at the Pittsburgh\nSupercomputing Center. This work is funded by National Institute of Mental Health (NIMH)\ngrant R01MH123184 and NSF grant DMS-2113072.\n26\nREFERENCES\nReferences\nAigner, Dennis J. (1973). Regression with a binary independent variable subject to errors of\nobservation. Journal of Econometrics 1(1), 49\u201359.\nBarry, Timothy and others. (2021). SCEPTRE improves calibration and sensitivity in single-\ncell CRISPR screen analysis. Genome Biology, 1\u201319.\nCand`es, Emmanuel and others. (2018). Panning for gold: \u2018model-X\u2019 knockoffs for high dimen-\nsional controlled variable selection. Journal of the Royal Statistical Society. Series B: Statistical\nMethodology 80(3), 551\u2013577.\nCarroll, Raymond J and others. (2006). Measurement error in nonlinear models: a modern\nperspective. Chapman and Hall/CRC.\nChoudhary, Saket and Satija, Rahul. (2022). Comparison and evaluation of statistical\nerror models for scrna-seq. Genome Biology 23(1), 1\u201320.\nDatlinger, Paul and others. (2017). Pooled crispr screening with single-cell transcriptome\nreadout. Nature Methods 14(3), 297\u2013301.\nFitzpatrick, Patrick. (2009). Advanced calculus, Volume 5. American Mathematical Soc.\nGallagher, Michael D. and Chen-Plotkin, Alice S. (2018). The post-gwas era: From\nassociation to function. American Journal of Human Genetics 102(5), 717\u2013730.\nGasperini, Molly and others. (2019). A genome-wide framework for mapping gene regulation\nvia cellular genetic screens. Cell 176(1-2), 377\u2013390.e19.\nGr\u00a8un, Bettina and Leisch, Friedrich. (2008). Finite Mixtures of Generalized Linear Re-\ngression Models. Heidelberg: Physica-Verlag HD, pp. 205\u2013230.\nREFERENCES\n27\nHafemeister, Christoph and Satija, Rahul. (2019). Normalization and variance stabi-\nlization of single-cell RNA-seq data using regularized negative binomial regression. Genome\nBiology 20(1), 1\u201315.\nHill, Andrew J. and others. (2018). On the design of crispr-based single-cell molecular screens.\nNature Methods 15(4), 271\u2013274.\nIbrahim, Joseph G. (1990).\nIncomplete data in generalized linear models.\nJournal of the\nAmerican Statistical Association 85(411), 765\u2013769.\nLin, Kevin Z, Lei, Jing and Roeder, Kathryn. (2021).\nExponential-family embedding\nwith application to cell developmental trajectories for single-cell rna-seq data. Journal of the\nAmerican Statistical Association 116(534), 457\u2013470.\nLiu, Molei, Katsevich, Eugene, Janson, Lucas and Ramdas, Aaditya. (2022). Fast and\npowerful conditional randomization testing via distillation. Biometrika 109(2), 277\u2013293.\nLouis, By Thomas A. (1982). Finding the Observed Information Matrix when Using the EM\nAlgorithm. Journal of the Royal Statistical Society. Series B: Statistical Methodology 44(2),\n226\u2013233.\nMcCullagh, P. and Nelder, J. A. (1990). Generalized Linear Models, 2nd Edn.\nMimitou, Eleni P and others. (2021). Scalable, multimodal profiling of chromatin accessibility,\ngene expression and protein levels in single cells. Nature biotechnology 39(10), 1246\u20131258.\nMorris, John and others. (2023).\nDiscovery of target genes and pathways at gwas loci by\npooled single-cell crispr screens. Science 380(6646), eadh7699.\nMostafavi,\nHakhamanesh,\nSpence,\nJeffrey\nP,\nNaqvi,\nSahin\nand\nPritchard,\nJonathan K. (2023). Systematic differences in discovery of genetic effects on gene expression\nand complex traits. Nature Genetics, 1\u201310.\n28\nREFERENCES\nMusunuru, Kiran and others. (2021).\nIn vivo crispr base editing of pcsk9 durably lowers\ncholesterol in primates. Nature 593(7859), 429\u2013434.\nPapalexi, Efthymia and others. (2021). Characterizing the molecular regulation of inhibitory\nimmune checkpoints with multimodal single-cell screens. Nature genetics 53(3), 322\u2013331.\nPrzybyla, Laralynne and Gilbert, Luke A. (2022). A new era in functional genomics\nscreens. Nature Reviews Genetics 23(2), 89\u2013103.\nReplogle, Joseph M and others. (2020). Combinatorial single-cell crispr screens by direct\nguide rna capture and targeted sequencing. Nature biotechnology 38(8), 954\u2013961.\nRipley, Brian and others. (2013). Package \u2018mass\u2019. Cran r 538, 113\u2013120.\nRobinson, Mark D and Smyth, Gordon K. (2008). Small-sample estimation of negative\nbinomial dispersion, with applications to sage data. Biostatistics 9(2), 321\u2013332.\nSarkar, Abhishek and Stephens, Matthew. (2021). Separating measurement and expres-\nsion models clarifies confusion in single-cell RNA sequencing analysis. Nature Genetics 53(6),\n770\u2013777.\nSavoca, E. (2000). Measurement errors in binary regressors: An application to measuring the\neffects of specific psychiatric diseases on earnings. Health Services and Outcomes Research\nMethodology 1(2), 149\u2013164.\nStefanski, L. A. (2000).\nMeasurement Error Models.\nJournal of the American Statistical\nAssociation 95(452), 1353\u20131358.\nTownes, F. William and others. (2019). Feature selection and dimension reduction for single-\ncell RNA-Seq based on a multinomial model. Genome Biology 20(1), 1\u201316.\nREFERENCES\n29\nTrapnell, Cole and others. (2014). The dynamics and regulators of cell fate decisions are\nrevealed by pseudotemporal ordering of single cells. Nature Biotechnology 32(4), 381\u2013386.\nWang, Lingfei. (2021). Single-cell normalization and association testing unifying crispr screen\nand gene co-expression analyses with normalisr. Nature communications 12(1), 6395.\nXie, Shiqi and others. (2019). Global analysis of enhancer targets reveals convergent enhancer-\ndriven regulatory modules. Cell Reports 29(9), 2570\u20132578.e5.\n30\nREFERENCES\n10. Figures\na\ngene\ngRNA\nExpression\nDensity\nPerturbed\nUnperturbed\nc\nb\nd\nPerturbed\nUnperturbed\nDi\ufb00erential expression\ngRNA \nexpression\nGene \nexpression\nTechnical \nfactors\nPerturbation \n(latent)\n?\nGene \nexpres.\ngRNA \ncount\nPerturbation \n(latent)\nTechnical \nfactors\n25\n0\n0\nz1\n29\n1\n0\nz2\n11\n8\n1\nz3\n8\n3\n1\nz4\n?\nFig. 1. Experimental design and analysis challenges: a, Experimental design. For a given pertur-\nbation (e.g., the perturbation indicated in blue), we partition the cells into two groups: perturbed and\nunperturbed. Next, for a given gene, we conduct a differential expression analysis across the two groups,\nyielding an estimate of the impact of the given perturbation on the given gene. b, DAG representing all\nvariables in the system. The perturbation (latent) impacts both gene expression and gRNA expression;\ntechnical factors act as confounders, also impacting gene and gRNA expression. The target of estimation\nis the effect of the perturbation on gene expression. c, Schematic illustrating the \u201cbackground read\u201d phe-\nnomenon. Due to errors in the sequencing and alignment processes, unperturbed cells exhibit a nonzero\ngRNA count distribution (bottom). The target of estimation is the change in mean gene expression in\nresponse to the perturbation (top). d, Example data on four cells for a given perturbation-gene pair.\nNote that (i) the perturbation is unobserved, and (ii) the gene and gRNA data are discrete counts.\nREFERENCES\n31\nFig. 2. Empirical challenges of thresholded regression. a-b, Estimates for fold change (i.e., exp(\u03b2m\n1 )\nin model (4.3)) produced by threshold = 5 versus threshold = 1 (a) and threshold = 5 versus threshold\n= 20 (b). The selected threshold substantially impacts the results. c-d, p-values (c) and CI widths (d)\nproduced by threshold = 5 versus threshold = 20. The p-values correspond to a test of the null hypothesis\nH0 : \u03b2m\n1 = 0, i.e., a log fold change in gene expression of zero. A threshold of 5 yields more significant\np-values and more confident estimates. e-f, Empirical distribution of a gRNA from Gasperini (e) and\nXie (f) data (0 counts not shown). These gRNA count distributions do not appear to imply an obvious\nthreshold.\n32\nREFERENCES\nFig. 3. Simulation study. Columns correspond to distributions (Poisson, NB with known s, NB with\nestimated s), and rows correspond to metrics (bias, MSE, coverage, CI width, and time). Methods\nare shown in different colors; GLM-EIV (green) is masked by accelerated GLM-EIV (red) in several\npanels. Generally, GLM-EIV (both accelerated and non-accelerated versions) outperformed the gRNA-\nmixture/NB-regression method, which in turn outperformed the thresholding/NB-regression method.\nThe rejection probability (i.e., the probability of rejecting the null hypothesis H0 : \u03b2m\n1\n= 0 at level\n\u03b1 = 0.05) was strictly 1 across methods and parameter settings, likely because the effect size was fairly\nlarge.\nREFERENCES\n33\nRelative \nestimate \nchange \n(REC)\nFig. 4. Applying GLM-EIV to analyze large-scale, high-MOI data. a-b, Estimates for fold change\nproduced by GLM-EIV and thresholded regression on Gasperini (a) and Xie (b) negative control pairs.\nc-d, Estimates produced by GLM-EIV and thresholded regression on two positive control pairs \u2013 LRIF1\n(a) and NDUFA2 (b) \u2013 plotted as a function of excess background contamination. Grey bands, 95%\nCIs for the target of inference outputted by the methods. e-f, Median relative estimate change (REC;\ne) and confidence interval coverage rate (f) across all 322 positive control pairs, plotted as a function of\nexcess background contamination. Panels (c-f) together illustrate that GLM-EIV demonstrated greater\nstability than thresholded regression as background contamination increased.\n34\nREFERENCES\nPerturbed\nUnperturbed\nPerturbed\n137\n4\nUnperturbed\n0\n20,588\nBalanced accuracy: 0.9998 \nSensitivity: 1.0000 \nSpeci\ufb01city: 0.9998\nMixture assignment\nGround truth assignment\nImpute gRNA \nassignments via \ngRNA mixture \nmodel \u2192 NB \nregression \n(standard setting) \nBackground \ncontamination\nhigh\nlow\nGLM-EIV for \nwhole data \nanalysis\nClassi\ufb01cation metrics for gRNA \nCUL3g2\nReal data analysis work\ufb02ow\nFig. 5. The gRNA-only mixture assignment functionality of GLM-EIV accurately assigns\ngRNAs to cells on real low-MOI data. a, Each point represents a cell. The position of each cell\nalong the vertical axis indicates the number of gRNA reads (from gRNA \u201cCUL3g2\u201d) observed in that\ncell. Cells in the left column were classified by the gRNA mixture model as perturbed, while those in\nthe right column were classified as unperturbed. Purple (resp., red) cells were classified by the ground\ntruth method as perturbed (resp., unperturbed). b, A confusion matrix comparing the gRNA-to-cell\nmixture model classifications against the ground truth classifications for gRNA \u201cCUL3g2.\u201d The two sets\nof classifications were highly concordant, as quantified by balanced accuracy, sensitivity, and specificity\nmetrics. c, The balanced accuracy (left), sensitivity (middle), and specificity (right) of the gRNA mixture\nassignment method across all gRNAs. d, The proposed data analysis workflow. If the level of background\ncontamination is low, then the gRNA mixture method can be used to impute perturbation identities onto\ncells, which can then be plugged into downstream analytic tools, such as negative binomial regression or\nSCEPTRE. On the other hand, if the level of background contamination is high, then the entire GLM-\nEIV model can be used to analyze the data.\nREFERENCES\n35\nAPPENDIX\nA. Theoretical details for thresholding estimator\nWe study the thresholding method from a theoretical perspective, recovering in a simplified Gaus-\nsian setting phenomena revealed in the empirical analysis. Suppose we observe gRNA expression\nand gene expression data (g1, m1), . . . , (gn, mn) on n cells from the following linear model:\nmi = \u03b2m\n0 + \u03b2m\n1 pi + \u03f5i;\ngi = \u03b2g\n0 + \u03b2g\n1pi + \u03c4i;\npi \u223cBern(\u03c0);\n\u03f5i, \u03c4i \u223cN(0, 1),\n(A.1)\nwhere pi, \u03c4i, and \u03f5i are independent. For a given threshold c \u2208R, the imputed perturbation\nassignment \u02c6pi is \u02c6pi = I(gi \u2a7ec). The thresholding estimator \u02c6\u03b2m\n1 is the OLS solution, i.e. \u02c6\u03b2m\n1 =\n\u0002Pn\ni=1(\u02c6pi \u2212\u02c6p)2\u0003\u22121 \u0002Pn\ni=1(\u02c6pi \u2212\u02c6p)(mi \u2212m)\n\u0003\n. We derive the almost sure limit of \u02c6\u03b2m\n1 :\nProposition 1 The almost sure limit (as n \u2192\u221e) of \u02c6\u03b2m\n1 is\n\u02c6\u03b2m\n1\na.s.\n\u2212\u2212\u2192\u03b2m\n1\n\u0012 \u03c0(\u03c9 \u2212E[\u02c6pi])\nE[\u02c6pi](1 \u2212E[\u02c6pi])\n\u0013\n\u2261\u03b2m\n1 \u03b3(\u03b2g\n1, \u03c0, c, \u03b2g\n0),\n(A.2)\nwhere E[\u02c6pi] = \u03b6(1 \u2212\u03c0) + \u03c9\u03c0, \u03c9 \u2261\u03a6 (\u03b2g\n1 + \u03b2g\n0 \u2212c), and \u03b6 \u2261\u03a6 (\u03b2g\n0 \u2212c) .\nThe function \u03b3 : R4 \u2192R does not depend on the gene expression parameters \u03b2m\n1 or \u03b2m\n0 . The\nasymptotic relative bias b : R4 \u2192R of \u02c6\u03b2m\n1 is given by\nb(\u03b2g\n1, \u03c0, c, \u03b2g\n0) \u2261\n1\n\u03b2m\n1\n\u0010\n\u03b2m\n1 \u2212lim\na.s.\n\u02c6\u03b2m\n1\n\u0011\n= 1 \u2212\u03b3(\u03b2g\n1, \u03c0, c, \u03b2g\n0).\nHaving derived an exact expression for the asymptotic relative bias of \u02c6\u03b2m\n1 , we can prove several\nresults about this quantity. We fix \u03c0 to 1/2 for simplicity. (In reality, \u03c0 is smaller, but the relevant\nstatistical phenomena emerge for \u03c0 = 1/2.) We start with informal proposition statements; we\nfollow up with formal proposition statements below. First, the thresholding estimator strictly\nunderestimates (in absolute value) the true value of \u03b2m\n1 over all choices of the threshold c and\nover all values of the regression coefficients (\u03b2m\n0 , \u03b2m\n1 ) and (\u03b2g\n0, \u03b2g\n1). This phenomenon, called\n36\nREFERENCES\nattenuation bias, is a common attribute of estimators that ignore measurement error in errors-\nin-variables models (Stefanski, 2000). Second, the magnitude of the bias decreases monotonically\nin \u03b2g\n1, comporting with the intuition that the problem becomes easier as the gRNA mixture\ndistribution becomes increasingly well-separated. Third, the Bayes-optimal decision boundary\ncbayes \u2208R (i.e., the most accurate decision boundary for classifying cells) is a critical value of\nthe bias function. Finally, and most subtly, there is no universally applicable rule for selecting a\nthreshold that yields minimal bias: when \u03b2g\n1 is small, setting the threshold to an arbitrarily large\nnumber yields smaller bias than setting the threshold to the Bayes decision boundary; when \u03b2g\n1\nis large, the reverse is true.\nWe state five propositions labeled 2 \u2013 6 corresponding to the informal claims above; these\npropositions are depicted visually in Figure 6.\nProposition 2 Fix \u03c0 = 1/2. For all (\u03b2g\n1, c, \u03b2g\n0) \u2208R3, the asymptotic relative bias is positive, i.e.\nb(\u03b2g\n1, 1/2, c, \u03b2g\n0) > 0.\nProposition 3 Fix \u03c0 = 1/2. The asymptotic relative bias b decreases monotonically in \u03b2g\n1, i.e.\n\u2202b\n\u2202(\u03b2g\n1) (\u03b2g\n1, 1/2, c, \u03b2g\n0) \u2a7d0.\nLet cbayes denote the Bayes-optimal decision boundary for classifying cells as perturbed or\nunperturbed, i.e. cbayes = (1/2)(\u03b2g\n0 + \u03b2g\n1) for \u03c0 = 1/2. We have that cbayes is a critical value of\nthe bias function:\nProposition 4 For \u03c0 = 1/2 and given (\u03b2g\n1, \u03b2g\n0) \u2208R2, the Bayes-optimal decision boundary cbayes\nis a critical value of the bias function b, i.e.\n\u2202b\n\u2202c (\u03b2g\n1, 1/2, cbayes, \u03b2g\n0) = 0.\nFurthermore, as the threshold tends to infinity, the asymptotic relative bias b tends to \u03c0:\nREFERENCES\n37\nProposition 5 Assume without loss of generality that \u03b2g\n1 > 0. As the threshold c tends to infinity,\nthe asymptotic relative bias b tends to \u03c0, i.e.\nlim\nc\u2192\u221eb(\u03b2g\n1, \u03c0, c, \u03b2g\n0) = \u03c0.\nAs a corollary, when \u03c0 = 1/2, asymptotic relative bias tends to 1/2 as c tends to infinity.\nFinally, we compare two threshold selection strategies head-to-head: setting the threshold to an\narbitrarily large number, and setting the threshold to the Bayes-optimal decision boundary:\nProposition 6 Assume without loss of generality that \u03b2g\n1 > 0. For \u03b2g\n1 \u2208[0, 2\u03a6\u22121(3/4)), we have\nthat\nb(\u03b2g\n1, 1/2, cbayes, \u03b2g\n0) > b(\u03b2g\n1, 1/2, \u221e, \u03b2g\n0).\nFor \u03b2g\n1 = 2\u03a6\u22121(3/4), we have that\nb(\u03b2g\n1, 1/2, cbayes, \u03b2g\n0) = b(\u03b2g\n1, 1/2, \u221e, \u03b2g\n0).\nFinally, for \u03b2g\n1 \u2208(2\u03a6\u22121(3/4), \u221e), we have that\nb(\u03b2g\n1, 1/2, cbayes, \u03b2g\n0) < b(\u03b2g\n1, 1/2, \u221e, \u03b2g\n0).\nIn other words, setting the threshold to a large number yields a smaller bias when \u03b2g\n1 is small\n(i.e., \u03b2g\n1 < 2\u03a6\u22121(3/4) \u22481.35; Figure 7a, left); setting the threshold to the Bayes-optimal decision\nboundary yields a smaller bias when \u03b2g\n1 is large (i.e., \u03b2g\n1 > 2\u03a6\u22121(3/4); Figure 7a, right); and the\ntwo approaches coincide when \u03b2g\n1 is intermediate (i.e., \u03b2g\n1 = 2\u03a6\u22121(3/4); Figure 7a, middle).\nNext, we study the variance of the thresholding estimator, considering a slightly simpler\nmodel for this purpose. Suppose the intercepts in (A.1) are fixed at 0 (i.e., \u03b2m\n0 = \u03b2g\n0 = 0). For\nnotational simplicity we write \u03b2m = \u03b2m\n1\nand \u03b2g = \u03b2g\n1. The thresholding estimator \u02c6\u03b2m is the\nno-intercept OLS solution \u02c6\u03b2m =\n\u0002Pn\ni=1 \u02c6p2\ni\n\u0003\u22121 [Pn\ni=1 \u02c6pimi] . The following proposition derives the\nscaled, asymptotic distribution of \u02c6\u03b2m :\n38\nREFERENCES\nFig. 6. Bias as a function of threshold. This figure visually depicts Propositions 2-6, which were\nstated informally above. Asymptotic relative bias is plotted on the vertical axis, and the threshold is\nplotted on the horizontal axis. Panels correspond to different values of \u03b2g\n1. Vertical blue lines indicate\nthe Bayes-optimal decision boundary. Observe that (a) bias is strictly nonzero (proposition 2); (b) bias\ndecreases monotonically in \u03b2g\n1 (Proposition 3); (c) the Bayes-optimal decision boundary is a critical value\nof the bias function (Proposition 4), in some cases a maximum and in other cases a minimum; (d) as\nthe threshold tends to infinity, the bias converges to 1/2 (Proposition 5); and (e) when \u03b2g\n1 < 1.35, an\narbitrarily large number yields a smaller bias; by contrast, when \u03b2g\n1 > 1.35, the Bayes-optimal decision\nboundary yields a smaller bias (Proposition 6). Together, these results illustrate that selecting a good\nthreshold is deceptively challenging.\nProposition 7 The limiting distribution of \u02c6\u03b2m is\n\u221an(\u02c6\u03b2m \u2212l)\nd\u2212\u2192N\n \n0, \u03b2m\u03c9\u03c0(\u03b2m \u22122l) + E[\u02c6pi](1 + l2)\n(E[\u02c6pi])2\n!\n,\nwhere\nl \u2261\u03b2m\u03c9\u03c0/[\u03b6(1 \u2212\u03c0) + \u03c9\u03c0];\nE[\u02c6pi] = \u03c0\u03c9 + (1 \u2212\u03c0)\u03b6;\n\u03c9 \u2261\u03a6(\u03b2g \u2212c);\n\u03b6 \u2261\u03a6(\u2212c).\nThis proposition yields an asymptotically exact bias-variance decomposition for \u02c6\u03b2m: as the\nthreshold tends to infinity, the bias decreases and the variance increases. Figure 7 plots the\nbias-variance decomposition as a function of the threshold.\nA.1\nOrganization\nThe following subsections prove all propositions. Section A.2 introduces some notation. Section\nA.3 establishes almost sure convergence of the thresholding estimator in the model (A.1), proving\nProposition 1. Section A.4 simplifies the expression for the attenuation function \u03b3, and section\nREFERENCES\n39\nFig. 7. Thresholding method bias-variance decomposition. Bias decreases and variance increases\nas the threshold tends to infinity. \u03b2g\n1 = 1, \u03b2m\n1 = 1, and \u03c0 = 0.1 in this plot.\nA.5 computes derivatives of \u03b3 to be used throughout the proofs. Section A.6 establishes the\nlimit in c of \u03b3, proving Proposition 5. Section A.7 establishes that the Bayes-optimal decision\nboundary is a critical value of \u03b3, proving Proposition 4, and section A.8 compares the competing\nthreshold selection strategies head-to-head, proving Proposition 6. Section A.9 demonstrates that\n\u03b3 is monotone in \u03b2g\n1, proving Proposition 3, and Section A.10 establishes attenuation bias of the\nthresholding estimator, proving Proposition 2. Finally, Section A.11 derives the bias-variance de-\ncomposition of the thresholding estimator in the no-intercept version of A.1, proving Proposition\n7.\nA.2\nNotation\nAll notation introduced in this subsection (i.e., A.2) pertains to the Gaussian model with inter-\ncepts (A.1). Recall that the attenuation function \u03b3 : R4 \u2192R is defined by\n\u03b3(\u03b2g\n1, c, \u03c0, \u03b2g\n0) =\n\u03c0(\u03c9 \u2212E[\u02c6pi])\nE[\u02c6pi](1 \u2212E[\u02c6pi]),\nwhere\nE[\u02c6pi] = \u03b6(1 \u2212\u03c0) + \u03c9\u03c0;\n\u03c9 = \u03a6 (\u03b2g\n1 + \u03b2g\n0 \u2212c) ;\n\u03b6 = \u03a6 (\u03b2g\n0 \u2212c) .\n40\nREFERENCES\nAdditionally, recall that the asymptotic relative bias function b : R4 \u2192R is b(\u03b2g\n1, c, \u03c0, \u03b2g\n0) =\n1 \u2212\u03b3(\u03b2g\n1, c, \u03c0, \u03b2g\n0). Next, we define the functions g and h : R4 \u2192R by\ng(\u03b2g\n1, c, \u03c0, \u03b2g\n0) = (1 \u2212\u03c0) (\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c)) \u2212(1 \u2212\u03c0) (\u03a6(\u03b2g\n0 \u2212c))\n(A.3)\nand\nh(\u03b2g\n1, c, \u03c0, \u03b2g\n0) = [(1 \u2212\u03c0) (\u03a6(\u03b2g\n0 \u2212c)) + \u03c0 (\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c))] \u00d7\n[(1 \u2212\u03c0) (\u03a6(c \u2212\u03b2g\n0)) + \u03c0 (\u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1))] .\n(A.4)\nWe use f : R \u2192R to denote the N(0, 1) density, and we denote the right-tail probability\nprobability of f by \u00af\u03a6, i.e.,\n\u00af\u03a6(x) =\nZ \u221e\nx\nf = \u03a6(\u2212x).\nThe parameter \u03b2g\n0 is a given, fixed constant throughout the proofs. Therefore, to minimize\nnotation, we typically use \u03b3(\u03b2g\n1, c, \u03c0) (resp., b(\u03b2g\n1, c, \u03c0), g(\u03b2g\n1, c, \u03c0), h(\u03b2g\n1, c, \u03c0)) to refer to the\nfunction \u03b3 (resp., b, g, h) evaluated at (\u03b2g\n1, c, \u03c0, \u03b2g\n0). Finally, for a given function r : Rp \u2192R,\npoint x \u2208Rp, and index i \u2208{1, . . . , p}, we use the symbol Dir(x) to refer to the derivative of the\nith argument of r evaluated at x (sensu Fitzpatrick (2009)). For example, D1\u03b3(\u03b2g\n1, c, 1/2) is the\nderivative of the first argument of \u03b3 (the argument corresponding to \u03b2g\n1) evaluated at (\u03b2g\n1, c, 1/2).\nLikewise, D2g(\u03b2g\n1, c, \u03c0) is the derivative of the second argument of g (the argument corresponding\nto c) evaluated at (\u03b2g\n1, c, \u03c0).\nA.3\nAlmost sure limit of \u02c6\u03b2m\n1\nWe derive the limit in probability of \u02c6\u03b2m\n1 for the Gaussian model with intercepts (A.1). Dividing\nby n in (A.2), we can express \u02c6\u03b2m\n1 as\n\u02c6\u03b2m\n1 =\n1\nn\nPn\ni=1(\u02c6pi \u2212\u02c6pi)(mi \u2212m)\n1\nn\nPn\ni=1(\u02c6pi \u2212\u02c6p)\n.\nREFERENCES\n41\nBy weak LLN, \u02c6\u03b2m\n1\nP\u2212\u2192Cov(\u02c6pi, mi)/V (\u02c6pi) . To compute this quantity, we first compute several\nsimpler quantities:\n1. Expectation of mi: E[mi] = \u03b2m\n0 + \u03b2m\n1 \u03c0.\n2. Expectation of \u02c6pi:\nE[\u02c6pi] = P [\u02c6pi = 1] = P [\u03b2g\n0 + \u03b2g\n1pi + \u03c4i \u2a7ec] =\n(By LOTP) P [\u03b2g\n0 + \u03c4i \u2a7ec] P [pi = 0] + P [\u03b2g\n0 + \u03b2g\n1 + \u03c4i \u2a7ec] P[pi = 1]\n= P [\u03c4i \u2a7ec \u2212\u03b2g\n0] (1 \u2212\u03c0) + P [\u03c4i \u2a7ec \u2212\u03b2g\n1 \u2212\u03b2g\n0] (\u03c0)\n=\n\u0000\u00af\u03a6(c \u2212\u03b2g\n0)\n\u0001\n(1 \u2212\u03c0) +\n\u0000\u00af\u03a6(c \u2212\u03b2g\n1 \u2212\u03b2g\n0)\n\u0001\n(\u03c0) =\n\u03a6(\u03b2g\n0 \u2212c)(1 \u2212\u03c0) + \u03a6(\u03b2g\n1 + \u03b2g\n0 \u2212c)\u03c0 = \u03b6(1 \u2212\u03c0) + \u03c9\u03c0.\n3. Expectation of \u02c6pipi: E [\u02c6pipi] = E [\u02c6pi|pi = 1] P [pi = 1] = P [\u03b2g\n0 + \u03b2g\n1 + \u03c4i \u2a7ec] \u03c0 = \u03c9\u03c0.\n4. Expectation of \u02c6pimi:\nE [\u02c6pimi] = E[\u02c6pi(\u03b2m\n0 + \u03b2m\n1 pi + \u03f5i)] = \u03b2m\n0 E [\u02c6pi] + \u03b2m\n1 E [\u02c6pipi] + E[\u02c6pi\u03f5i]\n= \u03b2m\n0 E[\u02c6pi] + \u03b2m\n1 \u03c9\u03c0 + E[\u02c6pi]E[\u03f5i] = \u03b2m\n0 E[\u02c6pi] + \u03b2m\n1 \u03c9\u03c0.\n5. Variance of \u02c6pi: Because \u02c6pi is binary, we have that V[\u02c6pi] = E[\u02c6pi] (1 \u2212E[\u02c6pi]) .\n6. Covariance of \u02c6pi, mi:\nCov (\u02c6pi, mi) = E [\u02c6pimi] \u2212E[\u02c6pi]E[mi] = \u03b2m\n0 E[\u02c6pi] + \u03b2m\n1 \u03c9\u03c0 \u2212E[\u02c6pi](\u03b2m\n0 + \u03b2m\n1 \u03c0)\n= \u03b2m\n1 \u03c9\u03c0 \u2212E[\u02c6pi]\u03b2m\n1 \u03c0 = \u03b2m\n1 \u03c0 (\u03c9 \u2212E[\u02c6pi]) .\nCombining these expressions, we have that\n\u02c6\u03b2m\n1\nP\u2212\u2192\u03b2m\n1 \u03c0(\u03c9 \u2212E[\u02c6pi])\nE[\u02c6pi](1 \u2212E[\u02c6pi]) = \u03b2m\n1 \u03b3(\u03b2g\n1, c, \u03c0).\n42\nREFERENCES\nA.4\nRe-expressing \u03b3 in a simpler form\nWe rewrite the attenuation fraction \u03b3 in a way that makes it more amenable to theoretical\nanalysis. We leverage the fact that f integrates to unity and is even. We have that\nE [\u02c6pi] = (1 \u2212\u03c0)\u00af\u03a6(c \u2212\u03b2g\n0) + \u03c0\u00af\u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1) = (1 \u2212\u03c0)\u03a6(\u03b2g\n0 \u2212c) + \u03c0\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c),\n(A.5)\nand so\n1 \u2212E [\u02c6pi] = (1 \u2212\u03c0) + \u03c0 \u2212E[\u02c6pi] = (1 \u2212\u03c0)\n\u00001 \u2212\u00af\u03a6(c \u2212\u03b2g\n0)\n\u0001\n+ \u03c0\n\u00001 \u2212\u00af\u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)\n\u0001\n= (1 \u2212\u03c0)\u03a6(c \u2212\u03b2g\n0) + \u03c0\u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1).\n(A.6)\nNext,\n\u03c9 = \u03a6(\u03b2g\n1 + \u03b2g\n0 \u2212c),\n(A.7)\nand so\n\u03c9 \u2212E[\u02c6pi] = \u03a6(\u03b2g\n1 + \u03b2g\n0 \u2212c) \u2212(1 \u2212\u03c0)\u03a6(\u03b2g\n0 \u2212c) \u2212\u03c0\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c)\n(1 \u2212\u03c0)\u03a6(\u03b2g\n1 + \u03b2g\n0 \u2212c) \u2212(1 \u2212\u03c0)\u03a6(\u03b2g\n0 \u2212c).\n(A.8)\nCombining (A.5, A.6, A.7, A.8), we find that\n\u03b3(\u03b2g\n1, c, \u03c0) =\n\u03c0(\u03c9 \u2212E[\u02c6pi])\nE[\u02c6pi](1 \u2212E[\u02c6pi])\n=\n\u03c0 [(1 \u2212\u03c0)\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2212(1 \u2212\u03c0)\u03a6(\u03b2g\n0 \u2212c)]\n[(1 \u2212\u03c0)\u03a6(\u03b2g\n0 \u2212c) + \u03c0\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c)] [(1 \u2212\u03c0)\u03a6(c \u2212\u03b2g\n0) + \u03c0\u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)].\n(A.9)\nAs a corollary, when \u03c0 = 1/2,\n\u03b3(\u03b2g\n1, c, 1/2) =\n\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2212\u03a6(\u03b2g\n0 \u2212c)\n[\u03a6(\u03b2g\n0 \u2212c) + \u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c)] [\u03a6(c \u2212\u03b2g\n0) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)].\n(A.10)\nRecalling the definitions of g (A.3) and h (A.4), we can write \u03b3 as\n\u03b3(\u03b2g\n1, c, \u03c0) = \u03c0g(\u03b2g\n1, c, \u03c0)\nh(\u03b2g\n1, c, \u03c0) .\nREFERENCES\n43\nThe special case (A.10) is identical to\n\u03b3(\u03b2g\n1, c, 1/2) = (4)(1/2)g(\u03b2g\n1, c, 1/2)\n4h(\u03b2g\n1, c, 1/2)\n= 2g(\u03b2g\n1, c, 1/2)\n4h(\u03b2g\n1, c, 1/2),\n(A.11)\ni.e., the numerator and denominator of (A.11) coincide with those of (A.10). We sometimes will\nuse the notation 2 \u00b7 g and 4 \u00b7 h to refer to the numerator and denominator of (A.10), respectively.\nA.5\nDerivatives of g and h in c\nWe compute the derivatives of g and h in c, which we will need to prove subsequent results. First,\nby the FTC (fundamental theorem of calculus) and the evenness of f, we have that\nD2g(\u03b2g\n1, c, \u03c0) = \u2212(1 \u2212\u03c0)f(\u03b2g\n0 + \u03b2g\n1 \u2212c) + (1 \u2212\u03c0)f(\u03b2g\n0 \u2212c)\n= (1 \u2212\u03c0)f(c \u2212\u03b2g\n0) \u2212(1 \u2212\u03c0)f(c \u2212\u03b2g\n0 \u2212\u03b2g\n1).\n(A.12)\nSecond, we have that\nD2h(\u03b2g\n1, c, \u03c0) = \u2212[(1 \u2212\u03c0)f(\u03b2g\n0 \u2212c) + \u03c0f(\u03b2g\n0 + \u03b2g\n1 \u2212c)] [(1 \u2212\u03c0)\u03a6(c \u2212\u03b2g\n0) + \u03c0\u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)]\n+ [(1 \u2212\u03c0)f(c \u2212\u03b2g\n0) + \u03c0f(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)] [(1 \u2212\u03c0)\u03a6(\u03b2g\n0 \u2212c) + \u03c0\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c)]\n= [(1 \u2212\u03c0)f(c \u2212\u03b2g\n0) + \u03c0f(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)] \u00d7\n\u0014\n(1 \u2212\u03c0)\u03a6(\u03b2g\n0 \u2212c) + \u03c0\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2212(1 \u2212\u03c0)\u03a6(c \u2212\u03b2g\n0) \u2212\u03c0\u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)\n\u0015\n.\n(A.13)\nA.6\nLimit of \u03b3 in c\nAssume (without loss of generality) that \u03b2g\n1 > 0. We compute limc\u2192\u221e\u03b3(\u03b2g\n1, c, \u03c0). Observe that\nlim\nc\u2192\u221eg(\u03b2g\n1, c, \u03c0) = lim\nc\u2192\u221eh(\u03b2g\n1, c, \u03c0) = 0.\nTherefore, we can apply L\u2019H\u02c6opital\u2019s rule. We have by (A.12) and (A.13) that\nlim\nc\u2192\u221e\u03b3(\u03b2g\n1, c, \u03c0) = lim\nc\u2192\u221e\n\u03c0D2g(\u03b2g\n1, c, \u03c0)\nD2h(\u03b2g\n1, c, \u03c0)\n44\nREFERENCES\n= lim\nc\u2192\u221e\n\u001a\n(1 \u2212\u03c0)f(c \u2212\u03b2g\n0) + \u03c0f(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)\n\u03c0(1 \u2212\u03c0)f(c \u2212\u03b2g\n0) \u2212\u03c0(1 \u2212\u03c0)f(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)\u00d7\n\u0014\n(1 \u2212\u03c0)\u03a6(\u03b2g\n0 \u2212c) + \u03c0\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2212(1 \u2212\u03c0)\u03a6(c \u2212\u03b2g\n0) \u2212\u03c0\u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)\n\u0015\u001b\u22121\n.\n(A.14)\nWe evaluate the two terms in the product (A.14) separately. Dividing by f(c \u2212\u03b2g\n0 \u2212\u03b2g\n1) > 0, we\nsee that\n(1 \u2212\u03c0)f(c \u2212\u03b2g\n0) + \u03c0f(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)\n\u03c0(1 \u2212\u03c0)f(c \u2212\u03b2g\n0) \u2212\u03c0(1 \u2212\u03c0)f(c \u2212\u03b2g\n0 \u2212\u03b2g\n1) =\n(1\u2212\u03c0)f(c\u2212\u03b2g\n0 )\nf(c\u2212\u03b2g\n0 \u2212\u03b2g\n1 ) + \u03c0\n\u03c0(1\u2212\u03c0)f(c\u2212\u03b2g\n0 )\nf(c\u2212\u03b2g\n0 \u2212\u03b2g\n1 )\n\u2212\u03c0(1 \u2212\u03c0)\n.\n(A.15)\nTo evaluate the limit of (A.15), we first evaluate the limit of\nf(c \u2212\u03b2g\n0)\nf(c \u2212\u03b2g\n0 \u2212\u03b2g\n1) =\nexp [\u2212(1/2)(c \u2212\u03b2g\n0)2]\nexp [\u2212(1/2)(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)2]\n=\nexp[\u2212(1/2)(c2 \u22122c\u03b2g\n0 + (\u03b2g\n0)2)]\nexp [\u2212(1/2)(c2 \u22122c\u03b2g\n0 \u22122c\u03b2g\n1 + (\u03b2g\n0)2 + 2(\u03b2g\n0\u03b2g\n1) + (\u03b2g\n1)2)]\n= exp\n\u0002\n\u2212c2/2 + c\u03b2g\n0 \u2212(\u03b2g\n0)2/2\n+ c2/2 \u2212c\u03b2g\n0 \u2212c\u03b2g\n1 + (\u03b2g\n0)2/2 + \u03b2g\n0\u03b2g\n1 + (\u03b2g\n1)2/2\n\u0003\n= exp[\u2212c\u03b2g\n1 + \u03b2g\n0\u03b2g\n1 + (\u03b2g\n1)2/2] = exp[\u03b2g\n0\u03b2g\n1 + (\u03b2g\n1)2/2] exp[\u2212c\u03b2g\n1].\n(A.16)\nTaking the limit in (A.16), we obtain\nlim\nc\u2192\u221e\nf(c \u2212\u03b2g\n0)\nf(c \u2212\u03b2g\n0 \u2212\u03b2g\n1) = exp[\u03b2g\n0\u03b2g\n1 + (\u03b2g\n1)2/2] lim\nc\u2192\u221eexp[\u2212c\u03b2g\n1] = 0\nfor \u03b2g\n1 > 0. We now can evaluate the limit of (A.15):\nlim\nc\u2192\u221e\n(1 \u2212\u03c0)f(c \u2212\u03b2g\n0) + \u03c0f(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)\n\u03c0(1 \u2212\u03c0)f(c \u2212\u03b2g\n0) \u2212\u03c0(1 \u2212\u03c0)f(c \u2212\u03b2g\n0 \u2212\u03b2g\n1) =\n\u2212\u03c0\n\u03c0(1 \u2212\u03c0) = \u2212\n1\n1 \u2212\u03c0 .\nNext, we compute the limit of the other term in the product (A.14):\nlim\nc\u2192\u221e\n\u0014\n(1 \u2212\u03c0)\u03a6(\u03b2g\n0 \u2212c) + \u03c0\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c)\n\u2212(1 \u2212\u03c0)\u03a6(c \u2212\u03b2g\n0) \u2212\u03c0\u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)\n\u0015\n= \u2212(1 \u2212\u03c0) \u2212\u03c0 = \u22121.\n(A.17)\nCombining (A.15) and (A.17), the limit (A.14) evaluates to\nlim\nc\u2192\u221e\u03b3(\u03b2g\n1, c, \u03c0) =\n\u0012\n1\n1 \u2212\u03c0\n\u0013\u22121\n= 1 \u2212\u03c0.\nREFERENCES\n45\nIt follows that the limit in c of the asymptotic relative bias b is\nlim\nc\u2192\u221eb(\u03b2g\n1, c, \u03c0) = 1 \u2212lim\nc\u2192\u221e\u03b3(\u03b2g\n1, c, \u03c0) = \u03c0.\nA corollary is that limc\u2192\u221eb(\u03b2g\n1, c, 1/2) = 1/2.\nA.7\nBayes-optimal decision boundary as a critical value of \u03b3\nLet cbayes = \u03b2g\n0 + (1/2)\u03b2g\n1. We show that c = cbayes is a critical value of \u03b3 for \u03c0 = 1/2 and given\n\u03b2g\n1, i.e, D2\u03b3(\u03b2g\n1, cbayes, 1/2) = 0. Differentiating (A.11), the quotient rule implies that\nD2\u03b3(\u03b2g\n1, c, 1/2) = D2[2g(\u03b2g\n1, c, 1/2)]4h(\u03b2g\n1, c, 1/2) \u22122g(\u03b2g\n1, c, 1/2)D2[4h(\u03b2g\n1, c, 1/2)]\n[4h(\u03b2g\n1, c, \u03c0)]2\n.\n(A.18)\nWe have by (A.12) that\nD2[2g(\u03b2g\n1, cbayes, 1/2)] = f(\u03b2g\n1/2) \u2212f(\u2212\u03b2g\n1/2) = f(\u03b2g\n1/2) \u2212f(\u03b2g\n1/2) = 0.\n(A.19)\nSimilarly, we have by (A.13) that\nD2[4h(\u03b2g\n1, cbayes, \u03c0)] = [f(\u03b2g\n1/2) + f(\u2212\u03b2g\n1/2)] [\u03a6(\u2212\u03b2g\n1/2) + \u03a6(\u03b2g\n1/2) \u2212\u03a6(\u03b2g\n1/2) \u2212\u03a6(\u2212\u03b2g\n1/2)] = 0.\n(A.20)\nPlugging in (A.20) and (A.19) to (A.18), we find that D2[\u03b3(\u03b2g\n1, cbayes, 1/2)] = 0. Finally, because\nb(\u03b2g\n1, c, 1/2) = 1 \u2212\u03b3(\u03b2g\n1, c, 1/2),\nit follows that\nD2[b(\u03b2g\n1, cbayes, 1/2)] = \u2212D2[\u03b3(\u03b2g\n1, cbayes, 1/2)] = 0.\nA.8\nComparing Bayes-optimal decision boundary and large threshold\nWe compare the bias produced by setting the threshold to a large number to the bias produced\nby setting the threshold to the Bayes-optimal decision boundary. Let r : R\u2a7e0 \u2192R be the value\n46\nREFERENCES\nof attenuation function evaluated at the Bayes-optimal decision boundary cbayes = \u03b2g\n0 + (1/2)\u03b2g\n1,\ni.e.\nr(\u03b2g\n1) = \u03b3(\u03b2g\n1, \u03b2g\n0 + (1/2)\u03b2g\n1, 1/2) =\n\u03a6(\u03b2g\n1/2) \u2212\u03a6(\u2212\u03b2g\n1/2)\n[\u03a6(\u2212\u03b2g\n1/2) + \u03a6(\u03b2g\n1/2)] [\u03a6(\u03b2g\n1/2) + \u03a6(\u2212\u03b2g\n1/2)]\n=\nR \u03b2g\n1 /2\n\u2212\u03b2g\n1 /2 f\n[1 \u2212\u03a6(\u03b2g\n1/2) + \u03a6(\u03b2g\n1/2)] [\u03a6(\u03b2g\n1/2) + 1 \u2212\u03a6(\u03b2g\n1/2)] = 2\nZ \u03b2g\n1 /2\n0\nf = 2\u03a6(\u03b2g\n1/2) \u22121.\nWe set r to 1/2 and solve for \u03b2g\n1:\nr(\u03b2g\n1) = 1/2 \u21d0\u21d22\u03a6(\u03b2g\n1/2) \u22121 = 1/2 \u21d0\u21d2\u03a6(\u03b2g\n1/2) = 3/4 \u21d0\u21d2\u03b2g\n1 = 2\u03a6\u22121(3/4) \u22481.35.\nBecause r is a strictly increasing function, it follows that r(\u03b2g\n1) < 1/2 for \u03b2g\n1 < 2\u03a6\u22121(3/4) and\nr(\u03b2g\n1) > 1/2 for \u03b2g\n1 > 2\u03a6\u22121(3/4). Next, because\nb(\u03b2g\n1, cbayes, 1/2) = 1 \u2212\u03b3(\u03b2g\n1, cbayes, 1/2) = 1 \u2212r(\u03b2g\n1),\nwe have that b(\u03b2g\n1, cbayes, 1/2) > 1/2 for \u03b2g\n1 < 2\u03a6\u22121(3/4) and b(\u03b2g\n1, cbayes, 1/2) < 1/2 for\n\u03b2g\n1 > 2\u03a6\u22121(3/4). Recall that the bias induced by sending the threshold to infinity (as stated\nin Proposition 5 and proven in Section A.6) is 1/2, i.e.\nb(\u03b2g\n1, \u221e, 1/2) = 1/2.\nWe conclude that b(\u03b2g\n1, cbayes, 1/2) > b(\u03b2g\n1, \u221e, 1/2) on \u03b2g\n1 \u2208[0, 2\u03a6\u22121(3/4)); b(\u03b2g\n1, cbayes, 1/2) =\nb(\u03b2g\n1, \u221e, 1/2) for \u03b2g\n1 = 2\u03a6\u22121(3/4); and b(\u03b2g\n1, cbayes, 1/2) < b(\u03b2g\n1, \u221e, 1/2) on \u03b2g\n1 \u2208(2\u03a6\u22121(3/4), \u221e).\nA.9\nMonotonicity in \u03b2g\n1\nWe show that \u03b3 is monotonically increasing in \u03b2g\n1 for \u03c0 = 1/2 and given threshold c. We begin\nby stating and proving two lemmas. The first lemma establishes an inequality that will serve as\nthe basis for the proof.\nLemma A.1 The following inequality holds:\nREFERENCES\n47\n[\u03a6(\u03b2g\n0 \u2212c) + \u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c)] \u00b7 [\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2212\u03a6(\u03b2g\n0 \u2212c) + \u03a6(c \u2212\u03b2g\n0) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)]\n\u2a7e[\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2212\u03a6(\u03b2g\n0 \u2212c)] [\u03a6(c \u2212\u03b2g\n0) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)] .\n(A.21)\nProof: We take cases on the sign on \u03b2g\n1.\nCase 1: \u03b21\ng < 0. Then \u03b2g\n1 + (\u03b2g \u2212c) < (\u03b2g\n0 \u2212c), implying \u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) < \u03a6(\u03b2g\n0 \u2212c), or\n[\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2212\u03a6(\u03b2g\n0 \u2212c)] < 0. Moreover, [\u03a6(c \u2212\u03b2g\n0) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)] is positive. Therefore,\nthe right-hand side of (A.21) is negative.\nTurning our attention of the left-hand side of (A.21), we see that\n\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1) = 1 \u2212\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1) = 1.\n(A.22)\nAdditionally, \u03a6(\u03b2g\n0 \u2212c) < 1 and \u03a6(c \u2212\u03b2g\n0) > 0. Combining these facts with (A.22), we find that\n[\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2212\u03a6(\u03b2g\n0 \u2212c) + \u03a6(c \u2212\u03b2g\n0) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)] > 0.\nFinally, because [\u03a6(\u03b2g\n0 \u2212c) + \u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c)] > 0, the entire left-hand side of (A.21) is positive.\nThe inequality holds for \u03b2g\n1 < 0.\nCase 2: \u03b21\ng \u2a7e0. We will show that the first term on the LHS of (A.21) is greater than the first\nterm on the RHS of (A.21), and likewise that the second term on the LHS is greater than the\nsecond term on the RHS, implying the truth of the inequality. Focusing on the first term, the\npositivity of \u03a6(\u03b2g\n0 \u2212c) implies that \u03a6(\u03b2g\n0 \u2212c) \u2a7e\u2212\u03a6(\u03b2g\n0 \u2212c), and so\n\u03a6(\u03b2g\n0 \u2212c) + \u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2a7e\u03a6(\u03b2g\n0 \u2212\u03b2g\n1 \u2212c) \u2212\u03a6(\u03b2g\n0 \u2212c).\nNext, focusing on the second term, \u03b2g\n1 \u2a7e0 implies that\n\u03b2g\n1 + \u03b2g\n0 \u2212c \u2a7e\u03b2g\n0 \u2212c =\u21d2\u03a6(\u03b2g\n1 + \u03b2g\n0 \u2212c) \u2212\u03a6(\u03b2g\n0 \u2212c) \u2a7e0.\n(A.23)\nAdding \u03a6(c \u2212\u03b2g\n0) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1) to both sides of (A.23) yields\n\u03a6(\u03b2g\n1 + \u03b2g\n0 \u2212c) \u2212\u03a6(\u03b2g\n0 \u2212c) + \u03a6(c \u2212\u03b2g\n0) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1) \u2a7e\u03a6(c \u2212\u03b2g\n0) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1).\n48\nREFERENCES\nThe inequality holds for \u03b2g\n1 \u2a7e0. Combining the cases, the inequality holds for all \u03b2g\n1 \u2208R. \u25a1\nThe second lemma establishes the derivatives of the functions 2 \u00b7 g and 4 \u00b7 h in \u03b2g\n1.\nLemma A.2 The derivatives in \u03b2g\n1 of 2 \u00b7 g and 4 \u00b7 h are\nD1[2g(\u03b2g\n1, c, 1/2)] = f(\u03b2g\n0 + \u03b2g\n1 \u2212c),\n(A.24)\nD1[4h(\u03b2g\n1, c, 1/2)] = f(\u03b2g\n0 + \u03b2g\n1 \u2212c) [\u03a6(c \u2212\u03b2g\n0) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)]\n\u2212f(\u03b2g\n0 + \u03b2g\n1 \u2212c) [\u03a6(\u03b2g\n0 \u2212c) + \u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c)].\n(A.25)\nProof: Apply FTC and product rule. \u25a1\nWe are ready to prove the monotonicity of \u03b3 in \u03b2g\n1. Subtracting\n[\u03a6(\u03b2g\n0 \u2212c) + \u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c)] [\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2212\u03a6(\u03b2g\n0 \u2212c)]\nfrom both sides of (A.21) and multiplying by f(\u03b2g\n0 + \u03b2g\n1 \u2212c) > 0 yields\nf(\u03b2g\n0 + \u03b2g\n1 \u2212c)[\u03a6(\u03b2g\n0 \u2212c) + \u03a6 (\u03b2g\n0 + \u03b2g\n1 \u2212c)] [\u03a6(c \u2212\u03b2g\n0) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)]\n\u2a7ef(\u03b2g\n0 + \u03b2g\n1 \u2212c) [\u03a6(c \u2212\u03b2g\n0) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)][\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2212\u03a6(\u03b2g\n0 \u2212c)]\n\u2212f(\u03b2g\n0 + \u03b2g\n1 \u2212c) [\u03a6(\u03b2g\n0 \u2212c) + \u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c)][\u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2212\u03a6(\u03b2g\n0 \u2212c)].\n(A.26)\nNext, recall that\n2g(\u03b2g\n1, c, 1/2) = \u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c) \u2212\u03a6(\u03b2g\n0 \u2212c).\n(A.27)\nand\n4h(\u03b2g\n1, c, 1/2) = [\u03a6(\u03b2g\n0 \u2212c) + \u03a6(\u03b2g\n0 + \u03b2g\n1 \u2212c)] [\u03a6(c \u2212\u03b2g\n0) + \u03a6(c \u2212\u03b2g\n0 \u2212\u03b2g\n1)].\n(A.28)\nSubstituting (A.24, A.25, A.27, A.28) into (A.26) produces\nD1[2g(\u03b2g\n1, c, 1/2)]4h(\u03b2g\n1, c, 1/2) \u2a7e2g(\u03b2g\n1, c, 1/2)D1[4h(\u03b2g\n1, c, 1/2)],\nREFERENCES\n49\nor\nD1[2g(\u03b2g\n1, c, 1/2)]4h(\u03b2g\n1, c, 1/2) \u22122g(\u03b2g\n1, c, 1/2)D1[4h(\u03b2g\n1, c, 1/2)] \u2a7e0.\n(A.29)\nThe quotient rule implies that\nD1\u03b3(\u03b2g\n1, c, 1/2) = D1[2g(\u03b2g\n1, c, 1/2)]4h(\u03b2g\n1, c, 1/2) \u22122g(\u03b2g\n1, c, 1/2)D1[4h(\u03b2g\n1, c, 1/2)]\n[4h(\u03b2g\n1, c, 1/2)]2\n.\n(A.30)\nWe conclude by (A.29) and (A.30) that \u03b3 is monotonically increasing in \u03b2g\n1. Finally, b(\u03b2g\n1, c, \u03c0) =\n1 \u2212\u03b3(\u03b2g\n1, c, \u03c0) is monotonically decreasing in \u03b2g\n1.\nA.10\nStrict attenuation bias\nWe begin by computing the limit of \u03b3 in \u03b2g\n1 given \u03c0 = 1/2. First,\nlim\n\u03b2g\n1 \u2192\u221e\u03b3(\u03b2g\n1, c, 1/2) =\n1 \u2212\u03a6(\u03b2g\n0 \u2212c)\n[1 + \u03a6(\u03b2g\n0 \u2212c)] [\u03a6(c \u2212\u03b2g\n0)]\n=\n\u03a6(c \u2212\u03b2g\n0)\n[1 + \u03a6(\u03b2g\n0 \u2212c)] [\u03a6(c \u2212\u03b2g\n0)] =\n1\n1 + \u03a6(\u03b2g\n0 \u2212c) < 1.\nSimilarly,\nlim\n\u03b2g\n1 \u2192\u2212\u221e\u03b3(\u03b2g\n1, c, 1/2) =\n\u2212\u03a6(\u03b2g\n0 \u2212c)\n[\u03a6(\u03b2g\n0 \u2212c)] [\u03a6(c \u2212\u03b2g\n0) + 1] =\n\u22121\n1 + \u03a6(c \u2212\u03b2g\n0) > \u22121.\nThe function \u03b3(\u03b2g\n1, c, 1/2, \u03b2g\n0) is monotonically increasing in \u03b2g\n1 (as stated in Proposition 3 and\nproven in section A.9). It follows that\n\u22121 < \u2212\n1\n1 + \u03a6(c \u2212\u03b2g\n0) \u2a7d\u03b3(\u03b2g\n1, c, 1/2, \u03b2g\n0) \u2a7d\n1\n1 \u2212\u03a6(\u03b2g\n0 \u2212c) < 1\nfor all \u03b2g\n1 \u2208R. But \u03b2g\n0 and c were chosen arbitrarily, and so\n\u22121 < \u03b3(\u03b2g\n1, c, 1/2, \u03b2g\n0) < 1\nfor all (\u03b2g\n1, c, \u03b2g\n0) \u2208R3. Finally, because b(\u03b2g\n1, c, 1/2, \u03b2g\n0) = 1 \u2212\u03b3(\u03b2g\n1, c, 1/2, \u03b2g\n0), it follows that\n0 < b(\u03b2g\n1, c, 1/2, \u03b2g\n0) < 2\nfor all (\u03b2g\n1, c, \u03b2g\n0) \u2208R3\n50\nREFERENCES\nA.11\nBias-variance decomposition in no-intercept model\nWe prove the bias-variance decomposition for the no-intercept version of (A.1). Define l (for\n\u201climit\u201d) by\nl = \u03b2m\n\u0012\n\u03c9\u03c0\n\u03b6(1 \u2212\u03c0) + \u03c9\u03c0\n\u0013\n,\nwhere\n\u03c9 = \u00af\u03a6(c \u2212\u03b2g) = \u03a6(\u03b2g \u2212c);\n\u03b6 = \u00af\u03a6(c) = \u03a6(\u2212c).\nWe have that\n\u02c6\u03b2m \u2212l =\nPn\ni=1 \u02c6pimi\nPn\ni=1 \u02c6p2\ni\n\u2212l =\nPn\ni=1 \u02c6pimi\nPn\ni=1 \u02c6p2\ni\n\u2212l Pn\ni=1 \u02c6p2\ni\nPn\ni=1 \u02c6p2\ni\n=\nPn\ni=1 \u02c6pi(mi \u2212l\u02c6pi)\nPn\ni=1 \u02c6p2\ni\n.\nTherefore,\n\u221an(\u02c6\u03b2m \u2212l) = (1/\u221an) Pn\ni=1 \u02c6pi(mi \u2212l\u02c6pi)\n(1/n) Pn\ni=1 \u02c6p2\ni\n.\n(A.31)\nNext, we compute the expectation and variance of \u02c6pi(mi \u2212l\u02c6pi). To do so, we first compute several\nsimpler quantities:\n1. Expectation of \u02c6pi: E[\u02c6pi] = P(pi\u03b2g + \u03c4i \u2a7ec) = P(\u03b2g + \u03c4i \u2a7ec)\u03c0 + P(\u03c4i \u2a7ec)(1 \u2212\u03c0) =\n\u03c0\u03c9 + (1 \u2212\u03c0)\u03b6.\n2. Expectation of \u02c6pipi: E [\u02c6pipi] = E [\u02c6pi|pi = 1] P [pi = 1] = \u03c9\u03c0.\n3. Expectation of \u02c6pimi:\nE[\u02c6pimi] = E [\u02c6pi(\u03b2mpi + \u03f5i)] = E [\u03b2m\u02c6pipi + \u02c6pi\u03f5i]\n= \u03b2mE [\u02c6pipi] + E[\u02c6pi]E[\u03f5i] = \u03b2m\u03c9\u03c0 + 0 = \u03b2m\u03c9\u03c0.\n4. Expectation of \u02c6pim2\ni :\nE\n\u0002\n\u02c6pim2\ni\n\u0003\n= E\n\u0002\n\u02c6pi(\u03b2mpi + \u03f5i)2\u0003\n= E\n\u0002\n\u02c6pi\n\u0000\u03b22\nmp2\ni + 2\u03b2mpi\u03f5i + \u03f52\ni\n\u0001\u0003\n= E\n\u0002\n\u02c6pipi\u03b22\nm + 2\u03b2mpi\u02c6pi\u03f5i + \u02c6pi\u03f52\ni\n\u0003\n= \u03b22\nmE[\u02c6pipi] + 2\u03b2mE[pi\u02c6pi]E[\u03f5i] + E[\u02c6pi]E[\u03f52\ni ]\n= \u03b22\nmE[\u02c6pipi] + E[\u02c6pi] = \u03b22\nm\u03c9\u03c0 + E[\u02c6pi].\nREFERENCES\n51\nNow, we can compute the expectation and variance of \u02c6pi(mi \u2212l\u02c6pi). First,\nE [\u02c6pi(mi \u2212l\u02c6pi)] = E[\u02c6pimi] \u2212lE[\u02c6pi] = \u03b2m\u03c9\u03c0 \u2212\n\u0012\n\u03b2m\u03c9\u03c0\n\u03b6(1 \u2212\u03c0) + \u03c9\u03c0\n\u0013\n[\u03b6(1 \u2212\u03c0) + \u03c9\u03c0] = 0.\n(A.32)\nAdditionally,\nV [\u02c6pi(mi \u2212l\u02c6pi)] = E\n\u0002\n\u02c6p2\ni (mi \u2212l\u02c6pi)2\u0003\n\u2212(E [\u02c6pi(mi \u2212l\u02c6pi)])2\n= E\n\u0002\n\u02c6pim2\ni\n\u0003\n\u22122lE[mi\u02c6pi] + l2E[\u02c6pi] = \u03b22\nm\u03c9\u03c0 + E[\u02c6pi] \u22122l\u03b2m\u03c9\u03c0 + l2E[\u02c6pi]\n= \u03b2m\u03c9\u03c0(\u03b2m \u22122l) + E[\u02c6pi](1 + l2).\n(A.33)\nTherefore, by CLT, (A.32), and (A.33),\n(1/\u221an)\nn\nX\ni=1\n\u02c6pi(mi \u2212l\u02c6pi)\nd\u2212\u2192N\n\u00000, \u03b2m\u03c9\u03c0(\u03b2m \u22122l) + E[\u02c6pi](1 + l2)\n\u0001\n.\n(A.34)\nNext, by weak LLN,\n(1/n)\nn\nX\ni=1\n\u02c6p2\ni = (1/n)\nn\nX\ni=1\n\u02c6pi\nP\u2212\u2192E[\u02c6pi].\n(A.35)\nFinally, by (A.31), (A.34), (A.35), and Slutsky\u2019s Theorem,\n\u221an(\u02c6\u03b2m \u2212l)\nd\u2212\u2192N\n \n0, \u03b2m\u03c9\u03c0(\u03b2m \u22122l) + E[\u02c6pi](1 + l2)\n(E[\u02c6pi])2\n!\n.\nThus, for large n \u2208N, we have that\nE[\u02c6\u03b2m] \u2248l;\nV[\u02c6\u03b2m] \u2248\n\u0002\n\u03b2m\u03c9\u03c0(\u03b2m \u22122l) + E[\u02c6pi](1 + l2)\n\u0003\n/[nE2[\u02c6pi]],\ncompleting the bias-variance decomposition.\nA.12\nBayes-optimal decision boundary for non-Gaussian mixture distributions and GLMs\nWe report the Bayes-optimal decision boundary for gRNA count distributions that are non-\nGaussian. First, consider a simple two-component Poisson mixture model with means \u00b50 and \u00b51\nand mixing probability \u03c0:\n52\nREFERENCES\np(k; \u00b50, \u00b51, \u03c0) = (1 \u2212\u03c0)f(k; \u00b50) + \u03c0f(k; \u00b51),\nwhere f(k; \u00b5) = (\u00b5ke\u2212k)/\u00b5! is a Poisson density. Suppose we draw an observation from this\ndistribution. The Bayes-optimal threshold for classifying the observation as having been drawn\nfrom the first or second component is\n\u00b50 \u2212\u00b51 + log(\u03c0) \u2212log(1 \u2212\u03c0)\nlog(\u00b50) \u2212log(\u00b51)\n.\n(A.36)\nNext, consider the slightly more complex Poisson mixture GLM:\ngi|(pi, zi, oi) \u223cPois(\u00b5i);\nr(\u00b5i) = \u03b20 + \u03b21pi + \u03b3T zi + oi,\nwhere pi \u223cBern(\u03c0) is unobserved. Conditional on the covariates and offset, the mean of the\nunperturbed component is \u00b5i(1) = r\u22121(\u03b20 + \u03b3T zi + oi), and that of the perturbed component\nis \u00b5i(1) = r\u22121(\u03b20 + \u03b21 + \u03b3T zi + oi.) The Bayes-optimal threshold is obtained by plugging in\n\u00b5i(1) for \u00b51 and \u00b5i(0) for \u00b50 in (A.36). To obtain a fixed gRNA assignment threshold across\ncells, we compute the Bayes-optimal decision boundary for each cell and then take the average\nacross cells. The situation is similar for the negative binomial (with known size s) distribution;\nthe Bayes-optimal decision boundary in this case is\ns [log(\u00b50 + s) \u2212log(\u00b51 + s)] + log(\u03c0) \u2212log(1 \u2212\u03c0)\nlog(\u00b50(\u00b51 + s)) \u2212log(\u00b51(\u00b50 + s))\n.\nB. Estimation and inference in the GLM-EIV model\nB.1\nDetailed specification of the model\nWe provide a more precise and technical specification of the GLM-EIV model than provided\nin the main text. Let \u02dcxi = [1, pi, zi]T \u2208Rd be the vector of covariates (including an intercept\nterm) for the ith cell. (We use the tilde as a reminder that the vector is partially unobserved.)\nLet \u03b2m = [\u03b2m\n0 , \u03b2m\n1 , \u03b3m]T \u2208Rd and \u03b2g = [\u03b2g\n0, \u03b2g\n1, \u03b3g]T \u2208Rd be the unknown coefficient vectors\nREFERENCES\n53\ncorresponding to the gene and gRNA expression models, respectively. Finally, let om\ni\nand og\ni be\nthe (possibly zero) offset terms for the gene and gRNA models; in practice, we typically set om\ni\nand og\ni to the log-transformed library sizes (i.e., log(dm\ni ) and log(dg\ni ), respectively).\nWe use a pair of GLMs to model the gene and gRNA expressions. Considering first the gene\nexpression model, let the ith linear component lm\ni\nof the model be lm\ni \u2261\u27e8\u02dcxi, \u03b2m\u27e9+ om\ni . Next, let\nthe mean \u00b5m\ni\nof the ith observation be rm(\u00b5m\ni ) \u2261lm\ni , where rm : R \u2192R is a strictly increasing,\ndifferentiable link function. Let \u03c8m : R \u2192R be the differentiable, cumulant-generating function of\nthe selected exponential family distribution. We can express the canonical parameter \u03b7m\ni in terms\nof \u03c8m and rm by \u03b7m\ni\n=\n\u0000[\u03c8\u2032\nm]\u22121 \u25e6r\u22121\nm\n\u0001\n(lm\ni ) \u2261hm(lm\ni ). Finally, let cm : R \u2192R be the carrying\ndensity of the selected exponential family distribution. The density fm of mi conditional on the\nthe canonical parameter \u03b7i is fm(mi; \u03b7m\ni ) = exp {mi\u03b7m\ni \u2212\u03c8m(\u03b7m\ni ) + cm(mi)} . We implicitly set\nthe \u201cscale\u201d term (i.e., the a(\u03d5) term in McCullagh and Nelder (1990), Eqn. 2.4, p. 28) to unity;\nthis slightly simplified model encompasses the most important distributions for our purposes,\nincluding the Poisson, negative binomial, and Gaussian (with unit variance) distributions.\nLet the terms lg\ni , og\ni , \u00b5g\ni , \u03b7g\ni , \u03c8g, rg, hg and cg be defined in an analogous way for the gRNA\nmodel, i.e. lg\ni \u2261\u27e8\u02dcxi, \u03b2g\u27e9+ og\ni , rg(\u00b5g\ni ) \u2261lg\ni , and \u03b7g\ni =\n\u0000[\u03c8\u2032\ng]\u22121 \u25e6r\u22121\ng\n\u0001\n(lg\ni ) \u2261hg(lg\ni ). The density\nfg of gi given the canonical parameter is fg(mi; \u03b7g\ni ) = exp {gi\u03b7g\ni \u2212\u03c8g(\u03b7g\ni ) + cg(gi)} . Finally, the\nunobserved variable pi is assumed to follow a Bernoulli distribution with mean \u03c0 \u2208(0, 1/2]. Its\nmarginal density fp is given by fp(pi) = \u03c0pi(1 \u2212\u03c0)1\u2212pi. The unknown parameters in the model\nare \u03b8 = [\u03b2m, \u03b2g, \u03c0]T \u2208R2d+1.\nB.2\nNotation\nWe briefly introduce notation that we will use throughout. For j \u2208{0, 1}, let \u02dcxi(j) \u2261[1, j, zi]T\ndenote the value of \u02dcxi that results from setting pi to j. Next, let lm\ni (j), \u03b7m\ni (j), and \u00b5m\ni (j)\nbe the values of lm\ni , \u03b7m\ni , and \u00b5m\ni , respectively, that result from setting pi to j, i.e., lm\ni (j) \u2261\n54\nREFERENCES\n\u27e8\u02dcxi(j), \u03b2m\u27e9+ om\ni , \u03b7m\ni (j) \u2261hm(lm\ni (j)), and \u00b5m\ni (j) \u2261r\u22121\nm (lm\ni (j)). Let the corresponding gRNA\nquantities lg\ni (j), \u03b7g\ni (j), and \u00b5g\ni (j) be defined analogously. Next, let X \u2208Rn\u00d7(d\u22121) be the observed\ndesign matrix, and let \u02dcX \u2208Rn\u00d7d be the augmented design matrix that results from concatenating\nthe column of (unobserved) pis to X, i.e.\nX \u2261\n\uf8ee\n\uf8ef\uf8f0\n1\nz1\n...\n...\n1\nzn\n\uf8f9\n\uf8fa\uf8fb;\n\u02dcX \u2261\n\uf8ee\n\uf8ef\uf8f0\n1\np1\nz1\n...\n...\n...\n1\npn\nzn\n\uf8f9\n\uf8fa\uf8fb=\n\uf8ee\n\uf8ef\uf8f0\n\u02dcxT\n1\n...\n\u02dcxT\nn\n\uf8f9\n\uf8fa\uf8fb.\nFurthermore, for j \u2208{0, 1}, let \u02dcX(j) \u2208Rn\u00d7d be the matrix that results from setting pi to j\nfor all i \u2208{1, . . . , n} in \u02dcX, and let [ \u02dcX(0)T , \u02dcX(1)T ]T denote the R2n\u00d7d matrix that results from\nvertically concatenating \u02dcX(0) and \u02dcX(1). Furthermore, define m := [m1, . . . , mn], and let g, p,\nom, and og be defined analogously. Finally, let [m, m]T \u2208R2n be the vector that results from\nconcatenating m to itself, i.e. [m, m]T \u2261[m1, . . . , mn, m1, . . . , mn], and let [g, g]T , [og, og]T , and\n[om, om]T be defined similarly.\nB.3\nLog likelihood and estimation\nWe conduct estimation and inference conditional on the library sizes and technical factors lm\ni , lg\ni ,\nand zi; therefore, we treat these quantities as fixed constants. We assume that the gene expression\nmi and gRNA expression gi are conditionally independent given the perturbation pi. The model\nlog-likelihood is\nL(\u03b8; m, g) =\nn\nX\ni=1\nlog [(1 \u2212\u03c0)fm(mi; \u03b7m\ni (0))fg(gi; \u03b7g\ni (0)) + \u03c0fm(mi; \u03b7m\ni (1))fg(gi; \u03b7g\ni (1))] .\n(B.1)\nWe see from (B.1) that the GLM-EIV model is equivalent to a two-component mixture of products\nof GLM densities. We estimate the parameters of the GLM-EIV model using an EM algorithm.\nE step\nThe E step entails computing the membership probability of each cell. Let \u03b8(t) =\n(\u03b2(t)\nm , \u03b2(t)\ng , \u03c0(t)) be the parameter estimate at the t-th iteration of the algorithm. For k \u2208{0, 1},\nlet [\u03b7m\ni (k)](t) be the ith canonical parameter at the t-th iteration of the algorithm of the gene ex-\nREFERENCES\n55\npression distribution that results from setting pi to k, i.e. [\u03b7m\ni (k)](t) \u2261hm\n\u0010\n\u27e8\u02dcxi(k), \u03b2(t)\nm \u27e9+ om\ni\n\u0011\n.\nSimilarly, let [\u03b7g\ni (k)](t) be defined by [\u03b7g\ni (k)](t) \u2261hg\n\u0010\n\u27e8\u02dcxi(k), \u03b2(t)\ng \u27e9+ og\ni\n\u0011\n. Next, for k \u2208{0, 1},\ndefine \u03b1(t)\ni (k) by\n\u03b1(t)\ni (k) \u2261P\n\u0010\nMi = mi, Gi = gi|Pi = k, \u03b8(t)\u0011\n= P\n\u0010\nMi = mi|Pi = k, \u03b8(t)\u0011\nP\n\u0010\nGi = gi|Pi = k, \u03b8(t)\u0011\n(because Gi\n|=\nMi|Pi)\n= fm\n\u0010\nmi; [\u03b7m\ni (k)](t)\u0011\nfg\n\u0010\ngi; [\u03b7g\ni (k)](t)\u0011\n.\nFinally, let \u03c0(t)(1) \u2261\u03c0(t) = P\n\u0000Pi = 1|\u03b8(t)\u0001\nand \u03c0(t)(0) \u22611 \u2212\u03c0(t) = P\n\u0000Pi = 0|\u03b8(t)\u0001\n. The ith\nmembership probability T (t)\ni\n(1) is\nT (t)\ni\n(1) = P(Pi = 1|Mi = mi, Gi = gi, \u03b8(t)) =\n\u03c0(t)(1)\u03b1(t)\ni (1)\nP1\nk=0 \u03c0(t)(k)\u03b1(t)\ni (k)\n(by Bayes rule)\n=\n1\n\u03c0(t)(0)\u03b1i(0)\n\u03c0(t)(1)\u03b1i(1) + 1\n=\n1\nexp\n\u0010\nlog\n\u0010\n\u03c0(t)(0)\u03b1i(0)\n\u03c0(t)(1)\u03b1i(1)\n\u0011\u0011\n+ 1\n=\n1\nexp\n\u0010\nq(t)\ni\n\u0011\n+ 1\n,\n(B.2)\nwhere we set\nq(t)\ni\n:= log\n \n\u03c0(t)(0)\u03b1(t)\ni (0)\n\u03c0(t)(1)\u03b1(t)\ni (1)\n!\n.\n(B.3)\nNext, we have that\nq(t)\ni\n= log\nh\n\u03c0(t)(0)\ni\n+ log\nh\nfm\n\u0010\nmi; [\u03b7m\ni (0)](t)\u0011i\n+ log\nh\nfg\n\u0010\ngi; [\u03b7g\ni (0)](t)\u0011i\n\u2212log\nh\n\u03c0(t)(1)\ni\n\u2212log\nh\nfm\n\u0010\nmi; [\u03b7m\ni (1)](t)\u0011i\n\u2212log\nh\nfg\n\u0010\ngi; [\u03b7g\ni (1)](t)\u0011i\n,\nWe therefore conclude that T (t)\ni\n= 1/\n\u0010\nexp\n\u0010\nq(t)\ni\n\u0011\n+ 1\n\u0011\n, which is easily computable.\nM step\nThe complete-data log-likelihood of the GLM-EIV model is\nL(\u03b8; m, g, p) =\nn\nX\ni=1\n[pi log(\u03c0) + (1 \u2212pi) log(1 \u2212\u03c0)] +\nn\nX\ni=1\nlog (fm(mi; \u03b7m\ni )) +\nn\nX\ni=1\nlog (fg(gi; \u03b7g\ni )) .\n(B.4)\n56\nREFERENCES\nDefine Q(\u03b8|\u03b8(t)) = E(P |M=m,G=g,\u03b8(t)) [L(\u03b8; m, g, p)] . We have that\nQ(\u03b8|\u03b8(t)) =\nn\nX\ni=1\nh\nT (t)\ni\n(1) log(\u03c0) + T (t)\ni\n(0) log(1 \u2212\u03c0)\ni\n+\n1\nX\nk=0\nn\nX\ni=1\nT (t)\ni\n(k) log [fm(mi; \u03b7m\ni (k))] +\n1\nX\nk=0\nn\nX\ni=1\nT (t)\ni\n(k) log\nh\nfg(gi; \u03b7g,b\ni\n(k))\ni\n.\n(B.5)\nThe three terms of (B.5) are functions of different parameters: the first is a function of \u03c0, the\nsecond is a function of \u03b2m, and the third is a function of \u03b2g. Therefore, to find the maximizer\n\u03b8(t+1) of (B.5), we maximize the three terms separately. Differentiating the first term with respect\nto \u03c0, we find that\n\u2202\n\u2202\u03c0\nn\nX\ni=1\nh\nT (t)\ni\n(1) log(\u03c0) + T (t)\ni\n(0) log(1 \u2212\u03c0)\ni\n=\nPn\ni=1 T (t)\ni\n(1)\n\u03c0\n\u2212\nPn\ni=1 T (t)\ni\n(0)\n1 \u2212\u03c0\n.\nSetting the derivative equal to 0 and solving for \u03c0,\nPn\ni=1 T (t)\ni\n(1)\n\u03c0\n\u2212\nPn\ni=1 T (t)\ni\n(0)\n1 \u2212\u03c0\n= 0 \u21d0\u21d2\nn\nX\ni=1\nT (t)\ni\n(1) \u2212\u03c0\nn\nX\ni=1\nT (t)\ni\n(1) = \u03c0\nn\nX\ni=1\nTi(0)\n\u21d0\u21d2\nn\nX\ni=1\nT (t)\ni\n(1) \u2212\u03c0\nn\nX\ni=1\nT (t)\ni\n(1) = \u03c0n \u2212\u03c0\nn\nX\ni=1\nTi(1) \u21d0\u21d2\u03c0 =\nPn\ni=1 T (t)\ni\n(1)\nn\n.\nThus, the maximizer \u03c0(t+1) of (B.5) in \u03c0 is \u03c0(t+1) = (1/n) Pn\ni=1 T (t)\ni\n(1). Next, define w(t) =\n[T (t)\n1 (0), . . . , T (t)\nn (0), T (t)\n1 (1), . . . , T (t)\nn (1)]T \u2208R2n. We can view the second term of (B.5) as the log-\nlikelihood of a GLM \u2013 call it GLM(t)\nm \u2013 that has exponential family density fm, link function rm,\nresponses [m, m]T , offsets [om, om]T , weights w(t), and design matrix [ \u02dcX(0)T , \u02dcX(1)T ]T . Therefore,\nthe maximizer \u03b2(t+1)\nm\nof the second term of (B.5) is the maximizer of GLM(t)\nm , which we can\ncompute using the iteratively reweighted least squares (IRLS) procedure, as implemented in R\u2019s\nGLM function. Similarly, the maximizer \u03b2(t+1)\ng\nof the third term of (B.5) is the maximizer of\nthe GLM with exponential family density fg, link function rg, responses [g, g]T , offsets [og, og]T ,\nweights w(t), and design matrix [ \u02dcX(0)T , \u02dcX(1)T ]T .\nREFERENCES\n57\nB.4\nInference\nWe derive the asymptotic observed information matrix of the GLM-EIV log likelihood, enabling\nus to perform inference on the parameters. First, we define some notation. For i \u2208{1, . . . , n},\nj \u2208{0, 1}, and \u03b8 = (\u03c0, \u03b2m, \u03b2g), let T \u03b8\ni (j) be defined by\nT \u03b8\ni (j) = P\u03b8 (Pi = j|Mi = mi, Gi = gi) .\nLet the n\u00d7n matrix T \u03b8(j) be given by T \u03b8(j) = diag\n\b\nT \u03b8\n1 (j), . . . , T \u03b8\nn(j)\n\t\n. Next, define the diagonal\nn \u00d7 n matrices \u2206m, [\u2206\u2032]m, V m, and Hm by\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u2206m = diag{h\u2032\nm(lm\n1 ), . . . , h\u2032\nm(lm\nn )}\n[\u2206\u2032]m = diag{h\u2032\u2032\nm(lm\n1 ), . . . , h\u2032\u2032\nm(lm\nn )}\nV m = diag{\u03c8\u2032\u2032\nm(\u03b7m\n1 ), . . . , \u03c8\u2032\u2032\nm(\u03b7m\nn )}\nHm = diag{m1 \u2212\u00b5m\n1 , . . . , mn \u2212\u00b5m\nn }.\nDefine the n \u00d7 n matrices \u2206g, [\u2206\u2032]g, V g, and Hg analogously. These matrices are unobserved, as\nthey depend on {p1, . . . , pn}. Next, for j \u2208{0, 1}, let the diagonal n\u00d7n matrices \u2206m(j), [\u2206\u2032]m(j), V m(j),\nand Hm(j) be given by\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u2206m(j) = diag{h\u2032\nm(lm\n1 (j)), . . . , h\u2032\nm(lm\nn (j))}\n[\u2206\u2032]m(j) = diag{h\u2032\u2032\nm(lm\n1 (j)), . . . , h\u2032\u2032\nm(lm\nn (j))}\nV m(j) = diag{\u03c8\u2032\u2032\nm(\u03b7m\n1 (j)), . . . , \u03c8\u2032\u2032\nm(\u03b7m\nn (j))}\nHm(j) = diag{m1 \u2212\u00b5m\n1 (j), . . . , mn \u2212\u00b5m\nn (j)}.\nDefine the matrices \u2206g(j), [\u2206\u2032]g(j), V g(j), and Hg(j) analogously. Finally, define the vectors\nsm(j), wm(j) \u2208Rn by\n(\nsm(j) = [m1 \u2212\u00b5m\n1 (j), . . . , mn \u2212\u00b5m\nn (j)]T\nwm(j) = [T1(0)T1(1)\u2206m\n1 (j)Hm\n1 (j), . . . , Tn(0)Tn(1)\u2206m\nn (j)Hm\nn (j)]T ,\nand let the vectors sg(j) and wg(j) be defined analogously. The quantities \u2206m(j), [\u2206\u2032]m(j), V m(j),\nHm(j), sm(j), wm(j), \u2206g(j), [\u2206\u2032]g(j), V g(j), Hg(j), sg(j), and wg(j) are all observed.\nThe observed information matrix J(\u03b8; m, g) evaluated at \u03b8 = (\u03c0, \u03b2m, \u03b2g) is the negative Hes-\nsian of the log likelihood (B.1) evaluated at \u03b8, i.e. J(\u03b8; m, g) = \u2212\u22072L(\u03b8; m, g). This quantity,\nunfortunately, is hard to compute, as the log likelihood (B.1) is a complicated mixture. Louis\n58\nREFERENCES\n(1982) showed that J(\u03b8; m, g) is equivalent to the following quantity:\nJ(\u03b8; m, g) = \u2212E\n\u0002\n\u22072L(\u03b8; m, g, p)|G = g, M = m\n\u0003\n+ E [\u2207L(\u03b8; m, g, p)|G = g, M = m] E [\u2207L(\u03b8; m, g, p)|G = g, M = m]T\n\u2212E\n\u0002\n\u2207L(\u03b8; m, g, p)\u2207L(\u03b8; m, g, p)T |G = g, M = m\n\u0003\n.\n(B.6)\nThe observed information matrix J(\u03b8; m, g) has dimension (2d + 1) \u00d7 (2d + 1). Recall that the\ncomplete-data log-likelihood (B.4) is the sum of three terms. The first term depends only on\n\u03c0, the second on \u03b2m, and the third on \u03b2g. Therefore, the observed information matrix can be\nviewed as block matrix consisting of nine submatrices (Figure 8; only six submatrices labelled).\nSubmatrix I depends on \u03c0, submatrix II on \u03b2m, submatrix III on \u03b2g, submatrix IV on \u03b2m and\n\u03b2g, submatrix V on \u03c0 and \u03b2m, and submatrix VI on \u03c0 and \u03b2g. We only need to compute these\nsix submatrices to compute the entire matrix, as the matrix is symmetric. The following sections\nderive formulas for submatrices I-VI. All expectations are understood to be conditional on m and\ng. The notation \u2207v and \u22072\nv represent the gradient and Hessian, respectively, with respect to the\nvector v.\nSubmatrix I\nDenote submatrix I by J\u03c0(\u03b8; m, g). The formula for J\u03c0(\u03b8; m, g) is\nJ\u03c0(\u03b8; m, g) = \u2212E\n\u0002\n\u22072\n\u03c0L(\u03b8; m, g, p)\n\u0003\n+ (E [\u2207\u03c0L(\u03b8; m, g, p)])2 \u2212E\n\u0002\n(\u2207\u03c0L(\u03b8; m, g, p))2\u0003\n.\n(B.7)\nWe begin by calculating the first and second derivatives of the log-likelihood L with respect\nto \u03c0. The first derivative is\n\u2207\u03c0L(\u03b8; m, g, p) = \u2202\n\u2202\u03c0\n n\nX\ni=1\npi log(\u03c0) +\nn\nX\ni=1\n(1 \u2212pi) log(1 \u2212\u03c0)\n!\n=\nPn\ni=1 pi\n\u03c0\n\u2212\nPn\ni=1(1 \u2212pi)\n1 \u2212\u03c0\n=\nPn\ni=1 pi\n\u03c0\n\u2212n \u2212Pn\ni=1 pi\n1 \u2212\u03c0\n=\n\u0012 1\n\u03c0 +\n1\n1 \u2212\u03c0\n\u0013\nn\nX\ni=1\npi \u2212\nn\n1 \u2212\u03c0 .\n(B.8)\nThe second derivative is\nREFERENCES\n59\nFig. 8. Block structure of the observed information matrix J(\u03b8; m, g) = \u2212\u22072L(\u03b8; m, g). The matrix is\nsymmetric, and so we only need to compute submatrices I-VI to compute the entire matrix.\n\u22072\n\u03c0L(\u03b8; m, g, p)\n=\n\u22022\n\u22022\u03c0\n\u0012Pn\ni=1 pi\n\u03c0\n\u2212n \u2212Pn\ni=1 pi\n1 \u2212\u03c0\n\u0013\n=\n(Pn\ni=1 pi) \u2212n\n(1 \u2212\u03c0)2\n\u2212\nPn\ni=1 pi\n\u03c02\n.\nWe compute the expectation of the first term of (B.7):\nE\n\u0002\n\u2212\u22072\n\u03c0L(\u03b8; m, g, p)\n\u0003\n= \u2212E\n\u0014(Pn\ni=1 pi) \u2212n\n(1 \u2212\u03c0)2\n\u2212\nPn\ni=1 pi\n\u03c02\n\u0015\n= \u2212E\n(\u0014\n1\n(1 \u2212\u03c0)2 \u22121\n\u03c02\n\u0015\nn\nX\ni=1\npi \u2212\nn\n(1 \u2212\u03c0)2\n)\n= \u2212\n(\u0014\n1\n(1 \u2212\u03c0)2 \u22121\n\u03c02\n\u0015\nn\nX\ni=1\nT \u03b8\ni (1) \u2212\nn\n(1 \u2212\u03c0)2\n)\n=\n\u0014 1\n\u03c02 \u2212\n1\n(1 \u2212\u03c0)2\n\u0015\nn\nX\ni=1\nT \u03b8\ni (1) +\nn\n(1 \u2212\u03c0)2 .\n(B.9)\nNext, we compute the difference of the second two pieces of (B.7). To this end, define a \u2261\n1/(1 \u2212\u03c0) + 1/\u03c0 and b \u2261n/(1 \u2212\u03c0). We have that\nE\n\u0002\n\u2207\u03c0L(\u03b8; m, g, p)2\u0003\n= E\n\uf8ee\n\uf8f0\n \na\nn\nX\ni=1\npi \u2212b\n!2\uf8f9\n\uf8fb= E\n\uf8ee\n\uf8f0a2\n n\nX\ni=1\npi\n!2\n\u22122ab\nn\nX\ni=1\npi + b2\n\uf8f9\n\uf8fb\n= a2\nn\nX\ni=1\nn\nX\nj=1\nE[pipj] \u22122ab\nn\nX\ni=1\nE[pi] + b2.\nNext,\n(E [\u2207\u03c0L(\u03b8; m, g, x)])2 =\n \na\nn\nX\ni=1\nE[pi] \u2212b\n!2\n= a2\nn\nX\ni=1\nn\nX\nj=1\nE[pi]E[pj] \u22122ab\nn\nX\ni=1\nE[pi] + b2.\n60\nREFERENCES\nTherefore,\n(E[\u2207\u03c0L(\u03b8; m, g, p)])2 \u2212E\n\u0002\n\u2207\u03c0L(\u03b8; m, g, p)2\u0003\n= a2\nn\nX\ni=1\nn\nX\nj=1\nE[pi]E[pj] \u2212a2\nn\nX\ni=1\nn\nX\nj=1\nE[pipj] = a2\n n\nX\ni=1\nE[pi]2 \u2212E[p2\ni ]\n!\n= a2\n n\nX\ni=1\n[T \u03b8\ni (1)]2 \u2212T \u03b8\ni (1)\n!\n=\n\u0012\n1\n(1 \u2212\u03c0) + 1\n\u03c0\n\u00132  n\nX\ni=1\n[T \u03b8\ni (1)]2 \u2212T \u03b8\ni (1)\n!\n.\n(B.10)\nStringing (B.7), (B.9) and (B.10) together, we obtain\nJ\u03c0(\u03b8; m, g) =\n\u0014 1\n\u03c02 \u2212\n1\n(1 \u2212\u03c0)2\n\u0015\nn\nX\ni=1\nT \u03b8\ni (1) +\nn\n(1 \u2212\u03c0)2\n+\n\u0012\n1\n(1 \u2212\u03c0) + 1\n\u03c0\n\u00132  n\nX\ni=1\n[T \u03b8\ni (1)]2 \u2212T \u03b8\ni (1)\n!\n.\n(B.11)\nSubmatrix II\nDenote submatrix II by J\u03b2m(\u03b8; m, g). The formula for J\u03b2m(\u03b8; m, g) is\nJ\u03b2m(\u03b8; m, g) = \u2212E\n\u0002\n\u22072\n\u03b2mL(\u03b8; m, g, p)\n\u0003\n+ E [\u2207\u03b2mL(\u03b8; m, g, p)] E [\u2207\u03b2mL(\u03b8; m, g, p)]T \u2212E\n\u0002\n\u2207\u03b2mL(\u03b8; m, g, p)\u2207\u03b2mL(\u03b8; m, g, p)T \u0003\n.\n(B.12)\nStandard GLM results imply that \u2212\u22072\n\u03b2mL(\u03b8; m, g, p) = \u02dcXT (\u2206mV m\u2206m\u2212[\u2206\u2032]mHm) \u02dcX and \u2207\u03b2mL(\u03b8; m, g, p) =\n\u02dcXT \u2206msm. We compute the first term of (B.12). The (k, l)th entry of this matrix is\n\u0000E\n\u0002\n\u2212\u22072\n\u03b2mL(\u03b8; m, g, p)\n\u0003\u0001\n[k, l] = E\nn\n\u02dcX[, k]T (\u2206mV m\u2206m \u2212[\u2206\u2032]mHm) \u02dcX[, l]\no\n=\nn\nX\ni=1\nE {\u02dcxi,k(\u2206m\ni V m\ni \u2206m\ni \u2212[\u2206\u2032]m\ni Hm\ni )\u02dcxi,l}\n=\nn\nX\ni=1\n\u02dcxi,k(0)T \u03b8\ni (0)[\u2206m\ni (0)V m\ni (0)\u2206m\ni (0) \u2212[\u2206\u2032]m\ni (0)Hm\ni (0)]\u02dcxi,l(0)\n+\nn\nX\ni=1\n\u02dcxi,k(1)T \u03b8\ni (1)[\u2206m\ni (1)V m\ni (1)\u2206m\ni (1) \u2212[\u2206\u2032]m\ni (1)Hm\ni (1)]\u02dcxi,l(1)\n=\n1\nX\ns=0\n\u02dcX(s)[, k]T T \u03b8(s) [\u2206m(s)V m(s)\u2206m(s) \u2212[\u2206\u2032]m(s)Hm(s)] \u02dcX(s)[, l].\nWe therefore have that\nREFERENCES\n61\nE\n\u0002\n\u2212\u22072\n\u03b2mL(\u03b8; m, g, p)\n\u0003\n=\n1\nX\ns=0\n\u02dcX(s)T T \u03b8(s) [\u2206m(s)V m(s)\u2206m(s) \u2212[\u2206\u2032]m(s)Hm(s)] \u02dcX(s).\n(B.13)\nNext, we compute the difference of the last two terms of (B.12). The (k, l)th entry is\n\u0014\nE [\u2207\u03b2mL(\u03b8; m, g, p)] E [\u2207\u03b2mL(\u03b8; m, g, p)]T\n\u2212E\n\u0002\n\u2207\u03b2mL(\u03b8; m, g, p)\u2207\u03b2mL(\u03b8; m, g, p)T \u0003 \u0015\n[k, l]\n=\n\u0014\nE\nh\n\u02dcXT \u2206msmi\nE\nh\n\u02dcXT \u2206msmiT \u0015\n[k, l] \u2212E\nh\n\u02dcXT \u2206msm(sm)T \u2206m \u02dcX\ni\n[k, l]\n= E\nh\n\u02dcX[, k]T \u2206msmi\nE\nh\n\u02dcX[, l]T \u2206msmi\n\u2212E\nh\n\u02dcX[, k]T \u2206msm(sm)T \u2206m \u02dcX[, l]\ni\n= E\n n\nX\ni=1\n\u02dcxik\u2206m\ni sm\ni\n!\nE\n\uf8eb\n\uf8ed\nn\nX\nj=1\n\u02dcxjl\u2206m\nj sm\nj\n\uf8f6\n\uf8f8\u2212E\n\uf8eb\n\uf8ed\nn\nX\ni=1\nn\nX\nj=1\n\u02dcxik\u2206m\ni sm\ni sm\nj \u2206m\nj \u02dcxjl\n\uf8f6\n\uf8f8\n=\nn\nX\ni=1\nn\nX\nj=1\nE[\u02dcxik\u2206m\ni sm\ni ]E[\u02dcxjl\u2206m\nj sm\nj ] \u2212\nn\nX\ni=1\nn\nX\nj=1\nE[\u02dcxik\u2206m\ni sm\ni sm\nj \u2206m\nj \u02dcxjl]\n=\nn\nX\ni=1\nn\nX\nj=1\nE[\u02dcxik\u2206m\ni sm\ni ]E\n\u0002\n\u02dcxjl\u2206m\nj sm\nj\n\u0003\n\u2212\nX\ni\u0338=j\nE[\u02dcxik\u2206m\ni sm\ni ]E[sm\nj \u2206m\nj \u02dcxjl]\n\u2212\nn\nX\ni=1\nE[\u02dcxik\u2206m\ni sm\ni sm\ni \u2206m\ni \u02dcxil]\n=\nn\nX\ni=1\nE[\u02dcxik\u2206m\ni sm\ni ]E[\u02dcxil\u2206m\ni sm\ni ] \u2212\nn\nX\ni=1\nE[\u02dcxik(\u2206m\ni )2(Hm\ni )2\u02dcxil]\n=\nn\nX\ni=1\n\u0002\n\u02dcxik(0)\u2206m\ni (0)T \u03b8\ni (0)Hm\ni (0) + \u02dcxik(1)\u2206m\ni (1)T \u03b8\ni (1)Hm\ni (1)\n\u0003\n\u00b7\n\u0002\n\u02dcxil(0)\u2206m\ni (0)T \u03b8\ni (0)Hm\ni (0) + \u02dcxil(1)\u2206m\ni (1)T \u03b8\ni (1)Hm\ni (1)\n\u0003\n\u2212\nn\nX\ni=1\n\u0002\n\u02dcxik(0)T \u03b8\ni (0)(\u2206m\ni (0))2(Hm\ni (0))2\u02dcxil(0) + \u02dcxik(1)T \u03b8\ni (1)(\u2206m\ni (1))2(Hm\ni (1))m\u02dcxil(1)\n\u0003\n=\n1\nX\ns=0\n1\nX\nt=0\n\" n\nX\ni=1\n\u02dcxik(s)T \u03b8\ni (s)\u2206m\ni (s)Hm\ni (t)T \u03b8\ni (t)\u2206m\ni (t)Hm\ni (t)\u02dcxil(t)\n#\n\u2212\n1\nX\ns=0\n\" n\nX\ni=1\n\u02dcxik(s)T \u03b8\ni (s)(\u2206m\ni (s))2(Hm\ni (s))2\u02dcxil(s)\n#\n=\n1\nX\ns=0\n1\nX\nt=0\n\u02dcX(s)[, k]T T \u03b8(s)\u2206m(s)Hm(s)T \u03b8(t)\u2206m(t)Hm(t) \u02dcX(k)[, l]\n\u2212\n1\nX\ns=0\nX(s)[, k]T T \u03b8(s)(\u2206m(s))2(Hm(s))2 \u02dcX(s)[, l].\n62\nREFERENCES\nThe sum of the last two terms on the right-hand side of (B.12) is therefore\nE [\u2207\u03b2mL(\u03b8; m, g, p)] E [\u2207\u03b2mL(\u03b8; m, g, p)]T \u2212E\n\u0002\n\u2207\u03b2mL(\u03b8; m, g, p)\u2207\u03b2mL(\u03b8; m, g, p)T \u0003\n=\n1\nX\ns=0\n1\nX\nt=0\n\u02dcX(s)T T \u03b8(s)\u2206m(s)Hm(s)T \u03b8(t)\u2206m(t)Hm(t) \u02dcX(t)\n\u2212\n1\nX\ns=0\n\u02dcX(s)T T \u03b8(s)(\u2206m(s))2(Hm(s))2 \u02dcX(s).\n(B.14)\nCombining (B.12), (B.13), (B.14), we find that\nJ\u03b2m(\u03b8; m, g) =\n1\nX\ns=0\n\u02dcX(s)T T \u03b8(s) [\u2206m(s)V m(s)\u2206m(s) \u2212[\u2206\u2032]m(s)Hm(s)] \u02dcX(s)\n+\n1\nX\ns=0\n1\nX\nt=0\n\u02dcX(s)T T \u03b8(s)\u2206m(s)Hm(s)T \u03b8(t)\u2206m(t)Hm(t) \u02dcX(t)\n\u2212\n1\nX\ns=0\n\u02dcX(s)T T \u03b8(s)(\u2206m(s))2(Hm(s))2 \u02dcX(s).\n(B.15)\nSubmatrix III\nDenote submatrix III by J\u03b2g(\u03b8; m, g). The formula for sub-matrix III is similar to\nthat of sub-matrix II (B.15). Substituting g for m in this equation yields\nJ\u03b2g(\u03b8; m, g) =\n1\nX\ns=0\n\u02dcX(s)T T \u03b8(s) [\u2206g(s)V g(s)\u2206g(s) \u2212[\u2206\u2032]g(s)Hg(s)] \u02dcX(s)\n+\n1\nX\ns=0\n1\nX\nt=0\n\u02dcX(s)T T \u03b8(s)\u2206g(s)Hg(s)T \u03b8(t)\u2206g(t)Hg(t) \u02dcX(t)\n\u2212\n1\nX\ns=0\n\u02dcX(s)T T \u03b8(s)(\u2206g(s))2(Hg(s))2 \u02dcX(s).\n(B.16)\nSubmatrix IV\nDenote sub-matrix IV by J(\u03b2g,\u03b2m)(\u03b8; m, g). The formula for J(\u03b2g,\u03b2m)(\u03b8; m, g) is\nJ(\u03b2g,\u03b2m)(\u03b8; m, g) = E [\u2212\u2207\u03b2g\u2207\u03b2mL(\u03b8; m, g, p)]\n+ E [\u2207\u03b2gL(\u03b8; m, g, p)] E [\u2207\u03b2mL(\u03b8; m, g, p)]T \u2212E\n\u0002\n\u2207\u03b2gL(\u03b8; m, g, p)\u2207\u03b2mL(\u03b8; m, g, p)T \u0003\n.\n(B.17)\nFirst, we have that\nE [\u2212\u2207\u03b2g\u2207\u03b2mL(\u03b8; m, g, p)] = 0,\n(B.18)\nREFERENCES\n63\nas differentiating L with respect to \u03b2g yields a vector that is a function of \u03b2g, and differentiating\nthis vector with respect to \u03b2m yields 0. Next, recall from GLM theory that \u2207\u03b2gL(\u03b8; m, g, p) =\n\u02dcXT \u2206gsg and \u2207\u03b2mL(\u03b8; m, g, p) = \u02dcXT \u2206msm. The (k, l)th entry of the last two terms of (B.17) is\n\u0014\nE [\u2207\u03b2gL(\u03b8; m, g, p)] E [\u2207\u03b2mL(\u03b8; m, g, p)]T\n\u2212E\n\u0002\n\u2207\u03b2gL(\u03b8; m, g, p)\u2207\u03b2mL(\u03b8; m, g, p)T \u0003 \u0015\n[k, l]\n=\n\u0014\nE\nh\n\u02dcXT \u2206gsgi\nE\nh\n\u02dcXT \u2206msmiT \u0015\n[k, l] \u2212E\nh\n\u02dcXT \u2206gsg(sm)T \u2206m \u02dcX\ni\n[k, l]\n= E\nh\n\u02dcX[, k]T \u2206gsgi\nE\nh\n\u02dcX[, l]T \u2206msmi\n\u2212E\nh\n\u02dcX[, k]T \u2206gsg(sm)T \u2206m \u02dcX[, l]\ni\n= E\n n\nX\ni=1\n\u02dcxik\u2206g\ni sg\ni\n!\nE\n\uf8eb\n\uf8ed\nn\nX\nj=1\n\u02dcxjl\u2206m\nj sm\nj\n\uf8f6\n\uf8f8\u2212E\n\uf8eb\n\uf8ed\nn\nX\ni=1\nn\nX\nj=1\n\u02dcxik\u2206g\ni sg\ni sm\nj \u2206m\nj \u02dcxjl\n\uf8f6\n\uf8f8\n=\nn\nX\ni=1\nn\nX\nj=1\nE[\u02dcxik\u2206g\ni sg\ni ]E[\u02dcxjl\u2206m\nj sm\nj ] \u2212\nn\nX\ni=1\nn\nX\nj=1\nE[\u02dcxik\u2206g\ni sg\ni sm\nj \u2206m\nj \u02dcxjl]\n=\nn\nX\ni=1\nn\nX\nj=1\nE[\u02dcxik\u2206g\ni sg\ni ]E\n\u0002\n\u02dcxjl\u2206m\nj sm\nj\n\u0003\n\u2212\nX\ni\u0338=j\nE[\u02dcxik\u2206g\ni sg\ni ]E[\u02dcxjl\u2206m\nj sm\nj ]\n\u2212\nn\nX\ni=1\nE[\u02dcxik\u2206g\ni sg\ni sm\ni \u2206m\ni \u02dcxil]\n=\nn\nX\ni=1\nE[\u02dcxik\u2206g\ni Hg\ni ]E[\u02dcxil\u2206m\ni Hm\ni ] \u2212\nn\nX\ni=1\nE[\u02dcxikHg\ni \u2206g\ni \u2206m\ni Hm\ni \u02dcxil]\n=\nn\nX\ni=1\n\u0002\n\u02dcxik(0)\u2206g\ni (0)T \u03b8\ni (0)Hg\ni (0) + \u02dcxik(1)\u2206g\ni (1)T \u03b8\ni (1)Hg\ni (1)\n\u0003\n\u00b7\n\u0002\n\u02dcxil(0)\u2206m\ni (0)T \u03b8\ni (0)Hm\ni (0) + \u02dcxil(1)\u2206m\ni (1)T \u03b8\ni (1)Hm\ni (1)\n\u0003\n\u2212\nn\nX\ni=1\n[\u02dcxik(0)T \u03b8\ni (0)\u2206g\ni (0)Hg\ni (0)\u2206m\ni (0)Hm\ni (0)\u02dcxil(0)\n+ \u02dcxik(1)T \u03b8\ni (1)\u2206g\ni (1)Hg\ni (1)\u2206m\ni (1)Hm\ni (1)\u02dcxil(1)]\n=\n1\nX\ns=0\n1\nX\nt=0\n\" n\nX\ni=1\n\u02dcxik(s)T \u03b8\ni (s)\u2206g\ni (s)Hg\ni (s)T \u03b8\ni (t)\u2206m\ni (t)Hm\ni (t)\u02dcxil(t)\n#\n\u2212\n1\nX\ns=0\n\" n\nX\ni=1\n\u02dcxik(s)T \u03b8\ni (s)\u2206g\ni (s)Hg\ni (s)\u2206m\ni (s)Hm\ni (s)\u02dcxil(s)\n#\n=\n1\nX\ns=0\n1\nX\nt=0\nh\n\u02dcX(s)[, k]T T \u03b8(s)\u2206g(s)Hg(s)T \u03b8(t)\u2206m(t)Hm(t) \u02dcX(t)[, l]\ni\n64\nREFERENCES\n\u2212\n1\nX\ns=0\nh\n\u02dcX[, k]T T \u03b8(s)\u2206g(s)Hg(s)\u2206m(s)Hm(s) \u02dcX[, l](s)\ni\n.\n(B.19)\nCombining (B.17), (B.18), and (B.19) produces\nJ(\u03b2g,\u03b2m)(\u03b8; m, g) =\n1\nX\ns=0\n1\nX\nt=0\n\u02dcX(s)T T \u03b8(s)\u2206g(s)Hg(s)T \u03b8(t)\u2206m(t)Hm(t) \u02dcX(t)\n\u2212\n1\nX\ns=0\n\u02dcX(s)T T \u03b8(s)\u2206g(s)Hg(s)\u2206m(s)Hm(s) \u02dcX(s).\n(B.20)\nSubmatrix V\nDenote submatrix V by J(\u03b2m,\u03c0)(\u03b8; m, g). The formula for J(\u03b2m,\u03c0)(\u03b8; m, g) is\nJ(\u03b2m,\u03c0)(\u03b8; m, g) = E [\u2212\u2207\u03b2m\u2207\u03c0L(\u03b8; m, g, p)]\n+ E [\u2207\u03b2mL(\u03b8; m, g, p)] E [\u2207\u03c0L(\u03b8; m, g, p)]T \u2212E\n\u0002\n\u2207\u03b2mL(\u03b8; m, g, p)\u2207\u03c0L(\u03b8; m, g, p)T \u0003\n.\n(B.21)\nWe have that\nE [\u2212\u2207\u03b2m\u2207\u03c0L(\u03b8; m, g, p)] = 0,\n(B.22)\nas \u03b2m and \u03c0 separate in the log likelihood. Next, set a \u22611/\u03c0 + 1/(1 \u2212\u03c0) and b \u2261n/(1 \u2212\u03c0).\nRecall from GLM theory that \u2207\u03b2mL(\u03b8; m, g, p) = \u02dcXT \u2206msm and from (B.8) that a Pn\ni=1 pi \u2212b.\nThe kth entry of the last two terms of (B.21) is\nE [\u2207\u03c0L(\u03b8; m, g, p)] E [\u2207\u03b2mL(\u03b8; m, g, p)[k]] \u2212E [\u2207\u03c0L(\u03b8; m, g, p)\u2207\u03b2mL(\u03b8; m, g, p)[k]]\n=\n \nE\n\"\na\nn\nX\ni=1\npi \u2212b\n#! \u0010\nE\nh\n\u02dcX[, k]T \u2206msmi\u0011\n\u2212E\n\" \na\nn\nX\ni=1\npi \u2212b\n!\n\u02dcX[, k]T \u2206msm\n#\n=\n \na\nn\nX\ni=1\nE[pi] \u2212b\n! \uf8eb\n\uf8ed\nn\nX\nj=1\nE[\u02dcxjk\u2206m\nj sm\nj ]\n\uf8f6\n\uf8f8\u2212E\n\uf8ee\n\uf8f0\n \na\nn\nX\ni=1\npi \u2212b\n! \uf8eb\n\uf8ed\nn\nX\nj=1\n\u02dcxjk\u2206m\nj sm\nj\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n= a\nn\nX\ni=1\nn\nX\nj=1\nE[pi]E[\u02dcxjk\u2206m\nj sm\nj ] \u2212b\nn\nX\nj=1\nE[\u02dcxjk\u2206m\nj sm\nj ]\n\u2212\n\uf8ee\n\uf8f0a\nn\nX\ni=1\nn\nX\nj=1\nE[pi\u02dcxjk\u2206m\nj sm\nj ] \u2212b\nn\nX\nj=1\nE[\u02dcxjk\u2206m\nj sm\nj ]\n\uf8f9\n\uf8fb\n= a\nn\nX\ni=1\nn\nX\nj=1\nE[pi]E[\u02dcxjk\u2206m\nj sm\nj ] \u2212a\nX\ni\u0338=j\nE[pi]E[\u02dcxjk\u2206m\nj sm\nj ] \u2212a\nn\nX\ni=1\nE[pi\u02dcxik\u2206m\ni sm\ni ]\nREFERENCES\n65\n= a\nn\nX\ni=1\nE[pi]E[\u02dcxik\u2206m\ni sm\ni ] \u2212a\nn\nX\ni=1\nE[pi\u02dcxik\u2206m\ni sm\ni ]\n= a\nn\nX\ni=1\nT \u03b8\ni (1)[T \u03b8\ni (0)\u2206m\ni (0)sm\ni (0)\u02dcxik(0)+T \u03b8\ni (1)\u2206m\ni (1)sm\ni (1)\u02dcxik(1)]\u2212a\nn\nX\ni=1\nT \u03b8\ni (1)\u2206m\ni (1)sm\ni (1)\u02dcxik(1)\n= a\nn\nX\ni=1\nT \u03b8\ni (0)T \u03b8\ni (1)\u2206m\ni (0)Hm\ni (0)\u02dcxik(0)\n+ a\nn\nX\ni=1\n\u0000[T \u03b8\ni (1)]2\u2206m\ni (1)Hm\ni (1) \u2212T \u03b8\ni (1)\u2206m\ni (1)Hm\ni (1)\n\u0001\n\u02dcxik(1)\n= a\n\" n\nX\ni=1\nT \u03b8\ni (0)T \u03b8\ni (1)\u2206m\ni (0)Hm\ni (0)\u02dcxik(0) +\nn\nX\ni=1\nT \u03b8\ni (1)\u2206m\ni (1)Hm\ni (1)[T \u03b8\ni (1) \u22121]\u02dcxik(1)\n#\n= a\n\" n\nX\ni=1\nT \u03b8\ni (0)T \u03b8\ni (1)\u2206m\ni (0)Hm\ni (0)\u02dcxik(0) \u2212\nn\nX\ni=1\nT \u03b8\ni (0)T \u03b8\ni (1)\u2206m\ni (1)Hm\ni (1)\u02dcxik(1)\n#\n= a\n\u0010\n\u02dcX(0)[, k]T wm(0) \u2212\u02dcX(1)[, k]T wm(1)\n\u0011\n.\n(B.23)\nCombining (B.21), (B.22), and (B.23), we conclude that\nJ(\u03b2m,\u03c0)(\u03b8; m, g, p) =\n\u0012 1\n\u03c0 +\n1\n1 \u2212\u03c0\n\u0013 \u0010\n\u02dcX(0)T wm(0) \u2212\u02dcX(1)T wm(1)\n\u0011\n.\n(B.24)\nSubmatrix VI\nDenote submatrix VI by J(\u03b2g,\u03c0)(\u03b8; m, g). Calculations similar to those for subma-\ntrix V show that\nJ(\u03b2g,\u03c0)(\u03b8; m, g, p) =\n\u0012 1\n\u03c0 +\n1\n1 \u2212\u03c0\n\u0013 \u0010\n\u02dcX(0)T wg(0) \u2212\u02dcX(1)T wg(1)\n\u0011\n.\n(B.25)\nCombining submatrices\nTo summarize, the formulas for submatrices I-VI are as follows:\nI\nJ\u03c0(\u03b8; m, g) =\n\u0014 1\n\u03c02 \u2212\n1\n(1 \u2212\u03c0)2\n\u0015\nn\nX\ni=1\nT \u03b8\ni (1) +\nn\n(1 \u2212\u03c0)2\n+\n\u0012\n1\n(1 \u2212\u03c0) + 1\n\u03c0\n\u00132  n\nX\ni=1\n[T \u03b8\ni (1)]2 \u2212T \u03b8\ni (1)\n!\n.\nII\nJ\u03b2m(\u03b8; m, g) =\n1\nX\ns=0\n\u02dcX(s)T T \u03b8(s) [\u2206m(s)V m(s)\u2206m(s) \u2212[\u2206\u2032]m(s)Hm(s)] \u02dcX(s)\n66\nREFERENCES\n+\n1\nX\ns=0\n1\nX\nt=0\n\u02dcX(s)T T \u03b8(s)\u2206m(s)Hm(s)T \u03b8(t)\u2206m(t)Hm(t) \u02dcX(t)\n\u2212\n1\nX\ns=0\n\u02dcX(s)T T \u03b8(s)(\u2206m(s))2(Hm(s))2 \u02dcX(s).\nIII\nJ\u03b2g(\u03b8; m, g) =\n1\nX\ns=0\n\u02dcX(s)T T \u03b8(s) [\u2206g(s)V g(s)\u2206g(s) \u2212[\u2206\u2032]g(s)Hg(s)] \u02dcX(s)\n+\n1\nX\ns=0\n1\nX\nt=0\n\u02dcX(s)T T \u03b8(s)\u2206g(s)Hg(s)T \u03b8(t)\u2206g(t)Hg(t) \u02dcX(t)\n\u2212\n1\nX\ns=0\n\u02dcX(s)T T \u03b8(s)(\u2206g(s))2(Hg(s))2 \u02dcX(s).\nIV\nJ(\u03b2g,\u03b2m)(\u03b8; m, g) =\n1\nX\ns=0\n1\nX\nt=0\n\u02dcX(s)T T \u03b8(s)\u2206g(s)Hg(s)T \u03b8(t)\u2206m(t)Hm(t) \u02dcX(t)\n\u2212\n1\nX\ns=0\n\u02dcX(s)T T \u03b8(s)\u2206g(s)Hg(s)\u2206m(s)Hm(s) \u02dcX(s).\nV\nJ(\u03b2m,\u03c0)(\u03b8; m, g, p) =\n\u0012 1\n\u03c0 +\n1\n1 \u2212\u03c0\n\u0013 \u0010\n\u02dcX(0)T wm(0) \u2212\u02dcX(1)T wm(1)\n\u0011\n.\nVI\nJ(\u03b2g,\u03c0)(\u03b8; m, g, p) =\n\u0012 1\n\u03c0 +\n1\n1 \u2212\u03c0\n\u0013 \u0010\n\u02dcX(0)T wg(0) \u2212\u02dcX(1)T wg(1)\n\u0011\n.\nWe stitch these pieces together and transpose submatrices IV, V, and VI to produce the whole\ninformation matrix J(\u03b8; m, g). Evaluating this matrix at the EM estimate \u03b8EM and inverting\nyields the asymptotic covariance matrix, which we can use to compute standard errors.\nB.5\nImplementation\nTo evaluate the observed information matrix, we need to compute the matrices \u2206m(j), [\u2206\u2032]m(j),\nV m(j), and Hm(j) and the vectors sm(j) and wm(j) for j \u2208{0, 1}. We likewise need to compute\nREFERENCES\n67\nthe analogous gRNA quantities. The procedure that we propose for this purpose is general, but\nfor concreteness, we describe how to implement this procedure using the glm function in R by\nextending base family objects. We implicitly condition on pi, zm\ni , and om\ni .\nAn R family object contains several functions, including linkinv, variance, and mu.eta.\nlinkinv is the inverse link function r\u22121\nm . variance takes as an argument the mean \u00b5m\ni of the ith\nexample and returns its variance [\u03c3m\ni ]2. mu.eta is the derivative of the inverse link function [r\u22121\nm ]\n\u2032.\nWe extend the R family object by adding two additional functions: skewness and mu.eta.prime.\nskewness returns the skewness \u03b3m\ni\nof the distribution as a function of the mean \u00b5i, i.e.\nskewness (\u00b5i) = E\n\"\u0012mi \u2212\u00b5m\ni\n\u03c3m\ni\n\u00133#\n:= \u03b3m\ni .\nFinally, mu.eta.prime is the second derivative of the inverse link function [r\u22121\nm ]\u2032\u2032. Algorithm 2\ncomputes the matrices \u2206m(j), [\u2206\u2032]m(j), V m(j), and Hm(j) and vector sm(j) for given \u03b2m and\ngiven family object. (The vector wm(j) can be computed in terms of \u2206m(j) and Hm(j).) We use\n\u03c3m\ni (j) (resp. \u03b3m\ni (j)) to refer to the standard deviation (resp. skewness) of the gene expression\ndistribution the ith cell when the perturbation pi is set to j.\nAll steps of the algorithm are obvious except the calculation of h\u2032\nm(lm\ni (j)) (line 6), h\u2032\u2032(lm\ni (j))\n(line 9), and V m\ni (j) (line 12). We omit the (j) notation for compactness. First, we prove the\ncorrectness of the expression for h\u2032\nm(lm\ni ). Recall the basic GLM identities\n\u03c8\u2032\u2032\nm(\u03b7m\ni ) = [\u03c3m\ni ]2\n(B.26)\nand, for all t \u2208R,\nr\u22121\nm (t) = \u03c8\u2032\nm(hm(t)).\n(B.27)\nDifferentiating (B.27) in t, we find that\n(r\u22121\nm )\u2032(t) = \u03c8\u2032\u2032\nm(hm(t))h\u2032\nm(t) \u21d0\u21d2h\u2032\nm(t) =\n(r\u22121\nm )\u2032(t)\n\u03c8\u2032\u2032m(hm(t)).\n(B.28)\n68\nREFERENCES\nTable 1. linkinv, variance, mu.eta, skewness, mu.eta.prime for common family objects (i.e., pairs of\ndistributions and link functions).\nGaussian response,\nidentity link\nPoisson response, log\nlink\nNB response\n(s > 0 fixed),\nlog link\nlinkinv\nx\nexp(x)\nexp(x)\nvariance\nx\nx\nx + x2/s\nmu.eta\n1\nx\nexp(x)\nskewness\n0\nx\u22121/2\n2x+s\n\u221asx\u221ax+s\nmu.eta.prime\n0\nexp(x)\nexp(x)\nFinally, plugging in lm\ni\nfor t,\nh\u2032\nm(li) = (r\u22121\nm )\u2032(lm\ni )\n\u03c8\u2032\u2032(hm(lm\ni )) = (r\u22121\nm )\u2032(lm\ni )\n\u03c8\u2032\u2032m(\u03b7m\ni )\n= by (B.26) (r\u22121\nm )\u2032(lm\ni )\n[\u03c3m\ni ]2\n.\nNext, we prove the correctness for the expression for h\u2032\u2032\nm(lm\ni ). Recall the exponential family\nidentity\n\u03c8\u2032\u2032\u2032\nm(\u03b7m\ni ) = \u03b3m\ni ([\u03c3m\ni ]2)3/2.\n(B.29)\nDifferentiating (B.28) in t, we obtain\n(r\u22121\nm )\u2032\u2032(t) = \u03c8\u2032\u2032\u2032\nm(hm(t))[h\u2032\nm(t)]2+\u03c8\u2032\u2032\nm(hm(t))h\u2032\u2032\nm(t) \u21d0\u21d2h\u2032\u2032\nm(t) = (r\u22121\nm )\u2032\u2032(t) \u2212\u03c8\u2032\u2032\u2032(hm(t))[h\u2032\nm(t)]2\n\u03c8\u2032\u2032m(hm(t))\n.\nPlugging in lm\ni\nfor t, we find that\nh\u2032\u2032\nm(lm\ni ) = (r\u22121\nm )\u2032\u2032(lm\ni ) \u2212\u03c8\u2032\u2032\u2032\nm(\u03b7m\ni )[h\u2032\nm(lm\ni )]2\n[\u03c3m\ni ]2\n= (by B.29) (r\u22121\nm )\u2032\u2032(lm\ni ) \u2212([\u03c3m\ni ]2)3/2(\u03b3m\ni )[h\u2032\nm(lm\ni )]2\n[\u03c3m\ni ]2\n.\nFinally, the expression for V m\ni\nfollows from (B.26). We can apply a similar algorithm to compute\nthe analogous matrices for the gRNA modality. Table 1 shows the linkinv, variance, mu.eta,\nskewness, and mu.eta.prime functions for several common family objects (which are defined by\na distribution and link function).\nREFERENCES\n69\nAlgorithm 2 Computing the matrices \u2206m(j), [\u2206\u2032]m(j), V m(j), Hm(j), and sm(j) given\ngiven \u03b2m.\nInput: A coefficient vector \u03b2m; data [m1, . . . , mn], [om\n1 , . . . , om\nn ], and [z1, . . . , zn]; and a family\nobject containing functions linkinv, variance, mu.eta, mu.eta.prime, and skewness.\nfor j \u2208{0, 1} do\nfor i \u2208{1, . . . , n} do\n3:\nlm\ni (j) \u2190\u27e8\u03b2m, \u02dcxi(j)\u27e9+ om\ni\n\u00b5m\ni (j) \u2190linkinv(lm\ni (j))\n[\u03c3m\ni (j)]2 \u2190variance(\u00b5m\ni (j))\n6:\nh\u2032\nm(lm\ni (j)) \u2190mu.eta(lm\ni (j))/[\u03c3m\ni (j)]2\n\u03b3m\ni (j) \u2190skewness(\u00b5m\ni (j))\n[r\u22121\nm ]\u2032\u2032(lm\ni (j)) \u2190mu.eta.prime(lm\ni (j))\n9:\nh\u2032\u2032\nm(lm\ni (j)) \u2190[r\u22121]\u2032\u2032(lm\ni (j)) \u2212[([\u03c3m\ni (j)]2)3/2][\u03b3m\ni (j)][h\u2032\nm(lm\ni (j))]2\n[\u03c3m\ni (j)]2\n\u25b7Assign quantities to matrices\n\u2206m\ni (j) \u2190h\u2032\nm(lm\ni (j))\n[\u2206\u2032]m\ni (j) \u2190h\u2032\u2032(lm\ni (j))\n12:\nV m\ni (j) \u2190[\u03c3m\ni (j)]2\nHm\ni (j) \u2190sm\ni (j) \u2190mi \u2212\u00b5m\ni (j)\nend for\n15: end for\n70\nREFERENCES\nC. Statistical accelerations and computing\nC.1\nStatistical accelerations\nWe describe in detail the procedure for obtaining the pilot parameter estimates (\u03c0pilot, \u03b2pilot\nm\n, \u03b2pilot\ng\n).\nThis procedure consists of two subroutines, which we label Algorithm 3 and Algorithm 4. The first\nstep (Algorithm 3) is to obtain good parameter estimates for [\u03b2m\n0 , \u03b3m]T and [\u03b2g\n0, \u03b3g]T via regres-\nsion. Recall that the underlying gene expression parameter vector \u03b2m is \u03b2m = [\u03b2m\n0 , \u03b2m\n1 , \u03b3m]T \u2208Rd,\nwhere \u03b2m\n0 is the intercept, \u03b2m\n1 is the effect of the perturbation, and \u03b3T\nm is the effect of the technical\nfactors. To produce estimates [\u03b2m\n0 ]pilot and [\u03b3T\nm]pilot, we regress the gene expressions m onto the\ntechnical factors X. The intuition for this procedure is as follows: the probability of perturbation\n\u03c0 is very small. Therefore, the true log likelihood is approximately equal to the log likelihood\nthat results from omitting pi from the model:\nn\nX\ni=1\nfm(mi; \u03b7m\ni ) =\nX\ni:pi=1\nfm(mi; hm(\u03b20 + \u03b21 + \u03b3T zi + om\ni ))\n|\n{z\n}\nfew terms\n+\nX\ni:pi=0\nfm(mi; hm(\u03b20 + \u03b3T zi + om\ni ))\n|\n{z\n}\nmany terms\n\u2248\nn\nX\ni=1\nfm(mi; hm(\u03b20 + \u03b3T zi + om\ni )).\nWe similarly can obtain pilot estimates [\u03b2g\n0]pilot and [\u03b3T\ng ]pilot by regressing the gRNA counts g\nonto the technical factors X. We extract the fitted values (on the scale of the linear component)\nfor use in a subsequent step: \u02c6f k\ni = [\u03b2k\n0]pilot + \u27e8[\u03b3T\nk ]pilot, zi\u27e9+ ok\ni , for k \u2208{m, g}.\nNext, we obtain estimates [\u03b2m\n1 ]pilot, [\u03b2g\n1]pilot, and \u03c0pilot for \u03b2m\n1 , \u03b2g\n1, and \u03c0 by fitting a \u201creduced\u201d\nGLM-EIV (Algorithm 4). The log likelihood of the no-intercept, univariate GLM with predictor\npi and offset \u02c6f m\ni\nis approximately equal to the true log likelihood:\nn\nX\ni=1\nfm(mi; \u03b7m\ni ) =\nn\nX\ni=1\nfm(mi; hm(\u03b20 + \u03b21pi + \u03b3T zi + om\ni )) \u2248\nn\nX\ni=1\nfm(mi; hm(\u03b21pi + \u02c6f m\ni )).\nTherefore, to estimate \u03b2m\n1 , \u03b2g\n1, and \u03c0, we fit a GLM-EIV model with gene expressions m,\ngRNA counts g, gene offsets \u02c6f m := [ \u02c6f m\n1 , . . . , \u02c6f m\nn ]T , gRNA offsets \u02c6f g := [ \u02c6f g\n1 , . . . , \u02c6f g\nn]T , and no\nREFERENCES\n71\nAlgorithm 3 Computing [\u03b2m\n0 ]pilot, [\u03b3T\nm]pilot, [\u03b2g\n0]pilot, and [\u03b3T\ng ]pilot .\nInput: Data m, g, om, og, and X; gene expression distribution fm and link function rm; gRNA\nexpression distribution fg and link function rg; number of EM starts B.\nfor k \u2208{m, g} do\n2:\nFit a GLM GLMk with responses k, offsets ok, design matrix X, distribution fk, and link\nfunction rk.\nSet [\u03b2k\n0]pilot and [\u03b3T\nk ]pilot to the fitted coefficients of GLMk.\n4:\nfor i \u2208{1, . . . , n} do\n\u02c6f k\ni \u2190[\u03b2k\n0]pilot + \u27e8[\u03b3T\nk ]pilot, zi\u27e9+ ok\ni\n\u25b7untransformed fitted values\n6:\nend for\nend for\n8: return ([\u03b2m\n0 ]pilot, \u02c6f m, [\u03b3T\nm]pilot, [\u03b2g\n0]pilot, [\u03b3T\ng ]pilot, \u02c6f g)\nintercept or covariate terms. Intuitively, we \u201cencode\u201d all information about technical factors,\nlibrary sizes, and baseline expression levels into \u02c6f m and \u02c6f g. We run the algorithm B \u224815 times\nover randomly-selected starting values for \u03b2m, \u03b2g, and \u03c0 and select the solution with greatest\nthe log likelihood.\nThe M step of the reduced GLM-EIV algorithm requires fitting two no-intercept, univariate\nGLMs with offsets. We derive analytic formulas for the MLEs of these GLMs in the three most\nimportant cases: Gaussian response with identity link, Poisson response with log link, and negative\nbinomial response with log link (see section C.2; the latter formula is asymptotically exact).\nConsequently, we do not need to run the relatively slow IRLS procedure to carry out the M step\nof the reduced GLM-EIV algorithm. Overall, the proposed method for obtaining the full set of\npilot parameter estimates requires fitting only two GLMs (via IRLS).\n72\nREFERENCES\nAlgorithm 4 Computing \u03c0pilot, [\u03b2m\n1 ]pilot, [\u03b2m\n1 ]pilot.\nInput: Data m, g; fitted offsets \u02c6f m, \u02c6f g.\nbestLik \u2190\u2212\u221e\n\u25b7Reduced GLM-EIV\n2: for i \u2208{1, . . . , B} do\nRandomly generate starting parameters \u03c0curr, [\u03b2m\n1 ]curr, [\u03b2g\n1]curr.\n4:\nwhile Not converged do\nfor i \u2208{1, . . . , n} do\n\u25b7E step\n6:\nTi(1) \u2190P(Pi = 1|Mi = mi, Gi = gi, \u03c0curr, [\u03b2g\n1]curr, [\u03b2m\n1 ]curr)\nTi(0) \u21901 \u2212Ti(1)\n8:\nend for\n\u03c0curr \u2190(1/n) Pn\ni=1 Ti(1)\n\u25b7M step\n10:\nw \u2190[T1(0), T2(0), . . . , Tn(0), T1(1), T2(1), . . . , Tn(1)]T\nfor k \u2208{g, m} do\n12:\nFit no-intercept, univariate GLM GLMk with predictors [0, . . . , 0\n| {z }\nn\n, 1, . . . , 1\n| {z }\nn\n], re-\nsponses [k, k]T , offsets [ \u02c6f k, \u02c6f k]T , and weights w.\nSet [\u03b2k\n1]curr to fitted coefficient of GLMk.\n14:\nend for\nCompute log likelihood currLik using \u03c0curr,[\u03b2m\n1 ]curr, and [\u03b2g\n1]curr.\n16:\nend while\nif currLik > bestLik then\n18:\nbestLik \u2190currLik\n\u03c0pilot \u2190\u03c0curr; [\u03b2m\n1 ]pilot \u2190[\u03b2m\n1 ]curr; [\u03b2g\n1]pilot \u2190[\u03b2g\n1]curr\n20:\nend if\nend for\n22: return (\u03c0pilot, [\u03b2m\n1 ]pilot, [\u03b2g\n1]pilot)\nREFERENCES\n73\nC.2\nIntercept-plus-offset models\nA key step in the algorithm for computing the pilot parameter estimates (Algorithm 4) is to\nfit a weighted, no-intercept, univariate GLM with nonzero offset terms and a binary predictor\nvariable. We derive an analytic formula for the MLE of this GLM for three important pairs\nof response distributions and link functions: Gaussian response with identity link, Poisson re-\nsponse with log link, and negative binomial response with log link. The GLM that we seek\nto estimate has responses [m, m]T , predictors [0, . . . , 0\n| {z }\nn\n, 1, . . . , 1\n| {z }\nn\n], offsets [ \u02c6f m, \u02c6f m], and weights\nw = [T1(0), . . . , Tn(0), T1(1), . . . , Tn(1)]T . Throughout, C denotes a universal constant. The log\nlikelihood of this GLM is\nL(\u03b21; m) =\nn\nX\ni=1\nTi(0)fm(mi; hm(\u03b21 + \u02c6f m\ni )) +\nn\nX\ni=1\nTi(1)fm(mi; hm( \u02c6f m\ni ))\n=\nn\nX\ni=1\nTi(1)fm(mi; hm(\u03b21 + \u02c6f m\ni )) + C.\n(C.1)\nThus, finding the MLE \u02c6\u03b21 is equivalent to estimating a GLM with intercept \u03b21, offsets \u02c6f m, weights\nTi(1), and no covariate terms. We term such a GLM a intercept-plus-offset model. Below, we study\nintercept-plus-offset models in generality.\nGeneral formulation\nLet \u03b2 \u2208R be an unknown constant. Let o1, . . . , on \u223cP1, where P1 is a\ndistribution. Let Yi|oi, . . . , Yn|oi be exponential family-distributed random variables with identity\nsufficient statistic. Suppose the mean \u00b5i of Yi|oi is given by r(\u00b5i) = \u03b2 + oi, where r : R \u2192R is a\nstrictly increasing, differentiable link function. We call this model the intercept-plus-offset model.\nWe derive the (weighted) log likelihood of this model. Let w1, . . . , wn \u223cP2 be weights, where\nP2 is a distribution bounded above by 1 and below by 0. (A special case, which corresponds to\nno weights, is wi = 1 for all i \u2208{1, . . . , n}.) Throughout, we assume that yiwi and exp(oi)wi\nhave finite first moment. Suppose the cumulant-generating function and carrying density of the\nexponential family distribution are \u03c8 : R \u2192R and c : R \u2192R, respectively. The canonical\n74\nREFERENCES\nparameter \u03b7i of the ith observation is\n\u03b7i = ([\u03c8\u2032]\u22121 \u25e6r\u22121)(\u03b2 + oi) := h(\u03b2 + oi),\n(C.2)\nand the density f of Yi|\u03b7i is f(yi; \u03b7i) = exp{yi\u03b7i \u2212\u03c8(\u03b7i) + c(yi)}. The weighted log likelihood is\nL(\u03b2; yi) =\nn\nX\ni=1\nwi log [f(yi; \u03b7i)] = C +\nn\nX\ni=1\nwi(yi\u03b7i \u2212\u03c8(\u03b7i)).\n(C.3)\nOur goal is to find the weighted MLE \u02c6\u03b2 of \u03b2. We consider three important choices for the\nexponential family distribution and link function. In the first two cases \u2013 Gaussian distribution\nwith identity link and Poisson distribution with log link \u2013 we find the finite-sample maximizer of\n(C.3); by contrast, in the third case \u2013 negative binomial distribution with log link \u2013 we find an\nasymptotically exact maximizer.\nGaussian\nFirst, consider a Gaussian response distribution and identity link function r(\u00b5) = \u00b5.\nThe cumulant-generating function \u03c8 is \u03c8(\u03b7) = \u03b72/2, and so, by (C.2),\nh(t) = [\u03c8\u2032]\u22121(r\u22121(t)) = [\u03c8\u2032]\u22121(t) = t.\nPlugging \u03b7i = h(\u03b2 + oi) = \u03b2 + oi and \u03c8(\u03b7i) = (1/2)(\u03b2 + oi)2 into (C.3), we obtain\nL(\u03b2; y) =\nn\nX\ni=1\nwi(yi(\u03b2 + oi) \u2212(\u03b2 + oi)2/2).\nThe derivative of this expression in \u03b2 is\n\u2202L(\u03b2; y)\n\u2202\u03b2\n=\nn\nX\ni=1\nwi(yi \u2212\u03b2 \u2212oi) =\nn\nX\ni=1\nwi(yi \u2212oi) \u2212\u03b2\nn\nX\ni=1\nwi.\nSetting this quantity to 0 and solving for \u03b2, we find that the MLE \u02c6\u03b2gauss is\n\u02c6\u03b2gauss =\nPn\ni=1 wi(yi \u2212oi)\nPn\ni=1 wi\n.\nPoisson\nNext, consider a Poisson response distribution and log link function r(\u00b5) = log(\u00b5). The\ncumulant-generating function \u03c8 is \u03c8(\u03b7) = e\u03b7. Therefore, by (C.2),\nh(t) = [\u03c8\u2032]\u22121(r\u22121(t)) = [\u03c8\u2032]\u22121 (exp(t)) = log(exp(t)) = t.\nREFERENCES\n75\nPlugging \u03b7i = h(\u03b2 + oi) = \u03b2 + oi and \u03c8(\u03b7i) = exp(\u03b2 + oi) into (C.3), we obtain\nL(\u03b2; y) =\nn\nX\ni=1\nwi (yi(\u03b2 + oi) \u2212exp(\u03b2 + oi)) .\nThe derivative of this function in \u03b2 is\n\u2202L(\u03b2; y)\n\u2202\u03b2\n=\nn\nX\ni=1\nwiyi \u2212wi exp(\u03b2 + oi) =\nn\nX\ni=1\nwiyi \u2212exp(\u03b2)\nn\nX\ni=1\nwi exp(oi).\nSetting to zero and solving for \u03b2, we find that the MLE \u02c6\u03b2pois is\n\u02c6\u03b2pois = log\n\u0012 Pn\ni=1 wiyi\nPn\ni=1 wieoi\n\u0013\n.\n(C.4)\nNegative binomial\nFinally, we consider a negative binomial response distribution (with fixed size\nparameter s > 0) and log link function r(\u00b5) = log(\u00b5). The cumulant-generating function \u03c8 is\n\u03c8(\u03b7) = \u2212s log(1 \u2212e\u03b7). The derivative \u03c8\u2032 of \u03c8 is\n\u03c8\u2032(t) = s\n\u0012\net\n1 \u2212et\n\u0013\n=\ns\ne\u2212t \u22121.\nDefine the function \u03b4 : R \u2192R by \u03b4(t) = \u2212log (s/t + 1) . We see that\n\u03c8\u2032(\u03b4(t)) =\ns\nexp (log(s/t + 1)) \u22121 = t,\nimplying \u03b4 = [\u03c8\u2032]\u22121. By (C.2), we have that\nh(t) = [\u03c8\u2032]\u22121(r\u22121(t)) = \u2212log\n\u0012\ns\nexp(t) + 1\n\u0013\n= log\n\u0012\nexp(t)\ns + exp(t)\n\u0013\n.\nTherefore,\n\u03b7i = h(\u03b2+oi) = log\n\u0012\nexp(\u03b2 + oi)\ns + exp(\u03b2 + oi)\n\u0013\n= \u03b2+oi\u2212log\n\u0000s + e\u03b2eoi\u0001\n= \u03b2\u2212log\n\u0000s + e\u03b2eoi\u0001\n+C, (C.5)\nand\n\u03c8(\u03b7i) = \u2212s log\n\u0012\n1 \u2212\nexp(\u03b2 + oi)\ns + exp(\u03b2 + oi)\n\u0013\n= \u2212s log\n\u0012\ns\ns + exp(\u03b2 + oi)\n\u0013\n= \u2212s log(s) + s log[s + exp(\u03b2 + oi)] = s log(s + eseoi) + C.\n(C.6)\n76\nREFERENCES\nPlugging (C.5) and (C.6) into (C.3), the log-likelihood (up to a constant) is\nL(\u03b2; y) = \u03b2\nn\nX\ni=1\nwiyi \u2212\nn\nX\ni=1\nwiyi log(s + e\u03b2eoi) \u2212s\nn\nX\ni=1\nwi log(s + e\u03b2eoi)\n= \u03b2\nn\nX\ni=1\nwiyi \u2212\nn\nX\ni=1\n(yi + s)wi log(s + e\u03b2eoi).\nThe derivative of L in \u03b2 is\n\u2202L(\u03b2; y)\n\u2202\u03b2\n=\nn\nX\ni=1\nwiyi \u2212\nn\nX\ni=1\nwi(yi + s)e\u03b2eoi\ns + e\u03b2eoi\n.\nSetting the derivative to zero, the equation defining the MLE is\ne\u03b2\nn\nX\ni=1\nwieoi(yi + s)\ne\u03b2eoi + s\n=\nn\nX\ni=1\nwiyi.\n(C.7)\nWe cannot solve for \u03b2 in (C.7) analytically. However, we can derive an asymptotically exact\nsolution. By the law of total expectation,\nE\n\u0014wieoi(yi + s)\ne\u03b2+oi + s\n\u0015\n= E\n\u0014\nE\n\u0014wieoi(yi + s)\ne\u03b2+oi + s\n\f\f\f\f(oi, wi)\n\u0015\u0015\n= E\n\u0014wieoi(e\u03b2+oi + s)\ne\u03b2+oi + s\n\u0015\n= E[wieoi];\nthe second equality holds because E[yi|oi] = \u00b5i = e\u03b2+oi. Dividing by n on both sides of (C.7)\nand rearranging,\n\u03b2 = log\n\u0012(1/n) Pn\ni=1 wieoi(yi + s)/(e\u03b2eoi + s)\n(1/n) Pn\ni=1 wiyi.\n\u0013\n.\n(C.8)\nBy weak LLN, the limit (in probability) of the MLE \u02c6\u03b2NB is\n\u02c6\u03b2NB\nP\u2212\u2192log\n\u0012 E[wiyi]\nE[wieoi]\n\u0013\n.\n(C.9)\nBut the Poisson MLE \u02c6\u03b2Pois (C.4) converges in probability to the same limit:\n\u02c6\u03b2pois = log\n\u0012 (1/n) Pn\ni=1 wiyi\n(1/n) Pn\ni=1 wieoi\n\u0013\nP\u2212\u2192log\n\u0012 E[wiyi]\nE[wieoi]\n\u0013\n.\nTherefore, for large n, we can approximate \u02c6\u03b2NB by \u02c6\u03b2pois.\nREFERENCES\n77\nApplication to GLM-EIV\nThe GLM that we seek to estimate (C.1) is an approximate intercept-\nplus-offset model: T1(1), . . . , Tn(1) are the weights w1, . . . , wn, and \u02c6f m\n1 , . . . , \u02c6f m\nn\nare the offsets\no1, . . . , om. Of course, T1(1), . . . , T1(n) are in general dependent random variables, as are \u02c6f m\n1 , . . . , \u02c6f m\nn .\nTi(1) depends on mi and gi, as well as the final parameter estimate (\u02c6\u03c0, \u02c6\u03b2m, \u02c6\u03b2g), which itself is a\nfunction of m and g; the situation is similar for the \u02c6f m\ni s. In practice, we find that the intercept-\nplus-offset model is very good approximation to the GLM (C.1), especially when the number of\ncells n is large. Additionally, we note that the GLM (C.1) is fitted as a subroutine of the algo-\nrithm for producing pilot parameter estimates (Algorithm 4). The quality of the pilot parameter\nestimates does not affect the validity of the estimation and inference procedures (Algorithm 1),\nbarring issues related to convergence to local optima.\nC.3\nComputing\nWe describe in detail the at-scale GLM-EIV pipeline. First, we run a round of \u201cprecomputations\u201d\non all dg genes and dp perturbations. The precomputations involve regressing the gene expressions\n(or gRNA counts) onto the technical factors, thereby \u201cfactoring out\u201d Algorithm 3. Next, we\nrun differential expression analyses on the full set of gene-perturbation pairs; for a given pair,\nthis amounts to obtaining the complete set of pilot parameters (by running a reduced GLM-\nEIV), fitting the GLM-EIV model (Algorithm 1), and performing inference. The three loops in\nAlgorithm 5 are embarrassingly parallel and therefore can be massively parallelized.\nD. The Nat. Biotech. 2020 method\nAs described in the main text, the Nat. Biotech. 2020 method (of Replogle and others (2020))\nfits a Poisson-Gaussian mixture model to the log-2 transformed gRNA counts and then assigns\ngRNAs to cells based on the posterior perturbation probabilities. If a given cell has a posterior\nperturbation probability greater than 1/2, then the gRNA is assigned to that cell; otherwise, the\n78\nREFERENCES\nAlgorithm 5 Applying GLM-EIV at scale.\nG \u2190{gene1, . . . , genedg}; P \u2190{perturbation1, . . . , perturbationdp}\nfor gene \u2208G do\nRun precomputation (Algorithm 3) on gene; save \u02c6f m, [\u03b2m\n0 ]pilot and [\u03b3T\nm]pilot.\nend for\nfor perturbation \u2208P do\nRun precomputation (Algorithm 3) on perturbation; save \u02c6f g, [\u03b2g\n0]pilot and [\u03b3T\ng ]pilot.\nend for\nfor (gene, perturbation) \u2208G \u00d7 P do\nLoad \u02c6f m, \u02c6f g, [\u03b2m\n0 ]pilot [\u03b3T\nm]pilot, [\u03b2g\n0]pilot and [\u03b3T\ng ]pilot.\nCompute [\u03b2m\n1 ]pilot, [\u03b2g\n1]pilot, \u03c0pilot by fitting a reduced GLM-EIV (Algorithm 4).\nRun GLM-EIV using the pilot parameters (Algorithm 1).\nend for\ngRNA is not assigned to that cell. Covariates (including gRNA library size, gene library size,\nbatch, etc.) are not included in the model.\nAs mentioned in the main text, the Nat. Biotech. 2020 method poses several conceptual and\npractical challenges. First, the log-2 transformed gRNA counts are not integer-valued. Thus, it is\nunclear how the Poisson component of the mixture distribution is fitted to the data. Second, the\nauthors of the Nat. Biotech. 2020 method used the Python package Pomegranate (github.com/\njmschrei/pomegranate; version <= 0.14.8) to implement their method. Unfortunately, due to\nrecent updates to the Pomegranate package, we and others have been unable to install version\n<= 0.14.8 (relevant Github issues: github.com/jmschrei/pomegranate/issues/1052, github.\ncom/jmschrei/pomegranate/issues/1057).\nThus, we attempted to implement the Nat. Biotech. 2020 method ourselves in R using the\nflexmix package, a popular package for mixture modeling. We found that flexmix throws an\nREFERENCES\n79\nerror when one attempts to fit a Poisson distribution to non-integer data. We therefore con-\nsidered a modification to the Nat. Biotech. 2020 method in which we fitted a two-component\nGaussian mixture to the log-transformed gRNA counts, adding a pseudocount of one to avoid\ntaking the log of zero. Unfortunately, our modified version of the Nat. Biotech. 2020 method\ndid not work well in practice, as it categorized all cells as unperturbed on both the simu-\nlated gRNA data (Figure 3) and the low-MOI gRNA data (Figure 5). The default CellRanger\nmethod for gRNA assignment \u2014 which is based on the Nat. Biotech. 2020 method \u2014 uses a two-\ncomponent Gaussian mixture model (www.10xgenomics.com/support/software/cell-ranger/\nlatest/algorithms-overview/cr-crispr-algorithm). The CellRanger method became open-\nsource shortly before the publication of this paper.\n80\nREFERENCES\nE. Data analysis details\nFirst, we performed quality control and basic pre-processing on both datasets. As is standard\nin single-cell analysis, we removed cells with a high fraction (> 8%) of mitochondrial reads\n(Choudhary and Satija, 2022). We additionally excluded genes that were expressed in fewer than\n10% of cells or that had a mean expression level of less than 1. We excluded cells in the Gasperini\ndataset with gene transcript UMI or gRNA counts below the 5th percentile or above the 95th\npercentile to reduce the effect of outliers. We did not repeat this latter quality control step on\nthe Xie data because the Xie data appeared to be less noisy. The quality-controlled Gasperini\nand Xie datasets contained n = 170, 645 (resp. n = 101, 508) cells, 2, 079 (resp. 1, 030) genes, and\n6, 598 (resp. 516) distinct perturbations.\nThe Gasperini dataset came with 17, 028 candidate cis pairs, 97, 818 negative control pairs,\nand 322 positive control pairs. The cis pairs consisted of genes paired to nearby enhancers with\nunknown regulatory effects. The negative control pairs consisted of non-targeting gRNAs paired\nto genes. The positive control pairs are described in the main text. The Xie data did not come\nwith either cis, negative control, or positive control pairs. Therefore, we constructed a set of 681\ncandidate cis pairs by pairing perturbations to nearby genes, and we constructed a set of 50, 000\nin silico negative control by pairing perturbations to genes on different chromosomes. See the\nMethods section of Barry and others (2021) for details on the construction of cis and in silico\nnegative control pairs on the Xie data. Because the negative control pairs are not expected to\nexhibit a regulatory relationship, the ground truth fold change in gene expression for these pairs\nis taken to be unity.\nWe modeled the gene expression counts using a negative binomial distribution with unknown\nsize parameter s; we estimated s using the glm.nb package. Choudhary and Satija (2022) report\nthat Poisson models accurately capture highly sparse single-cell data. Although Choudhary and\nSatija did not investigate the application of Poisson models gRNA data specifically, we modeled\nREFERENCES\n81\nthe gRNA counts using Poisson distributions, as the gRNA modality exhibited greater sparsity\nthan the gene modality.\nWe applied GLM-EIV and the thresholding method to analyze the entire set of pairs in both\ndatasets. We did not report results on the candidate cis pairs in the text because we do not know\nthe ground truth for these pairs, making them less useful for method assessment. We focused our\nattention instead on the negative control pairs in both datasets and the positive control pairs in\nthe Gasperini dataset.\nWe describe in more detail how we conducted the \u201cexcess background contamination\u201d anal-\nysis. For each positive control pair, we varied excess background contamination over the grid\n[0.0, 0.05, 0.1, . . . , 0.4]. For a given level of excess background contamination, we generated B = 50\nsynthetic gRNA datasets, holding fixed the raw gene expressions, covariates, library sizes, and\nfitted perturbation probabilities. We fitted GLM-EIV and the thresholding method to the data,\nyielding estimates [\u02c6\u03b2m\n1 ](1), . . . , [\u02c6\u03b2m\n1 ](B). Next, we averaged over the [\u02c6\u03b2m\n1 ](i)s to obtain the mean\nestimate for a given pair and level of background contamination, and we calculated the REC\nusing these mean estimates.\nF. Additional related work\nSeveral authors working on statistical methods for single-cell data recently have extended mod-\nels that (implicitly or explicitly) assume Gaussianity and homoscedasticity to a broader class\nof exponential family distributions. For example, Lin and others (2021) and Townes and others\n(2019) (separately) developed eSVD and GLM-PCA, generalizations of SVD and PCA, respec-\ntively, to exponential family response distributions. Unlike their vanilla counterparts, eSVD and\nGLM-PCA can model gene expression counts directly, improving performance on dimension re-\nduction tasks. We see our work (in part) as a continuation of this broad effort to \u201cport\u201d common\nstatistical methods and models to single-cell count data. Our focus, however, is on regression\n82\nREFERENCES\nrather than dimension reduction: we extend the classical errors-in-variables model in several key\ndirections (see above), enabling its direct and natural application to multimodal single-cell data.\nG. Simulation study details and additional simulation studies\nG.1\nMain text simulation study parameter values\nWe constructed a table (Table 2) that maps each model parameter to its (i) main text simu-\nlation study value and (ii) estimated value on real data. We obtained the real-data parameter\nestimates by applying GLM-EIV to analyze a representative gRNA-gene pair from the Gasperini\nand others (2019) data (namely, gene \u201cENSG00000213931\u201d paired to positive control gRNA\n\u201cpos control Klannchr1 HS4\u201d). The main difference between the simulation parameter values\nand real-data parameter values is that the perturbation effect size on gRNA expression (i.e.,\nexp(\u03b2g\n1)) is smaller in the simulation study than on the real data. This difference has the effect\nof placing the simulation study into a more challenging region of the problem space.\nParameter\nSimulation value\nEstimated real data value\nMeaning\nexp(\u03b2m\n0 )\n0.01\n0.02\nGene model intercept\nexp(\u03b2m\n1 )\n0.25\n0.68\nGene perturbation effect\nexp(\u03b3m\n1 )\n0.9\n1.0\nGene batch effect\nexp(\u03b2g\n0)\n5.0 \u00b7 10\u22123\n3.4 \u00b7 10\u22126\ngRNA model intercept\nexp(\u03b2g\n1)\n[1.0, 1.5, . . . , 4.0]\n6, 200\ngRNA perturbation effect\nexp(\u03b3g\n1)\n1.1\n1.05\ngRNA batch effect\n\u03c0\n0.02\n0.004\nPerturbation probability\nTable 2. A mapping of each model parameter to its (i) main text simulation study value and (ii) estimated\nvalue on real data.\nG.2\nAdditional simulation studies\nWe report the results of five additional simulation studies. Study 2 considers Gaussian (as opposed\nto negative binomial or Poisson) data; study 3 varies the negative binomial size parameter s;\nREFERENCES\n83\nstudy 4 varies the effect size of the perturbation on gene expression \u03b2m\n1 ; and study five (resp.,\nsix) considers gRNA (resp., gene) expression data that are contaminated by doublets and an\nunmeasured covariate. In all simulation studies we deployed the accelerated version of GLM-\nEIV.\nSimulation study 2. In simulation study 2 we modeled the gene and gRNA expressions\nusing a Gaussian distribution with an identity link. We generated data on n = 50, 000 cells, fix-\ning the target of inference \u03b2m\n1 to \u22124 and the probability of perturbation \u03c0 to 0.05. We included\n\u201csequencing batch\u201d (modeled as a Bernoulli-distributed variable) and \u201csequencing depth\u201d (mod-\neled as a Poisson-distributed variable) as covariates in the model. We did not include sequencing\ndepth as an offset because use of the identity link renders offsets meaningless. We varied \u03b2g\n1 over a\ngrid on the interval [0, 7]. We applied GLM-EIV, thresholded regression, and the gRNA mixture\nassignment method (coupled to linear regression) to analyze the simulated data. The ranking of\nthe methods was as follows: GLM-EIV (best), gRNA mixture assignment method (intermediate),\nand thresholding method (worst) (Figure 9).\nSimulation study 3. Simulation study 3 was similar to the main text simulation study.\nThe difference is that in simulation study 3, we held fixed \u03b2g\n1 = log(2.5) while varying the\nnegative binomial size parameter s over the grid 1 = 100/9, 102/9, 104/9, . . . , 1016/9, 1018/9 = 100.\nWe applied the three methods twice: once assuming known s and once under unknown s. All\nmethods demonstrated roughly uniform bias over the grid of s values: the bias of GLM-EIV\nwas near zero, while that of the thresholding method and the gRNA mixture method was about\n0.02. As s increased, the CI width of all methods decreased (as the gene expression data became\nmore Poisson-like, causing standard errors to shrink). The confidence interval coverage of the\nthresholding method and the gRNA mixture method degraded, while that of GLM-EIV remained\nat the roughly nominal level. The former two methods likely lost coverage because their biased\nestimates caused the increasingly-narrow confidence intervals to be centered at the wrong location.\n84\nREFERENCES\nThe results were broadly similar across known s and unknown s (though slightly better under\nknown s).\nSimulation study 4. Simulation study 4 also was similar to the main text simulation study.\nThe difference is that in simulation study 4, we held fixed the perturbation effect size on gRNA\nexpression (exp(\u03b2g\n1) = 2.5) and varied the perturbation effect size on gene expression exp(\u03b2m\n1 )\nover the grid 0.2, 0.3, . . . , 0.9, 1.0. We applied the three methods to analyze data generated from\nPoisson, negative binomial (with known s), and negative binomial (with unknown s) gene ex-\npression distributions. We observed that as the magnitude of the effect size increased (i.e., as\nexp(\u03b2m\n1 ) decreased from 1.0 to 0.2), GLM-EIV remained roughly unbiased, while the threshold-\ning method and the gRNA mixture assignment method exhibited increasingly severe attenuation\nbias. Furthermore, GLM-EIV maintained coverage at the nominal level, while the coverage of the\nthresholding method and the gRNA mixture assignment method degraded due to the aforemen-\ntioned attenuation bias. Results were broadly similar (albeit slightly worse) under estimated s\nthan known s.\nWe additionally plotted the rejection probability, i.e. the probability of rejecting the null\nhypothesis of H0 : exp(\u03b2m\n1 ) = 1 at level 0.05. When exp(\u03b2m\n1 ) = 1 (i.e., when we are under the\nnull hypothesis), the rejection probability (which corresponds to type-I error) should be 0.05,\nthe nominal level. When exp(\u03b2m\n1 ) < 1 (i.e., when we are under the alternative hypothesis), the\nrejection probability (which corresponds to power) should be as large as possible (with a value of\n1.0 being optimal). We observed that all methods exhibited a rejection probability of roughly 0.05\nunder the null hypothesis of exp(\u03b2m\n1 ) = 1 and a rejection probability of 1.0 under the alternative\nhypotheses of exp(\u03b2m\n1 ) = 0.9, 0.8, . . . , 0.2, 0.1. In other words, over the grid of values that we\nexamined, each method performed optimally with respect to testing the hypothesis exp(\u03b2m\n1 ) = 1.\n(We note that our goal in the simulation studies was to explore discrepancies in estimation\naccuracy and confidence interval coverage across methods, but we present type-I error control\nREFERENCES\n85\nand power results for completeness.)\nSimulation study 5. In simulation study 5 we applied the methods to analyze data drawn\nfrom a distribution that lay outside the GLM-EIV family of distributions. First, we simulated\ngRNA count data from a poisson GLM with two covariates: batch (modeled as a Bernoulli\nrandom variable with probability 1/2) and cell cycle (modeled as a uniform random variable on\nthe interval [0,1]). We treated cell cycle as an unmeasured covariate, i.e. we did not give any of the\nmethods access to cell cycle. Next, we randomly selected 1% of cells and doubled the gRNA count\nin these cells, thereby simulating the presence of doublets (i.e., droplets that contain two cells) in\nthe data. We simulated the gene expression data from the same negative binomial model that we\nused in the main text simulation (and so the gene expression model was correctly specified.) For\nsimplicity we assumed that the size parameter s = 20 was known. We varied the perturbation\neffect size on gRNA expression exp(\u03b2g\n1) over the grid 1, 2, . . . , 7 and the perturbation effect size\non gene expression exp(\u03b2m\n1 ) over the grid 0.25, 0.5, 0.75, 1.0.\nWe applied GLM-EIV, thresholded regression, and the gRNA mixture assignment method to\nanalyze the data. GLM-EIV exhibited generally lower bias, lower mean squared error, and better\nconfidence interval coverage than the other methods. The rightmost panel (i.e, exp(\u03b2m\n1 ) = 1)\ncorresponds to the null hypothesis of no perturbation effect on gene expression; the left panels\n(i.e., exp(\u03b2m\n1 ) = 0.75, 0.5, 0.25), by contrast, correspond to alternative hypotheses of varying\nstrength. All methods controlled type-I error at the nominal level of 0.05. GLM-EIV demonstrated\nequal or greater power than the competing methods.\nSimulation study 6. Simulation study 6 was similar to simulation study 5, the difference\nbeing that simulation study 6 considered a misspecified gene expression model (while simulation\nstudy 5 considered a misspecified gRNA count model). We generated the gene expression data\nfrom a negative binomial GLM containing the unmeasured covariate of cell cycle, and we doubled\nthe gene expression count in 1% of randomly selected cells to simulate doublets. We generated\n86\nREFERENCES\ngRNA counts from the same gRNA model that we used in the main text simulation (and so the\ngRNA count model was correctly specified.) Again, we varied exp(\u03b2g\n1) over the grid 1, 2, . . . , 7\nand exp(\u03b2m\n1 ) over the grid 0.25, 0.5, 0.75, 1.0. We found that GLM-EIV generally performed best:\nGLM-EIV exhibited lower bias, lower mean squared error, and better confidence interval coverage\nthan the other methods. There was one setting for \u03b2g\n1 (namely, exp(\u03b2g\n1) = 1.5) for which GLM-\nEIV did not control type-I error under the null hypothesis of exp(\u03b2m\n1 ) = 1. However, this was an\nextreme value for \u03b2g\n1, and GLM-EIV controlled type-I error under all other values of \u03b2g\n1.\nFig. 9. Simulation study 2. Analyzing data generated from a linear Gaussian model. Rejection proba-\nbility (not plotted) was strictly 1 across methods and parameter settings.\nREFERENCES\n87\nFig. 10. Simulation study 3. Varying the negative binomial size parameter s. Rejection probability\n(not plotted) was strictly 1 across methods and parameter settings.\n88\nREFERENCES\nFig. 11. Simulation study 4. Varying the perturbation effect size on gene expression, \u03b2m\n1 .\nREFERENCES\n89\nFig. 12. Simulation study 5. Analyzing data using a misspecified gRNA count model.\n90\nREFERENCES\nFig. 13. Simulation study 6. Analyzing data using a misspecified gene expression model.\n"}