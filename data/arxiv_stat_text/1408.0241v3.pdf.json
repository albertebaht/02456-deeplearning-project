{"text": "arXiv:1408.0241v3  [math.ST]  3 Jul 2016\nLinear and Conic Programming Estimators in\nHigh-Dimensional Errors-in-variables Models\nAlexandre Belloni\u2217,\nMathieu Rosenbaum\u2020\u2021 and Alexandre B. Tsybakov\u2021\nJuly 5, 2016\nAbstract\nWe consider the linear regression model with observation error in the design.\nIn\nthis setting, we allow the number of covariates to be much larger than the sample size.\nSeveral new estimation methods have been recently introduced for this model. Indeed,\nthe standard Lasso estimator or Dantzig selector turn out to become unreliable when\nonly noisy regressors are available, which is quite common in practice. In this work, we\npropose and analyse a new estimator for the errors-in-variables model. Under suitable\nsparsity assumptions, we show that this estimator attains the minimax e\ufb03ciency bound.\nImportantly, this estimator can be written as a second order cone programming minimi-\nsation problem which can be solved numerically in polynomial time. Finally, we show\nthat the procedure introduced in [17], which is almost optimal in a minimax sense, can\nbe e\ufb03ciently computed by a single linear programming problem despite non-convexities.\n1\nIntroduction\nWe consider the regression model with observation error in the design:\ny = X\u03b8\u2217+ \u03be,\nZ = X + W.\n(1)\nHere the random vector y \u2208Rn and the random n \u00d7 p matrix Z are observed, the n \u00d7 p\nmatrix X is unknown, W is an n \u00d7 p random noise matrix, \u03be \u2208Rn is a random noise vector,\nand \u03b8\u2217\u2208Rp is a vector of unknown parameters to be estimated. For example, the case\nwhere the entries of matrix X are missing at random can be reduced to this model. Such\nlinear regressions with errors in both variables have been widely investigated in the literature,\nsee for example [3, 6, 10]. Our work is di\ufb00erent in that we consider the setting where the\ndimension p can be much larger than the sample size n, and \u03b8\u2217is sparse.\nIt has been shown in [16] that the presence of observation noise has severe consequences\non the usual estimation procedures in the high-dimensional setting. In particular, the Lasso\nestimator and Dantzig selector turn out to be inaccurate and fail to identify the sparsity\npattern of the vector \u03b8\u2217. The same paper provides an alternative procedure, called Matrix\nUncertainty selector (MU selector for short), which is robust to the presence of noise. The\nMU selector \u02c6\u03b8MU is de\ufb01ned as a solution of the minimisation problem\nmin{|\u03b8|1 : \u03b8 \u2208\u0398,\n\f\f 1\nnZT (y \u2212Z\u03b8)\n\f\f\n\u221e\u2264\u00b5|\u03b8|1 + \u03c4},\n(2)\n\u2217The Fuqua School of Business, Duke University\n\u2020Laboratoire de Probabilit\u00b4es et Mod`eles Al\u00b4eatoires, Universit\u00b4e Pierre et Marie Curie (Paris 6)\n\u2021Centre de Recherche en Economie et Statistique, ENSAE-Paris Tech\n1\nwhere |\u00b7|q denotes the \u2113q-norm for 1 \u2264q \u2264\u221e, \u0398 is a given convex subset of Rp characterising\nthe prior knowledge about \u03b8\u2217, and the constants \u00b5 \u22650 and \u03c4 \u22650 depend on the level of the\nnoises W and \u03be respectively. An extension of the MU selector to generalized linear models\nis discussed in [20].\nIn [17], a modi\ufb01cation of the MU selector is suggested. It applies when W is a random\nmatrix with independent and zero mean entries Wij such that for any 1 \u2264j \u2264p, the sum of\nexpectations\n\u03c32\nj = 1\nn\nn\nX\ni=1\nE(W 2\nij)\nis \ufb01nite and admits a data-driven estimator. This is for example the case in the model with\nmissing data: eZij = Xij\u03b7ij, i = 1, . . . , n, j = 1, . . . , p, where for each \ufb01xed j = 1, . . . , p,\nthe factors \u03b7ij, i = 1, . . . , n, are i.i.d. Bernoulli random variables taking the value 1 with\nprobability 1\u2212\u03c0j and 0 with probability \u03c0j, 0 < \u03c0j < 1. Indeed, this model can be rewritten\nunder the form Zij = Xij+Wij, where Zij = eZij/(1\u2212\u03c0j) and Wij = Xij(\u03b7ij\u2212(1\u2212\u03c0j))/(1\u2212\u03c0j).\nThus, in this model, the \u03c32\nj satisfy \u03c32\nj = 1\nn\nPn\ni=1 X2\nij\n\u03c0j\n1\u2212\u03c0j , and it is easily shown that they\nadmit good data-driven estimators \u02c6\u03c32\nj , see [17]. There are of course other examples where \u03c32\nj\ncan be accurately estimated from the data. One of them is related to repeated measurement\nmodels where the values of Z are available on a \ufb01ner time scale. We refer here to examples\ngiven in [16], such as portfolio replication. In the problem of portfolio replication, assuming\nfor example that the entries of X are approximately constant on the daily scale, we can\nreadily estimate the variances using the additional \ufb01ner scale measurements of Z on the\nhourly scale.\nThe construction of this modi\ufb01ed estimator is based on the following idea. We cannot\nuse X in our estimation procedure since only its noisy version Z is available. In particular,\nthe MU selector involves the matrix ZTZ/n instead of XT X/n.\nCompared to XT X/n,\nthis matrix contains a bias induced by the diagonal entries of the matrix W TW/n whose\nexpectations \u03c32\nj do not vanish. Therefore, if the \u03c32\nj can be estimated, a natural idea is to\ncompensate this bias thanks to these estimates. This leads to a new estimator \u02c6\u03b8C, called\ncompensated MU selector, and de\ufb01ned as a solution of the minimisation problem\nmin{|\u03b8|1 : \u03b8 \u2208\u0398,\n\f\f 1\nnZT (y \u2212Z\u03b8) + bD\u03b8\n\f\f\n\u221e\u2264\u00b5|\u03b8|1 + \u03c4},\n(3)\nwhere bD is the diagonal matrix with entries \u02c6\u03c32\nj, which are estimators of \u03c32\nj, and \u00b5 \u22650 and\n\u03c4 \u22650 are constants chosen according to the level of the noises and the accuracy of the\nestimators \u02c6\u03c32\nj.\nSeveral aspects of the compensated MU selector are studied in [17], in particular the rates\nof convergence in \u2113q-norm, the prediction risk and the design of con\ufb01dence intervals. One of\nthe interests of this modi\ufb01cation of the MU selector is that it enables us to obtain bounds\nfor the estimation errors which are decreasing with the sample size n. This is in contrast to\nthe case of the MU selector, where the bounds are small only if the noise W is small. For\nexample, if \u03b8\u2217is s-sparse, it is shown in [17] that under appropriate assumptions,\n|\u02c6\u03b8C \u2212\u03b8\u2217|q \u2264Cs1/q\nr\nlog p\nn (|\u03b8\u2217|1 + 1),\n1 \u2264q \u2264\u221e,\n(4)\nwith probability close to 1, where C > 0 is a constant independent of s, p, n, and \u03b8\u2217.\nAn alternative Lasso type estimator (non-convex program) complemented by an iterative\nrelaxation procedure is introduced in [12]. This method requires the knowledge of the exact\nvalue of |\u03b8\u2217|1 (or of the property |\u03b8\u2217|1 \u2264b\u221as for a constant b), and of an upper bound on\n|\u03b8\u2217|2. Considering the setting where the entries of the regression matrix X are zero-mean\n2\nsubgaussian, it is shown in [12] that if \u03b8\u2217is s-sparse, under appropriate assumptions, the\nresulting estimator \u02c6\u03b8\u2032 satis\ufb01es\n|\u02c6\u03b8\u2032 \u2212\u03b8\u2217|2 \u2264C(\u03b8\u2217)s1/2\nr\nlog p\nn (|\u03b8\u2217|2 + 1),\n(5)\nwith probability close to 1, where C(\u03b8\u2217) > 0 depends on \u03b8\u2217in a non-speci\ufb01ed way. Related\ncovariates selection results are reported in [19]. In [4, 5], the authors propose yet another\nmethod of estimation of \u03b8\u2217, based on orthogonal matching pursuit (OMP). Their proce-\ndure needs the parameter s (the exact number of non-zero components of \u03b8\u2217) as an input.\nMoreover, they impose the additional assumption that the non-zero components \u03b8\u2217\nj of \u03b8\u2217are\nsu\ufb03ciently large:\n|\u03b8\u2217\nj| \u2265c\nr\nlog p\nn (|\u03b8\u2217|2 + 1),\nj = 1, . . . , p,\n(6)\nwhere c > 0 is a constant. Focusing as in [12] on the case where the entries of the regression\nmatrix X are zero-mean subgaussian, it is shown in [4, 5] that the OMP estimator satis\ufb01es\na bound analogous to (5) with constant C(\u03b8\u2217) \u2261C > 0 independent of \u03b8\u2217, as well as a\nconsistent support recovery property.\nThese recent developments shed light on errors-in-variables problems in high dimensional\nsettings. However, they are not fully satisfying. Indeed, the following issues are remaining:\n\u2022 From a practical viewpoint, the use of the above estimators can be intricate. In particular,\nthe minimisation problem (3) is not always a convex one, and [17] does not provide an\nalgorithm enabling to solve it in general case. Although the methods suggested in [12] and\n[4, 5] are computationally feasible, they need the knowledge of the parameters |\u03b8\u2217|1, |\u03b8\u2217|2 or\ns, which are not available in practice.\n\u2022 While the bound (4) is more general than (5) (it holds for all q and not only for zero-\nmean subgaussian X), it is less accurate than (5) in the case q = 2 assuming that (5) is\nestablished with C(\u03b8\u2217) \u2261C > 0 independent of \u03b8\u2217. Indeed, |\u03b8\u2217|2 is always smaller than\n|\u03b8\u2217|1. For example, if all components of \u03b8\u2217take the same value and \u03b8\u2217is s-sparse, then\n|\u03b8\u2217|2 = |\u03b8\u2217|1/\u221as. In fact, the optimal rate of convergence in \u2113q-norm on the class of s-sparse\nvectors, as a function of s, p, n and the norms |\u03b8\u2217|r, remains unknown. When q = 2 and X is\nzero-mean Gaussian, a minimax lower bound including the factor |\u03b8\u2217|2 and not |\u03b8\u2217|1 is stated\nwithout proof in [5]. This, however, does not answer the question in general situation.\nThe aim of this paper is to provide answers to the above two questions. It is organized\nas follows. After giving some de\ufb01nitions and assumptions in Sections 2 and 3, we introduce\nin Section 4 a new estimator \u02c6\u03b8 which is based on second order cone programming and thus\ncan be computed in polynomial time.\nWe show that, under appropriate conditions, this\nestimator attains bounds of the form\n|\u02c6\u03b8 \u2212\u03b8\u2217|q \u2264Cs1/q\nr\nlog p\nn (|\u03b8\u2217|2 + 1),\n1 \u2264q \u2264\u221e,\n(7)\nwith probability close to 1, where the constant C does not depend on s, p, n and \u03b8\u2217. Contrary\nto the procedures of [4, 5] and [12], this new estimator does not require the knowledge of |\u03b8\u2217|1,\n|\u03b8\u2217|2 or s to be computed. We also do not need a lower bound condition such as (6) on the\ncomponents of the target vector \u03b8\u2217. Another di\ufb00erence from the mentioned papers is that our\nmain results do not focus on zero-mean subgaussian regression matrices X, but rather deal\nwith deterministic matrices X commonly appearing in applications. As an easy consequence,\nwe show that the results extend to random matrices X by using suitable deviation properties\nfor the quantity\nm2 = max\nj=1,...,p\n1\nn\nn\nX\ni=1\nX2\nij,\n3\nwhere the Xij are the entries of X, as well as checking a Restricted Eigenvalue type condition\non the matrix XT X/n. This extension is possible under the assumption that X is indepen-\ndent of \u03be and W. Note that under mild moment conditions, m2 can be suitably controlled\nwith high probability, see Section 5 of the Supplementary Material. Su\ufb03cient conditions for\nRestricted Eigenvalue type conditions are well known in the high-dimensional setting, see\n[2].\nFurthermore, in Section 5, we prove minimax lower bounds showing that no estimator\ncan achieve faster rate than that given in (7), up to a logarithmic in s factor, uniformly on\na class of s-sparse vectors. Finally, Section 6 provides simulation results that compare the\nconic estimator, the compensated MU selector and variants of the Dantzig selector. The\nresults are in accordance with the theoretical \ufb01ndings.\nWhile the conic programming estimator solves a tractable convex minimisation problem,\nthe compensated MU selector is in general a non-convex program. Section 1 in the Supple-\nmentary Material is devoted to address this issue. We show that under mild assumptions,\nthe compensated MU selector can be reduced to convex programming. In fact, when \u0398 = Rp\nor \u0398 is de\ufb01ned by linear constraints, it can even be written as a single linear programming\nproblem, which is of course a computational advantage compared to the estimator based\non conic programming. However, the rate of convergence of the compensated MU selector\nis suboptimal. The proofs are relegated to the appendices and some additional results and\ntechnical lemmas are given in the Supplementary Material.\n2\nAssumptions on the model\nIn this section, we introduce the assumptions that will be used below to study the statistical\nproperties of the estimators. Recall that for \u03b3 > 0, the random variable \u03b7 is said to be\nsubgaussian with variance parameter \u03b32 (or shortly \u03b3-subgaussian) if, for all t \u2208R,\nE[exp(t\u03b7)] \u2264exp(\u03b32t2/2).\nA random vector \u03b6 \u2208Rp is said to be subgaussian with variance parameter \u03b32 (or shortly\n\u03b3-subgaussian) if the inner products (\u03b6, v) are \u03b3-subgaussian for any v \u2208Rp with |v|2 = 1.\nWe shall consider the following assumptions.\n(A1) The matrix X is deterministic.\n(A2) The elements of the random vector \u03be are independent zero-mean subgaussian random\nvariables with variance parameter \u03c32.\n(A3) The rows wi, i = 1, . . . , n, of the noise matrix W are independent zero-mean sub-\ngaussian random vectors with variance parameter \u03c32\n\u2217, and E(WijWik) = 0 for all\n1 \u2264j < k \u2264p. Furthermore, W is independent of \u03be.\n(A4) There exist statistics \u02c6\u03c32\nj such that for any \u03b5 > 0, we have\nP\n\u0002\nmax\nj=1,...,p |\u02c6\u03c32\nj \u2212\u03c32\nj | \u2265b(\u03b5)\n\u0003\n\u2264\u03b5,\n(8)\nwhere b(\u03b5) = cb\nq\nlog(c\u2032\nbp/\u03b5)\nn\nfor some constants cb > 0 and c\u2032\nb > 0.\nAssumptions (A1) \u2013 (A3) are quite standard. Note that we do not assume independence of\nthe components of each wi. Examples of su\ufb03cient conditions for (A4) in the model with\nmissing data are provided in [17].\n4\n3\nSensitivities\nIt is well known, see for example [2], that the performance of Lasso or Dantzig selector type\nestimators in high-dimensional linear models is determined by speci\ufb01c characteristics of the\nGram matrix\n\u03a8 = 1\nnXT X,\nsuch as the Restricted Eigenvalue constants.\nWe shall need similar characteristics here.\nFollowing [8], we de\ufb01ne them in a more general form, so that the required property is a\nconsequence of the Restricted Eigenvalue property whenever the latter is satis\ufb01ed. For a\nvector \u03b8 in Rp, we denote by \u03b8J the vector in Rp that has the same coordinates as \u03b8 on the\nset of indices J \u2282{1, . . . , p} and zero coordinates on its complement Jc. We denote by |J|\nthe cardinality of J.\nFor any u > 0 and any subset J of {1, . . . , p}, consider the cone\nCJ(u) =\n\b\n\u2206\u2208Rp : |\u2206Jc|1 \u2264u|\u2206J|1\n\t\n.\nThe use of such cones to de\ufb01ne the Restricted Eigenvalue constants and other related charac-\nteristics of the Gram matrix is standard in the literature on the Lasso and Dantzig selector,\nstarting from [2].\nFor q \u2208[1, \u221e] and an integer s \u2208[1, p], the paper [8] de\ufb01nes the \u2113q-\nsensitivity as follows:\n\u03baq(s, u) =\nmin\nJ: |J|\u2264s\n\u0010\nmin\n\u2206\u2208CJ(u): |\u2206|q=1 |\u03a8\u2206|\u221e\n\u0011\n.\nIn [8, 9], it is shown that meaningful bounds for various types of errors in sparse linear\nregression can be obtained in terms of the sensitivities \u03baq(s, u). In particular, it is proved in\n[8] that the approach based on sensitivities is more general than that based on the Restricted\nEigenvalue or the Coherence condition. In particular, under those assumptions,\n\u03baq(s, u) \u2265cs\u22121/q,\nfor some constant c > 0, which implies the rate optimal bounds for the errors of Lasso and\nDantzig selector estimators as in [2]. For convenience, some properties of \u03baq(s, u) proved\nin [8] are summarized in Section 3 of the Supplementary Material.\nIn addition to \u03baq(s, u), we introduce a prediction sensitivity as follows:\n\u03bapr(s, u) = min\nJ:|J|\u2264s\n\u0010\nmin\n\u2206\u2208CJ(u):|\u03a81/2\u2206|2=1\n|\u03a8\u2206|\u221e\n\u0011\n.\nThe sensitivity \u03bapr(s, u) is useful to establish convergence in the prediction norm with fast\nrates, see (17) in Theorem 2 below. Such rates can be obtained under more general assump-\ntions than rates of convergence in \u2113q-norm. A discussion of the case of repeated regressors\nis given in [1]. The prediction sensitivity is closely related to the identi\ufb01ability factor de-\n\ufb01ned in [7]. Lemma 8 in Section 3 of the Supplementary Material shows that \u03bapr(s, u) > 0\nquite generally. Also, \u03bapr(s, u) \u2265\np\n\u03ba1(s, u), see Lemma 7 in Section 3 in the Supplementary\nMaterial.\n4\nEstimator based on conic programming\nIn this section, we introduce our conic programming based estimator \u02c6\u03b8.\nThis estimator\nis computationally feasible and we provide upper bounds on its estimation and prediction\nerrors. It will be shown in Section 5 that these bounds cannot be improved in a minimax\n5\nsense. In what follows, we \ufb01x a (small) value \u03b5 > 0. The probabilities, with which the bounds\non the estimation and prediction errors hold will be of the form 1 \u2212c\u03b5 for some c > 0.\nTo de\ufb01ne the estimator \u02c6\u03b8, we consider the following minimisation problem:\nminimise |\u03b8|1 + \u03bbt\nover (\u03b8, t) such that :\n(9)\n\u03b8 \u2208\u0398, t \u2208R+,\n\f\f 1\nnZT(y \u2212Z\u03b8) + bD\u03b8\n\f\f\n\u221e\u2264\u00b5t + \u03c4, |\u03b8|2 \u2264t.\nHere \u03bb, \u00b5 and \u03c4 are positive tuning constants, and \u0398 is a given subset of Rp characterising\nthe prior knowledge about \u03b8. In the results below, \u03bb, \u00b5 and \u03c4 are of the form\n\u03bb \u2208[1/2, 2],\n\u00b5 = C\nr\nlog(p/\u03b5)\nn\n,\n\u03c4 = C\nr\nlog(p/\u03b5)\nn\n,\nwhere we denote by C > 0 constants depending only on m2 and on the constants appearing\nin Assumptions (A1) \u2013 (A4). More speci\ufb01cally, in the theory we take,\n\u00b5 = \u03b4\u2032\n1(\u03b5) + \u03b4\u2032\n4(\u03b5) + \u03b45(\u03b5) + b(\u03b5),\n\u03c4 = \u03b42(\u03b5) + \u03b43(\u03b5)\n(10)\nwhere \u03b4i(\u03b5) and \u03b4\u2032\ni(\u03b5) are de\ufb01ned in Lemmas 1 and 2 in Appendix A.\nWhen \u0398 = Rp or \u0398 is a subset of Rp de\ufb01ned by linear constraints, (9) is a conic pro-\ngramming problem. Therefore it can be e\ufb03ciently solved in polynomial time.\nLet (\u02c6\u03b8, \u02c6t) be a solution of (9). We take \u02c6\u03b8 as estimator of \u03b8\u2217. Under Assumptions (A1) \u2013\n(A4), it follows that the feasible set of the minimisation problem (9) is not empty with high\nprobability if \u03b5 is small enough (see Lemma 3 in Appendix B).\nThe following theorem, proved in Appendix B, is our main result about the statistical\nproperties of the estimator \u02c6\u03b8 based on conic programming.\nTheorem 1. Assume (A1)\u2013(A4), and that the true parameter \u03b8\u2217is s-sparse and belongs to\n\u0398. Let \u03b5 > 0, 1 \u2264q \u2264\u221e, and set \u00b5 and \u03c4 as in (10). Assume also that\n\u03baq(s, 1 + \u03bb) \u2265cs\u22121/q,\n(11)\nfor some constant c > 0 and that\ns \u2264c1(\u03bb\u22121 + \u03bb)\u22121p\nn/ log(p/\u03b5),\n(12)\nfor some small enough constant c1 > 0. Then, with probability at least 1 \u22128\u03b5,\n|\u02c6\u03b8 \u2212\u03b8\u2217|q\n\u2264\nCs1/q\nr\nlog(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|2 + 1),\n(13)\nfor some constants C > 0 and c\u2032 > 0 (here and in the sequel we set s1/\u221e= 1).\nUnder the same assumptions, the prediction risk admits the following bound, with probability\nat least 1 \u22128\u03b5:\n1\nn\n\f\fX(\u02c6\u03b8 \u2212\u03b8\u2217)\n\f\f2\n2\n\u2264\nCslog(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|2 + 1)2 .\n(14)\nThe constants C > 0 and c\u2032 > 0 in (13) and (14) depend only on m2 and on the constants\nappearing in Assumptions (A1)\u2013(A4).\n6\nSome remarks are in order here.\nThe results in Theorem 1 highlight the impact of\n\u03bb and suggest that we should set \u03bb \u224d1.\nTheorem 1 is established under the condition\n\u03baq(s, 1 + \u03bb) \u2265cs\u22121/q, which holds under standard assumptions on the matrix X. For ex-\nample, it holds simultaneously for all q under the Coherence assumption, see (49) in the\nSupplementary Material. For 1 \u2264q \u22642 this condition follows from the Restricted Eigen-\nvalue (RE) assumption, see (47), (48) in the Supplementary Material. It is shown in [18]\nthat the RE assumption is satis\ufb01ed with high probability for a large class of random matri-\nces, including random matrices with zero-mean subgaussian rows and non-trivial covariance\nstructure, as well as matrices with zero-mean independent rows and uniformly bounded en-\ntries. This allows us to extend Theorem 1 to random matrices X as follows. Assume that the\nconditional distribution of (\u03be, W) given X is such that (A2)\u2013(A4) are satis\ufb01ed conditionally\non X for some \ufb01xed constants cb and c\u2032\nb and all X \u2208\u2126, where \u2126is a given set of n \u00d7 p\nmatrices. For example, in the model with missing data considered in the Introduction, this\nis the case if \u2126= {X : maxj=1,...,p 1\nn\nPn\ni=1 X4\nij \u2264m4} for some \ufb01nite constant m4, see [17]\nand Section 5 in the Supplementary Material. Fix positive constants \u03b5, c, \u03bb, m2 and denote\nby P the class of probability distributions PX on the set of n \u00d7 p matrices X such that\nPX\nh\n\u03baq(s, 1 + \u03bb) \u2265cs\u22121/q,\nmax\nj=1,...,p\n1\nn\nn\nX\ni=1\nX2\nij \u2264m2,\nX \u2208\u2126\ni\n\u22651 \u2212\u03b5.\n(15)\nCorollary 1. Let X be a random matrix with distribution PX \u2208P, and let the above\nassumptions on the conditional distribution of (\u03be, W) given X hold. Assume that the true\nparameter \u03b8\u2217\u2208\u0398 is s-sparse, (12) holds and set \u00b5 and \u03c4 as in (10). Then, (13) and (14)\nhold with probability at least 1 \u22129\u03b5.\nAlthough we do not pursue it here, Theorem 1 implies results on the correct selection of\nthe sparsity pattern via a thresholding procedure, in the same spirit as it is done in [13].\nImportantly, the bound (13) shows that the conic programming estimator is optimal in\na minimax sense. Indeed, we give in Section 5 lower bounds for estimation errors which are\nin agreement with the upper bounds in (13). The conic programming estimator \u02c6\u03b8 achieves\nthis rate with a computationally feasible procedure and does not need the knowledge of the\nparameters |\u03b8\u2217|1, |\u03b8\u2217|2 or s.\nInspection of the proof reveals that if Condition (12) does not hold, the conclusions of\nTheorem 1 remain valid provided |\u03b8\u2217|2 is replaced by |\u03b8\u2217|1 in the bounds, thus leading to\nresults analogous to those for the compensated MU selector. The next theorem formally\nstates that. Note that the assumptions are di\ufb00erent and somewhat weaker than in Theorem\n1.\nTheorem 2. Assume (A1)\u2013(A4), and that the true parameter \u03b8\u2217is s-sparse and belongs to\n\u0398. Let \u03b5 > 0, 1 \u2264q \u2264\u221e, and set \u00b5 and \u03c4 as in (10). Then, with probability at least 1 \u22128\u03b5,\n|\u02c6\u03b8 \u2212\u03b8\u2217|q\n\u2264\nC\n\u03baq(s, 1 + \u03bb)\nr\nlog(c\u2032p/\u03b5)\nn\n{(\u03bb + \u03bb\u22121)|\u03b8\u2217|1 + 1},\n(16)\nfor some constants C > 0 and c\u2032 > 0. Under the same assumptions, the prediction risk\nadmits the following bound, with probability at least 1 \u22128\u03b5:\n1\nn\n\f\fX(\u02c6\u03b8 \u2212\u03b8\u2217)\n\f\f2\n2\n\u2264\nC\n\u03ba2pr(s, 1 + \u03bb)\nlog(c\u2032p/\u03b5)\nn\n{(\u03bb + \u03bb\u22121)|\u03b8\u2217|1 + 1}2 .\n(17)\nFurthermore, under no assumption on X, with the same probability, we have the following\n\u201cslow rate\u201d bound:\n1\nn\n\f\fX(\u02c6\u03b8 \u2212\u03b8\u2217)\n\f\f2\n2\n\u2264\nC\nr\nlog(c\u2032p/\u03b5)\nn\n(\u03bb2 + \u03bb\u22121)(|\u03b8\u2217|2\n1 + |\u03b8\u2217|1).\n(18)\n7\nThe constants C > 0 and c\u2032 > 0 in (16)\u2013(18) depend only on m2 and on the constants\nappearing in Assumptions (A1)\u2013(A4).\nThere are three di\ufb00erent results in Theorem 2.\nThe bound (16) is based on the \u2113q-\nsensitivity measures without the sparsity condition (12) and recovers the rates of the com-\npensated MU selector. The second result (17) presents a prediction rate but the prediction\nsensitivity allows for more general designs. Finally, the last result in Theorem 2 provides a\nslow rate of convergence that requires no assumption on the design matrix.\n5\nMinimax lower bounds for arbitrary estimators\nIn this section, we show that the rates of convergence obtained in Theorem 1 are optimal\n(up to a logarithmic in s term) in a minimax sense for all estimators over the intersection of\nthe class of s-sparse vectors and the \u21132-sphere, respectively\nB0(s) = {\u03b8 : |\u03b8|0 \u2264s}\nand\nS2(R) = {\u03b8 : |\u03b8|2 = R},\nwhere R > 0. De\ufb01ning the parameter set as the intersection \u0398 = B0(s) \u2229S2(R) is motivated\nby the presence of both s and |\u03b8\u2217|2 in the upper bounds of Theorem 1. Note that considering\na deterministic X means that X is a nuisance parameter of the model. Thus, in the de\ufb01nition\nof the minimax risk, one should take the maximum not only over \u0398 but also over a class of\npossible matrices X. More generally, one can deal with random X and with the maximum\nover a class of distributions of X. We shall follow this approach with the class of distributions\nP introduced in Section 4. The result of Corollary 1 corresponding to (13) can be written as\nsup\nPX\u2208P\nsup\n\u03b8\u2208B0(s)\u2229S2(R)\nPX,\u03b8\nh\n|\u02c6\u03b8 \u2212\u03b8|q \u2265Cs1/q\nr\nlog(c\u2032p/\u03b5)\nn\n(R + 1)\ni\n\u22649\u03b5,\n(19)\nwhere, for \u03b8 \u2208Rp, we denote by PX,\u03b8 the probability measure of the pair (y, Z) satisfying\n(1). Our aim now is to prove the reverse inequality to (19) valid for all estimators. For this\npurpose, instead of the maximum over all PX \u2208P, it su\ufb03ces in principle to consider one\nparticular distribution PX. We choose it to be the distribution of Gaussian matrix X with\ni.i.d. rows and positive de\ufb01nite covariance matrix. This enables us, in addition, to obtain\nminimax optimality when restricting the class P to one such Gaussian distribution only.\nWith high probability, these matrices satisfy the RE condition, which implies the inequality\n\u03baq(s, 1 + \u03bb) \u2265cs\u22121/q under the probability in (15) (see for example [18] for details). Also,\nwe shall assume that \u03be and W are Gaussian with i.i.d. entries. In summary, we make the\nfollowing assumption.\n(A5) The elements of the triplet (\u03be, X, W) are jointly independent. The components of \u03be\nand W are i.i.d. Gaussian zero-mean random variables with positive variances \u03c32 and\n\u03c32\n\u2217respectively. The rows of X are i.i.d. Gaussian zero-mean random vectors with\ncovariance matrix \u03a3 > 0.\nDenote by \u03bb\u03a3\nmin and \u03bb\u03a3\nmax the smallest and largest eigenvalues of \u03a3. The next theorem,\nproved in Appendix C, provides the desired minimax lower bound.\nTheorem 3. Let p \u22652, 1 \u2264q \u2264\u221e, 2 \u2264s \u2264p, and R > 0. Let Assumption (A5) hold and\ns log(p/s)/n \u2264\u00afcR2/(R2 + 1) for some constant \u00afc > 0. Then there exist constants c > 0 and\nc\u2032 > 0, depending only on q, \u03c32, \u03c32\n\u2217, \u00afc, \u03bb\u03a3\nmin and \u03bb\u03a3\nmax, such that\ninf\n\u02c6T\nsup\n\u03b8\u2208B0(s)\u2229S2(R)\nPX,\u03b8\nh\n| \u02c6T \u2212\u03b8|q \u2265cs1/q\nr\nlog(p/s)\nn\n(R + 1)\ni\n> c\u2032,\n(20)\nwhere inf\n\u02c6T\ndenotes the in\ufb01mum over all estimators, and we set s1/\u221e= 1.\n8\n6\nMonte Carlo study\nIn this section, we brie\ufb02y illustrate the empirical performance of the estimators discussed\nabove. We consider the proposed conic programming estimator with \u03bb = 0.5, 0.75and 1\n(denoted as Conic (\u03bb)) and the Compensated MU selector (CompMU). To have benchmarks,\nwe also compute the (unfeasible) Dantzig selector which knows X (Dantzig X), and the\nDantzig selector that uses only Z (Dantzig Z), ignoring the errors-in-variables issue.\nThe simulation study uses the following data generating process\nyi = xT\ni \u03b8\u2217+ \u03bei,\nzi = xi + wi.\nHere, \u03bei, wi, xi are independent and \u03bei \u223cN(0, \u03c32), wi \u223cN(0, \u03c32\n\u2217Ip\u00d7p), xi \u223cN(0, \u03a3) where\nIp\u00d7p is the identity matrix and \u03a3 is a p \u00d7 p matrix with elements \u03a3ij = \u03c1|i\u2212j|.\nWe set\n\u03c3 = 0.128, \u03c3\u2217= 0.45, and \u03c1 = 0.25. For simplicity, we assume that \u03c3\u2217and \u03c3 are known\nand we set \u02c6D = D = \u03c32\n\u2217Ip\u00d7p. The penalty parameters are set as \u03c4 = \u00b5 = \u03c3\np\nlog(p/\u03b5)/n for\n\u03b5 = 0.05. We consider two choices for the vector of unknown parameters \u03b8\u2217. The \ufb01rst choice is\n\u03b8\u2217= (1, 1, 1, 1, 1, 0, . . . , 0)T , which captures the case where the coe\ufb03cients are well separated\nfrom zero. The second choice is \u03b8\u2217= (1, 1/2, 1/3, 1/4, 1/5, 0, . . . , 0)T , which represents the\nsituation where \u03b8\u2217is sparse with components that are not necessarily well separated from\nzero. The results are based on 100 replications. The implementation of these estimators\nwas based on interior point methods which might not be suitable for large instances. The\naverage running times of the six estimators were within a factor of two. For p = 10, 50 the\naverage running time of all six estimators was below one second, for p = 100 all estimators\nwere below 3 seconds and for p = 500 the average running times were between 40 and 100\nseconds. We note that Conic(1) and CompMU had very comparable running times, both\nDantzigX and DantzigZ had faster running times than the other estimators, while Conic(0.5)\nand Conic(0.75) requires slightly more time to be computed.\nFirst \u03b8\u2217\nn = 300 and p = 10\nn = 300 and p = 50\nMethod\nBias\nRMSE\nPR\nBias\nRMSE\nPR\nConic (0.5)\n0.0838151\n0.1846383\n0.1710643\n0.0955776\n0.2245046\n0.2170111\nConic (0.75)\n0.0838151\n0.1846383\n0.1710643\n0.0953689\n0.2250219\n0.2176691\nConic (1)\n0.0838151\n0.1846383\n0.1710643\n0.0956858\n0.2253614\n0.2180705\nCompMU\n0.1566904\n0.2191588\n0.2225818\n0.1840462\n0.2362394\n0.2507162\nDantzig X\n0.0265486\n0.0321528\n0.0349530\n0.0301636\n0.0349420\n0.0386731\nDantzig Z\n0.2952845\n0.3300527\n0.3645317\n0.3078976\n0.4166192\n0.4174840\nFirst \u03b8\u2217\nn = 300 and p = 100\nn = 300 and p = 500\nMethod\nBias\nRMSE\nPR\nBias\nRMSE\nPR\nConic (0.5)\n0.1101178\n0.2556778\n0.2474407\n0.1668239\n0.2656095\n0.263529\nConic (0.75)\n0.0943678\n0.2711839\n0.2606997\n0.1425789\n0.2846916\n0.2789745\nConic (1)\n0.0942906\n0.2734750\n0.2631424\n0.1276741\n0.3121221\n0.3093194\nCompMU\n0.1910509\n0.2539411\n0.2658907\n0.2052520\n0.2657204\n0.2772154\nDantzig X\n0.0317776\n0.0366155\n0.0403419\n0.0352309\n0.0403134\n0.0448000\nDantzig Z\n0.3081669\n0.4994041\n0.4652972\n0.3536668\n0.6865989\n0.5921541\nTable 1: Simulation results for the \ufb01rst choice of \u03b8\u2217. For each estimator we provide average bias (Bias),\naverage root-mean squared error (RMSE), and average prediction risk (PR).\nTable 1 reports the simulation results in the case \u03b8\u2217= (1, 1, 1, 1, 1, 0, . . . , 0)T . As ex-\npected, the performance of all the estimators deteriorates as p grows but only slightly. Also,\nthe (unfeasible) estimator based on Dantzig selector that observes X outperforms all feasible\n9\noptions. The estimator that ignores the errors-in-variables issue appears with a higher bias\nleading to the worse result in terms of root mean square error and empirical risk. The per-\nformance of the feasible estimators discussed in this paper is between these two benchmarks.\nThe three conic estimators exhibit a better result than the compensated MU selector when\np = 10, 50. For the larger dimensions p = 100, 500, their performance becomes similar to\nthat of the compensated MU selector.\nNonetheless, the conic estimator with \u03bb = 0.5 is\nslightly better than all the other feasible estimators. We also note that for the small dimen-\nsion p = 10, all three conic estimators give the same results. The reason is that the conic\nconstraint was not active in the simulations for p = 10 so that the estimators were the same\nfor the range of \u03bb under consideration. This was not the case for p = 50, 100, 500. These\n\ufb01ndings are very much aligned with the theoretical properties of each estimator and sustain\nthat the impact of errors-in-variables can be substantial.\nTable 2 reports the results for the second choice of \u03b8\u2217, where the coe\ufb03cients are not\nwell separated from zero. They are qualitatively the same as before. The results con\ufb01rms\nthe robustness of the conclusions with respect to possible model selection errors which are\nunavoidable when the coe\ufb03cients are not well separated from zero.\nSecond \u03b8\u2217\nn = 300 and p = 10\nn = 300 and p = 50\nMethod\nBias\nRMSE\nPR\nBias\nRMSE\nPR\nConic (0.5)\n0.0564816\n0.1020763\n0.0987642\n0.0684162\n0.1236275\n0.1223037\nConic (0.75)\n0.0564816\n0.1020763\n0.0987642\n0.0682720\n0.1229000\n0.1219398\nConic (1)\n0.0564816\n0.1020763\n0.0987642\n0.0683291\n0.1227749\n0.1218898\nCompMU\n0.0839431\n0.1171633\n0.1204765\n0.1007774\n0.1318303\n0.1396494\nDantzig X\n0.0265486\n0.0321528\n0.0349530\n0.0301636\n0.0349420\n0.0386731\nDantzig Z\n0.1885828\n0.2024266\n0.2138763\n0.1949159\n0.2314964\n0.2319208\nSecond \u03b8\u2217\nn = 300 and p = 100\nn = 300 and p = 500\nMethod\nBias\nRMSE\nPR\nBias\nRMSE\nPR\nConic (0.5)\n0.0714621\n0.1374637\n0.1349551\n0.0945558\n0.1472914\n0.1477198\nConic (0.75)\n0.0713670\n0.1378301\n0.1353203\n0.0824510\n0.1589884\n0.1565416\nConic (1)\n0.0716242\n0.1381810\n0.1357000\n0.0783823\n0.1682841\n0.1658849\nCompMU\n0.1063728\n0.1405472\n0.1479579\n0.1131005\n0.1477336\n0.1545960\nDantzig X\n0.0317776\n0.0366155\n0.0403419\n0.0352309\n0.0403134\n0.0448000\nDantzig Z\n0.1978958\n0.2536633\n0.2432222\n0.2152972\n0.3145349\n0.2766815\nTable 2: Simulation results for the second choice of \u03b8\u2217. For each estimator we provide average bias (Bias),\naverage root-mean squared error (RMSE), and average prediction risk (PR).\nAcknowledgement We would like to thank Anatoli Juditsky for helpful remarks. This\nwork is supported by GENES and by the French National Research Agency (ANR) as part\nof Idex Grant ANR -11- IDEX-0003-02, Labex ECODEC (ANR - 11-LABEX-0047), and\nIPANEMA grant (ANR-13-BSH1-0004-02), and by the \u201cChaire Economie et Gestion des\nNouvelles Donn\u00b4ees\u201d, under the auspices of Institut Louis Bachelier, Havas-Media and Paris-\nDauphine.\nAppendix A. Bounds on stochastic error terms\nIn this appendix, we give upper bounds on the stochastic error terms appearing in the main\nresults. In what follows, D is the diagonal matrix with diagonal elements \u03c32\nj , j = 1, . . . , p,\nand for a square matrix A, we denote by Diag{A} the matrix with the same dimensions as A,\n10\nthe same diagonal elements as A and all o\ufb00-diagonal elements equal to zero. The following\nlemma is proved in [17].\nLemma 1. Let 0 < \u03b5 < 1 and assume (A1)-(A3). Then, with probability at least 1 \u2212\u03b5,\n\f\f 1\nnXT W\n\f\f\n\u221e\u2264\u03b41(\u03b5),\n\f\f 1\nnXT \u03be\n\f\f\n\u221e\u2264\u03b42(\u03b5),\n\f\f 1\nnW T\u03be\n\f\f\n\u221e\u2264\u03b43(\u03b5),\n\f\f 1\nn(W T W \u2212Diag{W T W})\n\f\f\n\u221e\u2264\u03b44(\u03b5),\n\f\f 1\nnDiag{W T W} \u2212D\n\f\f\n\u221e\u2264\u03b45(\u03b5),\nwhere \u03b41(\u03b5) = e\u03b4(\u03b5, \u03c3\u2217, 2p2), \u03b42(\u03b5) = e\u03b4(\u03b5, \u03c3, 2p), \u03b43(\u03b5) = \u03b45(\u03b5) = \u00af\u03b4(\u03b5, 2p), \u03b44(\u03b5) = \u00af\u03b4(\u03b5, p(p \u22121))\nand for an integer N,\ne\u03b4(\u03b5, a, N) = a\nr\n2m2 log(N/\u03b5)\nn\n,\n\u00af\u03b4(\u03b5, N) = max\n \n\u03b30\nr\n2 log(N/\u03b5)\nn\n, 2 log(N/\u03b5)\nt0n\n!\n,\nwith \u03b30, t0 are positive constants depending only on \u03c3 and \u03c3\u2217.\nWe now give the second lemma.\nLemma 2. Let 0 < \u03b5 < 1, \u03b8\u2217\u2208Rp and assume (A1)-(A3). Then, with probability at least\n1 \u2212\u03b5,\n\f\f 1\nnXT W\u03b8\u2217\f\f\n\u221e\u2264\u03b4\u2032\n1(\u03b5)|\u03b8\u2217|2,\n(21)\nwhere \u03b4\u2032\n1(\u03b5) = \u03c3\u2217\nq\n2m2 log(2p/\u03b5)\nn\n. In addition, with probability at least 1 \u2212\u03b5,\n\f\f 1\nn(W T W \u2212Diag{W T W})\u03b8\u2217\f\f\n\u221e\u2264\u03b4\u2032\n4(\u03b5)|\u03b8\u2217|2,\n(22)\nwhere\n\u03b4\u2032\n4(\u03b5) = max\n \n\u03b32\nr\n2 log(2p/\u03b5)\nn\n, 2 log(2p/\u03b5)\nt2n\n!\n,\nand \u03b32, t2 are positive constants depending only on \u03c3\u2217.\nProof. If \u03b8\u2217= 0, the result is obvious. So we assume that \u03b8\u2217\u0338= 0. Let v = \u03b8\u2217/|\u03b8\u2217|2. We can\nwrite\n\f\f 1\nnXT W\u03b8\u2217\f\f\n\u221e= |\u03b8\u2217|2 max\nj=1,...,p\n\f\f\f\f\f\n1\nn\nn\nX\ni=1\nXij(wi, v)\n\f\f\f\f\f ,\n(23)\nwhere (wi, v) = Pp\nk=1 Wikvk and we denote by Wik and vk the elements of the matrix W and\nthe vector v respectively. By Assumption (A3), the random variable (wi, v) is subgaussian\nwith variance parameter \u03c32\n\u2217. Using this together with the independence of the wi for di\ufb00erent\ni, we get that for all t \u2208R,\nE\nh\nexp\n\u0010 t\nn\nn\nX\ni=1\nXij(wi, v)\n\u0011i\n=\nn\nY\ni=1\nE\nh\nexp\n\u0010 t\nnXij(wi, v)\n\u0011i\n\u2264\nn\nY\ni=1\nexp\n\u0010\u03c32\n\u2217t2X2\nij\n2n2\n\u0011\n\u2264exp\n\u0010\u03c32\n\u2217m2t2\n2n\n\u0011\n.\nThus, the random variable\n\u03b7j = 1\nn\nn\nX\ni=1\nXij(wi, v)\nis \u03b31-subgaussian with \u03b31 = \u03c3\u2217\np\nm2/n. This implies the classical tail bound\nP[|\u03b7j| \u2265\u03b4] \u22642 exp\n\u0000\u2212\u03b42/(2\u03b32\n1)\n\u0001\n,\n11\nfor any \u03b4 > 0. This together with (23) and the union bound yields (21).\nTo prove (22), we shall use the following fact (see for example Lemma 5.14 in [22]): If \u03b7 is\na subgaussian random variable with variance parameter \u03b32, then \u03b72 is sub-exponential, that\nis there exist constants \u03b30 = \u03b30(\u03b3) and t0 = t0(\u03b3) such that\nE[exp(t\u03b72)] \u2264exp(\u03b32\n0t2/2),\n|t| \u2264t0.\n(24)\nAnalogously to (23), we obtain\n\f\f 1\nn(W T W \u2212Diag{W T W})\u03b8\u2217\f\f\n\u221e= |\u03b8\u2217|2 max\nj=1,...,p |\u03b7\u2032\nj|,\n(25)\nwhere\n\u03b7\u2032\nj = 1\nn\nn\nX\ni=1\nWij\np\nX\nk=1,k\u0338=j\nWikvk.\nNow, for all t \u2208R, we have\nE[exp(t\u03b7\u2032\nj)] =\nn\nY\ni=1\nE\nh\nexp\n\u0010tWij\nn\np\nX\nk=1,k\u0338=j\nWikvk\n\u0011i\n\u2264\nn\nY\ni=1\nE\nh\nexp\nn t\n2n\n\u0010\nW 2\nij +\n\u0000p\nX\nk=1,k\u0338=j\nWikvk\n\u00012\u0011oi\n.\nThen, using Cauchy-Schwarz inequality, we get\nE[exp(t\u03b7\u2032\nj)] \u2264\nn\nY\ni=1\nn\nE\nh\nexp\n\u0010tW 2\nij\nn\n\u0011i\nE\nh\nexp\n\u0010 t\nn\n\u0000p\nX\nk=1,k\u0338=j\nWikvk\n\u00012\u0011io1/2\n.\nRecall that Assumption (A3) implies that both Wij and Pp\nk=1,k\u0338=j Wikvk are \u03c3\u2217-subgaussian.\nConsequently, in view of (24), their squared values are (\u03b30(\u03c3\u2217), t0(\u03c3\u2217))-sub-exponential, which\nyields\nE[exp(t\u03b7\u2032\nj)] \u2264\nn\nY\ni=1\nexp\n\u0010\u03b30(\u03c3\u2217)2\n2\n\u0010 t\nn\n\u00112\u0011\n= exp\n\u0010\u03b30(\u03c3\u2217)2t2\n2n\n\u0011\n,\n|t| \u2264t0(\u03c3\u2217)n.\nSet \u03b32 = \u03b30(\u03c3\u2217) and t2 = t0(\u03c3\u2217). The last display states that \u03b7\u2032\nj is (\u03b32/\u221an, t2n)-sub-\nexponential. This implies the tail bound\nP(|\u03b7\u2032\nj| \u2265\u03b4) \u22642 max\n\u0000exp(\u2212n\u03b42/(2\u03b32\n2)), exp(\u2212\u03b4t2n/2)\n\u0001\n,\nfor any \u03b4 > 0. This together with (25) and the union bound yields (22). \u25a1\nAppendix B. Proofs of Theorem 1 and Theorem 2\nSet for brevity \u03b4i = \u03b4i(\u03b5), \u03b4\u2032\ni = \u03b4\u2032\ni(\u03b5), b = b(\u03b5). We \ufb01rst prove some preliminary lemmas.\nLemma 3. Assume (A1)\u2013(A4).\nThen with probability at least 1 \u22126\u03b5, the pair (\u03b8, t) =\n(\u03b8\u2217, |\u03b8\u2217|2) belongs to the feasible set of the minimisation problem (9).\nProof. First, note that ZT(y \u2212Z\u03b8\u2217) + n bD\u03b8\u2217is equal to\n\u2212XT W\u03b8\u2217+ XT \u03be + W T \u03be \u2212(W T W \u2212Diag{W T W})\u03b8\u2217\n\u2212(Diag{W T W} \u2212nD)\u03b8\u2217+ n( bD \u2212D)\u03b8\u2217.\n12\nBy de\ufb01nition of \u03b4i and b, with probability at least 1 \u22124\u03b5, we have\n| 1\nnXT \u03be|\u221e+ | 1\nnW T \u03be|\u221e\u2264\u03b42 + \u03b43\n(26)\n|( 1\nnDiag{W T W} \u2212D)\u03b8\u2217|\u221e\u2264| 1\nnDiag{W T W} \u2212D|\u221e|\u03b8\u2217|\u221e\u2264\u03b45|\u03b8\u2217|2\n(27)\n|( bD \u2212D)\u03b8\u2217|\u221e\u2264b|\u03b8\u2217|\u221e\u2264b|\u03b8\u2217|2,\n(28)\nwhere in (27) and (28) we have used that the considered matrices are diagonal. Also, by\nLemma 2, with probability at least 1 \u22122\u03b5, we have\n| 1\nnXT W\u03b8\u2217|\u221e\u2264\u03b4\u2032\n1|\u03b8\u2217|2\n(29)\n| 1\nn(W T W \u2212Diag{W T W})\u03b8\u2217|\u221e\u2264\u03b4\u2032\n4|\u03b8\u2217|2.\n(30)\nCombining the decomposition of ZT(y \u2212Z\u03b8\u2217) + n bD\u03b8\u2217together with (26)-(30), we \ufb01nd that\n\f\f 1\nnZT (y \u2212Z\u03b8\u2217) + bD\u03b8\u2217\f\f\n\u221e\u2264\u00b5|\u03b8\u2217|2 + \u03c4,\nwith probability at least 1 \u22126\u03b5, which implies the lemma. \u25a1\nWe now give two lemmas which will be crucial in the proof of our main theorem on the\naccuracy of the conic programming based estimator (Theorem 1).\nLemma 4. Assume (A1)\u2013(A4). Let J = {j : \u03b8\u2217\nj \u0338= 0}. Then with probability at least 1 \u22126\u03b5\n(on the same event as in Lemma 3), we have\n|(\u02c6\u03b8 \u2212\u03b8\u2217)Jc|1 \u2264(1 + \u03bb)|(\u02c6\u03b8 \u2212\u03b8\u2217)J|1,\n(31)\n\u02c6t \u2264(1/\u03bb)|\u02c6\u03b8 \u2212\u03b8\u2217|1 + |\u03b8\u2217|2.\n(32)\nProof. Set \u2206= \u02c6\u03b8 \u2212\u03b8\u2217. On the event of Lemma 3, (\u03b8\u2217, |\u03b8\u2217|2) belongs to the feasible set of\nthe minimisation problem (9). Consequently,\n|\u02c6\u03b8|1 + \u03bb|\u02c6\u03b8|2 \u2264|\u02c6\u03b8|1 + \u03bb\u02c6t \u2264|\u03b8\u2217|1 + \u03bb|\u03b8\u2217|2.\n(33)\nThis implies |\u2206Jc|1 \u2264|\u2206J|1 + \u03bb(|\u03b8\u2217|2 \u2212|\u02c6\u03b8|2) \u2264|\u2206J|1 + \u03bb|\u2206J|2 \u2264(1 + \u03bb)|\u2206J|1, and (31)\nfollows. To prove (32), it su\ufb03ces to note that (33) implies\n\u03bb\u02c6t \u2264|\u03b8\u2217|1 \u2212|\u02c6\u03b8|1 + \u03bb|\u03b8\u2217|2 \u2264|\u02c6\u03b8 \u2212\u03b8\u2217|1 + \u03bb|\u03b8\u2217|2.\n\u25a1\nLemma 5. Assume (A1)\u2013(A4). Then, on a subset of the event of Lemma 3 having probability\nat least 1 \u22128\u03b5, we have\n\f\f 1\nnXT X(\u02c6\u03b8 \u2212\u03b8\u2217)\n\f\f\n\u221e\u2264\u00b51|\u03b8\u2217|2 + \u00b52|\u02c6\u03b8 \u2212\u03b8\u2217|1 + \u03c41,\n(34)\nwhere \u00b51 = \u00b5 + b + \u03b4\u2032\n1 + \u03b4\u2032\n4 + \u03b45, \u00b52 = \u00b5/\u03bb + b + 2\u03b41 + \u03b44 + \u03b45 and \u03c41 = \u03c4 + \u03b42 + \u03b43.\nProof. Throughout the proof, we assume that we are on the event of probability at least\n1 \u22126\u03b5 where Inequalities (26) \u2013 (30) hold and (\u03b8\u2217, |\u03b8\u2217|2) belongs to the feasible set of the\nminimisation problem (9). We have that | 1\nnXT X\u2206|\u221eis smaller than\n| 1\nnZT (Z \u02c6\u03b8 \u2212y) \u2212bD\u02c6\u03b8|\u221e+ |( 1\nnZTW \u2212D)\u02c6\u03b8|\u221e+ |( bD \u2212D)\u02c6\u03b8|\u221e+ | 1\nnZT \u03be|\u221e+ | 1\nnW TX\u2206|\u221e.\n13\nUsing the fact that (\u02c6\u03b8, \u02c6t) belongs to the feasible set of the minimisation problem (9) together\nwith (32), we obtain\n| 1\nnZT(Z \u02c6\u03b8 \u2212y) \u2212bD\u02c6\u03b8|\u221e\u2264\u00b5\u02c6t + \u03c4 \u2264(\u00b5/\u03bb)|\u02c6\u03b8 \u2212\u03b8\u2217|1 + \u00b5|\u03b8\u2217|2 + \u03c4.\nTherefore, | 1\nnXT X\u2206|\u221edoes not exceed\n(\u00b5/\u03bb)|\u02c6\u03b8 \u2212\u03b8\u2217|1 + \u00b5|\u03b8\u2217|2 + \u03c41 + |( 1\nnZT W \u2212D)\u02c6\u03b8|\u221e+ |( bD \u2212D)\u02c6\u03b8|\u221e+ | 1\nnW TX\u2206|\u221e.\nWe now bound the last expression using the fact that \u02c6\u03b8 = \u03b8\u2217+ \u2206together with Assumption\n(A4) and (28). This gives\n| 1\nnXT X\u2206|\u221e\u2264((\u00b5/\u03bb) + b)|\u2206|1 + (\u00b5 + b)|\u03b8\u2217|2 + \u03c41 + |( 1\nnZTW \u2212D)\u03b8\u2217|\u221e\n(35)\n+ |( 1\nnZT W \u2212D)\u2206|\u221e+ | 1\nnW T X\u2206|\u221e.\nRemark that\n|( 1\nnZT W \u2212D)\u2206|\u221e\u2264| 1\nnZTW \u2212D|\u221e|\u2206|1\n\u2264\n\u0000| 1\nn(W T W \u2212Diag{W T W})|\u221e+ | 1\nnDiag{W T W} \u2212D|\u221e+ | 1\nnXT W|\u221e\n\u0001\n|\u2206|1.\nTherefore,\n|( 1\nnZTW \u2212D)\u2206|\u221e\u2264(\u03b41 + \u03b44 + \u03b45)|\u2206|1,\n(36)\nwith probability at least 1 \u22128\u03b5 (since we intersect the initial event of probability at least\n1 \u22126\u03b5 with the event of probability at least 1 \u22122\u03b5 where the bounds \u03b41 and \u03b44 hold for the\ncorresponding terms). Next, on the same event of probability at least 1 \u22128\u03b5,\n| 1\nnW T X\u2206|\u221e\u2264| 1\nnXT W|\u221e|\u2206|1 \u2264\u03b41|\u2206|1.\n(37)\nFinally, in view of Lemma 2 and (27), on the initial event of probability at least 1 \u22126\u03b5,\n|( 1\nnZTW \u2212D)\u03b8\u2217|\u221e\n\u2264| 1\nn(W T W \u2212Diag{W T W})\u03b8\u2217|\u221e+ |( 1\nnDiag{W T W} \u2212D)\u03b8\u2217|\u221e+ | 1\nnXT W\u03b8\u2217|\u221e\n\u2264(\u03b4\u2032\n1 + \u03b4\u2032\n4 + \u03b45)|\u03b8\u2217|2.\n(38)\nTo complete the proof, it su\ufb03ces to plug (36) \u2013 (38) in (35) and to set \u00b51 = \u00b5+b+\u03b4\u2032\n1+\u03b4\u2032\n4+\u03b45\nand \u00b52 = \u00b5/\u03bb + b + 2\u03b41 + \u03b44 + \u03b45. \u25a1\nWe are ready to give the proof of Theorem 1. Throughout the proof, we assume that we\nare on the event of probability at least 1 \u22128\u03b5 of Lemma 5 where the results of Lemmas 3, 4\nand 5 hold. Property (31) in Lemma 4 implies that \u2206is in the cone CJ(1 + \u03bb). Therefore,\nby de\ufb01nition of \u2113q-sensitivity and Lemma 5, we have\n\u03baq(s, 1 + \u03bb)|\u2206|q \u2264\n\f\f 1\nnXT X\u2206\n\f\f\n\u221e\u2264\u00b51|\u03b8\u2217|2 + \u00b52|\u2206|1 + \u03c41.\nFurthermore, using again (31), we have\n|\u2206|1\n=\n|\u2206Jc|1 + |\u2206J|1 \u2264(2 + \u03bb)|\u2206J|1\n\u2264\n(2 + \u03bb)s1\u22121/q|\u2206J|q \u2264(2 + \u03bb)s1\u22121/q|\u2206|q.\nIt follows that\n(\u03baq(s, 1 + \u03bb) \u2212(2 + \u03bb)\u00b52s1\u22121/q)|\u2206|q \u2264\u00b51|\u03b8\u2217|2 + \u03c41.\n14\nFurther, the assumption s \u2264c1(\u03bb\u22121 + \u03bb)\u22121p\nn/ log(p/\u03b5) implies\n{c \u2212\u00b52c1(2 + \u03bb)(\u03bb\u22121 + \u03bb)\u22121p\nn/ log(p/\u03b5)}s\u22121/q|\u2206|q \u2264\u00b51|\u03b8\u2217|2 + \u03c41.\nRecall that \u00b52 \u2264(1 + \u03bb\u22121)a\np\nlog(p/\u03b5)/n, where a > 0 is a constant. Therefore, (13) follows\nif (2+ \u03bb)(1+ \u03bb\u22121)(\u03bb\u22121 + \u03bb)\u22121c1a < c/2. Since (2+ \u03bb)(1+ \u03bb\u22121)(\u03bb\u22121 + \u03bb)\u22121 \u22645 we have that\nc1 < c/(10a) yields (13). To prove (14), write \ufb01rst\n1\nn |X\u2206|2\n2 \u22641\nn\n\f\fXT X\u2206\n\f\f\n\u221e|\u2206|1.\nNext remark that from (13), we have\n|\u2206|1 \u2264Cs\nr\nlog(c\u2032p/\u03b5)\nn\n(|\u03b8\u2217|2 + 1)\n(39)\nand recall that from Lemma 5,\n\f\f 1\nnXT X\u2206\n\f\f\n\u221e\u2264C\nr\nlog(c\u2032p/\u03b5)\nn\n(1 + |\u03b8\u2217|2 + |\u2206|1).\n(40)\nFurthermore, from (39) and (12) we also have |\u2206|1 \u2264C(|\u03b8\u2217|2 + 1), for some constant C > 0.\nThen (14) is easily deduced. This ends the proof of Theorem 1.\nWe now give the proof of Theorem 2. We place ourselves in the same framework as in\nthe proof of Theorem 1. By the de\ufb01nition of the estimator, |\u2206|1 \u2264|\u02c6\u03b8|1 + |\u03b8\u2217|1 \u2264(|\u03b8\u2217|1 +\n\u03bb|\u03b8\u2217|2) + |\u03b8\u2217|1 \u2264(2+ \u03bb)|\u03b8\u2217|1, where we have used that |\u03b8\u2217|2 \u2264|\u03b8\u2217|1. This and Lemma 5 yield\n\f\f 1\nnXT X\u2206\n\f\f\n\u221e\u2264\u00b51|\u03b8\u2217|2 + \u00b52|\u2206|1 + \u03c41 \u2264(\u00b51 + (2 + \u03bb)\u00b52)|\u03b8\u2217|1 + \u03c41.\nTherefore, arguing as in the proof of Theorem 1, we \ufb01nd\n\u03baq(s, 1 + \u03bb)|\u2206|q \u2264(\u00b51 + (2 + \u03bb)\u00b52)|\u03b8\u2217|1 + \u03c41,\nwhich implies (16) since \u00b52 \u2264(1 + \u03bb\u22121)a\np\nlog(p/\u03b5)/n for some constant a > 0. To prove\n(17), we note that by de\ufb01nition of \u03bapr and the fact that \u2206\u2208CJ(1 + \u03bb),\n\u03ba2\npr(s, 1 + \u03bb)\nn\n|X\u2206|2\n2 \u22641\nn\n\f\fXT X\u2206\n\f\f2\n\u221e\u2264{(\u00b51 + (2 + \u03bb)\u00b52)|\u03b8\u2217|1 + \u03c41}2 .\nFinally, since |\u2206|1 \u2264(2 + \u03bb)|\u03b8\u2217|1 and \u00b52 \u2264(1 + \u03bb\u22121)a\np\nlog(p/\u03b5)/n, (18) follows from\n1\nn |X\u2206|2\n2 \u22641\nn\n\f\fXT X\u2206\n\f\f\n\u221e|\u2206|1 \u2264{(\u00b51 + (2 + \u03bb)\u00b52)|\u03b8\u2217|1 + \u03c41}(2 + \u03bb)|\u03b8\u2217|1.\nThis concludes the proof of Theorem 2.\nAppendix C. Proof of Theorem 3\nAgain, throughout, we denote by c a positive constant which may vary from line to line.\nTo derive the lower bounds, we apply Theorem 2.7 in [21]. Thus, we de\ufb01ne a \ufb01nite set of\n\u201chypotheses\u201d included in B0(s) \u2229S2(R). To this end, we \ufb01rst introduce\nM =\n\b\nx \u2208{0, 1}p\u22121 : \u03c1H(0, x) = s \u22121\n\t\n,\n15\nwhere \u03c1H denotes the Hamming distance between elements of {0, 1}p\u22121, and 0 is the zero\nvector. Then, there exists a subset M\u2032 of M such that for any x, x\u2032 in M\u2032 with x \u0338= x\u2032, we\nhave \u03c1H(x, x\u2032) > s/16, and moreover,\nlog|M\u2032| \u2265c\u2032\n1s log\n\u0010p\ns\n\u0011\nfor some absolute constant c\u2032\n1 > 0. Indeed, this follows from the Varshamov-Gilbert bound\n(see Lemma 2.9 in [21]) if s \u22121 > (p \u22121)/2 and from Lemma A.3 in [15] if s \u22121 \u2264(p \u22121)/2.\nWe denote by \u03c9\u2032\nj the elements of the \ufb01nite set M\u2032. For j = 1, . . . , |M\u2032|, we de\ufb01ne vectors\n\u03c9j \u2208{0, 1}p with components \u03c9j1 = 0 and \u03c9jk = \u03c9\u2032\nj(k\u22121) for k \u22652, where \u03c9jk is the k-th\ncomponent of \u03c9j. We also de\ufb01ne \u03c90 as the vector in {0, 1}p with all components equal to 0\nexcept the \ufb01rst one equal to 1.\nWe now de\ufb01ne the set of \u201chypotheses\u201d (\u00af\u03c9j, j = 0, . . . , |M\u2032| + 1), where \u00af\u03c90 = R\u03c90, and\n\u00af\u03c9j =\nR\np\n1 + \u03b32(s \u22121)\n(\u03c90 + \u03b3\u03c9j),\nj = 1, . . . , |M\u2032| + 1.\nHere, \u03b3 is a positive parameter to be de\ufb01ned. Note that the sparsity of \u00af\u03c9j is equal to s and\nthat |\u00af\u03c9j|2 = R. Thus all \u00af\u03c9j belong to B0(s) \u2229S2(R). Let\ne\u03a3 = \u03c32\n\u2217(\u03a3 + \u03c32\n\u2217Ip\u00d7p)\u22121 and \u0393 = Ip\u00d7p \u2212e\u03a3.\nFor \u03b8 \u2208Rp, we set c\u03b8 = \u03b8T\u0393\u03b8 and we write K(P, Q) for the Kullback-Leibler divergence\nbetween two probability measures P and Q. For j \u22651, by Lemma 9 in the Supplementary\nMaterial, we have\nK(P\u00af\u03c9j, P\u00af\u03c90) \u2264\ncn\n1 + R2\n\u0010\nR2\u0010p\n1 + \u03b32(s \u22121) \u22121\np\n1 + \u03b32(s \u22121)\n\u00112\n+\nR2\n1 + \u03b32(s \u22121)\u03b32s + |c\u00af\u03c9j \u2212c\u00af\u03c90|\n\u0011\n\u2264\ncn\n1 + R2\n\u0010\n\u03b32R2s\n1 + \u03b32(s \u22121) + |c\u00af\u03c9j \u2212c\u00af\u03c90|\n\u0011\n.\nNow,\n|c\u00af\u03c9j \u2212c\u00af\u03c90| = |\u00af\u03c9T\nj \u0393\u00af\u03c9j \u2212\u00af\u03c9T\n0 \u0393\u00af\u03c90| =\n\f\f\f\fR2\u03c90\nT \u0393\u03c90 \u2212\nR2\n1 + \u03b32(s \u22121)\u03c90T \u0393\u03c90 \u2212\n\u03b32R2\n1 + \u03b32(s \u22121)\u03c9jT \u0393\u03c9j\n\f\f\f\f\n\u2264\n\u03b32R2\n1 + \u03b32(s \u22121)\n\u0000(s \u22121)\u03c9T\n0 \u0393\u03c90 + \u03c9jT \u0393\u03c9j\n\u0001\n\u22642\u03bbmax\u03b32R2(s \u22121)\n1 + \u03b32(s \u22121)\n,\nwhere \u03bbmax denotes the largest eigenvalue of \u0393 and the last inequality is due to the fact that\n|\u03c90|2 = 1, |\u03c9j|2\n2 = s \u22121. Combining the last two displays yields\nK(P\u00af\u03c9j, P\u00af\u03c90) \u2264c\u2032\n2n\u03b32s\nR2\n1 + R2 ,\nwhere c\u2032\n2 > 0 is a constant that does not depend on s, p, R, and n. Now, taking\n\u03b3 =\n\u0010\nc\u2032\n1\n16c\u2032\n2n\n1 + R2\nR2\nlog\n\u0010p\ns\n\u0011 \u00111/2\n,\n(41)\nwe obtain, for all j,\nK(P\u00af\u03c9j, P\u00af\u03c90) \u22641\n16 log |M\u2032|.\n16\nNext, for j and j\u2032 both di\ufb00erent from 0,\n|\u00af\u03c9j \u2212\u00af\u03c9j\u2032|q =\nR\u03b3\np\n1 + \u03b32(s \u22121)\n\u0010 p\u22121\nX\nk=1\n|\u03c9jk \u2212\u03c9j\u2032k|q\u00111/q\n\u2265cs1/q\nR\u03b3\np\n1 + \u03b32(s \u22121)\nand for j \u0338= 0,\n|\u00af\u03c9j \u2212\u00af\u03c90|q \u2265\nR\u03b3|\u03c9j|q\np\n1 + \u03b32(s \u22121)\n\u2265cs1/q\nR\u03b3\np\n1 + \u03b32(s \u22121)\n.\nThe de\ufb01nition of \u03b3 in (41) and the conditions in Theorem 3 imply that, for any j and j\u2032,\n|\u00af\u03c9j \u2212\u00af\u03c9j\u2032|q \u2265cs1/qR\u03b3 \u2265cs1/q(R + 1)\nr\nlog(p/s)\nn\n.\nWe can now apply Theorem 2.7 in [21] to obtain the result.\nReferences\n[1] A. Belloni, V. Chernozhukov, and L. Wang (2014). Pivotal estimation via square-root\nLasso in nonparametric regression. Annals of Statistics, 42, 757 \u2013788.\n[2] P. J. Bickel, Y. Ritov, and A. B. Tsybakov (2009). Simultaneous analysis of Lasso and\nDantzig selector. Annals of Statistics, 37, 1705 \u20131732.\n[3] G. Casella and R. L. Berger (1990) Statistical Inference. Paci\ufb01c Grove.\n[4] Y. Chen and C. Caramanis (2012) Orthogonal matching pursuit with noisy and missing\ndata: Low and high-dimensional results. arxiv:1206.0823\n[5] Y. Chen and C. Caramanis (2013) Noisy and missing data regression: Distribution-\noblivious support recovery. Proc. of International Conference on Machine Learning\n(ICML).\n[6] C. L. Cheng and J. W. van Ness (1999) Statistical Regression with Measurement Error.\nJohn Wiley & Sons.\n[7] V. Chernozhukov, D. Chetverikov and K. Kato (2013) Gaussian approximations and\nmultiplier bootstrap for maxima of sums of high-dimensional random vectors. Annals\nof Statistics, 41, 2786\u20132819.\n[8] E. Gautier and A. B. Tsybakov (2011) High-dimensional instrumental variables regres-\nsion and con\ufb01dence sets. arxiv:1105.2454\n[9] E. Gautier and A. B. Tsybakov (2013) Pivotal estimation in high-dimensional regression\nvia linear programming. In: Empirical Inference \u2013 Festschrift in Honor of Vladimir N.\nVapnik, B.Sch\u00a8olkopf, Z. Luo, V. Vovk eds., 195 - 204. Springer, New York e.a.\n[10] M. G. Kendall and A. Stuart (1973) The Advanced Theory of Statistics Volume Two.\nCharles Gri\ufb03n and Co Ltd, London, Third edition.\n[11] M. Ledoux and M. Talagrand (1991) Probability in Banach Spaces. Springer-Verlag,\nBerlin.\n[12] P-L. Loh and M. J. Wainwright (2012) High-dimensional regression with noisy and\nmissing data: Provable guarantees with non-convexity. Annals of Statistics, 40, 1637\u2013\n1664.\n17\n[13] K. Lounici (2008) Sup-norm convergence rate and sign concentration property of Lasso\nand Dantzig estimators. Electronic J. of Statistics, 2, 90\u2013102.\n[14] K. Lounici, M. Pontil, A.B. Tsybakov and S. van de Geer (2011) Oracle inequalities and\noptimal inference under group sparsity. Annals of Statistics, 39, 2164\u20132204.\n[15] P. Rigollet and A.B. Tsybakov (2011) Exponential screening and optimal rates of sparse\nestimation. Annals of Statistics, 39, 731\u2013771.\n[16] M. Rosenbaum and A.B. Tsybakov (2010) Sparse recovery under matrix uncertainty.\nAnnals of Statistics, 38, 2620\u20132651.\n[17] M. Rosenbaum and A.B. Tsybakov (2013) Improved matrix uncertainty selector. In:\nFrom Probability to Statistics and Back: High-Dimensional Models and Processes \u2013 A\nFestschrift in Honor of Jon A. Wellner, M.Banerjee et al. eds. IMS Collections, vol.9,\n276\u2013290, Institute of Mathematical Statistics.\n[18] M. Rudelson and S. Zhou (2013) Reconstruction From anisotropic random measure-\nments. IEEE Trans. on Information Theory, 56, 3434\u20133447.\n[19] \u00d8. S\u00f8rensen, A. Frigessi and M. Thoresen (2012) Measurement error in Lasso: Impact\nand likelihood bias correction. arxiv:1210.5378\n[20] \u00d8. S\u00f8rensen, A. Frigessi and M. Thoresen (2014) Covariate selection in high-dimensional\ngeneralized linear models with measurement error. arxiv:1407.1070\n[21] A.B. Tsybakov (2009) Introduction to Nonparametric Estimation. Springer, New York.\n[22] R. Vershynin (2012)\nIntroduction to the non-asymptotic analysis of random matri-\nces. In: Compressed Sensing, Theory and Applications, ed. Y. Eldar and G. Kutyniok,\nChapter 5. Cambridge University Press.\n18\nSupplementary Material\n1\nComputation of the compensated MU selector\nThe goal of this section is to show that the minimisation problem (3) de\ufb01ning the com-\npensated MU selector can be solved numerically in an e\ufb03cient way. This algorithmic issue\ncan be intricate since the problem is, in general, not convex, except in some speci\ufb01c situa-\ntions. For example, if \u0398 = (R+)p, it obviously reduces to linear programming. However, we\nshall see that under an additional mild technical hypothesis, solutions can be obtained using\nconvex or even linear programming. It is therefore computationally simpler than the conic\nprogramming estimator \u02c6\u03b8. We focus here only on algorithmic aspects. Therefore, we do not\nrecall the assumptions under which the problem admits a solution and the estimator enjoys\nrelevant properties. We refer to [17] where these issues are addressed in detail.\nFor brevity, we write\nS(\u03b8) = 1\nnZT (y \u2212Z\u03b8) + bD\u03b8\nand denote by (Ur)r\u22650 the family of sets\nUr = {\u03b8 \u2208\u0398 : |S(\u03b8)|\u221e\u2264\u00b5r + \u03c4} .\nWe also de\ufb01ne the function \u03d5 by\n\u03d5(r) = min\n\u03b8\u2208Ur |\u03b8|1.\nWe assume in the next theorem that the equation r = \u03d5(r) has a solution.\nNote that\n\u03d5 is decreasing on [0, \u221e) and \u03d5(r) \u22650. Moreover, for r, r\u2032 \u22650 and \u03b1 \u2208[0, 1] we have\n\u03b1Ur + (1 \u2212\u03b1)Ur\u2032 \u2286U\u03b1r+(1\u2212\u03b1)r\u2032 so that \u03d5 is a convex function and therefore continuous in its\ndomain. In particular, a solution exists provided \u03d5(0) < \u221e.1\nWe now present our algorithm. Consider the following minimisation problem:\nminimise t\n(42)\nover (t, \u03b8+, \u03b8\u2212) such that :\n\u03b8+ \u2212\u03b8\u2212\u2208\u0398, \u03b8+\nj \u22650, \u03b8\u2212\nj \u22650, j = 1, . . . , p,\nt =\np\nX\nj=1\n(\u03b8+\nj + \u03b8\u2212\nj ),\n\f\f 1\nnZT(y \u2212Z(\u03b8+ \u2212\u03b8\u2212)) + bD(\u03b8+ \u2212\u03b8\u2212)\n\f\f\n\u221e\u2264\u00b5t + \u03c4.\nHere the \u03b8+\nj and \u03b8\u2212\nj are the components of \u03b8+ and \u03b8\u2212respectively. As previously, \u00b5 and \u03c4 are\npositive tuning constants, and \u0398 is a given subset of Rp characterising the prior knowledge\nabout \u03b8. Note that (42) is a convex program if \u0398 is a convex set, and it reduces to a linear\nprogram if \u0398 = Rp or if \u0398 is de\ufb01ned by linear constraints.\nLet (\u02c6t, \u02c6\u03b8+, \u02c6\u03b8\u2212) be a solution of (42). We set b\u03b8C\u2032 = \u02c6\u03b8+ \u2212\u02c6\u03b8\u2212. The use of this algorithm is\njusti\ufb01ed by the following theorem.\nTheorem 4. Assume that there exists a solution \u00afr to the equation r = \u03d5(r). Then b\u03b8C\u2032\nis a solution of the minimisation problem (3). Moreover, any solution b\u03b8C of (3) induces\na solution (\u00afr, \u03b8+, \u03b8\u2212) of the problem (42), where \u03b8+ and \u03b8\u2212are vectors with components\n\u03b8+\nj = max{\u02c6\u03b8C\nj , 0} and \u03b8\u2212\nj = max{\u2212\u02c6\u03b8C\nj , 0}.\n1More generally, since \u03d5(r) < \u221e\u21d4Ur \u0338= \u2205, we can de\ufb01ne r := inf{r \u22650 : \u03d5(r) < \u221e}. A solution exists if\nand only if r \u2264\u03d5(r).\n19\nThe proof of Theorem 4 is given in Section 2 of this Supplementary Material.\nWe would like to emphasize that (42) is not an obvious reformulation because the problem\n(3) is non-convex. The proof of Theorem 4 exploits the structure of the \u21131-norm regularisa-\ntion. Again, recall that the rates attained by the compensated MU selector are suboptimal.\nHowever, it remains attractive compared to the conic programming estimator thanks to the\nsimplicity of its computation.\n2\nProof of Theorem 4\nLet \u00afr be a solution of the equation r = \u03d5(r). We set\nU\u2217= {\u03b8 \u2208\u0398 : |S(\u03b8)|\u221e\u2264\u00b5|\u03b8|1 + \u03c4} .\nThe minimisation problem (3) has the form\nmin\n\u03b8\u2208U\u2217|\u03b8|1.\nFirst remark that\nmin\n\u03b8\u2208U\u2217|\u03b8|1 \u2265\u00afr.\n(43)\nIndeed, with the convention that the minimum over an empty set is equal to +\u221e, we get\nmin\n\u03b8\u2208U\u2217|\u03b8|1 = min\n\u0000min\n\u03b8\u2208U\u2217:|\u03b8|1\u2264\u00afr |\u03b8|1,\nmin\n\u03b8\u2208U\u2217:|\u03b8|1>\u00afr |\u03b8|1\n\u0001\n\u2265min\n\u0000min\n\u03b8\u2208U\u00afr:|\u03b8|1\u2264\u00afr |\u03b8|1, \u00afr\n\u0001\n\u2265min\n\u0000min\n\u03b8\u2208U\u00afr |\u03b8|1, \u00afr\n\u0001\n= min(\u03d5(\u00afr), \u00afr) = \u00afr.\nLet now \u00af\u03b8 be any solution of\nmin\n\u03b8\u2208U\u00afr |\u03b8|1.\n(44)\nThen \u00af\u03b8 \u2208\u0398, |\u00af\u03b8|1 = \u00afr and\n|S(\u00af\u03b8)|\u221e\u2264\u00b5\u00afr + \u03c4 = \u00b5|\u00af\u03b8|1 + \u03c4.\nThus \u00af\u03b8 \u2208U\u2217, which implies\nmin\n\u03b8\u2208U\u2217|\u03b8|1 \u2264|\u00af\u03b8|1 = \u00afr.\nThis and (43) imply that \u00af\u03b8 is also a solution of (3) and\nmin\n\u03b8\u2208U\u2217|\u03b8|1 = \u00afr.\n(45)\nHence all solutions of (44) are also solutions of (3). Conversely, if \u03b8\u2032 is a solution of (3), then,\nin view of (45), |\u03b8\u2032|1 = \u00afr. This and the fact that \u03b8\u2032 \u2208U\u2217imply that \u03b8\u2032 \u2208\u0398 and\n|S(\u03b8\u2032)|\u221e\u2264\u00b5\u00afr + \u03c4.\nThis means that \u03b8\u2032 \u2208U\u00afr. Since\nmin\n\u03b8\u2208U\u00afr |\u03b8|1 = \u00afr = |\u03b8\u2032|1,\nwe get that \u03b8\u2032 is a solution of (44). Consequently, the solutions of (3) and (44) coincide.\nLet now \u02c6\u03b8C = (\u03b8C\n1 , . . . , \u03b8C\np ) be a solution of (3).\nThen setting \u03b8+\nj\n= max{\u02c6\u03b8C\nj , 0},\n\u03b8\u2212\nj = max{\u2212\u02c6\u03b8C\nj , 0}, t = |\u03b8+|1 + |\u03b8\u2212|1, we have that \u02c6\u03b8C = \u03b8+ \u2212\u03b8\u2212and |\u02c6\u03b8C|1 = t. Thus,\n20\n(|\u02c6\u03b8C|1, \u03b8+, \u03b8\u2212) is feasible for the problem (42). This implies that the minimum in (42) is\nsmaller than the minimum in (3), which yields |\u02c6\u03b8C\u2032|1 \u2264t = |\u02c6\u03b8C|1. Moreover, for any solution\n(\u02c6t, \u02c6\u03b8+, \u02c6\u03b8\u2212) of (42) the di\ufb00erence \u02c6\u03b8C\u2032 = \u02c6\u03b8+ \u2212\u02c6\u03b8\u2212satis\ufb01es\n\f\f 1\nnZT (y \u2212Z \u02c6\u03b8C\u2032) + bD\u02c6\u03b8C\u2032\f\f\n\u221e\n\u2264\u00b5\u02c6t + \u03c4 \u2264\u00b5|\u02c6\u03b8C|1 + \u03c4 = \u00b5\u00afr + \u03c4\nsince \u03d5(\u00afr) = \u00afr. Thus, \u02c6\u03b8C\u2032 \u2208U\u00afr. Hence, by de\ufb01nition of \u03d5, we have \u03d5(\u00afr) \u2264|\u02c6\u03b8C\u2032|1. Therefore,\nsince we have shown before that |\u02c6\u03b8C\u2032|1 \u2264|\u02c6\u03b8C|1, we obtain |\u02c6\u03b8C\u2032|1 = |\u02c6\u03b8C|1 = \u00afr, \u02c6\u03b8C\u2032 is a solution\nof (3) and (\u00afr, \u03b8+, \u03b8\u2212) is a solution of (42).\n3\nProperties of the sensitivities\nHere we collect some properties of the sensitivities \u03baq(s, u) and \u03bapr(s, u). First, following [8],\nwe give a relation between \u03baq(s, u) and the Restricted Eigenvalue (RE) and Coherence (C)\nconstants. For completeness, we recall the Restricted Eigenvalue and Coherence assumptions.\nAssumption RE(s, u). Let u > 0, 1 \u2264s \u2264p. There exists a constant \u03baRE(s, u) > 0\nsuch that\nmin\n\u2206\u2208CJ(u)\\{0}\n|\u2206T \u03a8\u2206|\n|\u2206J|2\n2\n\u2265\u03baRE(s, u),\nfor all subsets J of {1, . . . , p} of cardinality |J| \u2264s.\nAssumption C. All diagonal elements of \u03a8 are equal to 1 and all its o\ufb00-diagonal ele-\nments \u03a8ij satisfy the Coherence condition: maxi\u0338=j |\u03a8ij| \u2264\u03c1 for some \u03c1 < 1.\nAssumption C with \u03c1 < (cs)\u22121 and c > 0 depending only on u implies Assumption\nRE(s, u), see [2].\nThe following lemma due to [8] provides useful relations between the\nconstants \u03baRE, \u03c1 and \u03baq. In this lemma, we denote by c positive constants that do not\ndepend on s.\nLemma 6. Let u > 0, 1 \u2264s \u2264p. For any \u03b1 \u2208(0, 1), there exists c > 0 such that if\nAssumption C holds with \u03c1 < (cs)\u22121, then\n\u03ba\u221e(s, u) \u2265\u03b1.\n(46)\nNext, under Assumption RE(s, u),\n\u03ba1(s, u) \u2265(cs)\u22121\u03baRE(s, u)\n(47)\nand under Assumption RE(2s, u), for any s \u2264p/2, 1 < q \u22642, we have\n\u03baq(s, u) \u2265c(q)s\u22121/q\u03baRE(2s, u),\n(48)\nwhere c(q) > 0 depends only on u and q. Furthermore, for any 1 \u2264q \u2264\u221e,\n\u03baq(s, u) \u2265(2s)\u22121/q\u03ba\u221e(s, u).\n(49)\nNote that (46) and (49) yield the control of the sensitivities \u03baq under the Coherence\nassumption for all 1 \u2264q \u2264\u221e. The next lemma relates \u03bapr to \u03ba1.\nLemma 7. For any u > 0, 1 \u2264s \u2264p,\n\u03bapr(s, u) \u2265\np\n\u03ba1(s, u).\n21\nProof. Fix a set J such that |J| \u2264s. Since \u2206T \u03a8\u2206\u2264|\u03a8\u2206|\u221e|\u2206|1, we obtain\nmin\n\u2206\u2208CJ(u):|\u03a81/2\u2206|2=1 |\u03a8\u2206|\u221e=\nmin\n\u2206\u2208CJ(u):|\u03a81/2\u2206|2>0 |\u03a8\u2206|\u221e/\n\u221a\n\u2206T\u03a8\u2206\n\u2265\nmin\n\u2206\u2208CJ(u):|\u03a81/2\u2206|2>0\np\n|\u03a8\u2206|\u221e/|\u2206|1\n\u2265\nmin\n\u2206\u2208CJ(u):|\u2206|1>0\np\n|\u03a8\u2206|\u221e/|\u2206|1\n=\nmin\n\u2206\u2208CJ(u):|\u2206|1=1\np\n|\u03a8\u2206|\u221e,\nwhere we used the fact that {\u2206: |\u03a81/2\u2206|2 > 0} \u2286{\u2206: |\u2206|1 > 0, |\u03a8\u2206|\u221e> 0}. Taking\nthe minimum over J such that |J| \u2264s and using the de\ufb01nitions of \u03bapr(s, u) and \u03ba1(s, u) we\nobtain the result. \u25a1\nLemma 8. If rank(X) = min{n, p}, then for any u > 0, 1 \u2264s \u2264p,\n\u03bapr(s, u) > 0.\nProof. If rank(X) = p, the result follows trivially, so we assume that rank(X) = n < p. We\nhave\nmin\n\u2206\u2208CJ(u):|\u03a81/2\u2206|2=1 |\u03a8\u2206|\u221e=\nmin\n\u2206\u2208CJ(u):|X\u2206/\u221an|2=1 |XT X\u2206/n|\u221e\n\u2265\nmin\n\u2206\u2208Rp:|X\u2206/\u221an|2=1 |XT X\u2206/n|\u221e\u2265\nmin\n\u03b4\u2208Rn:|\u03b4|2=1 |XT \u03b4/\u221an|\u221e.\nSince rank(XT ) = rank(X) = n, we have XT \u03b4/\u221an \u0338= 0 for all \u03b4 \u2208Rn \\ {0}. Moreover,\n{\u03b4 \u2208Rn : |\u03b4|2 = 1} being compact, the minimum is achieved at some \u03b4\u2217, with XT \u03b4\u2217/\u221an \u0338= 0,\nso that |XT \u03b4\u2217/\u221an|\u221e> 0. Taking the minimum over (the \ufb01nite collection of) J such that\n|J| \u2264s yields the result. \u25a1\n4\nKullback-Leibler divergence\nFor \u03b8 \u2208Rp, recall that c\u03b8 = \u03b8T\u0393\u03b8. Denote by \u03bbmin and \u03bbmax the smallest and largest\neigenvalues of \u0393. It is easy to see that they are functions of \u03c32\n\u2217, \u03bb\u03a3\nmin, and \u03bb\u03a3\nmax only. Since\nthe distribution of X is now \ufb01xed, we write for brevity PX,\u03b8 = P\u03b8. The following lemma is a\ncrucial element in the proof of the lower bounds.\nLemma 9. Let \u03b81 \u2208Rp and \u03b82 \u2208Rp be such that |\u03b81|2 = |\u03b81|2. Under Assumption (A5),\nK(P\u03b81, P\u03b82) \u2264\nc n\n1 + |\u03b81|2\n2\n\u0000|\u03b81 \u2212\u03b82|2\n2 + |c\u03b81 \u2212c\u03b82|\n\u0001\n,\nwhere c is a constant depending only on \u03c32\n\u2217, \u03c32, \u03bb\u03a3\nmin, and \u03bb\u03a3\nmax.\nProof. In view of the properties of Kullback divergence between product measures, it su\ufb03ces\nto prove the lemma for n = 1. In the following, we denote by c positive constants depending\nonly on \u03c32\n\u2217, \u03c32, \u03bb\u03a3\nmin, and \u03bb\u03a3\nmax, which may vary from line to line. Let \u03b8 \u2208Rp. Consider the\nrandom vector (U, V ) where\nV = (A1 + B1, . . . , Ap + Bp),\nwith A = (A1, . . . , Ap)T a Gaussian vector with covariance matrix \u03a3 and B = (B1, . . . , Bp)T\na Gaussian vector with covariance matrix \u03c32\n\u2217Ip\u00d7p, independent of A and\nU =\np\nX\nj=1\n\u03b8j(Vj \u2212Bj) + \u03b7,\n22\nwhere \u03b7 is a zero-mean Gaussian random variable with variance \u03c32. We now \ufb01nd the con-\nditional distribution L\u03b8(U|V ) of U given V . Remark \ufb01rst that (V1, . . . , Vp, B1, . . . Bp)T is a\nzero-mean Gaussian random vector with covariance matrix\n\u0012 \u03a3 + \u03c32\n\u2217Ip\u00d7p\n\u03c32\n\u2217Ip\u00d7p\n\u03c32\n\u2217Ip\u00d7p\n\u03c32\n\u2217Ip\u00d7p\n\u0013\n,\nso that L\u03b8(B|V ) is a Gaussian distribution with mean e\u03a3V and covariance matrix \u03c32\n\u2217(Ip\u00d7p\u2212e\u03a3).\nThis easily implies that L\u03b8(U|V ) is Gaussian with mean \u03b8T\u0393V and variance \u03c32 +c\u03b8\u03c32\n\u2217. Then\nthe logarithm of the density of L\u03b8(U|V ), denoted by l\u03b8(U|V ) satis\ufb01es\nl\u03b8(U|V ) = \u22121\n2log(2\u03c0) \u22121\n2log(\u03c32 + c\u03b8\u03c32\n\u2217) \u2212\n1\n2(\u03c32 + c\u03b8\u03c32\u2217)(U \u2212\u03b8T \u0393V )2.\nLet now \u03b81 \u2208Rp and \u03b82 \u2208Rp with |\u03b81|2 = |\u03b81|2. Then,\nl\u03b81(U|V ) \u2212l\u03b82(U|V ) = 1\n2\n\u0000log(\u03c32 + c\u03b82\u03c32\n\u2217) \u2212log(\u03c32 + c\u03b81\u03c32\n\u2217)\n\u0001\n+\n1\n2(\u03c32 + c\u03b82\u03c32\u2217)\n\u0000(U \u2212\u03b8T\n2 \u0393V )2 \u2212(U \u2212\u03b8T\n1 \u0393V )2\u0001\n+\n\u0012\n1\n2(\u03c32 + c\u03b82\u03c32\u2217) \u2212\n1\n2(\u03c32 + c\u03b81\u03c32\u2217)\n\u0013\n(U \u2212\u03b8T\n1 \u0393V )2.\nSince the distribution of V does not depend on \u03b8, we obtain that in the case n = 1,\nK(P\u03b81, P\u03b82) = 1\n2\n\u0000log(\u03c32 + c\u03b82\u03c32\n\u2217) \u2212log(\u03c32 + c\u03b81\u03c32\n\u2217)\n\u0001\n+\n1\n2(\u03c32 + c\u03b82\u03c32\u2217)E\u03b81\n\u0000(U \u2212\u03b8T\n2 \u0393V )2 \u2212(U \u2212\u03b8T\n1 \u0393V )2\u0001\n+\n\u0012\n1\n2(\u03c32 + c\u03b82\u03c32\u2217) \u2212\n1\n2(\u03c32 + c\u03b81\u03c32\u2217)\n\u0013\nE\u03b81(U \u2212\u03b8T\n1 \u0393V )2,\nwhere E\u03b81[\u00b7] denotes the expectation when \u03b8 = \u03b81 in the de\ufb01nition of U. Using the inequality\n|log(\u03c32 + x1) \u2212log(\u03c32 + x2)| \u2264|x1 \u2212x2|/ min(\u03c32 + x1, \u03c32 + x2), x1, x2 > 0 and the fact that\nc\u03b8 \u2265\u03bbmin|\u03b8|2\n2, we get\nK(P\u03b81, P\u03b82) \u2264\nc\n1 + |\u03b81|2\n2\n\f\fE\u03b81\n\u0002\n(U \u2212\u03b8T\n2 \u0393V )2 \u2212(U \u2212\u03b8T\n1 \u0393V )2\u0003\f\f\n+ c |c\u03b81 \u2212c\u03b82|\n(1 + |\u03b81|2\n2)2\n\u0000(1 + |\u03b81|2\n2) + E\u03b81[(U \u2212\u03b8T\n1 \u0393V )2]\n\u0001\n.\nWe have\nK(P\u03b81, P\u03b82) \u2264\nc\n1 + |\u03b81|2\n2\n\f\f\fE\u03b81\nh\u0000(\u03b8T\n1 \u2212\u03b8T\n2 \u0393)A \u2212\u03b8T\n2 \u0393B\n\u00012 \u2212\n\u0000(\u03b8T\n1 \u2212\u03b8T\n1 \u0393)A \u2212\u03b8T\n1 \u0393B\n\u00012i\f\f\f\n+ c |c\u03b81 \u2212c\u03b82|\n(1 + |\u03b81|2\n2)2\n\u00001 + |\u03b81|2\n2 + |\u03b81|2\n2\u03bb\u03a3\nmax + \u03c32 + |\u03b81|2\n2(\u03bb\u03a3\nmax + \u03c32\n\u2217)\u03bb2\nmax\n\u0001\n.\nHence\nK(P\u03b81, P\u03b82) \u2264\nc\n1 + |\u03b81|2\n2\n\f\f\fE\u03b81\nh\u0000(\u03b8T\n1 \u2212\u03b8T\n2 \u0393)A \u2212\u03b8T\n2 \u0393B\n\u00012 \u2212\n\u0000(\u03b8T\n1 \u2212\u03b8T\n1 \u0393)A \u2212\u03b8T\n1 \u0393B\n\u00012i\f\f\f\n+ c|c\u03b81 \u2212c\u03b82|\n1 + |\u03b81|2\n2\n.\n23\nThen using the independence of A and B, we get\nK(P\u03b81, P\u03b82) \u2264\nc\n1 + |\u03b81|2\n2\n\f\f(\u03b8T\n1 \u2212\u03b8T\n2 \u0393)\u03a3(\u03b81 \u2212\u0393\u03b82) \u2212(\u03b8T\n1 \u2212\u03b8T\n1 \u0393)\u03a3(\u03b81 \u2212\u0393\u03b81) + (\u03b8T\n2 \u03932\u03b82 \u2212\u03b8T\n1 \u03932\u03b81)\n\f\f\n+ c|c\u03b81 \u2212c\u03b82|\n1 + |\u03b81|2\n2\n.\nDeveloping the preceding expression yields\nK(P\u03b81, P\u03b82) \u2264\nc\n1 + |\u03b81|2\n2\n\f\f2(\u03b81 \u2212\u03b82)T \u0393\u03a3\u03b81 + \u03b8T\n2 \u0393(\u03a3 + \u03c32\n\u2217Ip\u00d7p)\u0393\u03b82 \u2212\u03b8T\n1 \u0393(\u03a3 + \u03c32\n\u2217Ip\u00d7p)\u0393\u03b81\n\f\f\n+ c|c\u03b81 \u2212c\u03b82|\n1 + |\u03b81|2\n2\n.\nThe right hand side here can be rewritten as\nc\n1 + |\u03b81|2\n2\n\u0010\f\f(\u03b81 \u2212\u03b82)T \u0393\n\u00002\u03a3\u03b81 \u2212(\u03a3 + \u03c32\n\u2217Ip\u00d7p)\u0393(\u03b81 + \u03b82)\n\u0001\f\f + |c\u03b81 \u2212c\u03b82|\n\u0011\n.\nSince (\u03a3 + \u03c32\n\u2217Ip\u00d7p)\u0393 = \u03a3, we \ufb01nally get\nK(P\u03b81, P\u03b82) \u2264\nc\n1 + |\u03b81|2\n2\n\u0000\f\f(\u03b81 \u2212\u03b82)T \u0393\u03a3(\u03b81 \u2212\u03b82)\n\f\f + |c\u03b81 \u2212c\u03b82|\n\u0001\n,\nwhich implies the lemma. \u25a1\n5\nSu\ufb03cient conditions for bounded mk\nIn this section, we provide conditions on the matrix X guaranteeing that the quantity mk =\nmaxj=1,...,p 1\nn\nPn\ni=1 |Xij|k concentrates around \u00afmk = maxj=1,...,p 1\nn\nPn\ni=1 E[|Xij|k].\nWe are\nparticularly interested in the cases k = 2 and k = 4 since such a concentration can be used\nto assure that assumptions of Corollary 1 hold. However, the argument below is valid for\nany positive k. We assume that the independent random vectors X1, . . . , Xn \u2208Rp satisfy\n(B) E[maxi\u2264n |Xi|k\n\u221e]n\u22121 log(p) = o( \u00afmk) as n \u2192\u221e.\nThe condition is fairly mild and allows for p > n under sub-exponential tail conditions.\nSharper bounds are available in the Gaussian case (see remark below).\nThe argument is as follows. Lemma 9.1 in [14] with m = 1 yields\na := E[max\nj\u2264p | 1\nn\nn\nX\ni=1\n(|Xij|k \u2212E[|Xij|k])|] \u2264\nr\n8 log(2p)\nn\nE[max\nj\u2264p { 1\nn\nn\nX\ni=1\nX2k\nij }1/2].\nThus,\na\n\u2264\nq\n8 log(2p)\nn\nE[maxi\u2264n |Xi|k/2\n\u221emaxj\u2264p{ 1\nn\nPn\ni=1 |Xij|k}1/2]\n\u2264\nq\n8 log(2p)\nn\nE[maxi\u2264n |Xi|k\n\u221e]1/2{E[maxj\u2264p 1\nn\nPn\ni=1 |Xij|k]}1/2\n\u2264\u03b4n(a + \u00afmk)1/2\nwhere \u03b4n =\nq\n8 log(2p)\nn\nE[maxi\u2264n |Xi|k\n\u221e]1/2. This implies that a \u2264\u03b42\nn + \u03b4n \u00afm1/2\nk\n, and \ufb01nally\nE[maxj\u2264p 1\nn\nPn\ni=1 |Xij|k] \u2264\u00afmk + a \u2264\u00afmk + \u03b42\nn + \u03b4n \u00afm1/2\nk\n.\nTherefore, by (B), we have E[mk] = \u00afmk + o( \u00afmk) as the sample size n grows.\n24\nRemark. (Gaussian Case) Let X1, . . . , Xn \u2208Rp be independent Gaussian random vectors\nsuch that Xij \u223cN(0, \u03c32\nj ) for j = 1, . . . , p. By [11], page 21, equation (1.6), and the union\nbound, there exists a universal constant \u00afC \u22651 such that for any k \u22652 and \u03b3 \u2208(0, 1)\nP\n\u0014\nm1/k\nk\n\u2265\u00afC\n\u221a\nk max\n1\u2264j\u2264p \u03c3j + n\u22121/kp\n2 log(2p/\u03b3) max\n1\u2264j\u2264p \u03c3j\n\u0015\n\u2264\u03b3.\n25\n"}