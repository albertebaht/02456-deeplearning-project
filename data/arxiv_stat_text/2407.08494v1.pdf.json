{"text": "arXiv:2407.08494v1  [math.ST]  11 Jul 2024\nMultivariate root-n-consistent smoothing parameter free matching\nestimators and estimators of inverse density weighted expectations\nHajo Holzmann\u2217\nFachbereich Mathematik und Informatik\nPhilipps-Universit\u00a8at Marburg\nholzmann@mathematik.uni-marburg.de\nAlexander Meister\nInstitut f\u00a8ur Mathematik\nUniversit\u00a8at Rostock\nalexander.meister@uni-rostock.de\nJuly 12, 2024\nAbstract\nExpected values weighted by the inverse of a multivariate density or, equivalently, Lebesgue\nintegrals of regression functions with multivariate regressors occur in various areas of applications,\nincluding estimating average treatment e\ufb00ects, nonparametric estimators in random coe\ufb03cient re-\ngression models or deconvolution estimators in Berkson errors-in-variables models. The frequently\nused nearest-neighbor and matching estimators su\ufb00er from bias problems in multiple dimensions.\nBy using polynomial least squares \ufb01ts on each cell of the Kth-order Voronoi tessellation for su\ufb03-\nciently large K, we develop novel modi\ufb01cations of nearest-neighbor and matching estimators which\nagain converge at the parametric \u221an-rate under mild smoothness assumptions on the unknown re-\ngression function and without any smoothness conditions on the unknown density of the covariates.\nWe stress that in contrast to competing methods for correcting for the bias of matching estimators,\nour estimators do not involve nonparametric function estimators and in particular do not rely on\nsample-size dependent smoothing parameters. We complement the upper bounds with appropriate\nlower bounds derived from information-theoretic arguments, which show that some smoothness of\nthe regression function is indeed required to achieve the parametric rate. Simulations illustrate the\npractical feasibility of the proposed methods.\nKeywords: Average treatment e\ufb00ects; Berkson errors in variables models; bias correction; inverse\ndensity weighted expectations; matching estimators; random coe\ufb03cients; Voronoi tessellation\n1\nIntroduction\nWe consider estimation of inverse density weighted expectations, or, equivalently, of Lebesgue in-\ntegrals of regression functions, with multivariate regressors.\nAs we shall discuss in detail, such\nfunctionals occur in various areas of applications, including estimating average treatment e\ufb00ects\n(Abadie and Imbens, 2006, 2011), random coe\ufb03cient regression models (Hoderlein et al., 2017),\n(Gaillac and Gautier, 2022), Berkson errors in variables models (Delaigle et al., 2006) or transfer\nlearning under covariate shift (Kouw and Loog, 2019; Portier et al., 2023). Frequently used nearest-\nneighbor as well as matching estimators involve a bias or order n\u22121/d in d-dimensions (Abadie and Imbens,\n\u2217Corresponding author. Prof. Dr. Hajo Holzmann, Fachbereich Mathematik und Informatik, Philipps-Universit\u00a8at\nMarburg, Hans-Meerweinstr. 6, 35043 Marburg, Germany\n1\n2006). Therefore, methods for bias correction have been proposed in the treatment e\ufb00ect literature\n(Abadie and Imbens, 2011; Lin et al., 2023). These however involve nonparametric estimates and rates\nof convergence for regression functions and their derivatives, potentially under the strong smoothness\nassumptions.\nIn this paper we introduce simple modi\ufb01cations of nearest-neighbor and matching estimators for such\nfunctionals which again converge at the parametric \u221an-rate under mild smoothness assumptions on the\nunknown regression function. These novel estimators do not involve nonparametric function estimators\nand in particular do not rely on sample-size dependent smoothing parameters. Our upper bounds are\ncomplemented by a discussion of lower bounds which show that some smoothness is required for\n\u221an-consistent estimation to be possible.\nMore precisely, consider a d-vector of covariates Z supported on some compact and convex set S \u2286Rd\nand having a strictly positive density fZ on S. Further let S\u2217\u2286S be some given non-empty Borel\nset. We consider the following two closely related estimation problems, in which we focus on the\nmultivariate setting d \u22652.\nFirst we intend to estimate from i.i.d. data (Uj, Zj), j = 1, . . . , n, distributed as (U, Z), for some given\nreal-valued measurable function g(u, z) the functional\n\u03a8 := E\nh\ng(U, Z) 1S\u2217(Z)\nfZ(Z)\ni\n=\nZ\nS\u2217G(z) dz ,\n(1.1)\nwhere\nG(z) := E\n\u0002\ng(U, Z) | Z = z\n\u0003\n(1.2)\nis the regression function of Z on g(U, Z). The issue in estimating \u03a8 in (1.1) in the representation as\nexpected value is the unknown density fZ which occurs in the denominator.\nSecond, from independent and identically distributed observations (Yi, Zi), i = 1, . . . , n, where Yi is\nreal-valued, we consider estimation of the functional\n\u03a6 = E\nh\nY 1S\u2217(Z) f(Z)\nfZ(Z)\ni\n.\n(1.3)\nHere f is an additional Lebesgue density in Rd. If f is known, by putting g(y, z) = y f(z), this is a\nspecial case of the functional \u03a8 with Y taking the role of U. We shall devise an estimator in case of\nunknown f, from which an additional sample X1, . . . , Xm, independent of the (Yi, Zi), is supposed to\nbe available.\nIn dimension d = 1, estimating \u03a8 in (1.1) has been studied in Lewbel and Schennach (2007). Based\non the theory of order statistics and spacings they construct estimators which do not require nonpara-\nmetric tuning parameters and converge at the parametric \u221an-rate. Further, they even show how to\nmake their estimator asymptotically e\ufb03cient by letting the order of the spacings diverge. However, as\nnoted in Lewbel and Schennach (2007) their method does not easily extend to multiple dimensions.\nEstimating functionals of the form \u03a8 occurs frequently in applications which require d \u22652. Exam-\nples include random coe\ufb03cient regression models (Hoderlein et al., 2017; Gaillac and Gautier, 2022;\nGautier and Kitamura, 2013) or series estimators which involve inner products of the regression func-\ntion and basis functions with respect to the Lebesgue measure, such as nonparametric deconvolution\nestimators in the Berkson errors-in-variables models (Delaigle et al., 2006). We construct an estima-\ntor of \u03a8 in (1.1) which achieves the parametric \u221an-rate with neither dividing by a nonparametric\n2\ndensity estimate nor estimating the regression function G nonparametrically. Our methods involve\nleast squares polynomial \ufb01ts on cells of the Kth-order Voronoi tessellation, that is on the K-nearest\nneighbor partition induced by the sample Z1, . . . , Zn. We stress that our method does not have tuning\nparameters which must be chosen depending on the sample size and on the unknown smoothness of G,\nwhich would be typical of nonparametric methods. Compared to the analysis of the one-dimensional\ncase d = 1 in Lewbel and Schennach (2007), higher-order smoothness assumptions on G are required\nto achieve the parametric rate, however, no smoothness conditions on the density fZ of the covari-\nates are needed in our analysis. Moreover, using information-theoretic arguments we show that some\nsmoothness of the regression function G is indeed required in order to achieve the parametric rate.\nFurthermore we extend the methodology to construct a \u221an-consistent estimator for the functional\n\u03a6 in (1.3) in case of unknown density f. We discuss how this estimator can be applied for average\ntreatment e\ufb00ects, and show that for the special case of constant approximations it coincides with\nthe classical matching estimator from the treatment e\ufb00ect literature. Using polynomial least squares\n\ufb01ts of appropriate order depending on the dimension, we achieve a bias correction resulting in \u221an-\nconsistent estimators under much milder smoothness assumptions than in Abadie and Imbens (2011)\nand without invoking nonparametric regression estimates. The functional \u03a6 also occurs in transfer\nlearning under covariate shift, when we have at our disposal labeled data from a source distribution\nbut only unlabeled data from the target distribution. In such settings our method allows for a novel\napproach to estimating importance-weighted averages.\nIn our proofs we obtain results on the moments of the volume of higher-order Voronoi cells, as well as on\nproperties of design matrices for multivariate polynomial regression under random design which might\nbe of some independent interest. Let us brie\ufb02y discuss some further methodologically related work.\nDevroye et al. (2017) study the asymptotic distribution for the volume of a \ufb01rst-order Voronoi cells,\nand Sharpnack (2023) analyzes matching estimators by relying on properties of \ufb01rst-order Voronoi\ncells. Kallus (2020) investigate weighted matching estimators with emphasis on weights from repro-\nducing kernels, and Samworth (2012) provides comprehensive analysis of weighted nearest neighbor\nestimators. Finally, Hu et al. (2022) study nonparametric estimates of a function of bounded total\nvariation by functions which are constant on elements of the \ufb01rst-order Voronoi tessellation.\nThe structure of the paper is as follows. In Section 2.1 we \ufb01rst motivate our method and the need\nfor higher-order bias correction by considering and brie\ufb02y analyzing a simple special case. Then in\nSection 2.2 we introduce our general method and state our main theoretical result for estimating \u03a8,\nthat is, the parametric rate of convergence under smoothness assumptions on the regression function\nG. This is extended in Section 2.3 to a \u221an-consistent estimator of \u03a6. Section 2.4 complements these\nupper bounds by providing lower bounds for estimating \u03a8, which show that some smoothness of G\n(or fZ) is generally required to estimate at the parametric rate. In Section 3 we discuss in detail\nhow to apply the methodology to series estimators and in particular the Berkson errors-in-variables\nproblem, to random coe\ufb03cient regression models, to the estimation of average treatment e\ufb00ects and to\nestimating importance-weighted averages in transfer learning under covariate shift. Section 4 contains\na simulation study, while Section 5 concludes. The main proofs of the upper bounds are given in\nSection 6, while additional technical arguments as well as further simulations results are deferred to\nthe supplementary appendix.\nWe provide a brief overview of some important notation: the Euclidean norm is denoted by \u2225\u00b7 \u2225, the\ndimension by d; and we write \u03bb for the d-dimensional Lebesgue measure; for \u03ba = (\u03ba1, . . . , \u03bad)\u22a4\u2208Nd\n0\nwe denote |\u03ba| = \u03ba1 + \u00b7 \u00b7 \u00b7 + \u03bad; and, for a function f(z) in z = (z1, . . . , zd), we denote its partial\n3\nderivatives by\n\u2202\u03baf(z) =\n\u2202|\u03ba|\n\u2202z\u03ba1\n1\n. . . \u2202z\u03bad\nd\nf(z).\n2\nEstimation method and rate of convergence\nIn this section we propose estimators for the functionals \u03a8 in (1.1) and \u03a6 in (1.3). We start in Section\n2.1 with a special case of our method which illustrates the need for higher-order bias correction in\nmultiple dimensions. Smoothing-parameter free, \u221an-consistent estimators of the functionals \u03a8 and \u03a6\nare introduced in Sections 2.2 and 2.3, respectively. These are complemented in Section 2.4 with a\ndiscussion of lower bounds.\n2.1\nMotivation\nTo stress the need for a higher-order bias correction, let us start by considering and brie\ufb02y analyzing\na simple special case for the estimator of the functional \u03a8 in (1.1). Here we approximate the integral\nin (1.1) by a Riemann sum based on the \ufb01rst-order Voronoi tessellation of S\u2217. Consider the \ufb01rst order\nVoronoi cells\nCj :=\n\b\nz \u2208S\u2217| \u2225z \u2212Zj\u2225< \u2225z \u2212Zk\u2225, \u2200k \u0338= j\n\t\n.\nAn estimator of \u03a8 is then given by\n\u02c6\u03a8 :=\nn\nX\nj=1\ng(Uj, Zj) \u00b7 \u03bb(Cj) ,\n(2.1)\nwhere \u03bb denotes the d-dimensional Lebesgue measure.\nDenoting by \u03c3Z the \u03c3-\ufb01eld generated by\nZ1, . . . , Zn we obtain that\nE\n\u0002\u02c6\u03a8 | \u03c3Z\n\u0003\n=\nn\nX\nj=1\nG(Zj) \u00b7 \u03bb(Cj),\na discretized version of (1.1). Here G is the regression function de\ufb01ned in (1.2).\nTo show why this simple estimator does not achieve the parametric rate in dimensions d > 3 and even\nhas issues for d = 2, consider the expected conditional variance and expected squared conditional bias\ndecomposition of the mean squared error\nE\n\u0002\f\f\u02c6\u03a8 \u2212\u03a8\n\f\f2\u0003\n= E\n\u0002\nvar\n\u0000\u02c6\u03a8|\u03c3Z\n\u0001\u0003\n+ E\n\u0002\f\f E[\u02c6\u03a8|\u03c3Z] \u2212\u03a8\n\f\f2\u0003\n.\n(2.2)\nUnder the Assumptions 1 and 2 below, if the conditional variance var\n\u0000g(U1, Z1)|Z1\n\u0001\nis uniformly\nbounded, say by the constant CV > 0, we have that\nE\n\u0002\nvar\n\u0000\u02c6\u03a8|\u03c3Z\n\u0001\u0003\n= E\nh\nn\nX\nj=1\n\u03bb2(Cj) \u00b7 var\n\u0000g(Uj, Zj)|Zj\n\u0001i\n\u2264n CV E\n\u0002\n\u03bb2(C1)\n\u0003\n\u2272n\u22121,\nsince the \u03bb2(Cj) are identically distributed and Lemma 6.2 in Section 6.1.1 implies that E\n\u0002\n\u03bb2(C1)\n\u0003\n\u2272\nn\u22122. However, for the squared conditional bias, if G is Lipschitz continuous with constant CH > 0 we\n4\nhave that\nE\n\u0002\f\f E[\u02c6\u03a8|\u03c3Z] \u2212\u03a8\n\f\f2\u0003\n= E\nh\f\f\f\nn\nX\nj=1\nZ\nCj\n\u0000G(Zj) \u2212G(z)\n\u0001\ndz\n\f\f\f\n2i\n\u2264C2\nH\nn\nX\nj,k=1\nZZ\n(S\u2217)2 E\n\u0002\n1Cj(z) 1Ck(z\u2032) \u2225z \u2212Z(1)(z)\u2225\u2225z\u2032 \u2212Z(1)(z\u2032)\u2225\n\u0003\ndz dz\u2032 \u2272n\u22122/d,\nwhere Z(1)(z) denotes the \ufb01rst-nearest neighbor of z in Z1, . . . , Zn, and the \ufb01nal bound follows from\nLemma 6.4 in the Section 6.1.2. Thus in dimensions d \u22653 the order of the bias is slower than the\nparametric rate, and even for d = 2 it is not negligible compared to the variance.\nTo improve the rate of the bias, instead of the constant approximation to G on each Voronoi cell, we\nuse polynomial approximation. In order to make this well-de\ufb01ned and computable we use higher-order\nVoronoi cells.\n2.2\nBias corrected, \u221an-consistent estimation of \u03a8\nOur general method for estimating \u03a8 in (1.1) uses polynomial approximation of degree L and the K-th\norder Voronoi tessellation of S\u2217for su\ufb03ciently large integers K, L \u2208N depending on the dimension d.\nFor each J \u2286{1, . . . , n} with #J = K we set\nC(J) :=\n\b\nz \u2208S\u2217| \u2225z \u2212Zj\u2225< \u2225z \u2212Zk\u2225\n\u2200j \u2208J, k \u2208{1, . . . , n}\\J\n\t\n.\n(2.3)\nThen for z \u2208C(J) we let bG(z) be the least squares polynomial of degree L in d variables when\nregressing Zj on g(Uj, Zj) for indices j \u2208J. Let JK be the collection of all subsets of {1, . . . , n} with\nexactly K elements. We set\n\u02c6\u03a8 = \u02c6\u03a8(L, K) :=\nX\nJ\nZ\nC(J)\nbG(z) dz .\n(2.4)\nThe parameters K and L need not be chosen depending on the sample size n and hence do not take the\nrole of smoothing parameters. They simply have to be su\ufb03ciently large depending on the dimension\nd, see Remark 1. Also note that bG(z) is a weighted average of g(Uj, Zj) for those j corresponding to\nthe K-nearest neighbors of z in Z1, . . . , Zn. Speci\ufb01cally, for L = 0 we obtain\n\u02c6G(z) = 1\nK\nX\nj\u2208J\ng(Uj, Zj)\nif z \u2208C(J),\nand\n\u02c6\u03a8 =\nX\nJ\u2208JK\n\u0010 1\nK\nX\nj\u2208J\ng(Uj, Zj)\n\u0011\n\u03bb(C(J)) .\n(2.5)\nFor general L, more formally we introduce the notation\nK := K(d, L) :=\n\b\n\u03ba \u2208Nd\n0 | |\u03ba| \u2264L\n\t\n,\n\u03be\u03ba(z, w) :=\nd\nY\nk=1\n(wk \u2212zk)\u03bak , \u03ba \u2208K, w \u2208Rd ,\n\u03be(z, w) =\n\u0000\u03be\u03ba(z, w)\n\u0001\n\u03ba\u2208K.\nMonomials of degree \u2264L are indexed by elements in K, so that K\u2217is the number of free parameters\nof a polynomial in d variables of total degree \u2264L. We consider \u03be\u03ba(z, w) as a monomial function in w,\ncentered at z. Then \u03be(z, w) is a vector of functions in w of dimension K\u2217.\n5\nFor z \u2208S\u2217let J(z) denote that J \u2208JK which satis\ufb01es z \u2208C(J), and note that Lebesgue-almost all\nz \u2208S\u2217belong to some C(J), since the boundaries of the C(J) are sets of measure 0.\nThe least squares polynomial bG(z) of degree L in d variables at z when regressing Zj on g(Uj, Zj) for\nindices j \u2208J is obtained as the \ufb01rst entry \u03b3(0,...,0) of the vector\nargmin\u03b3\u2208RK\nX\nj\u2208J\n\u0000g(Uj, Zj) \u2212\u03b3\u22a4\u00b7 \u03be(z, Zj)\n\u00012 = M(z)\u22121 \u02c6G(z),\nwhere\nM(z) :=\nX\nj\u2208J(z)\n\u03be(z, Zj) \u03be(z, Zj)\u22a4=\n\u0010 X\nj\u2208J(z)\n\u03be\u03ba+\u03ba\u2032(z, Zj)\n\u0011\n\u03ba,\u03ba\u2032\u2208K ,\nbG(z) :=\nX\nj\u2208J(z)\ng(Uj, Zj) \u00b7 \u03be(z, Zj) ,\nand the matrix M(z) is invertible and, hence, strictly positive de\ufb01nite almost surely for su\ufb03ciently\nlarge K, making the estimator bG(z) uniquely de\ufb01ned, see Section 6.1.1. Thus, we can write\nbG(z) := e\u22a4\n0 M(z)\u22121 bG(z) =\nX\nj\u2208J(z)\ng(Uj, Zj) \u00b7 e\u22a4\n0 M(z)\u22121 \u03be(z, Zj)\n(2.6)\nwith the unit row vector e0 whose component for \u03ba = 0 is 1.\nOur main result requires the following two assumptions.\nAssumption 1 (Support and design density). The support S of Z is a compact and convex subset\nof S \u2286Rd. The diameter of S is called \u03c1\u2032\u2032. The distribution of Z has a Lebesgue-density fZ that is\nbounded away from zero by some \u03c1 > 0 on S and bounded from above by \u03c1.\nAssumption 2 (Exclusion of boundary e\ufb00ects). We assume that the Euclidean balls B\u03c1\u2032(z) with the\ncenter z, for all z \u2208S\u2217, and the radius \u03c1\u2032 > 0 are included in S as a subset.\nLet B\u03c1\u2032(S\u2217) = S\nz\u2208S\u2217B\u03c1\u2032(z) denote the open \u03c1-neighborhood of S\u2217. We assume that the restriction of\nthe regression function G to B\u03c1\u2032(S\u2217) belongs to the H\u00a8older class G(l, \u03b2, CH) with parameters l \u2208N0,\n0 < \u03b2 \u22641, CH > 0 of functions on B\u03c1\u2032(S\u2217) de\ufb01ned as follows:\nG(l, \u03b2, CH) =\nn\nh : B\u03c1\u2032(S\u2217) \u2192R | h has partial derivatives of order \u2264l,\nfor each \u03ba \u2208Nd\n0, with |\u03ba| = l we have that\n(2.7)\n\f\f\u2202\u03bah(z) \u2212\u2202\u03bah(z\u2032)\n\f\f \u2264CH \u2225z \u2212z\u2032\u2225\u03b2,\nz, z\u2032 \u2208B\u03c1\u2032(S\u2217)\no\n.\nFurthermore we set\nK\u2217= K\u2217(d, L) = #K =\nL\nX\nl=0\n\u0012d + l \u22121\nl\n\u0013\n,\nD = D(d, L) =\nL\nX\nl=1\nl\n\u0012d + l \u22121\nl\n\u0013\n.\n(2.8)\nTheorem 1. Consider estimation of the functional \u03a8 in (1.1) in dimensions d \u22652 under the Assump-\ntions 1 and 2. Suppose that the regression function G in (1.2) belongs to the H\u00a8older class G(l, \u03b2, CH)\nin (2.7) with parameters l \u2208N0, 0 < \u03b2 \u22641, CH > 0. Moreover assume that the conditional vari-\nance var\n\u0000g(U1, Z1)|Z1\n\u0001\nis uniformly bounded by a constant CV > 0. In the estimator \u02c6\u03a8(L, K) in (2.4),\n6\nchoose L = l. If L = l = 0 take any \ufb01xed any K \u22651, while for L \u22651 choose some K \u22652+(2D+1) K\u2217,\nwith K\u2217and D de\ufb01ned in (2.8). Then for the mean conditional variance and the mean squared con-\nditional bias given the \u03c3-\ufb01eld \u03c3Z generated by Z1, . . . , Zn we have that\nsup\nG\u2208G(l,\u03b2,CH)\nE\n\u0002\nvar\n\u0000\u02c6\u03a8|\u03c3Z\n\u0001\u0003\n\u2264CV \u00b7 C \u00b7 n\u22121,\nsup\nG\u2208G(l,\u03b2,CH)\nE\n\u0002\f\f E[\u02c6\u03a8|\u03c3Z] \u2212\u03a8\n\f\f2\u0003\n\u2264C2\nH \u00b7 C \u00b7 n\u22122(l+\u03b2)\nd\n, (2.9)\nwhere the constant C > 0 only depends on L, K, \u03c1, \u03c1\u2032, \u03c1\u2032\u2032, \u00af\u03c1, d and \u03bb(S\u2217). Thus, if l + \u03b2 \u2265d/2,\nsup\nG\u2208G(l,\u03b2,CH)\nE\n\u0002\f\f\u02c6\u03a8 \u2212\u03a8\n\f\f2\u0003\n\u2272n\u22121,\nand the contribution of the squared bias is negligible if l + \u03b2 > d/2.\nThe main steps of the proof are provided in Section 6.1, with some technical details being deferred to\nSection B in the supplementary appendix.\nRemark 1 (Choice of L and K). For given dimension d and order of the polynomial approximation\nL, the theorem speci\ufb01es a minimal choice K \u22652 + (2D + 1) K\u2217, K\u2217in (2.8), under which the\nupper risk bounds are theoretically guaranteed. Speci\ufb01cally, the minimal value of K for L = 1 is\nK = 2+(2 d+1) (1+d). For d = 2 the bias is negligible for L = l = 1 if \u03b2 > 0, and are result requires\nK = 17. Similarly, for d = 3 the bias is negligible for L = l = 1 if \u03b2 > 1/2, and we need K = 30 in\nour theoretical result. In the simulations, we illustrate that smaller values of K often seem to su\ufb03ce.\nRemark 2 (Higher moments of the volume of C(J)). For the derivation of upper bounds as in\nTheorem 1, the asymptotic properties of the d-dimensional Lebesgue measure \u03bb(C(J)) of the Voronoi\ncells as n \u2192\u221eare essential. Since\n\u03bb(S\u2217) =\nX\nJ\u2208JK\n\u03bb(C(J))\nand the C(J) are identically distributed we have that E[\u03bb(C(J))] = O(n\u2212K).\nHence \u03bb(C(J)) =\nOP(n\u2212K) which implies \u03bb(C(J))\u2113= OP(n\u2212\u2113K). As a surprising phenomenon the higher moments of\nthis term show an unusual behaviour since E[\u03bb(C(J))\u2113] \u2243n\u2212\u2113\u2212K+1. The upper bound is provided in\nLemma 6.2 for \u2113= 2, and for the lower bound see Section B in the supplementary appendix.\n2.3\nBias corrected matching estimators for \u03a6\nNow let us turn to estimating the functional \u03a6 in (1.3) in the setting where f is unknown. Suppose\nthat we have an additional independent sample X1, . . . , Xm, each Xj with density f, independent of\n(Y1, Z1), . . . , (Yn, Zn). One possibility to estimate (1.3) is to use a nonparametric estimate \u02c6f of f from\nX1, . . . , Xm, and then to employ (2.1) with the estimated function \u02c6g(y, z) = y \u02c6f(z). Alternatively, we\ncan avoid a nonparametric estimate of f and proceed as follows.\nConsider the Voronoi cells as in (2.3), and for J \u2286{1, . . . , n} with #J = K, that is J \u2208JK, and\nz \u2208C(J) we let bG(z) be the least squares polynomial \ufb01t of order L as in (2.6), with the observed Yj\nreplacing g(Uj, Zj). Then we set\nb\u03a6 = b\u03a6(L, K) = 1\nm\nm\nX\nk=1\nX\nJ\u2208JK\n1\n\u0000Xk \u2208C(J)\n\u0001 bG(Xk).\n(2.10)\nConsider the class of functions\nG(l, \u03b2, CH, CG) =\nn\nh : B\u03c1\u2032(S\u2217) \u2192R | h \u2208G(l, \u03b2, CH), |h(z)| \u2264CG, \u2200z \u2208B\u03c1\u2032(S\u2217)\no\n.\n7\nTheorem 2. Consider estimation of the functional \u03a6 in (1.3) in dimensions d \u22652 under the Assump-\ntions 1 and 2. Moreover assume that the conditional variance var\n\u0000Y1|Z1\n\u0001\nis uniformly bounded by a\nconstant CV > 0, and that the density f is bounded from above by Cf. In the estimator \u02c6\u03a6(L, K)\nin (2.10), choose L = l.\nIf L = l = 0 take any \ufb01xed K \u22651, while for L \u22651 choose some\nK \u22652 + (2D + 1) K\u2217, with K\u2217and D de\ufb01ned in (2.8). Then\nsup\nG\u2208G(l,\u03b2,CH,CG)\nE\n\u0002\f\f\u02c6\u03a6 \u2212\u03a6\n\f\f2\u0003\n\u2264C\n\u0000(CV + C2\nG) m\u22121 + CV n\u22121 + C2\nH n\u22122(l+\u03b2)\nd\n\u0001\n,\n(2.11)\nwhere the constant C > 0 only depends on L, K, \u03c1, \u03c1\u2032, \u03c1\u2032\u2032, \u00af\u03c1, d, \u03bb(S\u2217) and Cf. Thus, if l + \u03b2 \u2265d/2,\nsup\nG\u2208G(l,\u03b2,CH,CG)\nE\n\u0002\f\f\u02c6\u03a6 \u2212\u03a6\n\f\f2\u0003\n\u2272n\u22121 + m\u22121,\nand the contribution of the squared bias is negligible if l + \u03b2 > d/2.\nThe proof is provided in Section 6.2.\nRemark 3 (Matching estimators). For L = 0 the estimator in (2.10) is corresponds to the matching\nestimator from the treatment e\ufb00ect literature (Abadie and Imbens, 2006; Imbens, 2004): For k =\n1, . . . , m let {Zj | j \u2208Jk}, Jk \u2286{1, . . . , n} with #Jk = K be the set of K-nearest neighbors of Xk in\nZ1, . . . , Zn, and set \u02c6Yk = K\u22121 P\nj\u2208Jk Yj. Then (2.10) can be written as\nb\u03a6 = 1\nm\nm\nX\nk=1\nX\nJ\u2208JK\n1\n\u0000Xk \u2208C(J)\n\u0001 1\nK\nX\nj\u2208J\nYj = b\u03a6 = 1\nm\nm\nX\nk=1\n\u02c6Yk.\nSee Section 3.3 for further discussion of estimating average treatment e\ufb00ects.\n2.4\nDiscussion of lower bounds\nIn the oracle setting when fZ is perfectly known the usual parametric rate is always attained by the\nestimator\n\u02c6\u03a8oracle := 1\nn\nn\nX\nj=1\ng(Uj, Zj)/fZ(Zj) ,\nunder the Assumptions 1 and 2 and the condition that var(g(U1, Z1)|Z1) is bounded, where no smooth-\nness constraints are required.\nOn the other hand we can show, inspired by techniques from quadratic functional estimation (e.g.\nTsybakov (2009)), that, under unknown fZ, the parametric rate cannot be kept when no smoothness\nconditions are imposed. This occurs even in the univariate setting, as demonstrated in the following\ntheorem.\nTheorem 3. Consider the classical univariate nonparametric regression model Uj = h(Zj) + \u03b5j,\nj = 1, . . . , n. Impose that both the design density fZ is contained in the class F = F(\u03c1, \u03c1) of all\nmeasurable functions on [0, 1] which are bounded from above by \u03c1 > 1 and from below by \u03c1 > 0; the\nregression function h lies in the class H = H(\u02dc\u03c1) of all measurable functions bounded by \u02dc\u03c1 > 0; and the\nregression errors are \u03b5j are standard Gaussian and independent of Zj. Then,\nlim inf\nn\u2192\u221en1/2 \u00b7\nsup\nfZ\u2208F,h\u2208H\nEfZ,h\n\f\f\f \u02c6Hn \u2212\nZ 3/4\n1/4\nh(x)dx\n\f\f\f\n2\n> 0 ,\nfor any sequence of estimators \u02c6Hn based on the data (Uj, Zj), j = 1, . . . , n.\n8\nThe proof is given in Section A. Note that the regression model in Theorem 3 is included in the general\nframework by putting g(u, z) = u and the conditional density fU|Z(u|z) = f\u03b5(u\u2212h(z)) when h denotes\nthe regression function and f\u03b5 stands for the standard normal density.\nRemark 4 (Continuous design density and regression functions). We mention that the claim of\nTheorem 3 still holds when the classes F and H are changed to the corresponding sub-classes F0 and\nH0 which contain only the continuous functions in F and H, respectively, as F0 and H0 are dense\nsubsets with respect to the L1-metric.\nRemark 5 (Fixed design). Linear functional estimation in multivariate regression with \ufb01xed design\nis not included in the general framework of this work. In particular, Assumption 1 is violated. Still\nwe mention that, in this setting, no estimator can attain faster convergence rates than n\u22122(l+\u03b2)/d as\nthey occur as an upper bound in Theorem 1. This can be seen as follows. Assume that the design\npoints Z1, . . . , Zn form an equidistant grid of some d-dimensional cube S so that \u2225Zj \u2212Zk\u2225\u2265c\u00b7n\u22121/d\nfor all j \u0338= k and some \ufb01xed constant c > 0. Consider the competing regression functions h0 \u22610\nand hn(z) := Pn\nj=1 bl+\u03b2 K(\u2225z \u2212Zj\u2225/b) for some non-negative kernel function K which is in\ufb01nitely\noften di\ufb00erentiable on R, supported on [0, 1] and satis\ufb01es K(0) = 0 as well as K(1/2) > 0, while\nb = (c/2) \u00b7 n\u22121/d. Note that h0 and hn lead to the same distribution of the data, whereas the (non-\nsquared) distance between\nR\nS\u2217h0 and\nR\nS\u2217hn is bounded from below by the rate n\u00b7bl+\u03b2 \u00b7bd \u224dn\u2212(l+\u03b2)/d\nfor any open non-void subset S\u2217of S. Note that these arguments cannot be applied to the random\ndesign case as, then, the competing regression functions must not depend on the design variables\nZ1, . . . , Zn. The rate n\u2212(l+\u03b2)/d also occurs in Kohler (2014) for the L1-risk in noiseless regression.\n3\nApplications\nIn this section we present various applications of our methodology. In Section 3.1 we discuss series\nestimators in a given orthogonal system with respect to the Lebesgue measure, where for random\ndesign the Fourier coe\ufb03cients are of the form (1.1). We give an explicit analysis in the nonparametric\nBerkson errors in variables problem (Delaigle et al., 2006), where our estimate is an ingredient of a\nFourier series deconvolution estimator. In Section 3.2 we indicate how estimating functionals as in\n(1.1) arise in non- and semiparametric estimators in random coe\ufb03cients regression models.\nSections 3.3 on estimating average treatment e\ufb00ects and 3.4 on transfer learning involve the functional\n(1.3) with unknown density f0, for which as in Section 2.3 an additional sample is available. For the\naverage treatment e\ufb00ect on the treated and for the average treatment e\ufb00ect over a subset of the\ncovariate space we achieve a bias correction resulting in \u221an-consistent estimators without involving\nnonparametric estimates of the regression functions and under much milder smoothness assumptions\nthan in Abadie and Imbens (2011).\n3.1\nSeries estimators and Berkson errors in variables models\nSeries estimators of a regression function h(x) = E[Y |X = x] in a given orthogonal basis (\u03c6n) or more\ngenerally a frame in L2(\u00b5) for some measure \u00b5 require estimation of\n\u27e8h, \u03c6n\u27e9\u00b5 =\nZ\nh \u03c6n d\u00b5.\n9\nOften in the mathematical analysis it is simply assumed that \u00b5 is the distribution of the covariates\nX, so that \u27e8m, \u03c6n\u27e9= E[Y \u03c6n(X)] can directly be estimated by an average. But if \u00b5 is a given measure\nsuch as the Lebesgue measure \u03bb on some compact domain determined by an a-priori choice of the\n(\u03c6n) as e.g. the Fourier basis, wavelets (Kerkyacharian and Picard, 2004) or Gabor frames from time-\nfrequency analysis (Dahlke et al., 2022), this may not be a realistic assumption for random X. Then,\nassuming X has Lebesgue density fX, we rather have\n\u27e8h, \u03c6n\u27e9\u03bb = E\nhY \u03c6n(X)\nfX(X)\ni\n,\n(3.1)\nleading to the estimation problem considered in (1.1) when putting g(u, z) = u \u00b7 \u03c6n(z).\nLet us illustrate this further in the nonparametric Berkson errors-in-variables model, in which one\nobserves (U, Z) according to\nU = h(Z + \u03b4) + \u01eb,\n(3.2)\nwhere \u01eb, \u03b4 and Z are independent and \u01eb is centered and square integrable. When estimating h by\ndeconvolution methods, Delaigle et al. (2006) use a local linear estimator of the calibrated regression\nfunction g(z) = E[U|Z = z] and numerically compute its Fourier coe\ufb03cients. Meister (2009, Section\n3.4.2) shows how to avoid nonparametric estimation of g in one dimension d = 1 by using spacings.\nHere we extend these results to higher dimensions d \u22652 using our novel approach.\nWe choose the Lebesgue measure \u00b5 = \u03bb and the Fourier basis\n\u03c6j(x) = exp\n\u0010\ni\nd\nX\nk=1\njkxk\n\u0011\n,\nx = (x1, . . . , xd)\nand the multi-index j = (j1, . . . , jd) where all components jk, k = 1, . . . , d, are integers satisfying\n|jk| \u2264Jn for some smoothing parameter Jn. Assume that \u03b4 has a known d-dimensional Lebesgue\ndensity and let \u02dcf\u03b4 denote the density of \u2212\u03b4. We derive our result under the following set of assumptions.\nAssumption 3. Impose that h and f\u03b4 are supported on some \ufb01x compact subset of (\u2212\u03c0, \u03c0)d; that the\nsupport of the convolution h \u2217\u02dcf\u03b4 is included in some \ufb01xed compact set S\u2217, which is contained in the\ninterior of the compact and convex support S of fZ as a subset; that fZ is bounded away from zero\non S (such that the Assumptions 1 and 2 are satis\ufb01ed); that the known error density f\u03b4 is ordinary\nsmooth, i.e. its Fourier coe\ufb03cients f ft\n\u03b4 (j) := \u27e8f\u03b4, \u03c6j\u27e9\u03bb satisfy\nc\u03b4 \u00b7 (1 + \u2225j\u2225)\u2212\u03b3 \u2264\n\f\ff ft\n\u03b4 (j)\n\f\f \u2264C\u03b4 \u00b7 (1 + \u2225j\u2225)\u2212\u03b3 ,\nfor all j and some constants 0 < c\u03b4 < C\u03b4 and \u03b3 > d/2, which guarantees that f\u03b4 is squared integrable\non the whole of Rd.\nNow apply our estimator \u02c6\u03a8 = \u02c6\u03a8j in (2.4) for\ng(u, z) = gj(u, z) = u \u00b7 \u03c6j(z) ,\nso that G(z) = Gj(z) = \u03c6j(z) \u00b7\n\u0002\nh \u2217\u02dcf\u03b4\n\u0003\n(z) , and S\u2217as in Assumption 3. Finally we consider the\nestimator\n\u02c6h = (2\u03c0)\u2212d X\nj\u2208J(n)\n\u02c6\u03a8j \u00b7 \u03c6\u2212j/f ft\n\u03b4 (\u2212j)\nof the regression function h, where J(n) := {\u2212Jn, . . . , Jn}d.\n10\nTheorem 4. Consider the Berkson errors in variables model (3.2) under Assumption 3 for d \u22652.\nFurthermore suppose that the regression function h ful\ufb01lls the Sobolev condition\nX\nj\n|hft(j)|2 (1 + \u2225j\u22252\u03b1) \u2264C\u03b1 ,\n(3.3)\nfor constants \u03b1, C\u03b1 > 0, where \u03b1 is assumed to be su\ufb03ciently large such that\n4(\u03b1 \u22121)(\u03b1 + \u03b3) \u2265d(2\u03b1 + 2\u03b3 + d) .\nThen under the smoothing regime Jn \u224dn1/(2\u03b1+2\u03b3+d) we obtain uniformly over the class (3.3)\nE\nh Z\n[\u2212\u03c0,\u03c0]d |\u02c6h(x) \u2212h(x)|2dx\ni\n\u2272n\u22122\u03b1/(2\u03b1+2\u03b3+d).\n(3.4)\nThe proof is provided in Section C.\n3.2\nRandom coe\ufb03cient regression\nRandom coe\ufb03cient regression models form another \ufb01eld of application of our methods. We consider\nthe linear random coe\ufb03cient regression model\nY = \u03b1 + \u03b2\u22a4X,\n(3.5)\nwhere Y is the observed scalar response, X is a d-dimensional covariate vector, and \u03b2 \u2208Rd and \u03b1 \u2208R\nare random regression coe\ufb03cients, with (\u03b1, \u03b2) being independent of X. Note that \u03b1 contains the\ndeterministic intercept \u03b20 and the centered random errors \u01eb, that is \u03b1 = \u03b20 + \u01eb. The joint density f\u03b1,\u03b2\nis of interest and to be estimated. The model (3.5) contains heterogeneity in a linear equation, as is\noften required in econometric applications.\nGaillac and Gautier (2022) propose an estimator which allows for compactly supported covariates X.\nThey use the identi\ufb01ed relation\nE[exp(itY )|X = x] = f ft\n\u03b1,\u03b2(t, tx),\nwhere f ft\n\u03b1,\u03b2 is the (d + 1) - dimensional Fourier transform of f\u03b1,\u03b2. Their estimator involves a series\nestimator of a partial Fourier transform of f\u03b1,\u03b2 in the \ufb01rst coordinate using the prolate spheroidal wave\nfunctions as basis. Thus, estimates of a form related to (3.1) from the previous section are required,\nto which our method can be applied.\nHoderlein et al. (2017) consider a triangular model with random coe\ufb03cients. In case of additional\nexogenous covariates their semiparametric estimation method uses the simple estimator of the form\n(2.1) without higher-order bias correction to estimate the contrast function.\nIn their analysis of model (3.5), Hoderlein et al. (2010) normalize the covariates to the sphere (upper\nhemisphere) by setting S = (1, X)/\u2225(1, X)\u2225and U = Y/\u2225(1, X)\u2225. Setting (\u03b1, \u03b2\u22a4) =: \u00af\u03b2\u22a4this leads to\nthe equation\nU = S\u22a4\u00af\u03b2,\nin which S and \u00af\u03b2\u22a4remain independent. A regularized approximation to f \u00af\u03b2 is then given by\n\u00aff \u00af\u03b2(b; h) =\nZ\nSd E\n\u0002\nKh,p(S\u22a4b \u2212U)|S = s\n\u0003\nd\u00b5(s)\n(3.6)\n11\nwhere \u00b5 is the Lebesgue measure on the d -dimensional sphere Sd, h > 0 is a bandwidth parameter,\nand Kh,d is a kernel tailored to the estimation problem. See Hoderlein et al. (2010) for the formula.\n(3.6) is of similar form to (1.1), but replacing the integral over Euclidean space with an integral\nover the sphere. Similar expressions involving integrals of regression function over the sphere occur\nin Dunker et al. (2019) or in the analysis of the binary choice model with random coe\ufb03cients in\nGautier and Kitamura (2013).\nAn extension of our estimation approach to such situations would\ntherefore be of some interest.\n3.3\nAverage treatment e\ufb00ects\nLet us discuss how the bias-corrected matching estimator in Section 2.3 can be applied to estimate\naverage treatment e\ufb00ects in the potential outcome framework. Our methods complement the semi-\nnal work of Abadie and Imbens (2006, 2011); Imbens (2004) in several aspects. First we show that\nbias correction can be achieved without involving nonparametric estimators.\nFurther we quantify\na precise \ufb01nite amount of smoothness of the regression functions to achieve \u221an-consistent estima-\ntion, without assuming in\ufb01nite smoothness as in Abadie and Imbens (2011). Moreover, our results\non lower bounds show that some amount of smoothness of design densities or regression functions\nis required for \u221an-consistent estimation to be possible. Recently, Lin et al. (2023) further analyzed\nthe bias-corrected estimator of Abadie and Imbens (2011) in case of diverging K and under milder\nsmoothness assumptions (Assumptions 4.4 and 4.5 in Lin et al. (2023)) which seem to be closer to but\nsill stronger than those imposed in Theorem 2. Further, Lin et al. (2023) related to the recent liter-\nature on debiased/ doubly robust machine learning methods for treatment e\ufb00ects (Wang and Shah,\n2024; Chernozhukov et al., 2018, 2024).\nConsider the potential outcome framework: We have i.i.d. observations (Yi, Zi, Di), i = 1, . . . , n,\ndistributed as the generic (Y, Z, D). Here Y is the real-valued target quantity, Z a covariate vector,\nand D the binary treatment indicator. Suppose that the random variables Y (0) and Y (1) denote the\npotential outcomes, and that the observed outcome is generated as Y = Y (D).\nFirst consider estimation of the average treatment e\ufb00ect on the treated (ATET)\n\u03c4 t = E\n\u0002\nY (1) \u2212Y (0) | D = 1\n\u0003\n.\nAssuming that Y (0) and D are independent given Z (conditional ignorability), and that P(p(Z) < 1) =\n1, where p(Z) = P(D = 1|Z) is the propensity score, the ATET is identi\ufb01ed as (Chernozhukov et al.,\n2024, Section 5.C)\n\u03c4 t = E\n\u0002\nY |D = 1\n\u0003\n\u22121\n\u03c0 E\nh\n1(D = 0) Y\np(Z)\n1 \u2212p(Z)\ni\n:= \u03c4 t\n1 \u2212\u03c4 t\n0,\nwhere \u03c0 = P(D = 1). Let n1 = #{i | Di = 1}, n0 = n \u2212n1. We estimate \u03c4 t\n1 by the simple average\n\u02c6\u03c4 t\n1 = n\u22121\n1\nPn\ni=1 Yi1(Di = 1). Next assume that the conditional distribution of Z given D = i has\nLebesgue density fi, i = 0, 1. Then we can write\n\u03c4 t\n0 = E\nh\nY f1(Z)\nf0(Z)\n\f\fD = 0\ni\n.\nWe estimate this parameter by \u02c6\u03c4 t\n0 with the estimator (2.10), using those n0-observations (Yi, Zi) for\nwhich Di = 0, and as sample from f1 (which has the role of f in (1.3)) we take those Zi for which\nDi = 1. Finally set\n\u02c6\u03c4 t = \u02c6\u03c4 t\n1 \u2212\u02c6\u03c4 t\n0.\n12\nAssumption 4. The density f0 satis\ufb01es Assumption 1 for fZ.\nFurther, the support S\u2217of f1 is\ncontained in the support S as in Assumption 2.\nMoreover, f1 bounded, the conditional variance\nvar(Y |Z = z, D = 0) is uniformly bounded.\nUnder Assumption 4, if the regression function G0(z) = E[Y |Z = z, D = 0] is contained in G(l, \u03b2, CH, CG),\nwhen choosing K and L as in Theorem 2 in \u02c6\u03c4 t\n0, we obtain, conditionally on the treatment variables,\nthat\nE\n\u0002\u0000\u02c6\u03c4 t \u2212\u03c4 t\u00012\f\f\u03c3D\n\u0003\n\u2272n\u22121\n1\n+ n\u22121\n0\n+ n\n\u22122(l+\u03b2)\nd\n0\n,\nwhere \u03c3D = \u03c3{D1, . . . , Dn}. If l + \u03b2 \u2265d/2 we obtain a bound of order n\u22121\n1\n+ n\u22121\n0 . If n0 is large\ncompared to n1, n\u22122(l+\u03b2)/d\n0\nmight be small compared to n1 even for lower smoothness of G0. In an\nasymptotic analysis, this would require the distribution of D to depend on n with P(D = 1) tending\nto zero, see Assumption 3\u2019 in Abadie and Imbens (2006).\nNow let us turn brie\ufb02y to the average treatment e\ufb00ect (ATE). Here we require conditional ignorability\nfor both Y (1) and Y (0), that is Y (d) and D are independent given Z, d = 0, 1, and that P(0 < p(Z) <\n1) = 1. Let S\u2217be a subset of the support of the covariate Z. Then the average treatment e\ufb00ect over\nS\u2217is\n\u03c4S\u2217= E\n\u0002\nY (1) 1(Z \u2208S\u2217)\n\u0003\n\u2212E\n\u0002\nY (0) 1(Z \u2208S\u2217)\n\u0003\nThe average potential outcome of Y (1) over S\u2217is identi\ufb01ed as\nE\n\u0002\nY (1) 1(Z \u2208S\u2217)\n\u0003\n= E\nh\nY 1(D = 1) 1(Z \u2208S\u2217)\np(Z)\ni\n= E\n\u0002\nY 1(D = 1) 1(Z \u2208S\u2217)\n\u0003\n+ (1 \u2212\u03c0) E\nh\nY f0(X)\nf1(X) 1(Z \u2208S\u2217)| D = 1\ni\n,\n(3.7)\nwith a similar expression for E\n\u0002\nY (1) 1(Z \u2208S\u2217)\n\u0003\n. As for ATET we can apply the estimator (2.10) to\nthe second expression and under Assumption 4 for both f0 and f1 obtain the \u221an - rate of convergence.\n3.4\nReweighting in transfer learning under covariate shift\nIn the transfer learning problem for classi\ufb01cation, we have the target problem from a distribution\n(X, Y ) as well as the source problem (XS, Y S). Here Y and Y S are binary variables, which are to\npredicted from the covariate Z respectively ZS.\nInterest focuses on the classi\ufb01cation problem for\n(X, Y ), but labeled data are mainly or only available from the source problem (ZS\ni , Y S\ni ), i = 1, . . . , nS,\ntogether with unlabeled data Zi, i = 1, . . . , n from the target sample, that is with distribution PZ.\nUnder covariate shift it is assumed that the conditional distributions PY |X and PY S|XS are equal,\nand only the marginal distributions of covariates, PX and PXS di\ufb00er. See Kouw and Loog (2019);\nPortier et al. (2023); Sugiyama et al. (2007)\nIf \u2113(\u02c6y, y) is a loss function, the goal would be to train a classi\ufb01er h which minimizes the expected\nloss under the target distribution E[\u2113(h(X), Y )], which is however not directly accessible due to in-\nsu\ufb03cient trained data from the target classi\ufb01cation problem. Suppose that the distributions PX and\nPXS have Lebesgue densities fX and fXS, respectively. Then using the equality of the conditional\ndistributions PY |X and PY S|XS we have that the average loss for the target problem can be computed\nas a importance-weighted average of the source problem,\nE[\u2113(h(X), Y )] = E\nh fX(XS)\nfXS(XS) \u2113\n\u0000h(XS), Y S\u0001i\n,\n(3.8)\n13\nwhere the second expected value is over the source distribution, for which a labeled sample is available.\n(3.8) is of the form (1.3), and we can apply the matching estimator (2.10).\n4\nSimulations\nIn a brief simulation study we illustrate the practical feasibility of our methods.\nTo numerically\ncompute the estimator \u02c6\u03a8 in (2.4), we simulate x\u2217\n1, . . . , x\u2217\nm from the uniform distribution on S\u2217for\nlarge m, and then e\ufb00ectively compute the estimator \u02c6\u03a6 in (2.10) with f = 1 and the x\u2217\nj taking the role\nof observed Xj\u2019s, and normalize by \u03bb(S\u2217). Fast implementations to compute the K-nearest neighbors\nin Z1, . . . , Zn for a given set x\u2217\n1, . . . , x\u2217\nm of m points are available, also for moderately high values of\nK. We use the function kNN from the R-library dbscan, consider regressors in d = 2 and d = 3, and\nfocus on the orders L = 0 and L = 1 for polynomial approximation. Computing the estimator once\nfor n = 1000 and m = 10000 takes about 1 second on a computer with i7-10700 CPU, 2.90 GHz and\n32GB RAM.\nIn a simulation in d = 2, we choose the coordinates Zi,1 and Zi,2 of the regressors as independent and\nBeta-distributed with parameters \u03b1 = \u03b2 = 3 and S\u2217= [0.2, 0.8]2, and as regression function we take\nf1(z1, z2) = 5 z1 (z2\n2 + 1) + 10 cos(z1/z2).\nThe DGP then is Yi = f1(Zi,1, Zi,2) + 0.1 \u03b5i, with \u03b5i independent standard normal. We use sample\nsizes n \u2208{100, 1000}, m = 10000 for the generated sample x\u2217\n1, . . . , x\u2217\nm and N = 1000 repetitions\nin each scenario. We compute the true value of the functional by numerical integration and obtain\n\u03a81 = 2.4981. In Table 1 we report the values of the empirical bias, standard deviation and root mean\nsquared error over the N = 1000 repetitions, all scaled by \u221an. Overall, while the reduction in MSE\nfrom L = 0 to L = 1 is not that large in this setting, there seems to be some reduction in asymptotic\nbias when using the linear approximation over Voronoi cells.\nThe method does not seem to depend sensitively on the choice of K for L = 1 for n = 1000, while\nfor n = 100 a small choice of K = 5 performs best in this simulation setting. In the supplementary\nappendix, Section D we present another setting in d = 2 where higher values of K are required for\noptimal performance.\nFor a simulation with d = 3 we again choose coordinates Zi,1, Zi,2 and Zi,3 of regressors as independent\nand Beta-distributed with parameters \u03b1 = \u03b2 = 3, and let S\u2217= [0.2, 0.8]3. The regression function is\nchosen as\nf2(z1, z2, z3) = exp(2 cos(7 z1) sin(7 z2)) \u00b7 (4 \u22128 (z3 \u22120.5)2),\nand as DGP we take Yi = f2(Zi,1, Zi,2, Zi,3)+0.4 \u03b5i, with \u03b5i independent standard normally distributed.\nAgain we use N = 1000 repetitions in each scenario, and sample sizes n \u2208{100, 1000} and m = 10000\nfor the generated sample x\u2217\n1, . . . , x\u2217\nm. The results are displayed in Table 2. The true value in this\nsetting is \u03a83 = 1.4176. While, for the smaller sample size n = 100, using L = 0 is preferable, for\nn = 1000 using L = 1 leads to a substantial reduction in the bias.\n5\nConcluding remarks\nIn contrast to other proposals in literature our estimators for expected values weighted by the inverse\nof a multivariate density do not rely on nonparametric function estimators and therefore do not require\n14\nn = 100\nL = 0\nK\n1\n3\n4\n6\n8\n10\n\u221an\u00b7 BIAS\n0.1744\n0.3074\n0.3703\n0.4858\n0.5875\n0.6814\n\u221an\u00b7 STDV\n0.3498\n0.4536\n0.4976\n0.5740\n0.6388\n0.6934\n\u221an\u00b7 RMSE\n0.3908\n0.5479\n0.6202\n0.7520\n0.8679\n0.9722\nL = 1\nK\n4\n5\n6\n8\n10\n12\n\u221an\u00b7 BIAS\n0.0188\n0.0136\n0.0029\n-0.0150\n-0.0345\n-0.0606\n\u221an\u00b7 STDV\n0.4072\n0.3862\n0.4075\n0.4534\n0.4992\n0.5420\n\u221an\u00b7 RMSE\n0.4076\n0.3865\n0.4075\n0.4536\n0.5004\n0.5454\nn = 1000\nL = 0\nK\n1\n3\n4\n6\n8\n10\n\u221an\u00b7 BIAS\n0.0541\n0.1064\n0.1336\n0.1872\n0.2397\n0.2933\n\u221an\u00b7 STDV\n0.5869\n0.5843\n0.5841\n0.5876\n0.5920\n0.5960\n\u221an\u00b7 RMSE\n0.5894\n0.5939\n0.5992\n0.6167\n0.6387\n0.6643\nL = 1\nK\n4\n5\n6\n8\n10\n12\n\u221an\u00b7 BIAS\n0.0203\n0.0250\n0.0314\n0.0451\n0.0593\n0.0727\n\u221an\u00b7 STDV\n0.5856\n0.5797\n0.5792\n0.5785\n0.5783\n0.5785\n\u221an\u00b7 RMSE\n0.5859\n0.5802\n0.5800\n0.5802\n0.5813\n0.5831\nTable 1: Simulation scenario with f1 in d = 2\nn = 100\nL = 0\nK\n1\n3\n6\n8\n10\n12\n\u221an\u00b7 BIAS\n0.3265\n0.5211\n0.7081\n0.7912\n0.8535\n0.9004\n\u221an\u00b7 STDV\n0.7086\n0.8674\n1.0261\n1.1015\n1.1625\n1.2141\n\u221an\u00b7 RMSE\n0.7802\n1.0119\n1.2467\n1.3562\n1.4421\n1.5116\nL = 1\nK\n5\n6\n8\n10\n12\n14\n\u221an\u00b7 BIAS\n-0.4611\n-0.5416\n-0.6972\n-0.8296\n-0.9426\n-1.0446\n\u221an\u00b7 STDV\n0.7106\n0.6545\n0.6654\n0.7009\n0.7417\n0.7827\n\u221an\u00b7 RMSE\n0.8471\n0.8495\n0.9637\n1.0861\n1.1995\n1.3053\nn = 1000\nL = 0\nK\n1\n3\n6\n8\n10\n12\n\u221an\u00b7 BIAS\n0.2910\n0.4584\n0.6606\n0.7731\n0.8766\n0.9726\n\u221an\u00b7 STDV\n0.5137\n0.5482\n0.6058\n0.6386\n0.6715\n0.7009\n\u221an\u00b7 RMSE\n0.5904\n0.7146\n0.8963\n1.0028\n1.1043\n1.1988\nL = 1\nK\n5\n6\n8\n10\n12\n14\n\u221an\u00b7 BIAS\n-0.1133\n-0.1336\n-0.1844\n-0.2320\n-0.2798\n-0.3284\n\u221an\u00b7 STDV\n0.5013\n0.4336\n0.4330\n0.4338\n0.4360\n0.4382\n\u221an\u00b7 RMSE\n0.5139\n0.4537\n0.4707\n0.4919\n0.5180\n0.5476\nTable 2: Simulation scenario with f2 in d = 3\n15\nthe choice of data-dependent smoothing parameters. In our analysis we focus on risk bounds and the\namount of smoothness of the regression functions required for the parametric rate. A further novel\ncontribution, previously not available in the literature, are the lower bounds which we provide and\nwhich show that, for unknown design density, some smoothness is required for the parametric rate\nto be attainable.\nOur analysis is thus complementary to much of the literature which focuses on\nasymptotic normality, testing and e\ufb03ciency issues.\nAssumption 1 is sometimes called the strong density assumption in the statistics literature (Audibert and Tsybakov,\n2007). For nonparametric density estimation in random coe\ufb03cient models in d = 1, Holzmann and Meister\n(2020) consider the e\ufb00ect on the minimax rates of a design density which tends to zero on the bound-\nary of the support. Their method could be adapted to study estimation of the functionals \u03a8 and \u03a6\nin d = 1 under such weaker design assumptions. Multivariate extensions of such an analysis would\nbe of some interest. A point of departure could be the case d = 2, in which removal of the bias with\npolynomial approximation is not required at least to obtain the parametric rates. A further extension\nof some interest would be to estimate functionals \u03a8 in which the integral is over more abstract compact\nmanifolds, such as the unit sphere equipped with spherical Lebesgue measure, with the polynomial\napproximation being taken locally in the tangent space.\n6\nProofs\n6.1\nProof of Theorem 1\nWe use the notation introduced at the beginning of Section 2.2. Recall the estimator (2.4),\n\u02c6\u03a8 =\nX\nJ\u2208JK\nZ\nC(J)\nbG(z) dz ,\nwhere the Voronoi cells C(J) are given in (2.3), and bG(z) in (2.6).\nAgain consider the expected\nconditional variance and expected squared conditional bias decomposition of the mean squared error\n(2.2),\nE\n\u0002\f\f\u02c6\u03a8 \u2212\u03a8\n\f\f2\u0003\n= E\n\u0002\nvar\n\u0000\u02c6\u03a8|\u03c3Z\n\u0001\u0003\n+ E\n\u0002\f\f E(\u02c6\u03a8|\u03c3Z) \u2212\u03a8\n\f\f2\u0003\n,\nwhere \u03c3Z is the \u03c3-\ufb01eld generated by Z1, . . . , Zn\nWe provide the main steps in the proof of the bound (2.9) in Theorem 1 for the expected conditional\nvariance term in Section 6.1.1, and for the expected squared conditional bias in Section 6.1.2. Section\nB contains further technical details.\n6.1.1\nExpected conditional variance in (2.9)\nWe show the \ufb01rst inequality in (2.9), and proceed in the three steps.\nIn Step 1 we make use of conditional independence of the Y1, . . . , Yn given \u03c3Z, of the independence of\ntwo random Voronoi cells C(J) and C(J\u2032) for J \u2229J\u2032 = \u2205, as well as of the assumption of a bounded\nconditional variance var(g(U, Z)|Z).\nIn Step 2 we derive a tight bound on E [\u03bb(C(J)\n\u0001\n\u03bb\n\u0000C(J\u2032))] for J \u2229J\u2032 \u0338= \u2205, and conclude in the case\nL = 0.\n16\nFinally, in Step 3 we use a bound on the conditional moments of (e\u22a4\n0 MJ(z)\u22121e0) from Lemma 6.3,\nproven in Section B, to extend the bound to L \u22651. Here, the invertability of M(z) almost surely for\nsu\ufb03ciently large K follows from Lemma B.3 in Section B in the supplementary appendix.\nStep 1: Using conditional independence\nNote that if J(z) and J(z\u2032) are disjoint, the estimates bG(z) and bG(z\u2032) are independent conditionally\non \u03c3Z. Therefore\nE\n\u0002\nvar\n\u0000\u02c6\u03a8|\u03c3Z\n\u0001\u0003\n= E\nh\nX\nJ,J\u2032\u2208JK\n1{J \u2229J\u2032 \u0338= \u2205}\nZ\nC(J)\nZ\nC(J\u2032)\ncov\n\u0000 bG(z), bG(z\u2032) | \u03c3Z\n\u0001\ndz dz\u2032i\n\u2264\nX\nJ,J\u2032\u2208JK\n1{J \u2229J\u2032 \u0338= \u2205} E\nh Z\nC(J)\n\b\nvar\n\u0000 bG(z)|\u03c3Z\n\u0001\t1/2dz \u00b7\nZ\nC(J\u2032)\n\b\nvar\n\u0000 bG(z\u2032)|\u03c3Z\n\u0001\t1/2dz\u2032i\n(6.1)\nby applying the Cauchy-Schwarz inequality conditionally on \u03c3Z in the second step. By conditional\nindependence of the g(Uj, Zj), j = 1, . . . , n given \u03c3Z, observing (2.6) for all z \u2208S\u2217we obtain\nvar\n\u0000 bG(z)|\u03c3Z\n\u0001\n=\nX\nj\u2208J(z)\nvar\n\u0000g(Uj, Zj)|\u03c3Z\n\u0001\n\u00b7 e\u22a4\n0 M(z)\u22121 \u03be(z, Zj) \u03be(z, Zj)\u22a4M(z)\u22121 e0\n\u2264CV \u00b7 e\u22a4\n0 M(z)\u22121e0 .\n(6.2)\nTherefore, (6.1) can be upper bounded by\nE\n\u0002\nvar\n\u0000\u02c6\u03a8|\u03c3Z\n\u0001\u0003\n(6.3)\n\u2264CV\nX\nJ,J\u2032\u2208JK\n1{J \u2229J\u2032 \u0338= \u2205}\nZZ\n(S\u2217)2 E\nh\n1C(J)(z) 1C(J\u2032)(z\u2032)\n\u0000e\u22a4\n0 M(z)\u22121e0\n\u00011/2\u0000e\u22a4\n0 M(z\u2032)\u22121e0\n\u00011/2i\ndz dz\u2032.\nTo deal with the inner expected value, for J \u2208JK and z \u2208S\u2217let us introduce\nMJ(z) :=\nX\nj\u2208J\n\u03be(z, Zj) \u03be(z, Zj)\u22a4=\n\u0010 X\nj\u2208J\n\u03be\u03ba+\u03ba\u2032(z, Zj)\n\u0011\n\u03ba,\u03ba\u2032\u2208K ,\n(6.4)\nso that MJ(z)(z) = M(z). Then for all z, z\u2032 \u2208S\u2217and J, J\u2032 \u2208JK, since 1C(J)(z) = 1 if and only if\nJ = J(z) we have that\nE\n\u0002\n1C(J)(z) 1C(J\u2032)(z\u2032)\n\u0000e\u22a4\n0 M(z)\u22121e0\n\u00011/2\u0000e\u22a4\n0 M(z\u2032)\u22121e0\n\u00011/2\u0003\n= E\n\u0002\n1C(J)(z) 1C(J\u2032)(z\u2032)\n\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u00011/2\u0000e\u22a4\n0 MJ\u2032(z\u2032)\u22121e0\n\u00011/2\u0003\n= E\nh\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u00011/2 \u0000e\u22a4\n0 MJ\u2032(z\u2032)\u22121e0\n\u00011/2 P\n\u0000z \u2208C(J), z\u2032 \u2208C(J\u2032) | Zk , k \u2208J \u222aJ\u2032\u0001 i\n.\n(6.5)\nFurther,\nP\n\u0000z \u2208C(J), z\u2032 \u2208C(J\u2032) | Zk , k \u2208J \u222aJ\u2032\u0001\n= P\n\u0000\u2225z \u2212Zj\u2225< \u2225z \u2212Zk\u2225, \u2225z\u2032 \u2212Zj\u2032\u2225< \u2225z\u2032 \u2212Zk\u2032\u2225, \u2200j \u2208J, j\u2032 \u2208J\u2032, k \u0338\u2208J, k\u2032 \u0338\u2208J\u2032 | Zk , k \u2208J \u222aJ\u2032\u0001\n\u2264P\n\u0000\u2225z \u2212Zj\u2225< \u2225z \u2212Zk\u2225, \u2225z\u2032 \u2212Zj\u2032\u2225< \u2225z\u2032 \u2212Zk\u2032\u2225, \u2200j \u2208J, j\u2032 \u2208J\u2032, k, k\u2032 \u0338\u2208J \u222aJ\u2032 | Zk , k \u2208J \u222aJ\u2032\u0001\n=\n\u0010\n1 \u2212PZ\n\u0000Bd\n\u0000z, max\nj\u2208J \u2225z \u2212Zj\u2225\n\u0001\n\u222aBd\n\u0000z\u2032, max\nj\u2208J\u2032 \u2225z\u2032 \u2212Zj\u2225\n\u0001\u0001\u0011n\u2212#J\u222aJ\u2032\n,\n(6.6)\n17\nwhen we write PZ for the image measure of Z1, and Bd(x, r) denotes the d-dimensional Euclidean ball\naround x with the radius r. For \ufb01xed z, z\u2032 \u2208S\u2217and J, J\u2032 \u2208JK, put\n\u03b1 := \u03b1(z, z\u2032, J, J\u2032) := max\n\b\nmax\nj\u2208J \u2225z \u2212Zj\u2225, max\nj\u2208J\u2032 \u2225z\u2032 \u2212Zj\u2225\n\t\n.\n(6.7)\nThen using Assumptions 1 and 2, (6.6) is bounded from above by\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 \u03c1 \u00b7 \u03c0d/2 \u00b7 min{\u03c1\u2032, \u03b1}d/\u0393(d/2 + 1)\n\u0001\n,\nand using the Cauchy-Schwarz inequality (6.5) is smaller or equal to\n\u0010\nE\nh\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u0001\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 \u03c1 \u00b7 \u03c0d/2 \u00b7 min{\u03c1\u2032, \u03b1}d/\u0393(d/2 + 1)\n\u0001i\u00111/2\n\u00b7\n\u0010\nE\nh\u0000e\u22a4\n0 MJ\u2032(z\u2032)\u22121e0\n\u0001\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 \u03c1 \u00b7 \u03c0d/2 \u00b7 min{\u03c1\u2032, \u03b1}d/\u0393(d/2 + 1)\n\u0001 i\u00111/2\n.\n(6.8)\nTo further bound (6.8) we make use of the fact that the non-zero terms in (6.3) have J \u2229J\u2032 \u0338= \u2205. If J\nand J\u2032 are not disjoint there exists some j \u2208J \u2229J\u2032 so that\n\u03b1 \u2265(\u2225z \u2212Zj\u2225+ \u2225z\u2032 \u2212Zj\u2225)/2 \u2265\u2225z \u2212z\u2032\u2225/2 .\nOn the other hand,\n\u03b1 \u2265max\n\b\nmax\nj\u2208J \u2225z \u2212Zj\u2225, max\nj\u2208J\u2032 \u2225z \u2212Zj\u2225\u2212\u2225z \u2212z\u2032\u2225\n\t\n\u2265\nmax\nj\u2208J\u222aJ\u2032 \u2225z \u2212Zj\u2225\u2212\u2225z \u2212z\u2032\u2225,\nso that\n\u03b1 \u2265max\n\b\n\u2225z \u2212z\u2032\u2225/2, max\nj\u2208J\u222aJ\u2032 \u2225z \u2212Zj\u2225\u2212\u2225z \u2212z\u2032\u2225\n\t\n\u2265max\n\b\n\u2225z \u2212z\u2032\u2225/2, max\nj\u2208J\u222aJ\u2032 \u2225z \u2212Zj\u2225/3}\n=: \u03b1\u2217= \u03b1\u2217(z, z\u2032, J, J\u2032).\n(6.9)\nLet us summarize the bounds obtained so far in the following lemma.\nLemma 6.1. Under the assumptions of Theorem 1, setting MJ(z) as in (6.4), \u03b1\u2217= \u03b1\u2217(z, z\u2032, J, J\u2032)\nas in (6.9) and C\u03b1 = \u03c1 \u00b7 \u03c0d/2/\u0393(d/2 + 1) we have the bound\nE\n\u0002\nvar\n\u0000\u02c6\u03a8|\u03c3Z\n\u0001\u0003\n\u2264CV\nX\nJ,J\u2032\u2208JK\n1{J \u2229J\u2032 \u0338= \u2205}\nZZ\n(S\u2217)2\n\u0010\nE\nh\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u0001\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 C\u03b1 \u00b7 min{\u03c1\u2032, \u03b1\u2217}d\u0001i\u00111/2\n\u0010\nE\nh\u0000e\u22a4\n0 MJ\u2032(z\u2032)\u22121e0\n\u0001\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 C\u03b1 \u00b7 min{\u03c1\u2032, \u03b1\u2217}d\u0001i\u00111/2\ndz dz\u2032.\n(6.10)\nIn particular, for J, J\u2032 \u2208JK with J \u2229J\u2032 \u0338= \u2205,\nE\nh\n\u03bb\n\u0000C(J)\n\u0001\n\u03bb\n\u0000C(J\u2032)\n\u0001i\n\u2264\nZZ\n(S\u2217)2 E\nh\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 C\u03b1 \u00b7 min{\u03c1\u2032, \u03b1\u2217}d\u0001i\ndz dz\u2032.\n(6.11)\n18\nStep 2: Concluding for L = 0.\nFor L = 0 we can drop e\u22a4\n0 MJ(z)\u22121e0 = 1/K from the expected values, so that we can insert (6.11)\ninto (6.10). In the third step we show that for L \u22651,\nE\nh\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u0001\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 C\u03b1 \u00b7 min{\u03c1\u2032, \u03b1\u2217}d\u0001i\n\u2264C3 E\nh\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 C\u03b1 \u00b7 min{\u03c1\u2032, \u03b1\u2217}d\u0001i\n,\n(6.12)\nfor some constant C3 > 0. Then, by symmetry of (6.10) in z, J and z\u2032 and J\u2032 we obtain that the\nargument that we give here also applies to L \u22651.\nWriting\nn\u2217:= (n \u22122K) C\u03b1 \u224dn\nand using the inequality #J \u222aJ\u2032 \u22642K, we have that\nE\nh\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 C\u03b1 \u00b7 min{\u03c1\u2032, \u03b1\u2217}d\u0001i\n\u2264exp\n\u0000\u2212n\u2217\u00b7 (\u03c1\u2032)d\u0001\n+ E\n\u0002\nexp\n\u0000\u2212n\u2217\u00b7 (\u03b1\u2217)d\u0001\u0003\n.\n(6.13)\nSince the \ufb01rst term decreases exponentially in n, we focus on the second. It holds that\nE\n\u0002\nexp\n\u0000\u2212n\u2217\u00b7 (\u03b1\u2217)d\u0001\u0003\n=\nZ 1\n0\nP\n\u0002\n\u03b1\u2217<\n\u0000\u2212(log t)/n\u2217\u00011/d\u0003\ndt =\nZ \u221e\n0\nP\n\u0002\n\u03b1\u2217< (s/n\u2217)1/d\u0003\nexp(\u2212s) ds\n=\nZ\ns>n\u2217\u2225z\u2212z\u2032\u2225d/2d P\n\u0002\n\u2225z \u2212Zj\u2225< 3(s/n\u2217)1/d , \u2200j \u2208J \u222aJ\u2032\u0003\nexp(\u2212s) ds\n=\nZ\ns>n\u2217\u2225z\u2212z\u2032\u2225d/2d PZ\n\u0000Bd(z, 3(s/n\u2217)1/d)\n\u0001#J\u222aJ\u2032\nexp(\u2212s) ds\n\u2264\n\b\n\u03c1\u03c0d/23d/\n\u0000n\u2217\u0393(d/2 + 1)\n\u0001\t#J\u222aJ\u2032 Z\ns>n\u2217\u2225z\u2212z\u2032\u2225d/2d s#J\u222aJ\u2032 exp(\u2212s) ds .\nAs max{1, s2K} exp(\u2212s) \u2264(4K)2K exp(\u2212s/2) for all s > 0, it follows that\nZ\ns>n\u2217\u2225z\u2212z\u2032\u2225d/2d s#J\u222aJ\u2032 exp(\u2212s) ds \u2264(4K)2K\nZ\ns>n\u2217\u2225z\u2212z\u2032\u2225d/2d exp(\u2212s/2) ds\n= 2(4K)2K \u00b7 exp\n\u0000\u2212n\u2217\u2225z \u2212z\u2032\u2225d/2d+1\u0001\n.\nMoreover,\nZZ\n(S\u2217)2 exp\n\u0000\u2212n\u2217\u2225z \u2212z\u2032\u2225d/2d+1\u0001\ndz dz\u2032 \u2264\nZZ\nS\u2217\u00d7Rd exp\n\u0000\u2212n\u2217\u2225w\u2225d/2d+1\u0001\ndw dz\u2032\n= \u03bbd(S\u2217) \u00b7\nZ\nexp\n\u0000\u2212\u2225w\u2225d/2d+1\u0001\ndw / n\u2217.\nObserving (6.11) in Lemma 6.1, we obtain the \ufb01rst part of the following lemma.\nLemma 6.2. Under the assumptions of Theorem 1, for J, J\u2032 \u2208JK with J \u2229J\u2032 \u0338= \u2205,\nE\nh\n\u03bb\n\u0000C(J)\n\u0001\n\u03bb\n\u0000C(J\u2032)\n\u0001i\n\u2264\nZZ\n(S\u2217)2 E\nh\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 C\u03b1 \u00b7 min{\u03c1\u2032, \u03b1\u2217}d\u0001i\ndz dz\u2032\n\u2264const. \u00b7 n\u2212#J\u222aJ\u2032\u22121,\n(6.14)\nwhere the constant only depends on K, d, \u03c1, \u03c1, \u03c1\u2032 and \u03c1\u2032\u2032.\nIn particular, for L = 0 we obtain\nE\n\u0002\nvar\n\u0000\u02c6\u03a8|\u03c3Z\n\u0001\u0003\n\u2272n\u22121.\n19\nFor the second part, using Lemma 6.1 and the previous bound (6.14) we obtain\nE\n\u0002\nvar\n\u0000\u02c6\u03a8|\u03c3Z\n\u0001\u0003\n\u2264const. CV n\u22121 \u00b7\nX\nJ,J\u2032\u2208JK\n1{J \u2229J\u2032 \u0338= \u2205} n\u2212#J\u222aJ\u2032\nResults from combinatorics provide that JK contains exactly\n\u0000 n\nK\n\u0001\nelements; and that, for each J \u2208JK,\nthere exist exactly\n\u0000K\n\u2113\n\u0001\u0000n\u2212K\nK\u2212\u2113\n\u0001\nsets J\u2032 \u2208JK with #(J \u2229J\u2032) = \u2113. Therefore,\nX\nJ,J\u2032\u2208JK\n1{J \u2229J\u2032 \u0338= \u2205} n\u2212#J\u222aJ\u2032 =\nK\nX\n\u2113=1\nX\nJ,J\u2032\u2208JK\n1{#J \u2229J\u2032 = \u2113} n\u2113\u22122K\n\u2264nK\nK!\nK\nX\n\u2113=1\n\u0012K\n\u2113\n\u0013\n1\n(K \u2212l)! \u00b7 nK\u2212\u2113+\u2113\u22122K \u2264(2K \u22121)/K! ,\n(6.15)\nwhich proves the second part of the lemma.\nStep 3: Reducing the case L \u22651 to L = 0\nWe need to prove (6.12), then the calculation from the second step for L = 0 applies to L \u22651 as well.\nThis requires a quite sophisticated conditioning argument.\nBy bj we denote the smallest j \u2208J such that \u2225z \u2212Zj\u2225\u2265\u2225z \u2212Zk\u2225for all k \u2208J; moreover we de\ufb01ne\n\u03b1\u2032 := max\nj\u2208J\u2032\\J \u2225z \u2212Zj\u2225.\nThe \u03c3-\ufb01elds generated by bj, Zbj, \u03b1\u2032, on the one hand, and by bj, Zbj, on the other hand, are called A\u2032\nand A, respectively. Note that with this notation we can write \u03b1\u2217in (6.9) as\n\u03b1\u2217= max\n\b\nmax{\u2225z \u2212Zbj\u2225, \u03b1\u2032}/3 , \u2225z \u2212z\u2032\u2225/2\n\t\n,\nand that the random variable \u03b1\u2217is measurable with respect to A\u2032. Then the \ufb01rst factor in (6.8) has\nthe upper bound\nE\nh\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u0001\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 C\u03b1 \u00b7 min{\u03c1\u2032, \u03b1\u2217}d\u0001i\n= E\nh\nE\n\u0002\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u0001\n| A\u2032\u0003\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 C\u03b1 \u00b7 min{\u03c1\u2032, \u03b1\u2217}d\u0001i\n= E\nh\nE\n\u0002\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u0001\n| A\n\u0003\nexp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 C\u03b1 \u00b7 min{\u03c1\u2032, \u03b1\u2217}d\u0001i\n,\n(6.16)\nwhere in the last step we used that the random vector\n\u0000e\u22a4\n0 MJ(z)e0,bj, Zbj\n\u0001\nand \u03b1\u2032 are independent.\nLemma 6.3. Under the assumptions of Theorem 1, given \u03b7 \u22651 for K \u22651 + (\u230a2\u03b7D\u230b+ 1)K\u2217we have\nthat\nE\n\u0002\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u0001\u03b7 | A\n\u0003\n\u2264C3 ,\nalmost surely, where the deterministic constant C3 \u2208(0, \u221e) only depends on \u03b7, d, L, \u03c1, \u03c1, \u03c1\u2032, \u03c1\u2032\u2032.\nThe proof of the Lemma is deferred to Section B. This concludes the proof of the \ufb01rst inequality in\n(2.9).\n20\n6.1.2\nExpected squared conditional bias in (2.9)\nTo prove the second inequality in (2.9) we proceed in two steps. First, for the case L = 0 we use\na bound on expected integrated distances of points in S\u2217to their K-nearest neighbors provided in\nLemma 6.4 below. Second, we use a Taylor approximation and again the bound for the conditional\nmoments of (e\u22a4\n0 MJ(z)\u22121e0) from Lemma 6.3 to cover the case L \u22651.\nStep 1: The case L = 0.\nSuppose that L = 0 and that G \u2208G(0, \u03b2, CH). Then\nE\n\u0002\f\f E[\u02c6\u03a8|\u03c3Z] \u2212\u03a8\n\f\f2\u0003\n= E\nh\f\f\f\nX\nJ\u2208JK\nZ\nC(J)\n1\nK\nX\nj\u2208J\n\u0000G(Zj) \u2212G(z)\n\u0001\ndz\n\f\f\f\n2i\n\u2264C2\nH\nX\nJ,J\u2032\u2208JK\nZ\nS\u2217\nZ\nS\u2217E\n\u0002\n1C(J)(z) 1C(J\u2032)(z\u2032) \u2225z \u2212Z(K)(z)\u2225\u03b2 \u2225z\u2032 \u2212Z(K)(z\u2032)\u2225\u03b2 \u0003\ndz dz\u2032 ,\nwhere Z(K)(z) is the K - nearest neighbor of z in Z1, . . . , Zn.\nLemma 6.4. Under the Assumptions 1 and 2 we have for \u03b2 > 0 (not necessarily \u22641) and K \u2208N\nthat\nX\nJ,J\u2032\u2208JK\nZ\nS\u2217\nZ\nS\u2217E\n\u0002\n1C(J)(z) 1C(J\u2032)(z\u2032) \u2225z \u2212Z(K)(z)\u2225\u03b2 \u2225z\u2032 \u2212Z(K)(z\u2032)\u2225\u03b2 \u0003\ndz dz\u2032 \u2272n\u22122 \u03b2\nd .\nProof of Lemma 6.4. For \u03b1 = \u03b1(z, z\u2032, J, J\u2032) as in (6.7), with the argument leading to (6.8) we have\nthat\nE\n\u0002\n1C(J)(z) 1C(J\u2032)(z\u2032) \u2225z \u2212Z(K)(z)\u2225\u03b2 \u2225z\u2032 \u2212Z(K)(z\u2032)\u2225\u03b2 \u0003\n\u2264E\nh\n\u03b12 \u03b2 exp\n\u0000\u2212(n \u2212#J \u222aJ\u2032) \u00b7 C\u03b1 \u00b7 min{\u03c1\u2032, \u03b1}d\u0001i\n\u2264(\u03c1\u2032\u2032)2\u03b2 exp\n\u0000\u2212n\u2217(\u03c1\u2032)d\u0001\n+ E\n\u0002\n\u03b12 \u03b2 exp\n\u0000\u2212n\u2217\u00b7 \u03b1d\u0001\u0003\n,\nwhere as before C\u03b1 = \u03c1 \u00b7 \u03c0d/2/\u0393(d/2 + 1) and n\u2217= (n \u22122K) C\u03b1. Now\nE\n\u0002\n\u03b12 \u03b2 exp\n\u0000\u2212n\u2217\u00b7 \u03b1d\u0001\u0003\n=\nZ\nSJ\u222aJ\u2032\nmax\nj\u2208J\u222aJ\u2032 \u2225z \u2212zj\u22252\u03b2 \u00b7 exp\n\u0000\u2212n\u2217max\nj\u2208J\u222aJ\u2032 \u2225z \u2212zj\u2225d\u0001\nY\nj\u2208J\u222aJ\u2032\nfZ(zj)dzj\n\u2264(\u00af\u03c1)2K\nZ\n(Rd)J\u222aJ\u2032\nmax\nj\u2208J\u222aJ\u2032 \u2225uj\u22252\u03b2 \u00b7 exp\n\u0000\u2212n\u2217max\nj\u2208J\u222aJ\u2032 \u2225uj\u2225d\u0001\nY\nj\u2208J\u222aJ\u2032\nduj\n\u2264(\u00af\u03c1dCd)2K\nZ\n(0,\u221e)J\u222aJ\u2032\nmax\nj\u2208J\u222aJ\u2032 r2\u03b2\nj\n\u00b7 exp\n\u0000\u2212n\u2217max\nj\u2208J\u222aJ\u2032 rd\nj\n\u0001\nY\nj\u2208J\u222aJ\u2032\nrd\u22121\nj\ndrj\n= (\u00af\u03c1dCd)2K (n\u2217)\u22122\u03b2/d (n\u2217)\u2212#(J\u222aJ\u2032) (d\u22121)/d (n\u2217)\u2212#(J\u222aJ\u2032) /d\n\u00b7\nZ\n(0,\u221e)J\u222aJ\u2032\nmax\nj\u2208J\u222aJ\u2032 r2\u03b2\nj\n\u00b7 exp\n\u0000\u2212max\nj\u2208J\u222aJ\u2032 rd\nj\n\u0001\nY\nj\u2208J\u222aJ\u2032\nrd\u22121\nj\ndrj\n\u2264const. \u00b7 n\u22122\u03b2/d n\u2212#(J\u222aJ\u2032) ,\n21\nand observing as in (6.15),\nX\nJ,J\u2032\u2208JK\nn\u2212#J\u222aJ\u2032 \u22642K/K!\nconcludes the proof of the lemma.\nStep 2: Taylor expansion for L = l \u22651\nNow suppose that G \u2208G(l, \u03b2, CH), l \u22651 and that L = l.\nThe Taylor expansion of G in z, evaluated at Zj up to order L can be written as\nG(Zj) = \u03be(z, Zj)\u22a4G(z) + R(z, Zj),\nwhere\nG(z) :=\n\u0000\u2202\u03baG(z) / \u03ba!\n\u0001\n\u03ba\u2208K ,\nR(z, Zj) := L\nX\n|\u03ba\u2032|=L\n\u03be\u03ba\u2032(z, Zj)\n\u03ba\u2032!\n\u00b7\nZ 1\n0\n(1 \u2212s)L\u22121\u0010\n\u2202\u03ba\u2032G\n\u0000z + s(Zj \u2212z)\n\u0001\n\u2212\u2202\u03ba\u2032G(z)\n\u0011\nds.\nTherefore, observing e\u22a4\n0 G(z) = G(z) and (2.6) we obtain that\nE\n\u0002 \u02c6G(z)|\u03c3Z\n\u0003\n=\nX\nj\u2208J(z)\ne\u22a4\n0 M(z)\u22121 \u03be(z, Zj) G(Zj) = G(z) +\nX\nj\u2208J(z)\ne\u22a4\n0 M(z)\u22121 \u03be(z, Zj) R(z, Zj)\nTherefore, it holds that\nE[\u02c6\u03a8|\u03c3Z] \u2212\u03a8 =\nX\nJ\u2208JK\nZ\nC(J)\nX\nj\u2208J\ne\u22a4\n0 M(z)\u22121 \u03be(z, Zj) R(z, Zj) dz ,\nalmost surely, where we have used that the C(J), J \u2208JK, are pairwise disjoint and form an almost-\npartition of S\u2217; concretely, S\u2217\\ S\nJ\u2208JK C(J) is included in\nn[\nj,k=1\n\b\nz \u2208S\u2217| \u2225z \u2212Zk\u2225= \u2225z \u2212Zj\u2225\n\t\n,\nas a subset, which is a d-dimensional Borel set with the Lebesgue measure zero as Z1 has a Lebesgue\ndensity fZ. Using the Cauchy-Schwarz inequality and the multinomial theorem twice we deduce that\n\f\f\f\nX\nj\u2208J(z)\ne\u22a4\n0 M(z)\u22121 \u03be(z, Zj) R(z, Zj)\n\f\f\f\n\u2264CH\nn X\nj\u2208J(z)\n\u2225z \u2212Zj\u22252\u03b2 \u0010\nX\n|\u03ba\u2032|=L+1\n1\n\u03ba\u2032!\u03be\u03ba\u2032(z, Zj)2\u0011\nX\n|\u03ba\u2032|=L+1\n1\n\u03ba\u2032!\no1/2\n\u00b7\n\u0010 X\nj\u2208J(z)\n\f\fe\u22a4\n0 M(z)\u22121\u03be(z, Zj)\n\f\f2\u00111/2\n\u2264CH\nKL/2+1/2\n(L + 1)!\n\u0010 X\nj\u2208J(z)\n\u2225Zj \u2212z\u22252(L+\u03b2)\u00111/2\u0000e\u22a4\n0 M(z)\u22121e0\n\u00011/2\n\u2264CH\nKL/2+1\n(L + 1)! \u2225Z(K)(z) \u2212z\u2225L+\u03b2 \u0000e\u22a4\n0 M(z)\u22121e0\n\u00011/2.\n22\nThus the expected squared conditional bias is bounded from above as follows.\nE\n\u0002\f\fE(\u02c6\u03a8|\u03c3Z) \u2212\u03a8\n\f\f2\u0003\n\u2264C2\nH\nKL+2\n(L + 1)!2\nX\nJ,J\u2032\u2208JK\nZZ\n(S\u2217)2 E\nh\n1C(J)(z)1C(J\u2032)(z\u2032) \u00b7\n\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u00011/2\n\u0000e\u22a4\n0 MJ\u2032(z\u2032)\u22121e0\n\u00011/2 \u2225Z(K)(z) \u2212z\u2225L+\u03b2 \u2225Z(K)(z\u2032) \u2212z\u2032\u2225L+\u03b2 i\ndz dz\u2032\n\u2264C2\nH\nKL+2\n(L + 1)!2\nX\nJ,J\u2032\u2208JK\nZZ\n(S\u2217)2 E\nh \u0000e\u22a4\n0 MJ(z)\u22121e0\n\u00011/2 \u0000e\u22a4\n0 MJ\u2032(z\u2032)\u22121e0\n\u00011/2\n(6.17)\n\u03b12 \u03b2 exp\n\u0000\u2212n\u2217\u00b7 min{\u03c1\u2032, \u03b1}d\u0001 i\ndz dz\u2032\nwith \u03b1 = \u03b1(z, z\u2032, J, J\u2032) as in (6.7), and again n\u2217= (n \u22122K) C\u03b1 and using the argument leading to\n(6.8). Using the Cauchy Schwarz inequality,\nE\nh \u0000e\u22a4\n0 MJ(z)\u22121e0\n\u00011/2 \u0000e\u22a4\n0 MJ\u2032(z\u2032)\u22121e0\n\u00011/2 \u03b12 (L+\u03b2) exp\n\u0000\u2212n\u2217\u00b7 min{\u03c1\u2032, \u03b1}d\u0001 i\n\u2264\n\u0010\nE\nh \u0000e\u22a4\n0 MJ(z)\u22121e0\n\u00011/2 \u03b12 (L+\u03b2) exp\n\u0000\u2212n\u2217\u00b7 min{\u03c1\u2032, \u03b1}d\u0001 i\u00111/2\n\u00b7\n\u0010\nE\nh \u0000e\u22a4\n0 MJ\u2032(z\u2032)\u22121e0\n\u00011/2 \u03b12 (L+\u03b2) exp\n\u0000\u2212n\u2217\u00b7 min{\u03c1\u2032, \u03b1}d\u0001 i\u00111/2\nNow the conditioning argument from step 3 in Section 6.1.1 can again be applied to eliminate the\nterms\n\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u00011/2 and reduce to the setting with L = 0:\nLet bj denote the smallest j \u2208J such that \u2225z \u2212Zj\u2225\u2265\u2225z \u2212Zk\u2225for all k \u2208J and de\ufb01ne\n\u03b1\u2032 := max\nj\u2208J\u2032\\J \u2225z \u2212Zj\u2225,\nwhere now possibly J \u2229J\u2032 = \u2205. The \u03c3-\ufb01elds generated by bj, Zbj, \u03b1\u2032, on the one hand, and by bj, Zbj, on\nthe other hand, are again called A\u2032 and A, respectively. Note that with this notation we can write \u03b1\nas\n\u03b1 = max{\u2225z \u2212Zbj\u2225, \u03b1\u2032}.\nand that the random variable \u03b1 is measurable with respect to A\u2032. Then as in (6.16),\nE\nh\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u0001\n\u03b12 (L+\u03b2) exp\n\u0000\u2212n\u2217\u00b7 min{\u03c1\u2032, \u03b1}d\u0001 i\n= E\nh\nE\n\u0002\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u0001\n| A\u2032\u0003\n\u03b12 (L+\u03b2) exp\n\u0000\u2212n\u2217\u00b7 min{\u03c1\u2032, \u03b1}d\u0001 i\n= E\nh\nE\n\u0002\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u0001\n| A\n\u0003\n\u03b12 (L+\u03b2) exp\n\u0000\u2212n\u2217\u00b7 min{\u03c1\u2032, \u03b1}d\u0001 i\n\u2264C3 E\nh\n\u03b12 (L+\u03b2) exp\n\u0000\u2212n\u2217\u00b7 min{\u03c1\u2032, \u03b1}d\u0001 i\nwhere in the second step we used that the random vector\n\u0000e\u22a4\n0 MJ(z)e0,bj, Zbj\n\u0001\nand \u03b1\u2032 are independent,\nand in the last step we applied Lemma 6.3 with \u03b7 = 1. The argument is completed as in step 1.\n6.2\nProof of Theorem 2\nLet \u03c3X,Z denote the \u03c3-\ufb01eld generated by X1, . . . , Xm, Z1, . . . , Zn, and let \u03c3Z denote the \u03c3-\ufb01eld gener-\nated by Z1, . . . , Zn only. Then we have the decomposition\nE\n\u0002\f\fb\u03a6 \u2212\u03a6\n\f\f2\u0003\n= E\n\u0002\nvar\n\u0000b\u03a6|\u03c3X,Z\n\u0001\u0003\n+ E\n\u0002\nvar\n\u0000E[b\u03a6|\u03c3X,Z]|\u03c3Z\n\u0001\u0003\n+ E\n\u0002\f\f E[b\u03a6|\u03c3Z] \u2212\u03a6\n\f\f2\u0003\n.\n(6.18)\n23\nWe shall show the following bounds:\nsup\nG\u2208G(l,\u03b2,CH,CG)\nE\n\u0002\nvar\n\u0000b\u03a6|\u03c3X,Z\n\u0001\u0003\n\u2264CV \u00b7 C \u00b7\n\u0000n\u22121 + m\u22121\u0001\n,\n(6.19)\nsup\nG\u2208G(l,\u03b2,CH,CG)\nE\n\u0002\nvar\n\u0000E[b\u03a6|\u03c3X,Z]|\u03c3Z\n\u0001\u0003\n\u2264C2\nG \u00b7 C \u00b7 m\u22121,\n(6.20)\nsup\nG\u2208G(l,\u03b2,CH,CG)\nE\n\u0002\f\f E[\u02c6\u03a6|\u03c3Z] \u2212\u03a6\n\f\f2\u0003\n\u2264C2\nH \u00b7 C \u00b7 n\u22122(l+\u03b2)\nd\n,\n(6.21)\nwhere the constant C > 0 only depends on L, K, \u03c1, \u03c1\u2032, \u03c1\u2032\u2032, \u00af\u03c1, d, \u03bb(S\u2217) and Cf. These bounds imply\n(2.11), and hence the statement of the theorem.\nProof of (6.19): By conditional independence if J \u2229J\u2032 = \u2205,\nvar\n\u0000b\u03a6|\u03c3X,Z\n\u0001\n= 1\nm2\nm\nX\nk,k\u2032=1\nX\nJ,J\u2032\u2208JK\n1\n\u0000J\u2229J\u2032 \u0338= \u2205\n\u0001\n1\n\u0000Xk \u2208C(J), Xk\u2032 \u2208C(J\u2032)\n\u0001\ncov\n\u0010\nbG(Xk), bG(Xk\u2032)|\u03c3X,Z\n\u0011\n.\nThen\nE\nh\nvar\n\u0000b\u03a6|\u03c3X,Z\n\u0001\n|\u03c3Z\ni\n= 1\nm\nZ\nS\u2217var\n\u0000 bG(z) | \u03c3Z\n\u0001\nf(z) dz\n+m(m \u22121)\nm2\nX\nJ,J\u2032\u2208JK\n1\n\u0000J \u2229J\u2032 \u0338= \u2205\n\u0001 Z\nC(J)\nZ\nC(J\u2032)\ncov\n\u0000 bG(z), bG(z\u2032) | \u03c3Z\n\u0001\nf(z) f(z\u2032) dz dz\u2032.\n(6.22)\nSince\nX\nJ,J\u2032\u2208JK\n1{J \u2229J\u2032 \u0338= \u2205}\nZ\nC(J)\nZ\nC(J\u2032)\ncov\n\u0000 bG(z), bG(z\u2032) | \u03c3Z\n\u0001\nf(z) f(z\u2032) dz dz\u2032\n\u2264C2\nf\nX\nJ,J\u2032\u2208JK\n1{J \u2229J\u2032 \u0338= \u2205}\nZ\nC(J)\n\b\nvar\n\u0000 bG(z)|\u03c3Z\n\u0001\t1/2dz \u00b7\nZ\nC(J\u2032)\n\b\nvar\n\u0000 bG(z\u2032)|\u03c3Z\n\u0001\t1/2dz\u2032,\n(6.23)\nthe expected value of the second term in (6.22) can be upper bounded as the conditional variance\nterm in (2.9), see (6.1). For the \ufb01rst term in (6.22), we use the bound (6.2) on var( bG(z)|\u03c3Z) as well as\nLemma 6.3 to obtain that E[var( bG(z)|\u03c3Z)] is bounded from above uniformly in z. This proves (6.19).\nProof of (6.20) and (6.21):\nWrite\n\u00afG(z; Z) = E\n\u0002 \u02c6G(z)|\u03c3Z\n\u0003\n=\nX\nj\u2208J(z)\ne\u22a4\n0 M(z)\u22121 \u03be(z, Zj) G(Zj).\n(6.24)\nThen\nE\n\u0002 \u02c6G(Xk)|\u03c3X,Z\n\u0003\n= \u00afG(Xk; Z),\nand therefore\nE[b\u03a6|\u03c3X,Z] = 1\nm\nm\nX\nk=1\nX\nJ\u2208JK\n1\n\u0000Xk \u2208C(J)\n\u0001 \u00afG(Xk; Z).\nFurther,\n\f\f E[b\u03a6|\u03c3Z] \u2212\u03a6\n\f\f =\n\f\f\f\nZ\nS\u2217\n\u0000E\n\u0002 \u02c6G(z)|\u03c3Z\n\u0003\n\u2212G(z)\n\u0001\nf(z) dz\n\f\f\f \u2264Cf\nZ\nS\u2217\n\f\f E\n\u0002 \u02c6G(z)|\u03c3Z\n\u0003\n\u2212G(z)\n\f\f dz,\n24\nwhich can be bounded as the conditional bias term in (2.9) so that (6.21) follows. Moreover,\nvar\n\u0000E[b\u03a6|\u03c3X,Z]|\u03c3Z\n\u0001\n= 1\nm var\n\u0010 X\nJ\u2208JK\n1\n\u0000X1 \u2208C(J)\n\u0001 \u00afG(X1; Z)|\u03c3Z\n\u0011\n\u22641\nm\nZ\nS\u2217\n\u0000E\n\u0002 \u02c6G(z)|\u03c3Z\n\u0003\u00012 f(z) dz.\nUsing the Cauchy-Schwarz inequality in (6.24) we obtain\n\u0000E\n\u0002 \u02c6G(z)|\u03c3Z\n\u0003\u00012 \u2264K C2\nG \u00b7\nX\nj\u2208J(z)\n\f\fe\u22a4\n0 M(z)\u22121\u03be(z, Zj)\n\f\f2 = K C2\nG \u00b7 e\u22a4\n0 M(z)\u22121 e0.\nUsing Lemma 6.3 proves (6.20) and hence concludes the proof of the Theorem.\nReferences\nAbadie, A. and Imbens, G. W. (2006). Large sample properties of matching estimators for average\ntreatment e\ufb00ects. Econometrica, 74(1):235\u2013267.\nAbadie, A. and Imbens, G. W. (2011).\nBias-corrected matching estimators for average treatment\ne\ufb00ects. Journal of Business & Economic Statistics, 29(1):1\u201311.\nAudibert, J.-Y. and Tsybakov, A. B. (2007). Fast learning rates for plug-in classi\ufb01ers. Ann. Stat.,\n35(2):608\u2013633.\nChernozhukov, V., Chetverikov, D., Demirer, M., Du\ufb02o, E., Hansen, C., Newey, W., and Robins, J.\n(2018). Double/debiased machine learning for treatment and structural parameters. Econom. J.,\n21(1):c1\u2013c68.\nChernozhukov, V., Hansen, C., Kallus, N., Spindler, M., and Syrgkanis, V. (2024). Applied causal\ninference powered by ml and ai. arXiv preprint arXiv:2403.02467.\nDahlke, S., Heuer, S., Holzmann, H., and Tafo, P. (2022). Statistically optimal estimation of signals in\nmodulation spaces using gabor frames. IEEE Transactions on Information Theory, 68(6):4182\u20134200.\nDelaigle, A., Hall, P., and Qiu, P. (2006). Nonparametric methods for solving the Berkson errors-in-\nvariables problem. J. R. Stat. Soc., Ser. B, Stat. Methodol., 68(2):201\u2013220.\nDevroye, L., Gy\u00a8or\ufb01, L., Lugosi, G., and Walk, H. (2017). On the measure of voronoi cells. Journal of\nApplied Probability, 54(2):394\u2013408.\nDunker, F., Eckle, K., Proksch, K., and Schmidt-Hieber, J. (2019). Tests for qualitative features in\nthe random coe\ufb03cients model. Electron. J. Stat., 13(2):2257\u20132306.\nGaillac, C. and Gautier, E. (2022). Adaptive estimation in the linear random coe\ufb03cients model when\nregressors have limited variation. Bernoulli, 28(1):504\u2013524.\nGautier, E. and Kitamura, Y. (2013). Nonparametric estimation in random coe\ufb03cients binary choice\nmodels. Econometrica, 81(2):581\u2013607.\n25\nHoderlein, S., Holzmann, H., and Meister, A. (2017). The triangular model with random coe\ufb03cients.\nJournal of Econometrics, 201(1):144\u2013169.\nHoderlein, S., Klemel\u00a8a, J., and Mammen, E. (2010). Analyzing the random coe\ufb03cient model non-\nparametrically. Econom. Theory, 26(3):804\u2013837.\nHolzmann, H. and Meister, A. (2020). Rate-optimal nonparametric estimation for random coe\ufb03cient\nregression models. Bernoulli, 26(4):2790\u20132814.\nHu, A. J., Green, A., and Tibshirani, R. J. (2022). The voronoigram: minimax estimation of bounded\nvariation functions from scattered data. arXiv preprint arXiv: 2212.14514.\nImbens, G. W. (2004). Nonparametric estimation of average treatment e\ufb00ects under exogeneity: A\nreview. Review of Economics and Statistics, 86(1):4\u201329.\nKallus, N. (2020). Generalized optimal matching methods for causal inference. Journal of Machine\nLearning Research, 21(62):1\u201354.\nKerkyacharian, G. and Picard, D. (2004). Regression in random design and warped wavelets. Bernoulli,\n10(6):1053\u20131105.\nKohler, M. (2014). Optimal global rates of convergence for noiseless regression estimation problems\nwith adaptively chosen design. Journal of Multivariate Analysis, 132:197\u2013208.\nKouw, W. M. and Loog, M. (2019). An introduction to domain adaptation and transfer learning.\narXiv preprint arXiv:1812.11806.\nLewbel, A. and Schennach, S. M. (2007). A simple ordered data estimator for inverse density weighted\nexpectations. Journal of Econometrics, 136(1):189\u2013211.\nLin, Z., Ding, P., and Han, F. (2023). Estimation based on nearest neighbor matching: from density\nratio to average treatment e\ufb00ect. Econometrica, 91(6):2187\u20132217.\nMeister, A. (2009). Deconvolution problems in nonparametric statistics. Lecture Notes in Statistics,\nSpringer.\nPortier, F., Truquet, L., and Yamane, I. (2023). Scalable and hyper-parameter-free non-parametric\ncovariate shift adaptation with conditional sampling. arXiv preprint arXiv:2312.09969.\nSamworth, R. J. (2012). Optimal weighted nearest neighbour classi\ufb01ers. Ann. Statist., 40(5):2733\u2013\n2763.\nSharpnack, J. (2023). On L2-consistency of nearest neighbor matching. IEEE Trans. Inf. Theory,\n69(6):3978\u20133988.\nSugiyama, M., Nakajima, S., Kashima, H., Buenau, P., and Kawanabe, M. (2007). Direct importance\nestimation with model selection and its application to covariate shift adaptation. Advances in neural\ninformation processing systems, 20.\nTsybakov, A. (2009). Introduction to nonparametric estimation. Springer.\n26\nWang, Y. and Shah, R. D. (2024). Debiased inverse propensity score weighting for estimation of av-\nerage treatment e\ufb00ects with high-dimensional confounders. Ann. Statist., to appear; arXiv preprint\narXiv:2011.08661.\n27\nA\nProof of Theorem 3\nProof of Theorem 3: Set I\u2032\nk := [k/M, (k + 1/2)/M], I\u2032\u2032\nk := ((k + 1/2)/M, (k + 1)/M), k = 0, . . . , M \u22121\nfor some integer M > 1. Moreover, let\n\u03d5k := (1 + \u03b8k\u03b1)1I\u2032\nk + (1 \u2212\u03b8k\u03b1)1I\u2032\u2032\nk ,\nfor some \u03b1 > 0 and \u03b8 = (\u03b80, . . . , \u03b8M\u22121) \u2208{0} \u222a{\u22121, 1}M; and\nf\u03b8 :=\nM\u22121\nX\nk=0\n\u03d5k .\nNow write f (n) for the n-fold product density of a density f; and let \u02c6\u03b8 = (\u02c6\u03b80, . . . , \u02c6\u03b8M\u22121) be a random\nvector with i.i.d. components which satisfy P[\u02c6\u03b8k = 1] = P[\u02c6\u03b8k = \u22121] = 1/2 for all k = 0, . . . , M \u22121.\nThe densities f\u02c6\u03b8 and f0 are used as candidates for the design density fZ where \u03b1 is chosen su\ufb03ciently\nsmall such that f\u02c6\u03b8, f0 \u2208F are guaranteed. Put x = (x1, . . . , xn) and\nN +\nk (x) :=\nn\nX\nj=1\n1{1}(\u02c6\u03b8k) \u00b7 1I\u2032\nk(xj) + 1{\u22121}(\u02c6\u03b8k) \u00b7 1I\u2032\u2032\nk (xj) ,\nN \u2212\nk (x) :=\nn\nX\nj=1\n1{1}(\u02c6\u03b8k) \u00b7 1I\u2032\u2032\nk (xj) + 1{\u22121}(\u02c6\u03b8k) \u00b7 1I\u2032\nk(xj) .\nConsider that\nf (n)\n\u02c6\u03b8\n(x) =\nM\u22121\nY\nk=0\n(1 + \u03b1)N+\nk (x) \u00b7 (1 \u2212\u03b1)N\u2212\nk (x) .\nThe likelihood function \u03b7(n)\n\u03b8\nequals\n\u03b7(n)\n\u03b8 (u, z) =\nn\nY\nj=1\nf\u03b8(zj)f\u03b5\n\u0000uj \u2212h\u03b8(zj)\n\u0001\n,\nwith u = (u1, . . . , un) and z = (z1, . . . , zn), where we choose the regression function h\u03b8 = \u03b2 \u00b7 F(f\u03b8)\nfor the regression function where the function F satis\ufb01es F(1) = 1 but, apart from that, remains to\nbe selected; and f\u03b5 denotes the standard normal density. The parameter \u03b2 > 0 is chosen su\ufb03ciently\nsmall such that h\u02c6\u03b8, h0 \u2208H. Writing\nI+\nk :=\n(\nI\u2032\nk ,\nif \u02c6\u03b8k = 1 ,\nI\u2032\u2032\nk ,\nif \u02c6\u03b8k = \u22121 ,\nand\nI\u2212\nk :=\n(\nI\u2032\nk ,\nif \u02c6\u03b8k = \u22121 ,\nI\u2032\u2032\nk ,\nif \u02c6\u03b8k = 1 ,\nwe deduce that\n\f\fE\u03b7(n)\n\u02c6\u03b8 (u, z)\n\f\f2\n28\n=\n\u0002\nf (n)\n\u03b5\n(u)\n\u00032 \u00b7\n\f\f\f\nM\u22121\nY\nk=0\nE(1 + \u03b1)N+\nk (z)(1 \u2212\u03b1)N\u2212\nk (z) exp\nn\n\u22121\n2\u03b22\u0002\nF 2(1 + \u03b1)N +\nk (z) + F 2(1 \u2212\u03b1)N \u2212\nk (z)\n\u0003o\n\u00b7 exp\nn\n\u03b2F(1 + \u03b1)\nn\nX\nj=1\n1I+\nk (zj)uj + \u03b2F(1 \u2212\u03b1)\nn\nX\nj=1\n1I\u2212\nk (zj)uj\no\f\f\f\n2\n=\n\u0002\nf (n)\n\u03b5\n(u)\n\u00032 \u00b7\nM\u22121\nY\nk=0\nn1\n4(1 + \u03b1)2N\u2032\nk(z)(1 \u2212\u03b1)2N\u2032\u2032\nk (z) exp\nh\n\u2212\u03b22\u0002\nF 2(1 + \u03b1)N \u2032\nk(z) + F 2(1 \u2212\u03b1)N \u2032\u2032\nk (z)\n\u0003i\n\u00b7 exp\nh\n2\u03b2F(1 + \u03b1)\nn\nX\nj=1\n1I\u2032\nk(zj)uj + 2\u03b2F(1 \u2212\u03b1)\nn\nX\nj=1\n1I\u2032\u2032\nk (zj)uj\ni\n+ 1\n4(1 + \u03b1)2N\u2032\u2032\nk (z)(1 \u2212\u03b1)2N\u2032\nk(z) exp\nh\n\u2212\u03b22\u0002\nF 2(1 + \u03b1)N \u2032\u2032\nk (z) + F 2(1 \u2212\u03b1)N \u2032\nk(z)\n\u0003i\n\u00b7 exp\nh\n2\u03b2F(1 + \u03b1)\nn\nX\nj=1\n1I\u2032\u2032\nk (zj)uj + 2\u03b2F(1 \u2212\u03b1)\nn\nX\nj=1\n1I\u2032\nk(zj)uj\ni\n+ 1\n2(1 \u2212\u03b12)Nk(z) exp\nh\n\u2212\u03b22\n2\n\u0000F 2(1 + \u03b1) + F 2(1 \u2212\u03b1)\n\u0001\n\u00b7 Nk(z)\ni\n\u00b7 exp\nh\n\u03b2\n\u0002\nF(1 + \u03b1) + F(1 \u2212\u03b1)\n\u0003\nn\nX\nj=1\n1Ik(zj)uj\nio\n,\nwhere N \u2032\nk(x) := Pn\nj=1 1I\u2032\nk(xj), N \u2032\u2032\nk (x) := Pn\nj=1 1I\u2032\u2032\nk (xj) and Nk(x) := N \u2032\nk(x) + N \u2032\u2032\nk (x). On the other\nhand,\n\u03b7(n)\n0 (u, z) = f (n)\n\u03b5\n(u) \u00b7 exp(\u2212\u03b22n/2) \u00b7 exp\n\u0010\n\u03b2\nn\nX\nj=1\nuj\n\u0011\n,\non z \u2208[0, 1]n so that\nZ \f\fE\u03b7(n)\n\u02c6\u03b8 (u, z)\n\f\f2/\u03b7(n)\n0 (u, z)du\n= exp(\u03b22n/2) \u00b7\nM\u22121\nY\nk=0\nn1\n4(1 + \u03b1)2N\u2032\nk(z)(1 \u2212\u03b1)2N\u2032\u2032\nk (z) exp\nh\n\u2212\u03b22\u0002\nF 2(1 + \u03b1)N \u2032\nk(z) + F 2(1 \u2212\u03b1)N \u2032\u2032\nk (z)\n\u0003i\n\u00b7 exp\nh\u03b22\n2\n\u0002\n2F(1 + \u03b1) \u22121\n\u00032N \u2032\nk(z) + \u03b22\n2\n\u0002\n2F(1 \u2212\u03b1) \u22121\n\u00032N \u2032\u2032\nk (z)\ni\n+ 1\n4(1 + \u03b1)2N\u2032\u2032\nk (z)(1 \u2212\u03b1)2N\u2032\nk(z) exp\nh\n\u2212\u03b22\u0002\nF 2(1 + \u03b1)N \u2032\u2032\nk (z) + F 2(1 \u2212\u03b1)N \u2032\nk(z)\n\u0003i\n\u00b7 exp\nh\u03b22\n2\n\u0002\n2F(1 + \u03b1) \u22121\n\u00032N \u2032\u2032\nk (z) + \u03b22\n2\n\u0002\n2F(1 \u2212\u03b1) \u22121\n\u00032N \u2032\nk(z)\ni\n+ 1\n2(1 \u2212\u03b12)Nk(z) exp\nh\n\u2212\u03b22\n2\n\u0000F 2(1 + \u03b1) + F 2(1 \u2212\u03b1)\n\u0001\n\u00b7 Nk(z)\ni\n\u00b7 exp\nh\u03b22\n2\n\u0000F(1 + \u03b1) + F(1 \u2212\u03b1) \u22121\n\u00012 \u00b7 Nk(z)\nio\n.\nNow we integrate this term with respect to z. Thus consider Z as an n-dimensional random vector\nwhich consists of i.i.d. components that are uniformly distributed on [0, 1]. Given (N0(Z), . . . , NM\u22121(Z)),\nthe random variable N \u2032\nk(Z) are conditionally independent and conditionally binomially distributed\n29\nwith the parameters Nk(Z) and 1/2. Therefore,\nZZ \f\fE\u03b7(n)\n\u02c6\u03b8 (u, z)\n\f\f2/\u03b7(n)\n0 (u, z)du dz\n= E\nM\u22121\nY\nk=0\n\u0000\u03b3Nk(Z)\n0\n/2 + \u03b3Nk(Z)\n1\n/2\n\u0001\n= 2\u2212M\nX\nN \u2286{0,...,M\u22121}\nE \u03b3\nP\nk\u2208N Nk(Z)\n0\n\u03b3\nn\u2212P\nk\u2208N Nk(Z)\n1\n= E\nn\n\u03b30\nB\nM + \u03b31\n\u0010\n1 \u2212B\nM\n\u0011on\n,\n(A.1)\nwhere B \u223cB(M, 1/2) and\n\u03b30 := 1\n2(1 + \u03b1)2 exp\n\b\n\u03b22\u0002\n1 \u2212F(1 + \u03b1)\n\u00032\t\n+ 1\n2(1 \u2212\u03b1)2 exp\n\b\n\u03b22\u0002\n1 \u2212F(1 \u2212\u03b1)\n\u00032\t\n,\n\u03b31 := (1 \u2212\u03b12) exp\n\b\n\u03b22\u0002\n1 \u2212F(1 + \u03b1)\n\u0003\n\u00b7\n\u0002\n1 \u2212F(1 \u2212\u03b1)\n\u0003\t\n.\nThen, (A.1) equals\n\u00101\n2\u03b30 + 1\n2\u03b31\n\u0011n\n\u00b7 E\nn\n1 + 2\u03b30 \u22122\u03b31\n\u03b30 + \u03b31\n\u00b7\n\u0010 B\nM \u22121\n2\n\u0011on\n\u2264\n\u00101\n2\u03b30 + 1\n2\u03b31\n\u0011n\n\u00b7 exp\nn (\u03b30 \u2212\u03b31)2\n2(\u03b30 + \u03b31)2 \u00b7 n2\nM\no\n.\nBy Taylor expansion we obtain that\n\u03b30/2 + \u03b31/2 = 1\n4\n\u221e\nX\nk=0\n1\nk! \u03b22k \u0010\n(1 + \u03b1)\n\u0002\n1 \u2212F(1 + \u03b1)\n\u0003k + (1 \u2212\u03b1)\n\u0002\n1 \u2212F(1 \u2212\u03b1)\n\u0003k\u00112\n,\n\u03b30 \u2212\u03b31 = 1\n2\n\u221e\nX\nk=0\n1\nk! \u03b22k \u0010\n(1 + \u03b1)\n\u0002\n1 \u2212F(1 + \u03b1)\n\u0003k \u2212(1 \u2212\u03b1)\n\u0002\n1 \u2212F(1 \u2212\u03b1)\n\u0003k\u00112\n.\nThus the \u03c72-distance between E\u03b7(n)\n\u02c6\u03b8\nand \u03b7(n)\n0\nadmits the following upper bound\n\u03c72\u0000E\u03b7(n)\n\u02c6\u03b8 , \u03b7(n)\n0\n\u0001\n\u2264exp(c + c2/16) \u22121 ,\nfor all n, M whenever\n\u221e\nX\nk=1\n1\nk! \u03b22k \u0010\n(1 + \u03b1)\n\u0002\n1 \u2212F(1 + \u03b1)\n\u0003k + (1 \u2212\u03b1)\n\u0002\n1 \u2212F(1 \u2212\u03b1)\n\u0003k\u00112\n\u2264c/n ,\n(A.2)\n\u221e\nX\nk=0\n1\nk! \u03b22k \u0010\n(1 + \u03b1)\n\u0002\n1 \u2212F(1 + \u03b1)\n\u0003k \u2212(1 \u2212\u03b1)\n\u0002\n1 \u2212F(1 \u2212\u03b1)\n\u0003k\u00112\n\u2264c \u00b7 n\u22121M1/2 ,\n(A.3)\nfor all n, M, where the constant is c > 0 su\ufb03ciently small.\nImpose that M is an integer multiple of 4. Then,\n\f\f\f\nZ 3/4\n1/4\nh\u02c6\u03b8 \u2212\nZ 3/4\n1/4\nh0\n\f\f\f\n2\n=\n1\n16\u03b22 \u0000F(1 + \u03b1) + F(1 \u2212\u03b1) \u22122\n\u00012 ,\n(A.4)\n30\nholds true almost surely. Now we specify that F(x) = 1/x, which provides the lower bound \u03b22\u03b14 on\n(A.4) (up to a constant factor). Furthermore the left side of the inequality (A.2) is bounded from\nabove by \u03b14\u03b24 (again up to some constant factor), while (A.3) can always be satis\ufb01ed by selecting\nM su\ufb03ciently large. Note that there are no smoothness restrictions. Choose \u03b1 as a positive constant\nsu\ufb03ciently small; and \u03b2 = \u03b2n \u224dn\u22121/4 with a su\ufb03ciently small constant factor.\nUsing Le Cam\u2019s inequality, the consideration\nsup\nfZ\u2208F,h\u2208H\nEfZ,h\n\f\f\f \u02c6Hn \u2212\nZ 3/4\n1/4\nh(x)dx\n\f\f\f\n2\n\u22651\n2\nZZ\nE\n\f\f\f \u02c6Hn(u, z) \u2212\nZ 3/4\n1/4\nh\u02c6\u03b8(x)dx\n\f\f\f\n2\n\u03b7(n)\n\u02c6\u03b8 (u, z) du dz\n+ 1\n2\nZZ \f\f\f \u02c6Hn(u, z) \u2212\nZ 3/4\n1/4\nh0(x)dx\n\f\f\f\n2\n\u03b7(n)\n0 (u, z) du dz\n\u2265\n1\n64\u03b22\u0000F(1 + \u03b1) + F(1 \u2212\u03b1) \u22122\n\u00012 \u00b7\nZZ\nmin\n\b\nE\u03b7(n)\n\u02c6\u03b8 (u, z), \u03b7(n)\n0\n(u, z)\n\t\ndu dz\n\u2265\n1\n64\u03b22\u0000F(1 + \u03b1) + F(1 \u2212\u03b1) \u22122\n\u00012 \u00b7\nn\n1 \u2212\nh\n1 \u2212\n\u0010\n1 \u22121\n2\u03c72\u0000E\u03b7(n)\n\u02c6\u03b8 , \u03b7(n)\n0\n\u0001\u00112i1/2o\n,\nconcludes the proof of the theorem.\nB\nProof of Lemma 6.3, and further technical lemmas\nRecall that for J \u2208JK we take bj as the smallest j \u2208J for which \u2225z \u2212Zj\u2225\u2265\u2225z \u2212Zk\u2225for all k \u2208J,\nand denote by A the \u03c3 - \ufb01eld generated by (bj, Zbj). Further we set bJ := J\\{bj} and in the following\ndenote\nu0 = e\u22a4\n0 MJ(z)\u22121e0\nWe need to show that for \u03b7 \u22651 and K \u22651 + (\u230a2\u03b7D\u230b+ 1)K\u2217we have that\nE [u\u03b7\n0 | A] \u2264C3 .\n(B.1)\nNote that u0 is the \ufb01rst component of the column vector u which satis\ufb01es MJ(z)u = e0. Multiplying\nthis system of linear equation by u\u22a4from the left side we arrive at\nu0 =\nX\nj\u2208J\n\u0010 X\n\u03ba\u2208K\nu\u03ba \u03be\u03ba(z, Zj)\n\u00112\n.\nAs we are seeking for a positive upper bound on u0 we may assume that u0 > 0. Dividing by u2\n0 yields\nthat\nu0 = 1\n\u000en X\nj\u2208J\n\u0010 X\n\u03ba\u2208K\n(u\u03ba/u0) \u03be\u03ba(z, Zj)\n\u00112o\n\u22641\n\u000en\ninf\nP \u2208P1\nX\nj\u2208J\nP 2(z \u2212Zj)\no\n\u22641\n\u000en\ninf\nP \u2208P1\nX\nj\u2208b\nJ\nP 2(z \u2212Zj)\no\n,\n(B.2)\nwhere P1 denotes the set of all d-variate polynomials with the degree \u2264L which take on the value 1\nat 0, and we used the notation bJ := J\\{bj} introduced above.\n31\nLemma B.1. Given A, the random variables Zj, j \u2208bJ, are conditionally independent and each Zj\nhas the conditional Lebesgue density\nfZ|A(u) = fZ(u) \u00b7 1[0,\u2225z\u2212Zbj\u2225)(\u2225z \u2212u\u2225) /\nZ\nfZ(u\u2032) \u00b7 1[0,\u2225z\u2212Zbj\u2225)(\u2225z \u2212u\u2032\u2225) du\u2032 ,\nu \u2208Rd .\nProof of Lemma B.1. Let \u03a6 and \u2126denote two probe functions and consider that\nE\n\u0002\n\u03a6\n\u0000Zj , j \u2208bJ\n\u0001\n\u00b7 \u2126\n\u0000bj, Zbj\n\u0001\u0003\n=\nX\nj\u2032\u2208J\nE 1{bj = j\u2032} \u00b7 \u03a6\n\u0000Zk, k \u2208J\\{j\u2032}\n\u0001\n\u00b7 \u2126(j\u2032, Zj\u2032)\n=\nX\nj\u2032\u2208J\nE\nZ\n\u2126(j\u2032, u)fZ(u)\nZ\n\u00b7 \u00b7 \u00b7\nZ\n\u03a6\n\u0000z1, . . . , zK\u22121\n\u0001\nY\nk\u2208J\\{j\u2032}\nfZ(zk)1[0,\u2225u\u2212z\u2225)(\u2225zk \u2212z\u2225)dz1 \u00b7 \u00b7 \u00b7 dzK\u22121du.\nNow, putting \u03a6 \u22611 and changing \u2126to \u2126\u00b7 \u02dc\u03a6, we deduce that\nE\n\u0002\u02dc\u03a6\n\u0000bj, Zbj\n\u0001\n\u00b7 \u2126\n\u0000bj, Zbj\n\u0001\u0003\n=\nX\nj\u2032\u2208J\nE\nZ\n\u2126(j\u2032, u)\u02dc\u03a6\n\u0000j\u2032, Zj\u2032\u0001\nfZ(u)\n\u0010 Z\nfZ(\u03b6)1[0,\u2225u\u2212z\u2225)(\u2225\u03b6 \u2212z\u2225)d\u03b6\n\u0011K\u22121\ndu,\nso that\nE\n\b\n\u03a6\n\u0000Zj , j \u2208bJ\n\u0001\n| A\n\t\n=\nZ\n\u00b7 \u00b7 \u00b7\nZ\n\u03a6\n\u0000z1, . . . , zK\u22121\n\u0001\n\u00b7\nY\nk\u2208J\\{j\u2032}\n\u0010\nfZ(zk)1[0,\u2225z\u2212Zbj\u2225)(\u2225zk \u2212z\u2225)\n\u000e Z\nfZ(\u03b6)1[0,\u2225z\u2212Zbj\u2225)(\u2225\u03b6 \u2212z\u2225)d\u03b6\n\u0011\ndz1 \u00b7 \u00b7 \u00b7 dzK\u22121,\nholds almost surely, from what follows the claim of the lemma.\nAs an immediate consequence of Lemma B.1, the random variables Vj := (z \u2212Zj)/\u2225z \u2212Zbj\u2225, j \u2208bJ,\nare conditionally i.i.d. and have the conditional Lebesgue density\nfV |A(v) = 1Bd(0,1)(v) \u00b7 fZ\n\u0000z \u2212v\u2225z \u2212Zbj\u2225\n\u0001 \u000e Z\nBd(0,1)\nfZ\n\u0000z \u2212v\u2032\u2225z \u2212Zbj\u2225\n\u0001\ndv\u2032 ,\nv \u2208Rd ,\n(B.3)\ngiven A. Furthermore, continuing in (B.2) we have that\ninf\nP \u2208P1\nX\nj\u2208b\nJ\nP 2(z \u2212Zj) =\ninf\nP \u2208P1\nX\nj\u2208b\nJ\nP 2(Vj) ,\n(B.4)\nsince P \u2208P1 implies that x 7\u2192P(x/\u2225z \u2212Zbj\u2225) \u2208P1 and vice versa. Now \ufb01x any J\u2217\u2286bJ whose\ncardinality is denoted by K\u2217< K. The joint conditional density f \u2217\nV |A of (Vj, j \u2208J\u2217) given A equals\nf \u2217\nV |A(v) =\nY\nj\u2208J\u2217\nfV |A(vj) ,\nv = (vj, j \u2208J\u2217) \u2208(Rd)K\u2217.\nPreparatory to further consideration we are interested in the conditional distribution of \u2225V \u2212w\u2225given\nA and W := (V \u2212w)/\u2225V \u2212w\u2225when V has the conditional density f \u2217\nV |A given A; and w = (wj, j \u2208\nJ\u2217) \u2208(Rd)K\u2217is deterministic.\n32\nLemma B.2. (a) If V has the conditional density f \u2217\nV |A given A, then the conditional Lebesgue density\nof \u2225V \u2212w\u2225given A and W equals\nf \u2217\nV,w|A,W(r) = 1(0,\u221e)(r) \u00b7 f \u2217\nV |A(w + rW) rdK\u2217\u22121 \u000e Z \u221e\n0\nf \u2217\nV |A(w + sW) sdK\u2217\u22121 ds ,\nr \u2208R .\n(b) Writing\n\u03b21 := min{1/2, \u03c1\u2032/(2\u03c1\u2032\u2032)} ,\n\u03b22 :=\nq\n2K\u2217+ 2\u03b22\n1 ,\n\u03b23 := \u03b2dK\u2217\u22121\n2\n\u00b7 \u03c1K\u2217\u00b7 d \u00b7 K\u2217\u000e\b\n\u03c1K\u2217\u00b7 \u03b2dK\u2217\n1\n\t\n,\nwe assume that \u2225w\u2225\u2264\u03b21. Then, the support of f \u2217\nV,w|A,W is included in the interval [0, \u03b22] and f \u2217\nV,w|A,W\nis bounded from above by \u03b23.\nProof of Lemma B.2. (a) For two probe functions \u03a6 and \u2126, we consider that\nE\n\u0002\n\u03a6(\u2225V \u2212w\u2225)\u2126(W) | A\n\u0003\n=\nZ\n\u03a6(\u2225v \u2212w\u2225)\u2126\n\u0000(v \u2212w)/\u2225v \u2212w\u2225\n\u0001\nf \u2217\nV |A(v) dv\n= E\n\u0002\n\u03a6(\u2225U \u2212w\u2225)\u2126( \u02dcU) f \u2217\nV |A(U)\n\u0003\n\u00b7 \u03c0dK\u2217/2RdK\u2217/\u0393(dK\u2217/2 + 1)\n= E\nh\n\u2126( \u02dcU)\nZ R\n0\n\u03a6(r)f \u2217\nV |A(w + r \u02dcU)rdK\u2217\u22121dr\ni\n\u00b7 d \u00b7 K\u2217\u00b7 \u03c0dK\u2217/2/\u0393(dK\u2217/2 + 1) ,\n(B.5)\nwhere \u02dcU := (U \u2212w)/\u2225U \u2212w\u2225; and R > 0 is su\ufb03ciently large such that the dK\u2217-dimensional ball\naround w with the radius R contains the closure of Bd(0, 1)(K\u2217) \u2013 and, hence, the support of f \u2217\nV |A \u2013 as\na subset; and the random vector U is uniformly distributed on this ball. Therein we have used that\n\u2225U \u2212w\u2225and \u02dcU are independent. Now evaluating (B.5) for \u03a6 \u22611 and \u2126being replaced by \u02dc\u03a6 \u00b7 \u2126, we\ndeduce that\n\u02dc\u03a6( \u02dcU) =\nZ R\n0\n\u03a6(r)f \u2217\nV |A(w + r \u02dcU)rdK\u2217\u22121dr\n\u000e Z R\n0\nf \u2217\nV |A(w + s \u02dcU)sdK\u2217\u22121ds ,\na.s. ,\nso that\nE\n\u0000\u03a6(\u2225V \u2212w\u2225) | W, A\n\u0001\n= \u02dc\u03a6(W) =\nZ R\n0\n\u03a6(r)f \u2217\nV |A(w + rW)rdK\u2217\u22121dr\n\u000e Z R\n0\nf \u2217\nV |A(w + sW)sd \u02dc\nK\u22121ds ,\nholds true almost surely. As the left side does not depend on R the equality remains valid for R \u2192\u221e.\nThat completes the proof of part (a).\n(b) Part (a) yields that \u2225wj+rWj\u2225\u22641 for all j \u2208J\u2217with W = (Wj, j \u2208J\u2217) whenever f \u2217\nV,w|A,W(r) > 0\nsince the support of f \u2217\nV |A is included in Bd(0, 1)(K\u2217). Thus, r\u2225Wj\u2225\u22641 + \u2225wj\u2225holds for all j \u2208J\u2217so\nthat\nr2 =\nX\nj\u2208J\u2217\nr2\u2225Wj\u22252 \u2264\nX\nj\u2208J\u2217\n(2 + 2\u2225wj\u22252) = 2K\u2217+ 2\u03b22\n1 .\n33\nTherefore the support of f \u2217\nV,w|A,W is included in the interval [0, \u03b22]. Moreover, we have\nf \u2217\nV,w|A,W(r) = 1[0,\u03b22](r)\nn Y\nj\u2208J\u2217\n1Bd(0,1)(wj + rWj)fZ\n\u0000z \u2212(wj + rWj)\u2225z \u2212Zbj\u2225\n\u0001o\nrdK\u2217\u22121\n\u000e Z \u221e\n0\nn Y\nj\u2208J\u2217\n1Bd(0,1)(wj + sWj)fZ\n\u0000z \u2212(wj + sWj)\u2225z \u2212Zbj\u2225\n\u0001o\nsdK\u2217\u22121 ds\n\u2264\u03b2dK\u2217\u22121\n2\n\u00b7 \u03c1K\u2217\u000e \u0010\n\u03c1K\u2217Z \u03b21\n0\nsdK\u2217\u22121 ds\n\u0011\n= \u03b23 ,\nwhere we have used that, for s \u2208[0, \u03b21), it holds that \u2225wj +sWj\u2225< 1; that \u2225wj +sWj\u2225\u00b7\u2225z \u2212Zbj\u2225< \u03c1\u2032;\nand, hence, z \u2212(wj + sWj)\u2225z \u2212Zbj\u2225\u2208S as z \u2208S\u2217and Zbj \u2208S, for all j \u2208J\u2217.\nThese properties of f \u2217\nV,w|A,W are essential to establish the following result.\nLemma B.3. As above consider J\u2217\u2286\u02c6J. Assume that d \u22652; that L \u22651; and that K\u2217= #K(d, L) <\nK. De\ufb01ne D as in (2.8), that is\nD :=\nL\nX\n\u2113=1\n\u2113\u00b7 #\n\b\n\u03ba \u2208Nd\n0 | \u03ba1 + \u00b7 \u00b7 \u00b7 + \u03bad = \u2113\n\t\n.\nBy \u039ed,L(x) we denote the square matrix which consists of the row vectors\n\u0000\u03be\u03ba(0, xj)\n\u0001\n\u03ba\u2208K(d,L) for j =\n1, . . . , K\u2217where x :=\n\u0000x1, . . . , xK\u2217\u0001\n\u2208(Rd)K\u2217. Let us write \u03d1d,L(x) for the determinant of \u039ed,L(x).\nLet Vj, j \u2208J\u2217be a conditionally i.i.d. random sample, drawn from f \u2217\nV |A, given A. Write V for the\ndK\u2217-dimensional row vector V := (Vj , j \u2208J\u2217). Then, the random matrix \u039ed,L(V ) is invertible almost\nsurely, and\nP\n\u0002\n|\u03d1d,L(V )| \u2264\u03b5 | A\n\u0003\n\u2264C1 \u00b7 \u03b51/D ,\n\u2200\u03b5 \u2208(0, \u03b51) ,\nalmost surely for deterministic positive constants C1 and \u03b51 which only depend on d, L, \u03c1, \u03c1, \u03c1\u2032, \u03c1\u2032\u2032.\nProof of Lemma B.3. Clearly the function \u03d1d,L forms a polynomial in dK\u2217variables (namely the\ncomponents of x) with the degree of at most D. Note that \u03d1d,L is non-constant since it contains\nexactly K\u2217! \u22652 distinct monomials where each of them has the coe\ufb03cient of either 1 or \u22121. Hence\nthe zero set of \u03d1d,L has the dK\u2217-dimensional Lebesgue measure zero.\nIt follows from there that\n\u03d1d,L(V ) does not vanish and, thus, \u039ed,L(V ) is invertible almost surely as the random vector V has the\ndK\u2217-dimensional Lebesgue density f \u2217\nV |A.\nWe quantify the small ball probabilities of |\u03d1d,L(V )| around zero in terms of upper bounds. By M > 0\nwe denote the maximum of |\u03d1d,L| on the closure of the dK\u2217-dimensional ball with the center 0 and\nthe radius \u03b21/2 where \u03b21, \u03b22 and \u03b23 are as in Lemma B.2. We specify w from Lemma B.2 as some\nelement of that closed ball in which this maximum is taken on. Note that \u2225w\u2225\u2264\u03b21; and that any\nnon-negative, non-constant and continuous function takes on at least one maximum on any compact\ndomain. Consider that\nP\n\u0002\n|\u03d1d,L(V )| \u2264\u03b5 | A\n\u0003\n= E\n\b\nP\n\u0002\n|\u03d1d,L(w + \u2225V \u2212w\u2225W)| \u2264\u03b5 | W, A\n\u0003\n| A\n\t\n,\n(B.6)\n34\nwhere W = (V \u2212w)/\u2225V \u2212w\u2225.\nWe apply the fundamental theorem of algebra to the univariate\npolynomial r 7\u2192\u03d1d,L(w + rW) with the degree D\u2217\u2264D; and we exploit that the absolute value of this\npolynomial takes on the value M at 0 so that\n|\u03d1d,L(w + rW)| \u2265M \u00b7\nD\u2217\nY\nk=1\n\f\f1 \u2212r/|\u03b6j|\n\f\f ,\nfor all r \u22650 where \u03b6j, j = 1, . . . , D\u2217, are the complex roots of the polynomial. Hence,\nP\n\u0002\n|\u03d1d,L(w + \u2225V \u2212w\u2225W)| \u2264\u03b5 | W, A\n\u0003\n\u2264P\nh D\u2217\nY\nk=1\n\f\f1 \u2212\u2225V \u2212w\u2225/|\u03b6j|\n\f\f \u2264\u03b5/M | W, A\ni\n\u2264\nD\u2217\nX\nk=1\nP\nh\n|\u03b6j| \u00b7\n\u00001 + (\u03b5/M)1/D\u2217\u0001\n\u2265\u2225V \u2212w\u2225\u2265|\u03b6j| \u00b7\n\u00001 \u2212(\u03b5/M)1/D\u2217\u0001\n| W, A\ni\n,\n(B.7)\nfor all \u03b5 \u2208(0, M/2). If |\u03b6j| > \u03b22/\n\u00001 \u2212(1/2)1/D\u2217\u0001\nthen\nP\nh\n|\u03b6j| \u00b7\n\u00001 + (\u03b5/M)1/D\u2217\u0001\n\u2265\u2225V \u2212w\u2225\u2265|\u03b6j| \u00b7\n\u00001 \u2212(\u03b5/M)1/D\u2217\u0001\n| W, A\ni\n= 0 .\nOtherwise,\nP\nh\n|\u03b6j|\u00b7\n\u00001+(\u03b5/M)1/D\u2217\u0001\n\u2265\u2225V \u2212w\u2225\u2265|\u03b6j|\u00b7\n\u00001\u2212(\u03b5/M)1/D\u2217\u0001\n| W, A\ni\n\u22642\u03b22 \u03b23 (\u03b5/M)1/D/\n\u00001\u2212(1/2)1/D\u0001\n.\nTherefore, the term (B.7) is bounded from above by\n2D \u03b22 \u03b23 (\u03b5/M)1/D /\n\u00001 \u2212(1/2)1/D\u0001\n,\nfor all \u03b5 \u2208(0, M/2). Inserting this upper bound into (B.6) \ufb01nally completes the proof.\nThe inequality from Lemma B.3 can be extended to the term u0.\nLemma B.4. Grant the assumptions and the notation from Lemma B.3. Then,\nP\nh\ninf\nP \u2208P1\nX\nj\u2208J\u2217\nP 2(Vj) \u2264\u03b5 | A\ni\n\u2264C2 \u00b7 \u03b51/(2D) ,\n\u2200\u03b5 \u2208(0, \u03b52) ,\nfor deterministic positive constants C2 and \u03b52 which only depend on d, L, \u03c1, \u03c1, \u03c1\u2032, \u03c1\u2032\u2032.\nProof of Lemma B.4. Consider that\ninf\nP \u2208P1\nX\nj\u2208J\u2217\nP 2(Vj) =\ninf\n\u03b1\u2208RK,\u03b10=1 \u2225\u039ed,L(V )\u03b1\u22252 =\ninf\n\u03b1\u2208RK,\u03b10=1 \u03b1\u22a4\u039ed,L(V )\u22a4\u039ed,L(V )\u03b1 ,\nso that the above term is larger or equal to the smallest eigenvalue of the symmetric and positive\nde\ufb01nite matrix \u039ed,L(V )\u22a4\u039ed,L(V ). On the other hand, the largest eigenvalue of \u039ed,L(V )\u22a4\u039ed,L(V ) is\nbounded from above by the product of the spectral norms of \u039ed,L(V )\u22a4and \u039ed,L(V ) and, thus, by\n\u2225\u039ed,L(V )\u22252\nF where \u2225\u00b7 \u2225F denotes the Frobenius norm. We have\n\u2225\u039ed,L(V )\u22252\nF =\nX\nj\u2208J\u2217\nX\n\u03ba\u2208K\n\u03be2\n\u03ba(0, Vj) \u2264\nX\nj\u2208J\u2217\nL\nX\n\u2113=0\n\u2225Vj\u22252\u2113X\n|\u03ba|=\u2113\n\u0012\n\u2113\n\u03ba1, . . . , \u03bad\n\u0013\nd\nY\nk=1\n\u0000V 2\nj,k/\u2225Vj\u22252\u0001\u03bak\n=\nX\nj\u2208J\u2217\nL\nX\n\u2113=0\n\u2225Vj\u22252\u2113\u2264(L + 1) \u00b7 K\u2217,\n35\nas \u2225Vj\u2225\u22641 a.s., see (B.3). As the determinant of a symmetric matrix equals the product of all of its\neigenvalues it follows that\n\u03d12\nd,L(V ) = det\n\u0000\u039ed,L(V )\u22a4\u039ed,L(V )\n\u0001\n\u2264\n\u0000(L + 1)K\u2217\u0001K\u2217\u22121 \u00b7 inf\nP \u2208P1\nX\nj\u2208J\u2217\nP 2(Vj) .\nThen, Lemma B.3 yields that\nP\nh\ninf\nP \u2208P1\nX\nj\u2208J\u2217\nP 2(Vj) \u2264\u03b5 | A\ni\n\u2264P\n\u0002\f\f\u03d1d,L(V )\n\f\f \u2264\u03b51/2\u0000(L + 1)K\u2217\u00011/2\u2212K\u2217/2 | A\n\u0003\n\u2264C1\n\u0000(L + 1)K\u2217\u00011/(2D)\u2212K\u2217/(2D) \u00b7 \u03b51/(2D) ,\nfor all \u03b5 \u2208(0, \u03b52) where the constant \u03b52 > 0 only depends on d, L, \u03c1, \u03c1, \u03c1\u2032 and \u03c1\u2032\u2032.\nNow we ready to bound the conditional moments of u0 as stated in Lemma 6.3.\nProof of Lemma 6.3. Combining (B.2) and (B.4), we deduce that\nE\n\u0002\u0000e\u22a4\n0 MJ(z)\u22121e0\n\u0001\u03b7 | A\n\u0003\n\u2264\nZ \u221e\n0\nP\nh\ninf\nP \u2208P1\nX\nj\u2208\u02c6J\nP 2(Vj) < t\u22121/\u03b7 | A\ni\ndt .\n(B.8)\nThere exist pairwise disjoint subsets \u02c6Jk, k = 1, . . . , m, of \u02c6J with # \u02c6Jk = K\u2217for all k = 1, . . . , m, where\nm > 2\u03b7D. Clearly,\ninf\nP \u2208P1\nX\nj\u2208\u02c6J\nP 2(Vj) \u2265\nm\nX\nk=1\ninf\nP \u2208P1\nX\nj\u2208\u02c6Jk\nP 2(Vj) \u2265\nmax\nk=1,...,m inf\nP \u2208P1\nX\nj\u2208\u02c6Jk\nP 2(Vj) ,\nwhere the random variables infP \u2208P1\nP\nj\u2208\u02c6Jk P 2(Vj), k = 1, . . . , m, are i.i.d. (conditionally on A). By\nLemma B.4, the right side of (B.8) is smaller or equal to\nZ \u221e\n0\nP\nh\nmax\nk=1,...,m inf\nP \u2208P1\nX\nj\u2208\u02c6Jk\nP 2(Vj) < t\u22121/\u03b7 | A\ni\ndt\n\u2264\u03b5\u2212\u03b7\n2\n+\nZ\nt>\u03b5\u2212\u03b7\n2\nn\nP\nh\ninf\nP \u2208P1\nX\nj\u2208\u02c6J1\nP 2(Vj) < t\u22121/\u03b7 | A\niom\ndt\n\u2264\u03b5\u2212\u03b7\n2\n+ Cm\n2\nZ\nt>\u03b5\u2212\u03b7\n2\nt\u2212m/(2\u03b7D) dt\n\u2264\u03b5\u2212\u03b7\n2\n+ Cm\n2\n2\u03b7D\nm \u22122\u03b7D \u03b5\u2212\u03b7+m/(2D)\n2\n,\nso that the lemma has been shown.\nProof of Remark 2: For any J \u2208JK and integer \u2113\u22651, we deduce by Fubini\u2019s theorem that\nE\u03bb\u2113(C(J)) = E\n\u0010 Z\nS\u22171C(J)(x) dx\n\u0011\u2113\n=\nZ\nS\u2217\u00b7 \u00b7 \u00b7\nZ\nS\u2217P\n\u0002\nx1, . . . , x\u2113\u2208C(J)\n\u0003\ndx1 \u00b7 \u00b7 \u00b7 dx\u2113\n=\nZ\nS\u2217\u00b7 \u00b7 \u00b7\nZ\nS\u2217P\n\u0002\n\u2225xm \u2212Zj\u2225< \u2225xm \u2212Zk\u2225, \u2200m = 1, . . . , \u2113, j \u2208J, k \u0338\u2208J\n\u0003\ndx1 \u00b7 \u00b7 \u00b7 dx\u2113\n=\nZ\nS\u2217\u00b7 \u00b7 \u00b7\nZ\nS\u2217E PK\nZ\n\u0010\n\u2113\\\nm=1\nBd\n\u0000xm, min\nk\u0338\u2208J \u2225xm \u2212Zk\u2225\n\u0001\u0011\ndx1 \u00b7 \u00b7 \u00b7 dx\u2113,\n36\nwhere B(c, r) denotes the Euclidean ball around the center c with the radius r; and PZ stands for the\nimage measure of Z1. For some deterministic sequence (\u03b1n)n \u21930 we consider that\nP\n\u0002\nmin\nm=1,...,\u2113min\nk\u0338\u2208J \u2225xm \u2212Zk\u2225> \u03b1n\n\u0003\n= P\n\u0002\n\u2225xm \u2212Zk\u2225> \u03b1n , \u2200m = 1, . . . , \u2113, k \u0338\u2208J\n\u0003\n= Pn\u2212K\nZ\n\u0010\n\u2113\\\nm=1\nRd\\B(xm, \u03b1n)\n\u0011\n=\nn\n1 \u2212PZ\n\u0010\n\u2113[\nm=1\nB(xm, \u03b1n)\n\u0011on\u2212K\n\u2265\nn\n1 \u2212\n\u2113\nX\nm=1\nPZ\n\u0000B(xm, \u03b1n)\n\u0001on\u2212K\n\u2265\nn\n1 \u2212\u2113\u00b7 \u03c1 \u00b7\n\u03c0d/2\n\u0393(d/2 + 1) \u00b7 \u03b1d\nn\non\u2212K\n,\nfor n su\ufb03ciently large. We write En(x), x = (x1, . . . , x\u2113), for the event that minm=1,...,\u2113mink\u0338\u2208J \u2225xm \u2212\nZk\u2225> \u03b1n. We \ufb01x that \u03b1n \u224dn\u22121/d so that\nlim inf\nn\u2192\u221e\ninf\nx\u2208(S\u2217)\u2113P(En(x)) > 0 .\nFurthermore we introduce the set\nSn :=\n\b\nx = (x1, . . . , x\u2113) \u2208(S\u2217)\u2113: \u2225x1 \u2212xm\u2225< \u03b1n/2 , \u2200m = 1, . . . , \u2113\n\t\n.\nOn the event En(x) the ball B(x1, \u03b1n/2) is included in T\u2113\nm=1 Bd\n\u0000xm, mink\u0338\u2208J \u2225xm \u2212Zk\u2225\n\u0001\nand in S (for\nn su\ufb03ciently large) as a subset for any x \u2208Sn. Therefore,\nE \u03bb\u2113(C(J)) \u2265\n\u0010\n\u03c0d/2\n2d\u0393(d/2 + 1)\n\u0011K\n\u00b7 \u03c1K \u00b7 \u03b1Kd\nn\n\u00b7\nZ\n\u00b7 \u00b7 \u00b7\nZ\nSn\nP\n\u0000En(x)\n\u0001\ndx1 \u00b7 \u00b7 \u00b7 dx\u2113\n\u2265\n\u0010\n\u03c0d/2\n2d\u0393(d/2 + 1)\n\u0011K\n\u00b7 \u03c1K \u00b7 \u03b1Kd\nn\n\u00b7\ninf\nx\u2208(S\u2217)\u2113P\n\u0000En(x)\n\u0001\n\u00b7\nZ\n\u00b7 \u00b7 \u00b7\nZ\nSn\ndx1 \u00b7 \u00b7 \u00b7 dx\u2113.\nThe \u2113d-dimensional Lebesgue measure of Sn shall be bounded from below by const.\u00b7\u03b1d(\u2113\u22121)\nn\nas n tends\nto \u221e. Then, unfortunately, the \u2113th moment of \u03bb(C(J)) cannot attain the desired rate n\u2212\u2113K but only\nn\u2212\u2113\u2212K+1.\nC\nProofs for Section 3\nProof of Theorem 4. We show below that the estimators \u02c6\u03a8j of the Fourier coe\ufb03cients \u27e8h \u2217\u02dcf\u03b4, \u03c6j\u27e9\nresulting from (2.4) satisfy for each integer l \u2208N that\nsup\nj\u2208J(n) E\n\u0002\f\f\u02c6\u03a8j \u2212\u03a8j\n\f\f2\u0003\n\u2272n\u22121 + (Jn \u00b7 n\u22121/d)2l ,\n(C.1)\nIf we take l large enough, for our choice of Jn the \ufb01rst term dominates as Jn \u2272n1/d\u22121/(2l) and l > d/2.\nBy Parseval\u2019s identity, standard risk bounds for series estimators under the Sobolve condition (3.3)\nlead to\nE\nh Z\n[\u2212\u03c0,\u03c0]d |\u02c6h(x) \u2212h(x)|2dx\ni\n\u2272J\u22122\u03b1\nn\n+ Jd\nn \u00b7 J2\u03b3\nn \u00b7 n\u22121 ,\nfrom which the rate (3.4) follows upon inserting our choice of Jn.\n37\nConcerning (C.1): Since \u2225\u03c6j\u2225\u221e\u22641 the constant CV in Theorem 1 is set equal to the variance var \u01eb;\nwhereas\n\r\r\u2202i1 \u00b7 \u00b7 \u00b7 \u2202ilGj\n\r\r\n\u221e\u2264\nX\nM\u2286{1,...,l}\n\r\r\u2202iM\n\u0000h \u2217\u02dcf\u03b4\n\u0001\r\r\n\u221e\u00b7 Jl\u2212#M\nn\n,\nfor all j \u2208J(n) := {\u2212Jn, . . . , Jn}d where \u2202i1, . . . , \u2202il, i1, . . . , il \u2208{1, . . . , d}, denote the partial dif-\nferential operators with respect to the components zi1, . . . , zil where we allow for coincidence of the\ni1, . . . , il, and \u2202iM stands for the chaining of all \u2202ik, k \u2208M. Therein note the symmetry of the partial\nderivatives. By the Cauchy-Schwarz inequality and the Fourier representation of the Sobolev norm,\nwe consider that\n\r\r\u2202iM\n\u0000h \u2217\u02dcf\u03b4\n\u0001\r\r\n\u221e= \u2225(\u2202iM h) \u2217\u02dcf\u03b4\u2225\u221e\u2264\n\u0010 Z\n|\u2202iM h|2\u00111/2\n\u00b7\n\u0010 Z\n|f\u03b4|2\u00111/2\n\u2264(2\u03c0)\u2212d/2 \u00b7 C1/2\n\u03b1\n\u00b7\n\u0010 Z\n|f\u03b4|2\u00111/2\n,\nwhen l \u2264\u03b1, so that\n\r\r\u2202i1 \u00b7 \u00b7 \u00b7 \u2202ilGj\n\r\r\n\u221e\u2264CH = CH(j, l) := (2\u03c0)\u2212d/2 \u00b7 C1/2\n\u03b1\n\u00b7\n\u0010 Z\n|f\u03b4|2\u00111/2\n\u00b7 (1 + Jn)l ,\nfor j \u2208J(n) when l \u2264\u2308\u03b1\u2309\u22121. Then, Gj \u2208G(l \u22121, 1, CH(j, l)) and Theorem 1 yields (C.1).\nD\nAdditional simulations\nIn d = 2 we consider a further simulation setting with regressions function\nf3(z1, z2) = 5 z1 (z2\n2 + 1) + 10 cos(z1/z2)\nand Yi = fj(Zi,1, Zi,2) + 0.2 \u03b5i, otherwise the parameters are the same as before. Results are displayed\nin Table 3.\nHigher values of K perform better in this setting, but the method does not seem to depend sensitively\non the choice of K for L = 1 as long as K is not chosen too small.\n38\nn = 100\nL = 0\nK\n1\n3\n4\n6\n8\n10\n\u221an\u00b7 BIAS\n0.0140\n0.0280\n0.0343\n0.0457\n0.0557\n0.0650\n\u221an\u00b7 STDV\n0.0988\n0.0949\n0.0961\n0.0995\n0.1035\n0.1073\n\u221an\u00b7 RMSE\n0.0998\n0.0990\n0.1021\n0.1095\n0.1175\n0.1254\nL = 1\nK\n4\n8\n10\n12\n14\n16\n\u221an\u00b7 BIAS\n-0.0002\n-0.0011\n-0.0018\n-0.0035\n-0.0060\n-0.0091\n\u221an\u00b7 STDV\n0.1539\n0.0950\n0.0942\n0.0949\n0.0962\n0.0978\n\u221an\u00b7 RMSE\n0.1539\n0.0950\n0.0942\n0.0949\n0.0964\n0.0982\nn = 1000\nL = 0\nK\n1\n3\n4\n6\n8\n10\n\u221an\u00b7 BIAS\n0.0121\n0.0145\n0.0170\n0.0221\n0.0267\n0.0320\n\u221an\u00b7 STDV\n0.1242\n0.1160\n0.1145\n0.1126\n0.1122\n0.1116\n\u221an\u00b7 RMSE\n0.1248\n0.1169\n0.1157\n0.1148\n0.1153\n0.1161\nL = 1\nK\n4\n8\n10\n12\n14\n16\n\u221an\u00b7 BIAS\n0.0157\n0.0098\n0.0111\n0.0124\n0.0138\n0.0153\n\u221an\u00b7 STDV\n0.1758\n0.1165\n0.1139\n0.1123\n0.1111\n0.1103\n\u221an\u00b7 RMSE\n0.1765\n0.1169\n0.1145\n0.1130\n0.1120\n0.1114\nTable 3: Simulation scenario with f3 in d = 2\n39\n"}