{"text": "arXiv:1702.00842v2  [math.ST]  15 Mar 2017\nASYMPTOTIC NORMALITY OF ELEMENT-WISE WEIGHTED\nTOTAL LEAST SQUARES ESTIMATOR IN A MULTIVARIATE\nERRORS-IN-VARIABLES MODEL\nYA. V. TSAREGORODTSEV\nAbstract. A multivariable measurement error model AX \u2248B is considered. Here\nA and B are input and output matrices of measurements and X is a rectangular\nmatrix of \ufb01xed size to be estimated. The errors in [A, B] are row-wise independent,\nbut within each row the errors may be correlated. Some of the columns are observed\nwithout errors and the error covariance matrices may di\ufb00er from row to row. The\ntotal covariance structure of the errors is known up to a scalar factor.\nThe fully\nweighted total least squares estimator of X is studied. We give conditions for asymp-\ntotic normality of the estimator, as the number of rows in A is increasing. We provide\nthat the covariance structure of the limiting Gaussian random matrix is nonsingular.\n1. Introduction\nWe deal with an overdetermined set of linear equations AX \u2248B, which is common in\nlinear parameter estimation problems [12]. If both the data matrix A and observation\nmatrix B are contaminated with errors, and all the errors are uncorrelated and have equal\nvariances, the total least squares (TLS) technique is appropriate for solving this set [4],\n[12]. Under mild conditions, the TLS estimator of X is consistent and asymptotically\nnormal, as the number of rows in A is increasing [3], [7].\nIn this paper we consider heteroscedastic errors. The errors in [A, B] are row-wise\nindependent, but within each row the errors may be correlated. Some of the columns are\nobserved without errors, and the error covariance matrices may di\ufb00er from row to row.\nThe total error covariance structure is assumed known up to a scalar factor. For this\nmodel, the element-wise weighted total least squares (EW-TLS) estimator is introduced\nand its consistency is proven in [6]. Concerning the computation of the estimator see\n[10], [5]. The EW-TLS estimator \u02c6X is applied, e.g., in geodesy [9].\nOur goal is to extend the asymptotic normality result of [7] to the EW-TLS estimator.\nWe work under the conditions of Theorem 2, [6] about the consistency of \u02c6X. We use the\nobjective function of the estimator, see formula (22) in [6], and the rules of matrix\ncalculus [2].\nThe paper is organized as follows. In section 2, we describe the model, introduce main\nassumptions, refer to the consistency result for \u02c6X and present the objective function\nand the matrix estimating function. In Section 3, we state the asymptotic normality\nresult and provide a nonsingular covariance structure for a limiting random matrix. In\nSection 4, we derive consistent estimators for nuisance parameters of the model in order to\nestimate consistently the asymptotic covariance structure of \u02c6X, and Section 5 concludes.\nThe proofs are given in Appendix.\nThroughout the paper all vectors are column ones, E stands for expectation and acts\nas an operator on the total product, cov(x) denotes the covariance matrix of a ran-\ndom vector x, and for a sequence of random matrices {Xm, m \u22651} of the same size,\n2000 Mathematics Subject Classi\ufb01cation. 62E20; 62F12; 62J05; 62H12; 65F20.\nKey words and phrases. Asymptotic normality, element-wise weighted total least squares estimator,\nheteroscedastic errors, multivariate errors-in-variables model.\n1\n2\nYA. V. TSAREGORODTSEV\nnotation Xm = Op(1) means that the sequence {||Xm||} is stochastically bounded, and\nXm = op(1) means that ||Xm||\nP\u21920. Ip denotes the identity matrix of size p.\n2. Observation model and consistency of the estimator\n2.1. The EW-TLS promblem. We deal with the model AX \u2248B. Here A \u2208Rm\u00d7n and\nB \u2208Rm\u00d7d are matrices of observations, and the matrix X \u2208Rn\u00d7d is to be estimated.\nAssume that\nA = A0 + \u02dcA,\nB = B0 + \u02dcB,\n(2.1)\nand that there exists X0 \u2208Rn\u00d7d such that\nA0X0 = B0.\n(2.2)\nHere A0 is nonrandom true input matrix, B0 is a true output matrix, and \u02dcA, \u02dcB are error\nmatrices. X0 is the true value of the matrix parameter.\nIt is useful to rewrite the model (2.1) and (2.2) as a classical errors-in-variables (EIV)\nmodel [1]. Denote a\u22a4\ni , a\u22a4\n0i, \u02dca\u22a4\ni , b\u22a4\ni , b\u22a4\n0i, \u02dcb\u22a4\ni , i = 1, . . . , m, the rows of A, A0, \u02dcA, B, B0 and\n\u02dcB, respectively. Then the model above is equivalent to the EIV model\nai = a0i + \u02dcai,\nbi = b0i + \u02dcbi, b0i = X\u22a4\n0 a0i,\ni = 1, . . . , m.\n(2.3)\nVectors a0i are nonrandom and unknown, and vectors \u02dcai, \u02dcbi are random errors. Based\non observations ai, bi, i = 1, . . . , m, one has to estimate X0.\nRewrite the model (2.1) and (2.2) in an implicit way. Introduce matrices\nC = [A, B],\nC0 = [A0, B0],\n\u02dcC = [ \u02dcA, \u02dcB],\nZ0 =\n\u0014 X0\n\u2212Id\n\u0015\n.\n(2.4)\nThen (2.1), (2.2) is equivalent to the next relations:\nC = C0 + \u02dcC,\nC0Z0 = 0.\nLet \u02dcC = (\u02dccij, i = 1, . . . , m, j = 1, . . . , n+d). Following [6] we state global assumptions\nof the paper, conditions (i) to (iv).\n(i). Vectors \u02dcci := (\u02dcci1, . . . , \u02dcci,n+d)\u22a4, i = 1, 2, . . ., are independent with zero mean and\n\ufb01nite second moments.\nLet \u03c32\nij = E \u02dcc2\nij, i = 1, 2, . . ., j = 1, . . . , n + d. We allow that some of \u03c32\nij are vanishing.\n(ii). For a \ufb01xed J \u2282{1, 2, . . ., n + d}, every j /\u2208J and every i = 1, 2, . . . satisfy\n\u03c32\nij = 0. Moreover\ncov(\u02dccij, j \u2208J) = \u03c32\u03a3i,\ni = 1, 2, . . . ,\nwith unknown positive factor of proportionality \u03c32 and known matrices \u03a3i.\n(iii). There exists \u03ba > 0 such that for every i = 1, 2, . . . , it holds \u03bbmin(\u03a3i) \u2265\u03ba2.\nFor the matrix Z0 = (z0,jk) given in (2.4) and the set J from condition (ii), denote\nZ0J = (z0,jk, j \u2208J, k = 1, . . . , d).\n(iv).\nrank(Z0J) = d.\nThe EW-TLS problem consists in \ufb01nding the value \u02c6X of the unknown matrix X and\nvalues of disturbances \u2206\u02c6A, \u2206\u02c6B minimizing the weighted sum of squared corrections:\nmin\n(X\u2208Rn\u00d7d,\u2206A,\u2206B)\nm\nX\ni=1\n||\u03a3\u22121/2\ni\n\u2206cJ\ni ||2\n(2.5)\nsubject to constrains\n(A \u2212\u2206A)X = B \u2212\u2206B,\n\u2206cJ\ni = 0,\ni = 1, . . . , m,\nj /\u2208J.\nASYMPTOTIC NORMALITY OF EW-TLS ESTIMATOR IN A MULTIVARIATE EIV MODEL\n3\nHere C = [A, B] = (cij), \u2206C = [\u2206A, \u2206B] = (\u2206cij) and the column vectors\n\u2206cJ\ni := (\u2206cij, j \u2208J) \u2208R|J|.\n2.2. EW-TLS estimator and its consistency. For a random realization, it can hap-\npen that the problem (2.5) has no solution. Assume conditions (i) \u2013 (iv).\nDe\ufb01nition 1. The EW-TLS estimator \u02c6X = \u02c6XEW\u2212T LS of X0 in the model (2.1), (2.2)\nis a Borel measurable mapping of the data matrix C into Rn\u00d7d \u222a{\u221e}, which solves the\nproblem (2.5) under the additional constraint\nrank(ZJ) = d\n(2.6)\n\u0012\nhere Z =\n\u0014\nX\n\u2212Id\n\u0015\n= (zjk), ZJ := (zjk, j \u2208J, k = 1, . . . , d)\n\u0013\n, if there exists a solution, and\n\u02c6X = \u221eotherwise.\nThe EW-TLS estimator always exists due to [11]. We need more conditions to provide\nthe consistency of \u02c6X.\n(v). There exists r \u22652 with r > d\n\u0012\n|J| \u2212d + 1\n2\n\u0013\nsuch that\nsup\n(i\u22651,j\u2208J)\nE |\u02dccij|2r < \u221e.\n(vi). \u03bbmin(A\u22a4\n0 A0)\n\u221am\n\u2192\u221e, as m \u2192\u221e.\n(vii). \u03bb2\nmin(A\u22a4\n0 A0)\n\u03bbmax(A\u22a4\n0 A0) \u2192\u221e, as m \u2192\u221e.\nThe next result on weak consistency is stated in Theorem 2, [6].\nTheorem 2. Assume conditions (i) to (vii). Then the EW-TLS estimator \u02c6X is \ufb01nite\nwith probability tending to one, and \u02c6X tends to X0 in probability, as m \u2192\u221e.\nNotice that under a bit stronger assumptions on eigenvalues of A\u22a4\n0 A0, the estimator\n\u02c6X is strongly consistent, see Theorem 3, [6].\n2.3. The estimating function. Remember that error vectors \u02dcci enter condition (i) and\nthe matrix Z = Z(X) is introduced in De\ufb01nition 1. Let\nSi := 1\n\u03c32 cov(\u02dcci), i = 1, 2, . . .\nDenote also\nq(c, S; X) = c\u22a4Z(Z\u22a4SZ)\u22121Z\u22a4c,\n(2.7)\nwhere c =\n\u0014\na\nb\n\u0015\n\u2208R(n+d)\u00d71, S \u2208R(n+d)\u00d7(n+d), and\nQ(X) =\nm\nX\ni=1\nq(ci, Si; X),\nX \u2208Rn\u00d7d,\nrank(ZJ) = d.\n(2.8)\nNotice that due to (iv) |J| \u2265d, and under constraint (2.6) ZJ is of full rank. Then,\nunder conditions (i) \u2013 (iii) the matrix Z\u22a4SiZ is nonsingular, i = 1, 2, . . .\nThe EW-TLS estimator is known to minimize the objective function (2.7), see Theo-\nrem 1, [6].\nLemma 3. Assume conditions (i) to (iv). The EW-TLS estimator \u02c6X is \ufb01nite if, and\nonly if, there exists an unconditional minimum of the function (2.8), and then \u02c6X is a\nminimum point of this function.\n4\nYA. V. TSAREGORODTSEV\nIntroduce an estimating function related to the loss function (2.7):\ns(a, b, S; X) = \u02dcs \u00b7 (Z\u22a4SZ)\u22121,\n(2.9)\n\u02dcs = \u02dcs(a, b, S; X) := ac\u22a4Z \u2212[Sa, Sab]Z(Z\u22a4SZ)\u22121Z\u22a4cc\u22a4Z.\n(2.10)\nHere\nc =\n\u0014a\nb\n\u0015\n,\na \u2208Rn\u00d71;\nS =\n\u0014 Sa\nSab\nSba\nSb\n\u0015\n,\nSa \u2208Rn\u00d7n.\n(2.11)\nCorollary 4. Assume conditions (i) \u2013 (vii). Then the next two statements hold true.\n(a) With probability tending to one \u02c6X is a solution to the equation\nm\nX\ni=1\ns(ai, bi, Si; X) = 0,\nX \u2208Rn\u00d7d, rank(ZJ) = d.\n(b) The function (2.9) is an unbiased estimating function, i.e., for each i \u22651,\nEX0 s(ai, bi, Si; X0) = 0.\nFor \ufb01xed a, b, S, the function (2.9) maps X into Rn\u00d7d. The derivative s\u2032\nX is a linear\noperator in this space.\nLemma 5. Under conditions (i) \u2013 (vii), for each H \u2208Rn\u00d7d and i \u22651 it holds\nEX0[s\u2032\nX(ai, bi, Si; X0) \u00b7 H] = a0ia\u22a4\n0iH(Z0SiZ0)\u22121.\n(2.12)\n3. Asymptotic normality of the estimator\nIntroduce further assumptions.\n(viii). For some \u03b4 > 0,\nsup\n(i\u22651,j\u2208J)\nE |\u02dccij|4+2\u03b4 < \u221e.\n(ix). For \u03b4 from the condition (viii),\n1\nm1+\u03b4/2\nm\nX\ni=1\n||a0i||2+\u03b4 \u21920,\nas m \u2192\u221e.\n(x).\n1\nmA\u22a4\n0 A0 \u2192VA, as m \u2192\u221e, where VA is a nonsingular matrix.\nNotice that condition (x) implies assumptions (vi), (vii).\n(xi). For matrices from condition the (ii), \u03a3i \u2192\u03a3\u221e, as m \u2192\u221e, where \u03a3\u221eis certain\nmatrix.\nNotice that conditions (xi), (iii) imply that \u03a3\u221eis nonsingular.\n(xii). If p, q, r \u2208J (they are not necessarily distinct) and i \u22651, then\nE \u02dccip\u02dcciq\u02dccir = 0.\n(xiii). If p, q, r, u \u2208J (they are not necessarily distinct), then 1\nm\nm\nX\ni=1\nE \u02dccip\u02dcciq\u02dccir\u02dccin con-\nverges to a \ufb01nite limit \u00b54(p, q, r, u), as m tends to in\ufb01nity.\nIntroduce a random element in the space of couples of matrices:\nWi = (a0i\u02c6c\u22a4\ni , \u02dcci\u02dcc\u22a4\ni \u2212\u03c32Si).\n(3.1)\nHereafter\nd\u2192stands for the convergence in distribution.\nLemma 6. Assume conditions (i), (ii) and (viii) \u2013 (xiii). Then\n1\n\u221am\nm\nX\ni=1\nWi\nd\u2192\u0393 = (\u03931, \u03932),\nas m \u2192\u221e,\n(3.2)\nwhere \u0393 is a Gaussian centered random element with independent matrix components \u03931\nand \u03932.\nASYMPTOTIC NORMALITY OF EW-TLS ESTIMATOR IN A MULTIVARIATE EIV MODEL\n5\nNow, we state the asymptotic normality of the EW-TLS estimator.\nTheorem 7. Assume conditions (i) \u2013 (v) and (viii) \u2013 (xiii). Then\n\u221am( \u02c6X \u2212X0)\nd\u2192V \u22121\nA \u0393(X0), as m \u2192\u221e,\n(3.3)\n\u0393(X) := \u03931Z + Pa\u03932Z \u2212[S\u221e\na , S\u221e\nab]Z(Z\u22a4S\u221eZ)\u22121(Z\u22a4\u03932Z),\n(3.4)\nwhere VA enters condition (x), Pa is the projector with Pa\n\u0014a\nb\n\u0015\n= a, \u03931 and \u03932 enter\nrelation (3.2), and\nS\u221e=\n\u0014S\u221e\na\nS\u221e\nab\nS\u221e\nba\nS\u221e\nb\n\u0015\n= lim\ni\u2192\u221eSi,\nZ =\n\u0014 X\n\u2212Id\n\u0015\n.\n(3.5)\nMoreover the limiting random matrix X\u221e:= V \u22121\nA \u0393(X0) has a nonsingular covariance\nstructure, i.e., for each nonzero vector u \u2208Rd\u00d71, cov(X\u221eu) is a nonsingular matrix.\n4. Construction of confidence region for a linear functional of X0\n4.1. Estimation of nuisance parameters. Theorem 7 can be applied, e.g., to con-\nstruct a con\ufb01dence region for a linear functional of X0. For this purpose one has to\nestimate consistently a covariance structure of the limiting random matrix V \u22121\nA \u0393(X0).\nSuch a structure, besides of X0, depends on nuisance parameters. Some of them can be\nestimated consistently.\nHereafter bar means average for rows i = 1, . . . , m, e.g.,\nab\u22a4= m\u22121 \u00b7\nm\nX\ni=1\naib\u22a4\ni ,\n\u00afS = m\u22121\nm\nX\ni=1\nSi.\nLemma 8. Assume conditions of Theorem 7. De\ufb01ne\n\u02c6Z =\n\u0012 \u02c6X\n\u2212Id\n\u0013\n,\n\u02c6\u03c32 = 1\ndtr\nh\n( \u02c6Z\u22a4cc\u22a4\u02c6Z)( \u02c6Z\u22a4\u00afS \u02c6Z)\u22121i\n,\n(4.1)\n\u02c6VA = aa\u22a4\u2212\u02c6\u03c32 \u00afS.\n(4.2)\nThen, as m \u2192\u221e,\n\u02c6\u03c32\nP\u2192\u03c32,\n\u02c6VA\nP\u2192VA.\n4.2. Estimation of the asymptotic covariance structure of X0. Let u \u2208Rd\u00d71,\nu \u0338= 0. Theorem 7 implies the convergence\n\u221am( \u02c6Xu \u2212X0u)\nd\u2192N(0, Su),\nas m \u2192\u221e,\n(4.3)\nwith nonsingular matrix Su = cov(V \u22121\nA \u0393(X0)u).\nWe start with the case of normal errors \u02dcci, i = 1, 2, . . . Then condition (xii) holds\ntrue, and Theorem 7 is applicable. The asymptotic covariance matrix Su is a continuous\nfunction Su = Su(X0, VA, \u03c32, S\u221e) of unknown parameters (here the limiting covariance\nmatrix S\u221ecould be unknown, though for a given m, matrices S1, . . . , Sm are assumed\nknown). Due to Theorem 2 and Lemma 8 the matrix\n\u02c6Su := Su( \u02c6X, \u02c6VA, \u02c6\u03c32, \u00afS)\n(4.4)\nis a consistent estimator of Su.\nNow, we do not assume the normality of the errors. Then the exact formula for Su\ndoes not allow to estimate it consistently, because the formula involves higher moments\nof errors which are di\ufb03cult to estimate consistently.\nInstead, we use Corollary 4 to\nconstruct the so-called sandwich estimator [1] for Su. Denote\n\u02c6si = \u02dcs(ai, bi, Si; \u02c6X),\ni = 1, . . . , m,\n(4.5)\nwith \u02dcs introduced in (2.10)\n6\nYA. V. TSAREGORODTSEV\nLemma 9. Assume conditions of Theorem 7. For u \u2208Rd\u00d71, u \u0338= 0, de\ufb01ne\n\u02c6Su = \u02c6V \u22121\nA\n\u00b7 1\nm\nm\nX\ni=1\n\u02c6siuu\u22a4\u02c6s\u22a4\ni ,\n(4.6)\nwith \u02c6VA given in (4.2), (4.1). Then \u02c6Su\nP\u2192Su, as m \u2192\u221e.\nRemark.\nIn the case of normal errors, the estimator (4.4) is asymptotically more\ne\ufb03cient than the estimator (4.6), cf. the discussion in [1], p. 369.\nGiven a consistent estimator \u02c6Su of Su, we have from (4.3) that\n\u221am( \u02c6Su)\u22121/2( \u02c6Xu \u2212X0u)\nd\u2192N(0, In),\nas m \u2192\u221e.\n(4.7)\nBased on (4.7), one can construct in a standard way an asymptotic con\ufb01dence ellipsoid\nfor X0u. Similarly a con\ufb01dence ellipsoid can be constructed for any \ufb01nite set of linear\ncombinations of X0 entries.\n5. Conclusion\nWe proved the asymptotic normality of the EW-TLS estimator in a multivariate errors-\nin-variables model AX \u2248B with heteroscedastic errors. We assumed the convergence\n(xi) of the second error moments, vanishing third moments (xiii), and the convergence\nof averaged fourth moments (xiii).\nThe condition (xii) ensured that the asymptotic\ncovariance structure of \u02c6X is nonsingular. This condition holds true in two cases: (a) all\nthe error vectors \u02dcci are symmetrically distributed, or (b) for each i, random variables \u02dccip,\np \u2208J, are independent and have vanishing coe\ufb03cient of asymmetry.\nThe obtained asymptotic normality result made it possible to construct a con\ufb01dence\nellipsoid for a linear functional of X0. Another plausible application is goodness-of-\ufb01t test\nin the model AX \u2248B with heteroscedastic errors (see [7] for such a test in the model\nwith homoscedastic errors).\nThe author is grateful to Prof. A. Kukush for the problem statement and fruitful\ndiscussions.\nAppendix\nProof of Corollary 4. (a) The space Rn\u00d7d is endowed with natural inner product\n< A, B >= tr(AB\u22a4). The matrix derivative q\u2032\nX of the functional (2.7) is a linear func-\ntional on Rn\u00d7d, and based on the inner product, this functional can be identi\ufb01ed with\ncertain matrix from Rn\u00d7d.\nRemember that Z = Z(X) is introduced in De\ufb01nition 1. Using the rules of matrix\ncalculus [2], we have for H \u2208Rn\u00d7d :\n< q\u2032\nX, H >= c\u22a4\n\u0014H\n0\n\u0015\n(Z\u22a4SZ)\u22121Z\u22a4c + c\u22a4(Z\u22a4SZ)\u22121 \u00b7 [H\u22a4, 0]c\u2212\n\u2212c\u22a4Z(Z\u22a4SZ)\u22121\n\u0012\n[H\u22a4, 0]SZ + Z\u22a4S\n\u0014\nH\n0\n\u0015\u0013\n(Z\u22a4SZ)\u22121Z\u22a4c.\nRemember relations (2.11). Collecting similar terms, we obtain:\n1\n2 < q\u2032\nx, H >= a\u22a4H(Z\u22a4SZ)\u22121Z\u22a4c\u2212\n\u2212c\u22a4Z(Z\u22a4SZ)\u22121Z\u22a4\n\u0014\nSa\nSba\n\u0015\nH(Z\u22a4SZ)\u22121Z\u22a4c,\nand\n1\n2 < q\u2032\nx, H >= tr[ac\u22a4Z(Z\u22a4SZ)\u22121H\u22a4]\u2212\ntr\n\u0002\n[Sa, Sab]Z(Z\u22a4SZ)\u22121Z\u22a4cc\u22a4Z(Z\u22a4SZ)\u22121H\u22a4\u0003\n.\nASYMPTOTIC NORMALITY OF EW-TLS ESTIMATOR IN A MULTIVARIATE EIV MODEL\n7\nUsing the inner product in Rn\u00d7d we obtain\n1\n2q\u2032\nx = \u02dcs(X)(Z\u22a4SZ)\u22121,\nwith \u02dcs(X) = \u02dcs(a, b, S; X) given in (2.10). Now, Theorem 2 and Lemma 3 imply the\nstatement of Corollary 4(a).\n(b) We set\na = a0 + \u02dca,\nb = b0 + \u02dcb,\nb0 = X\u22a4a0,\nc = c0 + \u02dcc =\n\u0014a0\nb0\n\u0015\n+\n\u0014\u02dca\n\u02dcb\n\u0015\n,\n(A.1)\nwhere a0 is a nonrandom vector and like in (2.3),\ncov(\u02dcc) = \u03c32S = \u03c32\n\u0014 Sa\nSab\nSba\nSb\n\u0015\n,\nE \u02dcc = 0.\nThen\nEX ac\u22a4Z = a0c\u22a4\n0\n\u0014 X\n\u2212Id\n\u0015\n+ E \u02dca\u02dcc\u22a4Z = \u03c32[Sa, Sab]Z,\nEX cc\u22a4Z = c0c\u22a4\n0\n\u0014\nX\n\u2212Id\n\u0015\n+ E \u02dcc\u02dcc\u22a4Z = \u03c32SZ.\nTherefore, see (2.9),\nEX \u02dcs(a, b, S; X) = \u03c32[Sa, Sab]Z \u2212\u03c32[Sa, Sab]Z(Z\u22a4SZ)\u22121(Z\u22a4SZ) = 0\nThe statement (b) of Corollary 4 is proven.\nProof of Lemma 5. The derivative s\u2032\nX of the function (2.9) with respect to X is a\nlinear operator in Rn\u00d7d. Denote f = f(Z) = Z(Z\u22a4SZ)\u22121. For H \u2208Rn\u00d7d, it holds:\n\u02dcs\u2032\nXH = aa\u22a4H \u2212[Sa, Sab](f \u2032\nXH)(Z\u22a4cc\u22a4Z) \u2212[Sa, Sab]f \u00b7\n\u0012\n[H\u22a4, 0]cc\u22a4Z + Z\u22a4cc\u22a4\n\u0014H\n0\n\u0015\u0013\n.\nWe set (A.1), use relations\nE aa\u22a4= a0a\u22a4\n0 + \u03c32Sa,\nEX(cc\u22a4Z) = \u03c32SZ\nand get:\nEX(\u02dcs\u2032\nXH) = (a0a\u22a4\n0 + \u03c32Sa)H \u2212\u03c32[Sa, Sab](f \u2032\nXH)(Z\u22a4SZ)\u2212\n\u2212\u03c32[Sa, Sab]f \u00b7\n\u0012\n[H\u22a4, 0]SZ + Z\u22a4S\n\u0014H\n0\n\u0015\u0013\n.\n(A.2)\nNext,\nf \u2032\nXH =\n\u0014H\n0\n\u0015\n(Z\u22a4SZ)\u22121 \u2212Z(Z\u22a4SZ)\u22121\n\u0012\n[H\u22a4, 0]SZ + Z\u22a4S\n\u0014H\n0\n\u0015\u0013\n(Z\u22a4SZ)\u22121.\n(A.3)\nCombining (A.2) and (A.3) we see that on the right-hand side of (A.2) summands\ncontaining H\u22a4are cancelled out. We get \ufb01nally\nEX(\u02dcs\u2032\nXH) = a0a\u22a4\n0 H,\nwhich implies the statement, because by Corollary 4(b) it holds EX \u02dcs(a, b, S; X) = 0.\nProof of Lemma 6. The proof is similar to the proof of Lemmas 6 and 7 from [7] and\nbased on Lyapunov\u2019s Central Limit Theorem. We just notice that due to condition (xii)\nthe matrix components of Wi, namely a0i\u02dcc\u22a4\ni and \u02dcci\u02dcc\u22a4\ni \u2212\u03c32Si, are uncorrelated, and this\nimplies the independence of matrix components \u03931 and \u03932 in (3.2).\n8\nYA. V. TSAREGORODTSEV\nProof of Theorem 7. We follow the line of [7], see there the proof of Theorem 8(a).\nBy Corollary 4(a), it holds with probability tending to 1:\nm\nX\ni=1\ns(ai, bi, Si; \u02c6X) = 0.\n(A.4)\nDenote\n\u02c6\u2206= \u221am( \u02c6X \u2212X0),\nym =\nm\nX\ni=1\ns(ai, bi, Si; X0),\nUm =\nm\nX\ni=1\ns\u2032\nX(ai, bi, Si; X0).\nUsing Taylor\u2019s formula around X0 (see [2], Theorem 5.6.2), we obtain from (A.4) that\n\u0012 1\nmUm\n\u0013\n\u02c6\u2206= \u22121\n\u221amym + rest1,\n||rest1|| \u2264|| \u02c6\u2206|| \u00b7 || \u02c6X \u2212X0|| \u00b7 Op(1).\n(A.5)\nHere Op(1) is a multiplier of the form\n1\nm\nm\nX\ni=1\nsup\n(||X\u2212X0||\u2264\u03b50)\n||s\u2032\u2032\nX(ai, bi, Si; X)||,\n(A.6)\nwith positive \u03b50 chosen such that rank(ZJ) = d, for all X with ||X \u2212X0|| \u2264\u03b50; the choice\nis possible due to condition (iv), and expression (A.6) is indeed Op(1) (i.e., stochastically\nbounded), because s\u2032\u2032\nX is quadratic in ci and the averaged second moments of ci are\nassumed bounded.\nThus, the relation (A.5) holds true due to the consistency of \u02c6X\nstated in Theorem 2.\nWe have ||rest1|| \u2264|| \u02c6\u2206|| \u00b7 op(1). Now, by Lemma 5 and condition (x) and (xi) it holds\n1\nmUmH = VAH(Z\u22a4\n0 S\u221eZ0)\u22121 + op(1),\nH \u2208Rn\u00d7d,\nand we derive from (A.5) the relation\nVA \u02c6\u2206(Z\u22a4\n0 S\u221eZ0)\u22121 = \u22121\n\u221amym + rest2,\n||rest2|| \u2264|| \u02c6\u2206|| \u00b7 op(1).\n(A.7)\nThe summands in ym have zero expectation by Corollary 4(b). Remember that c0iZ0 = 0\nand the projector Pa is introduced in Theorem 7. Then, see (2.9),\n\u02dcs(ai, bi, Si; X0) = (a0i + \u02dcai)\u02dcc\u22a4\ni Z0 \u2212[Sai, Sbi]Z0(Z\u22a4\n0 SiZ0)\u22121(Z\u22a4\n0 \u02dcci\u02dcc\u22a4\ni Z0),\n\u02dcs(ai, bi, Si; X0) = Wi1Z0 + PaWi2Z0 \u2212[Sai, Sbi]Z0(Z\u22a4\n0 SiZ0)\u22121(Z\u22a4\n0 Wi2Z0).\nHere Wij are components of (3.1). By Lemma 6 it holds, see (3.4) and condition (xi):\n1\n\u221amym\nd\u2192\u0393(X0)(Z\u22a4\n0 S\u221eZ0)\u22121,\nas m \u2192\u221e.\n(A.8)\nNow, relations (A.7), (A.8) and nonsingularity of VA imply \u02c6\u2206= Op(1) and by Slutsky\u2019s\nlemma\nVA \u02c6\u2206(Z\u22a4\n0 S\u221eZ0)\u22121\nd\u2192\u0393(X0)(Z\u22a4\n0 S\u221eZ0)\u22121,\nas m \u2192\u221e.\nThis implies the desired convergence (3.3) \u2013 (3.5).\nLet u \u2208Rd\u00d71, u \u0338= 0. By Lemma 6 the components \u03931 and \u03932 are independent. We\nhave\ncov(\u0393(X0)u) \u2265cov(\u03931Z0u) = lim\nm\u2192\u221e\n1\nm\nm\nX\ni=1\nE(a0i\u02dcc\u22a4\ni Z0uu\u22a4Z\u22a4\n0 \u02dccia\u22a4\n0i) =\n= \u03c32VA[u\u22a4(Z\u22a4\n0 S\u221eZ0)u],\nand the latter matrix is positive de\ufb01nite, because VA and Z\u22a4\n0 S\u221eZ0 are positive de\ufb01nite\nunder the conditions of Theorem 7. Therefore, cov(X\u221eu) is a positive de\ufb01nite matrix\nas well.\nASYMPTOTIC NORMALITY OF EW-TLS ESTIMATOR IN A MULTIVARIATE EIV MODEL\n9\nProof of Lemma 8. We have\nE cic\u22a4\ni = c0ic\u22a4\n0i + \u03c32Si,\nZ\u22a4\n0 (E cic\u22a4\ni )Z0 = \u03c32Z\u22a4\n0 SiZ0,\n\u03c32 = 1\ndtr\nh\n(Z\u22a4\n0 cic\u22a4\ni Z0)(Z\u22a4\n0 \u00afSZ0)\u22121i\n+ op(1).\n(A.9)\nRelation (A.9) and the convergence \u02c6Z\nP\u2192Z0 imply the desired convergence \u02c6\u03c32\nP\u2192\u03c32,\nas m \u2192\u221e.\nNext,\n\u02c6VA = E aa\u22a4+ op(1) \u2212\u02c6\u03c32 \u00afS = a0a\u22a4\n0 + (\u03c32 \u2212\u02c6\u03c32) \u00afS + op(1),\n\u02c6VA\nP\u2192lim\nm\u2192\u221ea0a\u22a4\n0 = VA.\nProof of Lemma 9. Denote \u02dcsi = \u02dcs(ai, bi, Si; X0), i = 1, 2, . . . Then expansion (A.7)\nimplies that\nSu = V \u22121\nA\n\u00b7 1\nm\nm\nX\ni=1\n\u02dcsiuiu\u22a4\ni \u02dcs\u22a4\ni + op(1),\nand by Lemma 8\nSu = \u02c6V \u22121\nA\n\u00b7 1\nm\nm\nX\ni=1\n\u02dcsiuiu\u22a4\ni \u02dcs\u22a4\ni + op(1),\n\u02c6Su \u2212Su = \u02c6V \u22121\nA\n\u00b7 1\nm\nm\nX\ni=1\n(\u02c6siuiu\u22a4\ni \u02c6s\u22a4\ni \u2212\u02dcsiuiu\u22a4\ni \u02dcs\u22a4\ni ) + op(1).\nThen \u02c6Su \u2212Su\nP\u21920, as m \u2192\u221e, because \u02c6Z\nP\u2192Z0 and cc\u22a4= Op(1) (see formulas (2.9),\n(2.10) and (4.5)). Lemma 9 is proven.\nReferences\n[1] R. J. Carroll, D. Ruppert, L. A. Stefanski, and C. M. Crainiceanu, Measurement Error in Nonlinear\nModels: A Modern Perspective. 2nd ed. Boca Raton, Chapman and Hall/CRC, 2006.\n[2] H. Cartan, Di\ufb00erential Calculus. Hermann/Houghton Mi\ufb04in Co., Paris/Boston, MA. Translated\nfrom French, 1971.\n[3] L. J. Gleser, Estimation in a multivariate \u201cerrors in variables\u201d regression model: large sample\nresults, Ann. Stat. 9 (1981), no. 1, 24\u201344.\n[4] G. H. Golub and C. F. Van Loan, An analysis of the total least squares problem, SIAM J. Numer.\nAnal. 17 (1980), no. 6, 883\u2013893.\n[5] S. Jazaerti, A. R. Amiri-Simkooei, and M. A. Shari\ufb01, Iterative algorithm for weighted total least\nsquares adjustment, Survey Review 46, (2014), no. 334, 19\u201327.\n[6] A. Kukush and S. Van Hu\ufb00el, Consistency of elementwise-weighted total least squares estimator in\na multivariate errors-in-variables model AX = B, Metrika 59 (2004), no. 1, 75\u201397.\n[7] A. Kukush and Ya. Tsaregorodtsev, Asymptotic normality of total least squares estimator in a\nmultivariable errors-in-variables model AX = B, Modern Stochastics: Theory and Applications 3\n(2016), no. 1, 47\u201357.\n[8] A. Kukush and Ya. Tsaregorodtsev, Goodness-of-\ufb01t test in a multivariate errors-in-variables model\nAX = B, Modern Stochastics: Theory and Applications 3 (2016), no. 4, 287\u2013302.\n[9] V. Mahboud, On weighted total least squares for geodetic transformation, J. of Geodesy 86 (2012),\nno. 5, 359\u2013367.\n[10] I. Markovsky, M. L. Rastello, A. Premoli, A. Kukush, and S. Van Hu\ufb00el, The element-wise weighted\ntotal least-squares problem, Comput. Statist. Data Anal. 50 (2006), no. 1, 181\u2013209.\n[11] G. Pfanzagl, On the measurability and consistency of minimum contrast estimates, Metrika 14\n(1969), no. 1, 249\u2013273.\n[12] S. Van Hu\ufb00el and J. Vandewalle, The Total Least Squares Problem: Computational Aspects and\nAnalysis, Frontiers in Applied Mathematics, vol. 9, SIAM, Philadelphia, PA, 1991.\nDepartment of Mathematical Analysis, Faculty of Mechanics and Mathematics, Taras\nShevchenko National University of Kyiv, Building 4-e, Akademika Glushkova Avenue, Kyiv,\nUkraine, 03127\nE-mail address: 777Tsar777@mail.ru\n"}