{"text": "BAYESIAN ERROR-IN-VARIABLES MODELS FOR THE\nIDENTIFICATION OF POWER NETWORKS\nTECHNICAL REPORT\nJean-S\u00e9bastien Brouillon\nInstitute of Mechanical Engineering,\n\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL),\nCH-1015 Lausanne, Switzerland\njean-sebastien.brouillon@epfl.ch\nEmanuele Fabbiani\nIdenti\ufb01cation and Control of\nDynamic Systems Laboratory,\nUniversity of Pavia,\nPavia, Italy\nemanuele.fabbiani01@universitadipavia.it\nPulkit Nahata\nInstitute of Mechanical Engineering,\n\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL),\nCH-1015 Lausanne, Switzerland\npulkit.nahata@epfl.ch\nKeith Moffat\nDepartment of Electrical Engineering\nand Computer Science,\nUC Berkeley,\nBerkeley, USA\nkeithm@berkeley.edu\nFlorian D\u00f6r\ufb02er\nAutomatic Control Laboratory,\nSwiss Federal Institute of Technology (ETH),\nZurich, Switzerland\ndorfler@control.ee.ethz.ch\nGiancarlo Ferrari-Trecate\nInstitute of Mechanical Engineering,\n\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL),\nCH-1015 Lausanne, Switzerland\ngiancarlo.ferraritrecate@epfl.ch\n13th December 2021\nABSTRACT\nThe increasing integration of intermittent renewable generation, especially at the distribution level,\nnecessitates advanced planning and optimisation methodologies contingent on the knowledge of the\ngrid, speci\ufb01cally the admittance matrix capturing the topology and line parameters of an electric\nnetwork. However, a reliable estimate of the admittance matrix may either be missing or quickly\nbecome obsolete for temporally varying grids. In this work, we propose a data-driven identi\ufb01cation\nmethod utilising voltage and current measurements collected from micro-PMUs. More precisely,\nwe \ufb01rst present a maximum likelihood approach and then move towards a Bayesian framework,\nleveraging the principles of maximum a posteriori estimation. In contrast with most existing con-\ntributions, our approach not only factors in measurement noise on both voltage and current data,\nbut is also capable of exploiting available a priori information such as sparsity patterns and known\nline parameters. Simulations conducted on benchmark cases demonstrate that, compared to other\nalgorithms, our method can achieve signi\ufb01cantly greater accuracy.\nKeywords Bayesian inference \u00b7 Distribution grids \u00b7 Error-in-variables \u00b7 Line admittance estimation \u00b7 Power systems\nidenti\ufb01cation\n1\nIntroduction\nA major key to realising green energy systems is the large-scale integration of renewable energy sources (RESs)\nin the distribution grid. Nevertheless, RES proliferation leads to additional risks such as reverse power \ufb02ows and\nover-voltage\u2014especially during periods of peak generation and low consumption [1]. Distribution grid operators are\narXiv:2107.04480v2  [eess.SY]  19 Dec 2021\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nconsequently required to put in place intelligent monitoring and control algorithms in order to maintain the existing\nlevels of grid safety and reliability [2, 3, 4, 5]. Deploying such algorithms ef\ufb01ciently requires the knowledge of the\ntopology and the line parameters of the grid, embedded in its admittance matrix.\nAn exact estimate of the admittance matrix is hard to obtain for distribution grids, in particular as topological infor-\nmation and line parameter values either are unavailable for large chunks of the network, or become obsolete in the\nevent of a topology change. To circumvent this issue, many recent contributions work out an up-to-date admittance\nmatrix estimate by utilising data collected from micro-phasor measurement units (\u00b5PMUs). Although a more recent\ndevelopment than PMUs, which are commonly deployed in transmission systems, \u00b5PMUs have already been installed\nin distribution grids across America, Asia, and Europe, and their penetration is expected to steadily increase in the\ncoming years [6].\nDue to its increasing relevance, the problem of identifying the topology and line parameters of a power grid has\nattracted considerable attention in the last few years. In [7, 8], an approach based on inverter probing is explored.\nBoth works, besides employing approximate linearized power-\ufb02ow equations, are restricted to radial networks. Albeit\nrequiring voltage and current (or power) measurements at each bus of the grid, identi\ufb01cation methods in [9, 10, 11,\n12, 13] can be applied to both radial and meshed structures. In [12, 13], structural properties of the admittance matrix,\nsuch as symmetry and Laplacianity are used to eliminate redundant admittance matrix parameters. Moreover, [13]\nproposes an adaptive Lasso algorithm promoting sparsity.\nThe tradeoff between voltage stability and the energy of the signal is the main challenge for the identi\ufb01cation of\npower systems. To compensate for the lack of signal, online design-of-experiment procedures are presented in [9, 10];\nnonetheless, the proposed algorithms require control authority on the state of the grid, and additional measurements\nof line power \ufb02ows. They also neglect the structural and sparsity properties of the admittance matrix and stay limited\nby the small acceptable voltage variations.\nAll the foregoing works suffer from two limitations. First, they either completely disregard measurement errors or\nassume errors solely on certain measurements. This creates an estimation bias, for \u00b5PMUs introduce an measurement\nnoise on all electric variables [14, 15]. Second, they do not capitalize on grid information which may already be\navailable a priori, for instance sparsity patterns and known network sections and line parameters. To do away with the\n\ufb01rst limitation, [16, 17] introduce error-in-variable (EIV) models taking into consideration all sources of measurement\nerrors. That notwithstanding, they leave aside all prior information, including structural properties of the admittance\nmatrix, which can potentially improve grid identi\ufb01cation.\nIn this paper, we address the limitations of existing works by putting forth a novel Bayesian grid identi\ufb01cation frame-\nwork, which incorporates EIV models with an unbiased estimation of the error on both voltage and current data, and\ntakes advantage of the principles of maximum likelihood estimation (MLE). Our approach exploits not only the inher-\nent structural properties of the admittance matrix, but enables to exploit grid information known a priori. In particular,\nwe describe how to incorporate in the identi\ufb01cation algorithm different pieces of information which may be available\nto grid operators, partly through data-driven Bayesian priors.\nIn order to substantiate the ef\ufb01cacy of our method, we conduct simulations on a large network with realistic voltage\nand current pro\ufb01les, and \u00b5PMU noise levels compatible with the accuracy of actual commercial devices. We then\ncompare the performance of our proposed methods with other grid identi\ufb01cation procedures proposed in the literature.\nOur analysis shows not only that EIV models are needed to obtain reasonable grid estimates, but also that sparsity\nneeds to be enforced if the topology of the network is unknown. Moreover, it substantiates the value of injecting prior\ninformation into the estimation algorithm, in case it is available.\nThe paper proceeds as follows: Sections 2 to 4 de\ufb01ne the identi\ufb01cation problem, which we \ufb01rst solve using likelihood\nmaximization in Section 5. Sections 6 and 7 introduce prior knowledge using the Bayesian framework. Section 8\nprovides numerical methods for solving the optimization problem, Section 9 presents a realistic simulation, which is\nfurther discussed in Section 10. Section 11 concludes the paper and proposed future developments.\n1.1\nPreliminaries and Notation\nLet j = \u221a\u22121 denote the imaginary unit. For x \u2208Cn, x is its complex conjugate and |x| its magnitude, both taken\nelement-wise. The diagonal matrix of order n associated with x is denoted with [x]. The \u21131 and \u21132 norms of a\nvector x are represented by \u2225x\u22251 and \u2225x\u22252, respectively. Throughout, 1n and 0n are n-dimensional vectors of all\nones and zeros, whereas In and On\u00d7m represent n-by-n identity and m-by-n zero matrices, respectively. The unit\nvector ei, i = 1, ..., n is the ith column of In. For an (m, n) matrix A, A\u22a4denotes its transpose, Ai\u00b7 its ith row\nvector, and vec(A) = [A\u22a4\n1\u00b7 \u00b7 \u00b7 \u00b7 A\u22a4\nn\u00b7]\u22a4the mn-dimensional stacked column vector. Given a square matrix A, ve(A) is\nthe n(n \u22121)/2-dimensional vector obtained by removing diagonal and supra-diagonal elements from \u2212vec(A). The\n2\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nKronecker product between matrices A and B is A \u2297B. Given n elements xn, [xi]n\ni=1 is the stacked column vector\n[x\u22a4\n1 \u00b7 \u00b7 \u00b7 x\u22a4\nn ]\u22a4.\nRandom variables. X \u223cN(\u00b5, \u03c32) denotes a Gaussian random variable with expected value E[X] = \u00b5 and variance\nVar[X] = \u03c32.\nAlgebraic graph theory. We denote by G(V, E, W) an undirected, weighted, and connected graph, where V is the node\nset of cardinality n, E \u2286(V \u00d7V) the edge set, and W collects the edge weights. The adjacency matrix W \u2208Cn\u00d7n has\nelements whk corresponding to the weight of the edges (h, k) \u2208E and zero otherwise. The matrix L = [W 1n] \u2212W,\nL \u2208Cn\u00d7n, is the Laplacian matrix associated with G. By de\ufb01nition, a Laplacian matrix is symmetric and such that\nL1n = 0n.\n2\nGrid model and data collection\n2.1\nPower grid model\nAn electric distribution network is modeled as an undirected, weighted, and connected graph G(V, E, W), where\nthe nodes in V = {1, 2, . . . , n} represent buses, either generators or loads, and edges represent power lines, each\nconnecting two distinct buses and modeled after the standard lumped \u03c0\u2212model [18]. To each edge (h, k) \u2208E we\nassociate a complex weight equal to the line admittance yhk = ghk + jbhk, where ghk > 0 is the line conductance and\nbhk \u2208R the line susceptance.\nThe network is then completely represented by the n-by-n complex admittance matrix Y , with elements Yhk = \u2212yhk\nfor h \u0338= k and Yhk = Pn\nh=1,h\u0338=k yhk + ys,h, where ys,h is the shunt element at the hth bus. The admittance matrix Y\nis symmetric and typically sparse, as each bus is connected to few others: notably, this is the case in distribution grids,\nwhich are often characterized by a radial topology. Moreover, for network where shunt elements are negligible, Y is\nLaplacian [19].\nWe consider the network to be either single-phased or phase-balanced, and operating in sinusoidal regime. To each\nbus h \u2208V, we associate a phasor voltage vhej\u03b8h \u2208C, where vh > 0 is the voltage magnitude and \u03b8h \u2208R the voltage\nangle, and a phasor current ihej\u03c6h \u2208C, representing the injection at the bus. We do not assume the presence of a\npoint of common coupling (PCC), although one or more may be present as long as their \ufb01xed v0 and \u03b80 are known.\nThe current-voltage relation descending directly from Kirchhoff\u2019s and Ohm\u2019s laws is given by\ni = Y v,\n(1)\nwhere i \u2208Cn is the vector of nodal current injections, and v \u2208Cn the vector of nodal voltages [20].\n2.2\nData collection\nWe assume that each bus of interest is equipped with \u00b5PMU, while we do not require electrical variables to be measured\non the lines.\nAssumption 1. The network is either completely observable, that is current injections and voltages are measured at\neach node, or a reduced network between the observed nodes is identi\ufb01ed.\nIn transmission systems, where the reliability and the economic optimization of dispatch are primary concerns, syn-\nchronized phasor measurements are provided by PMUs, which sample the magnitude and phase of current and voltage\nphasors. Unfortunately, distribution networks are characterized by relatively small \ufb02ows of active power and predom-\ninantly resistive lines, resulting in small phase differences between nodes. The accuracy of standard PMUs is not high\nenough to reliably appreciate such differences, making the devices ineffective.\nMicro-syncrophasors improve the resolution and accuracy of PMUs by up to two orders of magnitude - see Table 1,\nwhile preserving low costs. As PMUs, these devices sample the magnitude and phase of current and voltage: state-\nof-the-art models achieve frequency up to 120 Hz [21]. While the fast sampling rate generates a large number of data\npoints in a short time span, the samples are highly correlated. Indeed, the characteristic frequencies of load variations\nare much lower than the sampling rate of \u00b5PMUs. Moreover, due to structure of the network and the low phase\ndifference between nodes, samples collected on different buses are highly correlated with each other [13].\n3\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nMetric\nclass 1 PMU\nclass 0.1 PMU\n\u00b5PMU\nMagnitude accuracy [%]\n1%\n0.1%\n0.03%\nPhase accuracy [rad]\n12 \u00b7 10\u22123\n1.5 \u00b7 10\u22123\n5.1 \u00b7 10\u22124\nTable 1: PMU and \u00b5PMU 99th percentile accuracy [15, 21]. Magnitude accuracy is a reported as a percentage of the\nrated value.\n3\nProblem Statement\nConsider a power distribution network as described in Section 2, ful\ufb01lling Assumption 1. The identi\ufb01cation problem\namounts to reconstruct the admittance matrix from a sequence of voltage and current measurements corresponding to\ndifferent steady states of the system [11, 13].\nLet N be the number of samples, and let vt \u2208Cn and it \u2208Cn be the vectors of current injections and voltages for\nt = 1, . . . , N. From (1), one can obtain\nI = V Y,\n(2)\nwhere V = [v1, v2, . . . , vN]\u22a4\u2208CN\u00d7n, and I = [i1, i2, . . . , iN]\u22a4\u2208CN\u00d7n.\nAs described in Section 2.2, the available current and voltage phasors are corrupted by measurement noise. Therefore,\nin place of the actual electrical variables V and I, only noisy samples \u02dcV and \u02dcI are available, where\n\u02dcV = V + \u2206V,\n(3a)\n\u02dcI = I + \u2206I,\n(3b)\nwith \u2206V \u2208CN\u00d7n and \u2206I \u2208CN\u00d7n denoting the complex measurement noise. The network identi\ufb01cation problem\nthen translates into the estimation of Y given \u02dcV and \u02dcI.\nAssumption 2. The admittance matrix is constant over the identi\ufb01cation period.\nIn order to estimate Y using the linear relationship (2), it must be constant over both time and the obserbed variables.\nIf not, the closest constant matrix will be identi\ufb01ed instead, and the variations will add uncertainty to the estimate.\n4\nNoise Model\nBy design, PMUs and \u00b5PMUs collect current and voltage measurements in polar coordinates, that is in terms of\nmagnitude and phase [15, Sec. 3.2]. Previous studies have shown, with both theoretical and empirical arguments, that\nthe measurement noise is approximately Gaussian in polar coordinates, with zero mean and constant variance [15,\nSec. 2.1].\nIn (1), the admittance matrix Y establishes a linear relationship between the real and the imaginary parts of i and v, but\nthe equation becomes non-linear if magnitude and phase are considered. To preserve linearity, one needs to transform\nthe measurements and their associated noise from polar to Cartesian coordinates. This transformation changes the\nstatistical distribution of the noise, which becomes non-Gaussian, with a non-zero mean, and with a time-varying,\nnon-diagonal covariance matrix, as shown hereafter.\nWe consider a generic phasor measured by a \u00b5PMU. Without loss of generality, we will discuss only the case of a\nvoltage phasor; the same arguments apply to currents. Let \u02dcv and \u02dc\u03b8 denote the measured magnitude and phase, and let v\nand \u03b8 be the noiseless (unobservable) variables. Then \u02dcv = v +\u03f5 and \u02dc\u03b8 = \u03b8 +\u03b4, where \u03f5 \u223cN(0, \u03c3\u03f5) and \u03b4 \u223cN(0, \u03c3\u03b4)\nare zero-mean Gaussian variables. Previous studies suggest that the following Assumption is usually satis\ufb01ed [15,\nSec. 2.1].\nAssumption 3. The samples taken at two different time steps on the same node, and the samples taken on two different\nnodes at the same time are independent.\nUsing Assumption 3, we can focus on a single sample. Our aim is to write the measured phasor \u02dcvej \u02dc\u03b8 in Cartesian\ncoordinate (\u02dcc+j \u02dcd). Let vej\u03b8 = c+jd. Highlighting the noise component \u2206c+j\u2206d gives \u02dcvej \u02dc\u03b8 = (c+\u2206c)+j(d+\u2206d),\nwhere\n\u2206c = \u02dcc \u2212c = v cos \u03b8(cos \u03b4 \u22121) \u2212v sin \u03b8 sin \u03b4 + \u03f5 cos \u03b8 cos \u03b4 \u2212\u03f5 sin \u03b8 sin \u03b4,\n(4a)\n\u2206d = \u02dcd \u2212d = v sin \u03b8(cos \u03b4 \u22121) + v cos \u03b8 sin \u03b4 + \u03f5 sin \u03b8 cos \u03b4 + \u03f5 cos \u03b8 sin \u03b4.\n(4b)\n4\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nFrom (4), it can be noted that \u2206c and \u2206d are not distributed as Gaussian variables, due to interaction terms like \u03f5 sin \u03b4.\nHowever, the \ufb01rst-order Taylor approximation of (4) is a linear combination of Gaussian variables, suggesting that, for\nlow noise levels, the distribution of \u2206c and \u2206d is closely approximated by a Gaussian variable. Similarly to [17], to\ncharacterize the noise we compute the expected value of \u2206c and \u2206d\nE[\u2206c] = v cos \u03b8(e\u2212\u03c32\n\u03b4/2 \u22121),\n(5a)\nE[\u2206d] = v sin \u03b8(e\u2212\u03c32\n\u03b4/2 \u22121),\n(5b)\nand the associated variance-covariance terms:\nVar[\u2206c] = v2e\u2212\u03c32\n\u03b4[cos2 \u03b8(cosh \u03c32\n\u03b4 \u22121) + sin2 \u03b8 sinh \u03c32\n\u03b4] + \u03c32\n\u03f5 e\u2212\u03c32\n\u03b4[cos2 \u03b8 cosh \u03c32\n\u03b4 + sin2 \u03b8 sinh \u03c32\n\u03b4],\n(6a)\nVar[\u2206d] = v2e\u2212\u03c32\n\u03b4[sin2 \u03b8(cosh \u03c32\n\u03b4 \u22121) + cos2 \u03b8 sinh \u03c32\n\u03b4] + \u03c32\n\u03f5 e\u2212\u03c32\n\u03b4[sin2 \u03b8 cosh \u03c32\n\u03b4 + cos2 \u03b8 sinh \u03c32\n\u03b4],\n(6b)\nCov[\u2206c, \u2206d] = sin \u03b8 cos \u03b8e\u22122\u03c32\n\u03b4[\u03c32\n\u03f5 + v2(1 \u2212e\u03c32\n\u03b4)].\n(6c)\nUnfortunately, the expressions in Eqs. (5) and (6) are of no practical use, as they rely on the actual unobservable values\nv and \u03b8. Similar issues arise in the context of state estimation based on Extended Kalman Filter (EKF): in particular,\nresearch on the \ufb01ltering of radar signal can be adapted to our case [22, 23, 24, 25, 15]. Following such developments,\nwe compute the expectation of the average (5) and the variance (6) conditioned on the measurements:\nE[\u2206c|\u02dcv, \u02dc\u03b8] = \u02dcv cos \u02dc\u03b8(e\u2212\u03c32\n\u03b4 \u2212e\u2212\u03c32\n\u03b4/2),\n(7a)\nE[\u2206d|\u02dcv, \u02dc\u03b8] = \u02dcv sin \u02dc\u03b8(e\u2212\u03c32\n\u03b4 \u2212e\u2212\u03c32\n\u03b4/2).\n(7b)\nThe same procedure can be applied to the variances:\nVar[\u2206c|\u02dcv, \u02dc\u03b8] = \u02dcv2e\u22122\u03c32\n\u03b4[cos2 \u02dc\u03b8(cosh 2\u03c32\n\u03b4 \u2212cosh \u03c32\n\u03b4) + sin2 \u02dc\u03b8(sinh 2\u03c32\n\u03b4 \u2212sinh \u03c32\n\u03b4)]+\n+ \u03c32\n\u03f5 e\u22122\u03c32\n\u03b4[cos2 \u02dc\u03b8(2 cosh 2\u03c32\n\u03b4 \u2212cosh \u03c32\n\u03b4) + sin2 \u02dc\u03b8(2 sinh 2\u03c32\n\u03b4 \u2212sinh \u03c32\n\u03b4)],\n(8a)\nVar[\u2206d|\u02dcv, \u02dc\u03b8] = \u02dcv2e\u22122\u03c32\n\u03b4[sin2 \u02dc\u03b8(cosh 2\u03c32\n\u03b4 \u2212cosh \u03c32\n\u03b4) + cos2 \u02dc\u03b8(sinh 2\u03c32\n\u03b4 \u2212sinh \u03c32\n\u03b4)]+\n+ \u03c32\n\u03f5 e\u22122\u03c32\n\u03b4[sin2 \u02dc\u03b8(2 cosh 2\u03c32\n\u03b4 \u2212cosh \u03c32\n\u03b4) + cos2 \u02dc\u03b8(2 sinh 2\u03c32\n\u03b4 \u2212sinh \u03c32\n\u03b4)],\n(8b)\nCov[\u2206c, \u2206d|\u02dcv, \u02dc\u03b8] = sin \u02dc\u03b8 cos \u02dc\u03b8e\u22124\u03c32\n\u03b4[\u03c32\n\u03f5 + (\u02dcv2 + \u03c32\n\u03f5 )(1 \u2212e\u03c32\n\u03b4)].\n(8c)\nEq. (7) suggests that the measurement in Cartesian coordinates are biased, as the noise has a non-zero average. How-\never, the arguments in Appendix D suggest that such bias is negligible for realistic noise levels. Moreover, it can\nalways be computed and substracted from the data. Hence, in the following, the noise will be considered unbiased.\nWe \ufb01nally model the noise on a phasor measurement as\n\u0014\n\u2206c\n\u2206d\n\u0015\n\u223cN (02, \u03a3) ,\n(9)\nwith the elements of \u03a3 de\ufb01ned by (8). The covariance matrix \u03a3 is not constant in time, but changes with the actual\nvalues of phase and magnitude: such property will be further discussed in Section 5, while presenting the estimation\nmethods.\n5\nFrequentist identi\ufb01cation\n5.1\nLeast squares\nFrom (2) and (3), noisy data are related by the model\n\u02dcI \u2212\u2206I = ( \u02dcV \u2212\u2206V )Y.\n(10)\nFor reconstructing Y , the works [13, 12] assume \u2206V = 0 (i.e. \u02dcV = V ) and use Ordinary Least Squares (OLS) as well\nas its recursive and regularized variants. For standard OLS, one obtains\n\u02c6YOLS = arg min\n\u02c6Y\n\u2225\u02dcI \u2212\u02dcV \u02c6Y \u22252\nF .\n(11)\n5\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nHowever, if \u2206V \u0338= 0, the OLS introduces a bias. In this case, the Total Least Squares (TLS) is an unbiased estimator\n[26]:\n\u02c6YTLS = arg min\n\u02c6Y\nmin\nI,V \u2225[ \u02dcV \u2212V, \u02dcI \u2212I]\u22252\nF\ns.t. I = V \u02c6Y .\n(12)\nClosed-form solutions for both estimators are well-known, and can be written column-wise, with centered data \u02dcVc and\n\u02dcIc, as\n\u02c6Yi,OLS = ( \u02dcV \u22a4\nc \u02dcVc)\u22121 \u02dcV \u22a4\nc \u02dcIi,c,\n(13a)\n\u02c6Yi,TLS = ( \u02dcV \u22a4\nc \u02dcVc \u2212\u03c32\nn+1I)\u22121 \u02dcV \u22a4\nc \u02dcIi,c,\n(13b)\nwhere \u03c3n+1 is the smallest singular value of [Vc, Ii,c]. The error covariance, computed in terms of the exact values V\n[27], is\nVar[ \u02c6Yi,OLS] \u2248\u03c32\nn+1\nN\n(V \u22a4\nc Vc)\u22121,\n(14a)\nVar[ \u02c6Yi,TLS] \u2248(1 + \u2225\u02c6Yi,T LS\u22252)\u03c32\nn+1\nN\n(V \u22a4\nc Vc)\u22121.\n(14b)\nIt appears that Yi,OLS has a smaller variance than Yi,TLS, but has a bias that grows with \u03c3n+1. In power systems, the\ndata covariance (V \u22a4\nc Vc)\u22121 is very small compared to the noise variance (approximated by \u03c3n+1). Hence, the bias of\nthe OLS can be very large, making the TLS a more suitable choice. However, the estimation of a sparse topology can\nbe dif\ufb01cult, because the TLS estimate of zero elements can be very large due to the large variance. Possible solutions\nfor this problem are regularization or more complex iterative methods such as the one presented in [16]. Both methods\nassume that the samples are independent and identically distributed (i.i.d). However, as shown in section 4, \u00b5PMU\nmeasurements are not identically distributed.\n5.2\nMaximum likelihood estimator\nThe high correlation between measurements observed in power systems suggest that large sample sizes N may be\nneeded in order to obtain a good estimate of Y . Moreover, the estimator should be unbiased and consistent [28, chapter\n7, 10]. The Maximum Likelihood Estimator (MLE) presents weak consistency conditions [29] that are satis\ufb01ed for\nlinear models. From (2) and (3), noisy data are related by the model\n\u02dcI \u2212\u2206I = ( \u02dcV \u2212\u2206V )Y.\n(15)\nConsidering Gaussian noise \u2206V and \u2206I, as described in Section 2.2, the MLE is a weighted Total Least Squares\n(TLS) estimator and thus shares most of its properties such as deregularization [26] and unbiasedness [30].\nThe variables V and I are deterministic, and the noises \u2206V and \u2206I are independent (see Assumption 3). The\nlikelihood l( \u02dcV , \u02dcI|V, I, \u02c6Y ) of ( \u02dcV , \u02dcI), can be written as follows.\nl( \u02dcV , \u02dcI|V, I, \u02c6Y ) \u221dp(V + \u2206V |V, \u02c6Y )p(I + \u2206I|I, \u02c6Y ),\n(16)\ns.t. ( \u02dcV \u2212\u2206V ) \u02c6Y = \u02dcI \u2212\u2206I.\nTo work with real variables, we de\ufb01ne\nv =\n\u0012\n\u211c(vec(V ))\n\u2111(vec(V ))\n\u0013\n,\ni =\n\u0012\n\u211c(vec(I))\n\u2111(vec(I))\n\u0013\n,\n(17)\nV =\n\u0012\n\u211c(In \u2297V )\n\u2212\u2111(In \u2297V )\n\u2111(In \u2297V )\n\u211c(In \u2297V )\n\u0013\n,\ny =\n\u0012\n\u211c(vec(Y ))\n\u2111(vec(Y ))\n\u0013\n.\nThe same transformations can be applied to \u02dcV , \u02dcI, \u2206V , \u2206I, and \u02c6Y , resulting in the vectors \u02dcv, \u02dci, \u2206v, \u2206i, \u02c6y, as well\nas the matrices \u02dcV and \u2206V. Note that v and V contain the same elements but arranged differently. We will therefore\nuse them interchangeably when describing optimization problems over \u2206v or \u2206V. The matrix V is introduced to\nrepresent the product V Y with real and vectorized quantities as shown in (18) below.\nUsing the vectorized notations (17), we assume that \u2206v \u223cN(0, \u03a3v) and \u2206i \u223cN(0, \u03a3i), as per the approximate\nnoise model discussed in Section 2.2. The covariance matrices \u03a3v and \u03a3i are computed from (8) as explained in\nAppendix A. The likelihood (16) becomes, up to a multiplicative constant,\n6\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nl(\u02dcv,\u02dci|v, i, \u02c6y) = e\u2212\u2206i\u22a4\u03a3\u22121\ni\n\u2206ie\u2212\u2206v\u22a4\u03a3\u22121\nv\n\u2206v,\n(18a)\ns.t. \u02dci \u2212\u2206i = (\u02dcV \u2212\u2206V)y.\n(18b)\nThe corresponding log-likelihood is\nL(\u02dcv,\u02dci|v, i, \u02c6y) = \u2212\u2206i\u22a4\u03a3\u22121\ni \u2206i \u2212\u2206v\u22a4\u03a3\u22121\nv \u2206v,\n(19)\nsubject to (18b). For a \ufb01xed, albeit unknown v and i, we use the shorthand notation L(\u02c6y, \u2206v, \u2206i). Minimizing \u2212L\nfor \u2206i, \u2206v and \u02c6y yields the MLE.\n5.3\nError covariance analysis\nLet \u00afV = 1\nN 1\u22a4\nNV be the vector of voltage means, and \u03a6 = In \u2297(V \u22121N \u00afV ) be the centered complex data matrix. The\nMLE with Gaussian noise as de\ufb01ned in Section 4 is a weighted TLS estimator, and therefore, expressions for its error\ncovariance can be found in the literature [27, 30]. For a time-varying, not identically distributed case such as (19),\n[30] proves that the error covariance \u03a3MLE is equal to the inverse of the Fischer information matrix FMLE, for which\na \ufb01rst order approximation can be found in [30, (48)]. We adapt this expression for the complex matrix \u03a6 using the\nsame transformation to real numbers as for V in (17), and obtain\nFMLE =\n2,n,N\nX\nq,h,t=1\n\uf8eb\n\uf8ed\n\u211c(\u03a6hN+t)\u22a4\u211c(\u03a6hN+t)\n\u211c(z)\u22a4R\u211c,qht\u211c(z)\n\u211c(\u03a6hN+t)\u22a4\u2111(\u03a6hN+t)\n\u211c(z)\u22a4R\u211c\u2111,qht\u2111(z)\n\u22c6\n\u2111(\u03a6hN+t)\u22a4\u2111(\u03a6hN+t)\n\u2111(z)\u22a4R\u2111,qht\u2111(z)\n\uf8f6\n\uf8f8,\n(20)\nwhere z = [vec(Y )\u22a4, \u22121 \u2212j]\u22a4and R\u211c,qht, R\u211c\u2111,qht, and R\u2111,qht are diagonal and constructed from \u03a3v and \u03a3i as in\n[30], and the \u22c6symbol means that FMLE is symmetric. The exact expression is presented in Appendix B.\nOne should note that \u02c6FMLE, computed from noisy data instead of the exact variables used for FMLE, is a good ap-\nproximation only if the signal to noise ratio is high enough. Although this is typically not the case in distribution\ngrids, FMLE can still be used for theoretical purposes, such as design of experiments [12] for avoiding unobservablility\nproblems (Section 8.1). FMLE also shows the following properties of the MLE.\nLemma 4. The columns of \u02c6YMLE are independent. The variance of each column ( \u02c6Y \u22a4\nMLE)h depends only on the same\ncolumn Y\u00b7h of the exact admittance matrix Y .\nCorollary 5. When Cov[\u211c( \u02dcVt), \u2111( \u02dcVt)] \u22480 for all t (i.e. with small voltage angles), the variances of both real and\nimaginary parts of ( \u02c6Y \u22a4\nMLE)h are monotone with their respective values, and their covariance is constant.\nProof. The proofs of both Lemma 4 and Corollary 5 can be found in Appendix B.\nRemark. It follows from (8c) that the assumption of Corollary 5 is also often true in in distribution networks, where\nvoltage angles are very small.\nLemma 4 and Corollary 5 are used in Section 9 to identify and explain what factors can make the parameter identi\ufb01-\ncation very imprecise or even impossible.\n6\nBayesian estimation\nLine admittances, even if measured, are typically known up to a tolerance. Some knowledge of Y \u2019s structure, such as\nits sparsity, may also not be certain or precisely de\ufb01ned. This kind of uncertainty can be modeled via Bayesian prior\ndistributions.\n6.1\nMaximum a posteriori estimation\nFollowing [31], we describe how to compute Maximum A Posteriori (MAP) estimates for the error-in-variables model\n(18). Using Bayes\u2019 rule, the posterior probability density is\np(V, I, \u02c6Y | \u02dcV , \u02dcI) = p( \u02dcV , \u02dcI|V, I, \u02c6Y ) p(V, I)\np( \u02dcV , \u02dcI)\np( \u02c6Y ),\n(21)\n7\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nwhere we assume that the line parameters y are independent of the grid state (V, I) or its measurement ( \u02dcV , \u02dcI). The\nfactor p(V,I)\np( \u02dcV ,\u02dcI) can be neglected as it is a quotient of non-informative priors [31], de\ufb01ned as uniform distributions on the\n\ufb01nite set of feasible voltages and currents. The negative log-posterior minimization of (21) is then written as\nmin\n\u02c6y,\u2206v,\u2206i \u2212L(\u02c6y, \u2206v, \u2206i) \u2212log (p(\u02c6y)) ,\n(22a)\n\u02dci \u2212\u2206i =\n\u0000\u02dcV \u2212\u2206V\n\u0001\n\u02c6y,\n(22b)\nwith L de\ufb01ned by (19). Optimizing (22) provides a maximum a posteriori (MAP) estimate \u02c6yMAP1.\nThe density function p(\u02c6y) can take many forms. If it is Gaussian, then \u2212log(p(\u02c6y)) corresponds to a weighted ridge\nregularization [32]. In this paper, we will focus mainly on the element-wise Laplace distribution p(\u02c6yh) \u221de\u2212\u03bb|\u02c6yh|,\nwhere all elements p(\u02c6yh) of the prior are assumed independent, and therefore p(\u02c6y) = Q\nh p(\u02c6yh) [33]. One obtains\n\u2212log(p(\u02c6y)) = \u03bb\u2225\u02c6y\u22251 + const, where the constant can be neglected in the optimization problem, obtaining an \u21131\nregularization term. The \u21131 regularization can also be interpreted as a robusti\ufb01cation of the MLE optimization problem\nmin\u02c6y,\u2206v,\u2206i \u2212L(\u02c6y, \u2206v, \u2206i) [34].\nPrior distributions are centered on the believed value of the exact admittance yi, which can be different from zero, e.g.,\nin case of an existing line. More generally, one can also believe that a linear combination of y has a particular value\n[35]. For example, the belief of two lines having the same admittance is equivalent to that of their difference being\nzero, not knowing the actual value. The probability density p(L\u02c6y \u2212\u00b5) of a linear transformation \u02c6y \u2192L\u02c6y \u2212\u00b5 can\ndescribe such a belief. The penalty function is then\n\u2212log(p(\u02c6y)) = \u03bb\u2225L\u02c6y \u2212\u00b5\u22251.\n(23)\nRemark. On one hand, if one has two priors about a line h, conditioned on independent events B and C, then similarly\nto (21), Bayes\u2019 rule gives that p(\u02c6yh) is proportional to p(\u02c6yh|B)p(\u02c6yh|C). If p(\u02c6yh|B) and p(\u02c6yh|C) are both Laplace\ndistributions, this is equivalent to adding a row to L and \u00b5 in (23). The same operation can be repeated for a larger\nnumber of priors on h.\nOn the other hand, if one has no prior about the line h, then the non-informative prior p(\u02c6yh) is a uniform distribution\non the bounded support of \u02c6yh, and can be factorized away in p(\u02c6y), leaving p(\u02c6y) \u221dQ\nk\u0338=h p(\u02c6yk).\nIntroducing additional information may reduce the variance of the estimate. Using the approximation \u2225\u02c6y\u22251 \u2248\u02c6y\u22a4[|y|+\n\u03b1]\u22121\u02c6y similarly to [36, Appendix A], we can make log(p(\u02c6y)) in (22) smooth. Hence, the Fisher information is given\nby the Hessian of the log-likelihood with smoothed prior.\nFMAP \u2248E\n\u0014 \u22022\n\u2202\u02c6y2 (\u2212L(\u02c6y, \u2206a, \u2206b) \u2212log(p(\u02c6y)))\n\u0015\n.\n(24)\nBoth the expected value and second derivative operators are distributive. For p(\u02c6y) \u221de\u2212\u03bb\u2225L\u02c6y\u2212\u00b5\u22251, FMAP is therefore\napproximated as\nFMAP \u2248FMLE + \u03bbL\u22a4([|Ly \u2212\u00b5|] + \u03b1I])\u22121L,\n(25)\nwhere \u03b1 > 0 is a small enough parameter. If the data matrix V is not full rank (so neither is FMLE), (25) can be used\nfor assessing where prior knowledge is required to make FMAP full rank.\n6.2\nPrior distributions from known grid parameters\nIn this section, we will explain how to inject various forms of prior knowledge into the estimation problem. This\nknowledge can be in the form of partially known parameter values, other known grid properties such as sparsity,\nconstraints on the signs of parameters, or on ratios between their values.\nA priori knowledge about the parameter yh can be done as follows.\n\u2212log(p(\u02c6y|\u03b2h)) = \u03bb\u2225e\u22a4\nh (\u02c6y \u2212yh)\u22251.\n(26)\nIf a set H of parameters is known, and \u03b2 \u2208Rn2 is a vector such that \u03b2h = yh for all h \u2208H and zero otherwise, the\nlog prior distribution becomes\n\u2212log(p(\u02c6y|\u03b2)) = \u03bb\u2225[Lhe\u22a4\nh ]h\u2208H(\u02c6y \u2212\u03b2)\u22251,\n(27)\n1The optimizer may not be unique, in this case \u02c6yMAP is one of the elements of the set of optimizers.\n8\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nwhere each Lh \u2208R+ is a parameter modelling the con\ufb01dence in the corresponding value \u03b2h.\nSparsity is the belief that each parameter has a high probability to be zero. In the Bayesian framework, this translates\ninto a zero-centered distribution, identical for all parameters, which is a standard \u21131 penalty \u03bb\u2225\u02c6y\u22251 in the log-space.\nIf some lines are known to exist or not, the sparsity prior can be removed or strengthened only for them. Let \u03b2s \u2208\n{0, 1, K}2n2 be a sparsity pattern containing a 0 for an existing line, an arbitrarily large K for an absent line, and a 1\notherwise. The corresponding sparsity promoting prior is\n\u2212log(p(\u02c6y|\u03b2s)) = \u03bb\u2225[\u03b2s]\u02c6y\u22251.\n(28)\nPrior knowledge may not directly concern the value of individual parameters. One could also know the sign of the\nelements of y. For instance, the conductance \u211c(Yhk) is always non-negative because of energy conservation laws, and\nin the absence of series capacitance, the susceptance \u2111(Yhk) is non-positive for all lines. In order to obtain a lower\nprobability for the wrong signs, we will use the asymmetric Laplace distribution [37] with a very large parameter on\nthe corresponding side. In this case, log(p(\u02c6y)) becomes a sum of skewed absolute values written as\n\u2212log(p(\u02c6y|s)) = \u03bb\u2225\u02c6y\u22251 + K\n2n2\nX\nh=1\n(1 \u2212shsgn(\u02c6yh)),\n(29)\nwhere K is an arbitrarily large constant and s is a vector with elements in {\u22121, 1} de\ufb01ning the believed signs of y.\nConstraining the ratios between parameters is another form of prior knowledge that can be useful in many cases, for\nexample if two lines are parallel, or if one knows the type of cable used for a certain line. For a ratio \u02c6\u03c1hk = yk\nyh , we\nintroduce the prior\n\u2212log(p(\u02c6y|\u02c6\u03c1hk)) = \u03bb\n\r\r(\u03c1hkeh \u2212ek)\u22a4\u02c6y\n\r\r\n1 .\n(30)\nWhile the exact value of the resistance R and the reactance X of a line depends on its length, the type of cable gives\ntheir ratio R/X. A constant R/X ratio for the entire network is a common assumption for distribution grids. By letting\n\u02c6\u03c1 be the estimated R/X ratio, the corresponding prior is\n\u2212log(p(\u02c6y|\u02c6\u03c1)) = \u03bb \u2225[\u02c6\u03c1In2, \u2212In2]\u02c6y\u22251 .\n(31)\n6.3\nData-driven prior distributions\nBayesian priors can incorporate model beliefs resulting from other estimation methods. Thereafter, we show how\nMLE can be used to re\ufb01ne the priors described in Section 6.2. Sparsity promotion using (28) can create a bias. This\nbias, can be reduced by using the weight L = [|\u02c6yMLE|]\u22121, similarly to the adaptive Lasso method [13]. This yields\n\u2212log(p(\u02c6y|yMLE)) = \u03bb\u2225[|\u02c6yMLE|]\u22121\u02c6y\u22251\n(32)\nNote that this method is only asymptotically unbiased (as N \u2192\u221e). For a \ufb01nite sample size, it does not cancel the \u21131\npenalty\u2019s bias but still reduces it [38].\nRemark. For constructing the prior (32), yMLE is considered constant rather than a random variable, preserving\nindependence towards other potential priors.\nThe hyperparameter \u03bb in (23) is very important because if it is too big, then the bias will be unnecessarily large, and if\nit is too small, some entries will not be effectively shrunk to zero. However, it can generally not be tuned using cross\nvalidation (which is the standard approach in Lasso) because \u03a3v and \u03a3i depend on the values of v and i. For very\nsparse systems such as power grids, there may not exist a \u03bb such that the MAP estimate is both sparse enough and\nmoderately biased. To solve this issue, one can add a prior on \u2225\u02c6y\u22251, centered on its believed value \u02c6\u03b3. If the sign of\nevery yi is known and with s from (29), one can construct this prior as\n\u2212log(p(\u02c6y|\u02c6\u03b3)) = \u03bb\u2032\n\u02c6\u03b3 |s\u22a4\u02c6y \u2212\u02c6\u03b3|.\n(33)\nIf \u03bb\u2032 > \u03bb, it limits the bias created by a too large \u03bb, which makes its tuning much more tolerant to errors and allows\nfor a more aggressive regularization.\nDirectly estimating \u02c6\u03b3 with maximum likelihood as \u02c6\u03b3 = \u2225\u02c6yh,MLE\u22251 may be biased. The error \u02c6yMLE \u2212y is a Gaussian\nrandom variable centered at zero [30]. Hence, the terms of \u2225\u02c6yMLE\u22251 are absolute values of Gaussian variables centered\nat the corresponding yh. For a random variable X \u223cN( \u00afX, \u03c32\nX), if \u00afX \u226b\u03c3X then |X| can be approximated as\n9\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nGaussian, but if \u00afX = 0, |X| has a half-Gaussian distribution and E[|X|] =\n\u221a\n2\n\u221a\u03c0\u03c3X. If y is very sparse, many\nparameters are centered at zero, which creates a large bias on \u02c6\u03b3. If one knows the sign sh \u2208{\u22121, 1} that each\nparameter yh should have, one can replace |\u02c6yh,MLE| by sh\u02c6yh,MLE. The resulting estimate\n\u02c6\u03b3 = s\u22a4\u02c6yMLE\n(34)\nhas the same variance as \u2225\u02c6yMLE\u22251 but is unbiased because E[\u02c6\u03b3] = P\nh shyh = \u2225y\u22251. Even without using (33), this\nvalue can be useful to assess if the chosen \u03bb is too large. Note that the same argument applies if \u02c6yMLE is split into\ngroups of elements (e.g. into columns). However, if the groups are too small, the variance of each \u02c6\u03b3 may be very large,\nleading to an erroneous prior.\n7\nStructural priors\nDue to the structure of power networks, Y has peculiar properties. If the network does not include phase-shifting\ntransformers and power lines are not compensated by series capacitors, Y is symmetric. Moreover, for networks\nwhere shunt elements are negligible, Y is Laplacian [19]. Since phase-shifting transformers are usually employed in\ntransmission systems, and shunt admittances are negligible for medium-sized grids, with line less than 60 km long, it\nis safe to assume that standard distribution networks have a Laplacian admittance matrix [39].\n7.1\nFormal de\ufb01nition\nUnder the assumption that Y is Laplacian, entries on and above the main diagonal of Y can be derived from the\nelements below the diagonal. Therefore, in order to avoid redundant variables, one can proceed as in [12] and use\nduplication and transformation matrices D and T to remove the redundant entries from the identi\ufb01cation problem and\nsolve for \u02c6yr = [\u211c(ve( \u02c6Y ))\u22a4, \u2111(ve( \u02c6Y ))\u22a4]\u22a4instead. In case some entries of Y are known to be zero, one can derive\nvariants of D and T and also remove these zero entries from \u02c6y by following a procedure similar to the one presented\nin [12, Appendix 2]. In both cases, \u02c6y = (I2 \u2297D \u00b7 T)\u02c6yr and the equation (18b) becomes\n\u02dci \u2212\u2206i = (\u02dcV \u2212\u2206V)(I2 \u2297D \u00b7 T)\u02c6y,\n(35)\nThe diagonal entries of Y are often the largest, as they are the sum of all other entries on the same rows. According to\nCorollary 5 and its following remark, the diagonal entries may not only have a large variance, but also cause one for\nall other elements. Using the D and T matrices also improves this last point.\n7.2\nImplications for Bayesian priors\nThe MAP estimate can be computed using (22) while replacing (22b) by (35). One can also simply replace \u02c6y by \u02c6yr\nin all the priors presented in Section 6. However, the Laplacianity of Y opens additional opportunities. Similarly to\nSection 6.2, if the line susceptance is assumed negative, diag(YMLE) provides the following alternative estimate for \u02c6\u03b3.\n\u2225\u02c6y\u22251 =\nn\nX\nh=1\nn\nX\nk=1,k\u0338=h\n\r\r\r\r\n\u211c( \u02c6Yhk)\n\u2111( \u02c6Yhk)\n\r\r\r\r\n1\n+\nn\nX\nh=1\n\r\r\r\r\n\u211c( \u02c6Yhh)\n\u2111( \u02c6Yhh)\n\r\r\r\r\n1\n,\n= 2\nn\nX\nh=1\n\r\r\r[\u211c( \u02c6Yhh), \u2111( \u02c6Yhh)]\n\r\r\r\n1 .\n(36)\nNote that \u2225\u02c6y\u22251 = 4\u2225\u02c6yr\u22251 because in \u02c6y, the diagonal elements make up half of the norm and the other elements are\npresent twice. Furthermore, a prior on Pn\nk=1,k\u0338=h Yhk for all h \u2208V can also limit the bias of a sparsity-promoting\none. If the exact value is not available and with a Laplacian Y , this prior can be centered on Yhh,MLE. This yields\n\u2212log(p(\u02c6yr|diag(YMLE))) =\n(37)\n\u03bb\u2032\nn\nX\nh=1\n\f\f\f\f\f\nPn\nk=1,k\u0338=h \u211c( \u02c6Yhk)\n\u211c( \u02c6YMLE,hh)\n\u22121\n\f\f\f\f\f +\n\f\f\f\f\f\nPn\nk=1,k\u0338=h \u2111( \u02c6Yhk)\n\u2111( \u02c6YMLE,hh)\n\u22121\n\f\f\f\f\f .\nMore details about the representation of p(\u02c6yr|diag( \u02c6YMLE)) in the same form as (23) are presented in Appendix C.\nIn the simulation Section 9, we have a large network with a very sparse, Laplacian admittance matrix. Hence, we\nwill us D and T with the prior p(\u02c6yr|s), as well as p(\u02c6yr|\u02c6yr,MLE) and p(\u02c6yr|diag( \u02c6YMLE)), which are built using the\nnon-diagonal and diagonal elements respectively. Both priors are combined using the remarks in Sections 6.1 and 6.3.\n10\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\n8\nNumerical methods\nBefore solving (22) to obtain an estimate, several improvements can be done by pre-processing available measure-\nments. This section will \ufb01rst explain how centering and \ufb01ltering the data, as well as removing hidden nodes can\nimprove the estimation. Then, we will present and compare different algorithms to solve the optimization problem.\n8.1\nData pre-processing\nSome networks may have nodes with no load attached. If a node is unloaded, then the corresponding column of V\nis a linear combination of the columns corresponding to neighboring nodes, as it is determined by a simple voltage\ndivider. In this case, \u03a6 in (20) does not have full rank, so FMLE is singular. In other words, the Cramer-Rao bound\nbecomes in\ufb01nite, at least for some parameters, which means that they cannot be reliably reconstructed by any unbiased\nestimator. In a similar spirit, if all the nodes are loaded but some nodes have much lower loads than others, FMLE can\nbe full rank, but some of its eigenvalues can be very small. When inverting FMLE, the small eigenvalues become very\nlarge, which means that the variance of the corresponding estimates will also be large.\nFrom (25), it is apparent that a prior on lines near an unloaded node may help with this issue as it potentially compen-\nsates for the rank de\ufb01ciency of FMLE. However, this makes the prior the only source of information. Another solution\nis to take out these nodes from the problem, and identify a reduced matrix Yred such that I\u2212h = YredV\u2212h. In most\napplications, such as sensitivity analysis or control, this reduced matrix is suf\ufb01cient as it only removes redundant pa-\nrameters, while keeping an equivalent model for the remaining nodes. Yred can be computed using the Kron reduction\nmethod [40].\nFor a power grid with a rated voltage Vrated, the matrix \u02dcV \u22a4\u02dcV is almost equal to Vrated1n1\u22a4\nn . This matrix is then\nalmost singular with one eigenvalue much larger than all others. This can be an issue for numerical stability both for\nminimizing (19) or for solving (22). If Y is Laplacian, one can use \u02dcVc = \u02dcV \u2212Vrated1N1\u22a4\nn instead, as the second term\nis cancelled by 1\u22a4\nn Y = 0\u22a4\nn . The covariance \u03a3v still needs to be computed with V and not Vc due to the non-linear\ntransformation (8).\nFinally, a low pass (moving average) \ufb01lter can help reducing the measurement noise. Measurements from \u00b5PMUs\nare usually very frequent (50 to 120Hz) [41], and load variation has an average period of a couple minutes. A low\npass \ufb01lter with a cutoff frequency equal to the one of the load pro\ufb01les will not remove relevant information from the\nsignal. However, as K measurements are averaged, the noise variance of a \ufb01ltered measurement is reduced since\n[\u03c32\n\u03b5, \u03c32\n\u03b4]\ufb01ltered = K\u22121[\u03c32\n\u03b5, \u03c32\n\u03b4].\n8.2\nOptimization algorithms\nIf \u2212log(p(\u02c6y)) is convex, the optimization problem (22) has a convex cost and bilinear constraints due to the multipli-\ncation of \u02c6y and \u2206V. Similarly to weighted TLS, no closed-form solution is available [26].\nThe most basic algorithm for solving (22) is the alternate block coordinate descent (BCD), which sets \u2206V to constant\nto solve \u02c6y for the next iteration (k) and vice versa, as explained in [42]. With c = \u2212L \u2212log(p(\u02c6y)), the update is\n\u2206Vk = arg min\n\u2206V\nc\n\u0010\n\u02c6yk\u22121, \u2206V,\u02dci \u2212\n\u0000\u02dcV \u2212\u2206V\n\u0001\n\u02c6yk\u22121\n\u0011\n,\n(38a)\n\u02c6yk = arg min\n\u02c6y\nc\n\u0010\n\u02c6y, \u2206Vk,\u02dci \u2212\n\u0000\u02dcV \u2212\u2206Vk\n\u0001\n\u02c6y\n\u0011\n.\n(38b)\n(38) shows two convex sub-problems that can be solved iteratively. However, the \u02c6y-subproblem (38b) may not admit\na closed-form solution, depending on p(\u02c6y). When it does not, (38b) can become the computational bottleneck.\nTo improve performance, one can use the approximation\n\u2225L(\u02c6y \u2212\u00b5)\u22251 \u2248(\u02c6y \u2212\u00b5)\u22a4L\u22a4[|L(\u02c6yk\u22121 \u2212\u00b5)| + \u03b11]\u22121L(\u02c6y \u2212\u00b5)\nin the expression of p(\u02c6y), with a small enough \u03b1. This algorithm is called broken adaptive ridge regression (BAR)[43,\n44], and provides a closed-from approximate solution to (38b). If L is diagonal, another possible alternative is use an\noperator splitting method such as ADMM [45, 46]. However, experimental evidence shows that more iterations are\nneeded for ADMM to converge. A comparison of the three algorithms can be found in table 2.\n11\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nAlgorithm\niterations to convergence\niterations/second\nBCD\n\u223c10000\n1.25\nBAR\n\u223c10000\n30\nADMM\n\u223c30000\n28\nTable 2: Comparison of the speed of three algorithms on a 9 nodes network with N = 400 measurement samples,\nusing a standard \u21131 penalty with the same \u03bb. In all cases, they are executed on a MacBook Pro with a 2.3GHz Intel i7\nprocessor running Python 3.8.\nRemark. The convergence speed highly depends on the regularization parameter \u03bb. To speed it up, a higher penalty\ncan be applied in the \ufb01rst iterations, and then decreased to the optimal one. This optimal value can be computed using\n\u02c6YMLE as shown in Section 6.3.\nIf L is diagonal, the proximal gradient method is also usable [46, 47], but it requires to tune the step size on top of\nthe regularization parameter. This can take many iterations, especially if the information contained in the data is very\nlimited. Also, it relies on thresholding providing a closed-form proximal operator of the \u21131 norm [48]. If L is not\ndiagonal, the proximal operator becomes a piece-wise linear function with a number of pieces scaling with the number\nof combinations of signs in L. It therefore becomes quickly prohibitive to compute. In practice, non-diagonal priors\nare used for cancelling bias (Section 6.3) or for keeping some parameters close to the same value, which can be needed\nfor a three phased network if some sub-networks have three times the same line. The proximal Newton method [47]\nis not usable: it relies on the transformation of the proximal operator by the Hessian matrix of c, which is generally\ndense and therefore requires the analysis of up to 22n2 different sign combinations.\n9\nSimulation results\nWe apply the estimation method presented in Section 6, as well as approaches from other works, to a 56-node network\nwith realistic parameters, admittances, noise levels and load pro\ufb01les. In order to obtain the results, the BAR algorithm\nis implemented using a hardware-accelerated linear solver from NVidia\u2019s CUDA tool. The framework is programmed\nin Python and available on GitHub [50]. It simulates the voltage and current measurements from the real network\nparameters, nominal loads, and given household load pro\ufb01les. It then computes the various least squares and MLE\nand MAP estimates from the simulated measurements.\n9.1\nSetup\nTo simulate the identi\ufb01cation problem, we use the IEEE 123 bus network. First, in Section 9.2, assuming that the loads\non each phase are balanced, the three-phased part of the network is transformed into a 56 nodes, single-phase feeder\nusing the method and parameters from [49]. Second, in Section 9.3 the three-phased part is simulated with its original\nunbalanced loads. Finally, in Section 9.4, the identi\ufb01cation is performed with partial information, which means that\nsome nodes are unobserved and the equivalent admittance between the observed nodes is identi\ufb01ed.\nThe load pro\ufb01les for each node are generated with the GENETX generator [51]. It creates random realistic loads for\nhouseholds according to parameters like penetration of renewable energies (set to 35%) or electric vehicles (40%).\nThe tool then generates a thousand one-minute-resolution demand pro\ufb01les, for households situated in the Netherlands\nduring week 12 of the year. Samples at different sampling frequencies are then extrapolated linearly from this data.\nTo create the load pro\ufb01les for each node (Fig. 1b), the demand pro\ufb01les of randomly selected households are summed\nuntil the nominal power is reached.\nVoltage and current values are generated by simulating the network using the PandaPower library [52] with a mea-\nsurement frequency of 50Hz. We then add 0.01% of Gaussian noise in polar coordinates, as described in Section 4.\nNote that the noise generated by a \u00b5PMU depends on its rating. Assuming that the \u00b5PMUs are adapted to their nodes,\nwe choose a rating of four times the nominal power. In order to reduce the computational complexity, the samples\ncollected over a minute are averaged as proposed in Section 8.1 and the identi\ufb01cation method is performed with the\naveraged samples.\nA whole week of data with 50Hz sampling rate may seem excessive, but it is required to reach a practical 1% to 2%\nerror. For different sample sizes, Fig. 2 shows the expected relative Frobenius error E[\u03b5F ], where\n\u03b5F = \u2225\u02c6Y \u2212Y \u2225F\n\u2225Y \u2225F\n.\n(39)\nThis quantity is obtained by sampling \u02c6Y as a Gaussian random variable centered on Y and with a covariance given by\nthe Cramer-Rao lower bound Cov[yMLE] = F \u22121\nMLE, where FMLE is computed using (20).\n12\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\n(a) Map of the network from [49].\n0\n200\n400\n600\n800\n1,000\n1,200\n1,400\n0\n0.1\n0.2\ntime [min]\nActive load [MW]\n(b) Sample day of generated load pro\ufb01les, with node 4\u2019s one highlighted.\nFigure 1: Graphic representation of the simulation settings.\n1\n2\n3\n5\n7\n10\n15\n21\n30\n45\n60\n0.5%\n1%\n2%\n5%\n10%\n20%\n50%\nsample size [days]\nrelative estimation error\n50Hz\n1Hz\nFigure 2: Graph in logarithm scale of the sample complexity for two sampling rates and a noise level of 0.01%.\n13\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nThe MLE is ef\ufb01cient so its expected error should be equal to the Cramer-Rao bound [28, chapter 7, 10]. However, this\nis true for the multivariate distribution. It then propagates non-linearly through (39) and the results in terms of norms\nmay be slightly different. The Cramer-Rao bound provides a method-free indication of the results to expect, which\nshows the dif\ufb01culty of the problem.\n9.2\nBalanced network\nWith the high accuracy of \u00b5PMUs, and to the data preprocessing described in Section 8.1, the Cramer-Rao lower\nbound is estimated around 3% using (20). The MLE manages to retrieve an approximate but fair estimate with 5.77%\nerror (Fig. 3a). Although the sparsity of the admittance matrix is above 98%, this estimate is dense. Fig. 3b shows\nthe MAP estimate with a prior distribution as described in Section 7.2. It does not use any exact information, but only\nthe MLE estimate described above as a starting point for the BAR algorithm, and achieves 1.21% error, beating the\nCramer-Rao bound.\n(a) MLE estimate.\n(b) MAP estimate.\n(c) Real admittance matrix.\nFigure 3: Heat-maps of real and estimated single-phase equivalent admittance matrices. Different colors correspond\nto log-spaced values.\n9.3\nUnbalanced network\nThree-phased identi\ufb01cation is a much more challenging problem. If some phases are not connected at any node\nor if any load is balanced, the voltage matrix V will not have full rank. To circumvent this issue, we identify the\nclosest balanced infrastructure network (i.e. a network with transposed lines) using sequence voltages, currents and\nadmittances [53]. In this case, zero, positive, and negative sequences can be estimated separately, which requires a\nrank of V three times lower, and any unbalances in the network infrastructure will be considered as noise. The prior\ndistribution presented in Section 7.2 can be used for each sequence. Fig. 4 shows the reconstructed phase admittance\nmatrix. The error is 6.9% for the MLE and 1.6% for MAP.\n(a) MLE estimate.\n(b) MAP estimate.\n(c) Real admittance matrix.\nFigure 4: Heat-maps of real and estimated three-phased admittance matrices.\n14\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nOne can observe that the error is similar to the single-phase cases, even though there are three times more parameters\nto estimate. This is due to the fact that the signal strength is similar but the admittance of the single phase equivalent\nis three times higher (three lines are considered as one). From (20), one can see that the information matrix is divided\nby 9 in the single-phase case, which increases the error. This also applies to parallel lines of the same phase.\n9.4\nReduced network\nFollowing guidelines on optimal \u00b5PMU placement [54], 40% of the nodes2 in the network represented in Fig. 1a are\nnot observed. The corresponding reduced admittance matrix (i.e. the matrix satisfying (1) with reduced i and v) is\nestimated with a 6.25% error using MLE (Fig. 5a), and 2.49% error using MAP (Fig. 5b). Note that because the\nnetwork is now smaller and less sparse, the MLE is better but the contribution of the sparsity-promoting prior is less\npronounced than for the full network.\n(a) MLE estimate.\n(b) MAP estimate.\n(c) Real admittance matrix.\nFigure 5: Heat-maps of real and estimated reduced three-phased admittance matrices.\n10\nDiscussion\n10.1\nComparison with state-of-the-art methods\nState-of-the-art approaches to network identi\ufb01cation include, besides MLE (or its TLS approximation), other methods\nsuch as OLS or Lasso. Fig. 6 compares these three approaches with MAP using the sparsity-promoting prior described\nin Section 9. It shows the relative Frobenius error (39) for various noise level, as well as its standard deviation over 4\ndifferent simulations of the reduced network (Section 9.4). This \ufb01gure highlights the low robustness to noise of non-\nEIV models, as well as the additional robustness provided by the regularization in MAP with physics-based priors.\n2The nodes 1, 3, 4, 6, 8, 9, 10, 12, 15, 16, 17, 18, 19, 22, 24, 26, 28, 32, 36, 37, 39, 40, 43, 44, 46, 47, 49, 50, 51, 52, 53, and 55\nare observed. (see Fig. 1a.)\n15\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\n10\u22125\n2 \u00b7 10\u22125\n5 \u00b7 10\u22125\n10\u22124\n2 \u00b7 10\u22124\n5 \u00b7 10\u22124\n10\u22123\n1%\n2%\n5%\n10%\n20%\n50%\n100%\nnoise level [p.u.]\nrelative estimation error\nOLS\nMLE\nLasso\nMAP\nFigure 6: Comparison of existing methods for various noise levels and 7 days of data.\nRemark. Many datasets suffer from incomplete measurements (e.g. data collected using smart meters miss the voltage\nphase). The EIV model generates estimates of the exact value of both currents and voltages, which could replace large\nvariance pseudo-measurements. However, doing so may greatly reduce the precision of the estimate and require very\nlarge sample sizes.\n10.2\nPrior improvement by measuring selected line admittances\nIf one needs to further improve the estimate and the sample size is limited, the remaining solution is to collect further\ninformation on topology or parameters of the electric network and integrate the results into the Bayesian prior (Sec-\ntion 6.2). To avoid collecting too much data, the additional information can be focused on estimates with the largest\nerror. Although the error covariance is practically not computable (Section 5.3), its properties show that in theory one\nexpects the estimation error to be concentrated on (i) high admittance elements such as short lines or switches, (ii)\nlines connecting a node with a load much lower than their power \ufb02ow, and (iii) lines with a very low admittance, which\nmay be strongly affected by regularization and estimated as zero.\nThe \ufb01rst point follows from Corollary 5, which implies that a high admittance element also affects the estimation error\nof the other lines connected to the same node. Point (ii) is equivalent to the remark in Section 8.1, that if a node has a\nlow load, the error variances of the admittances of all lines connected to it may be very high.\nAccording to these guidelines, we incorporate the knowledge of the lines connected to nodes 1, 50 and 51. Indeed,\nthese lines are short and have a very high power \ufb02ow due to neighbouring large loads or external grid connection,\nhence falling in the categories (i) and (ii). In total, we add information about 5 of the 870 possible connections, among\nwhich 35 actually exist, and obtain 2.24% error, which means a 10% improvement compared to the value without prior\nknowledge. As a comparison, we add the measurements of 5 random other lines and obtain 2.4% error, which means\nthat the estimate improves by only 3.6%. The guidelines (i), (ii), and (iii) can therefore help choosing lines to measure\nto achieve a better estimate.\n11\nConclusions\nThe penetration of distributed generation and smart devices in the distribution grid calls for the introduction of ad-\nvanced control schemes, which require the exact topology and line parameters. Such information is often unavailable\nfor distribution networks: as direct measurement is infeasible, data-driven estimators are needed.\nIn this work, we proposed to exploit samples collected by micro-PMUs. Considering a realistic statistical model for\nthe noise affecting both current and voltage measurements, we built maximum-likelihood and Bayesian estimators. We\nargued that the latter can outperform the former, due to their ability to exploit features of the grid, such as sparsity, as\nwell as available information on speci\ufb01c lines. Our argument is substantiated by numerical simulations on benchmark\ngrids: even without any network-speci\ufb01c prior information, Bayesian methods outperformed state-of-the-art estimators\nwith realistic noise levels.\n16\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nFurther research may focus on effective methods to selectively collect live measurements for improving the quality\nof estimates, as well as the development of alternative noise models for different sensors and physical quantities such\nas power. Formally de\ufb01ning the network\u2019s observability could lift the limitation to fully observed grids, introducing\npseudo-measurements and Bayesian priors on the grid\u2019s state. Learning a reduced network connecting only speci\ufb01c\nnodes could also provide an answer to missing measurements.\nA\nCovariance matrix\nIn order to solve the Maximum Likelihood problem (18) and all its subsequent re\ufb01nements, one needs the covariance\nmatrices \u03a3v and \u03a3i. The construction of the two is identical, thus we will focus on \u03a3v = Cov[\u2206v] \u2208R2nN\u00d72nN\nonly.\nFrom Section 4, the \u03a3v is sparse, having non-zero elements only on three diagonals:\n\u03a3v =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nVar[\u211c( \u02dcV11)]\n0\n...\n0\nVar[\u211c( \u02dcVNn)]\nCov[\u211c( \u02dcV11), \u2111( \u02dcV11)]\n0\n...\n0\nCov[\u211c( \u02dcVNn), \u2111( \u02dcVNn)]\n\u22c6\nVar[\u2111( \u02dcV11)]\n0\n...\n0\nVar[\u2111( \u02dcVNn)]\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\n(40)\nwhere \u22c6denotes a symmetric element. The main diagonal of \u03a3v hosts the variance while the nNth super- and sub-\ndiagonals, provide the covariance between the real and the imaginary part of the measurements. Such particular\nstructure makes it possible to split \u03a3v into four diagonal blocks and makes it easy to \ufb01nd \u03a3\u22121\nv\nanalytically.\nIt is also interesting to note that, up to a permutation of the elements in \u2206v, Cov[\u2206v] can be written as a block\ndiagonal matrix where the 2-by-2 blocks are given by (8c).\nB\nProof in Section 5.3\nB.1\nProof of Lemma 4\nSince \u03a6 = In \u2297(V \u22121N \u00afV ), one has that \u03a6\u22a4\nhN+t\u03a6hN+t is block diagonal for all h, t, with n blocks of size N by n.\nHence, FMLE is a 2-by-2 block matrix, with each block being a block diagonal matrix with n blocks, and its inverse has\nthe same shape. The error covariance matrix \u03a3y is then written using the compact notation \u02c6Y \u211c\nh + j \u02c6Y \u2111\nh = ( \u02c6Y \u22a4)h,MLE\nas\n\u03a3y =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nVar[ \u02c6Y \u211c\n1 ]\n0\n...\n0\nVar[ \u02c6Y \u211c\nn ]\nCov[ \u02c6Y \u211c\n1 , \u02c6Y \u2111\n1 ]\n0\n...\n0\nCov[ \u02c6Y \u211c\nn , \u02c6Y \u2111\nn ]\n\u22c6\nVar[ \u02c6Y \u2111\n1 ]\n0\n...\n0\nVar[ \u02c6Y \u2111\nn ]\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n.\n(41)\nThe zeros introduced by the Kronecker product for constructing \u03a6 are not random variables (i.e. their variance and\ncovariance is zero). Together with assumption 3, this yields\nR\u211c,1ht =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n[0n(h\u22121)]\nVar[\u211c( \u02dcVt)]\n0\n0\n[0n(n\u2212h)]\nVar[\u211c(\u02dcIht)]\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8,\n(42)\nwhere Var[\u211c( \u02dcVt)] is diagonal from Assumption 3. Thus, R\u211c,1ht is also diagonal. R\u211c,2ht has the same expression\nas (42), but with \u2111(\u02dcIht) replacing \u211c(\u02dcIht). Expressions of the same form can be derived using Cov[\u211c( \u02dcVt), \u2111( \u02dcVt)]\nand Var[\u2111( \u02dcVt)] for R\u211c\u2111,qht and R\u2111,qht, respectively. With q \u2208{1, 2} and Qqht such that Q1ht = Var[\u211c(\u02dcIht)] and\n17\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nQ2ht = Var[\u2111(\u02dcIht)], we de\ufb01ne\nD\u211c,qht(Y\u00b7h) = \u211c(Y\u00b7h)\u22a4Var[\u211c( \u02dcVt)]\u211c(Y\u00b7h) + Qqht,\n(43a)\nD\u211c\u2111,qht(Y\u00b7h) = \u211c(Y\u00b7h)\u22a4Cov[\u211c( \u02dcVt), \u2111( \u02dcVt)]\u2111(Y\u00b7h) + Qqht,\n(43b)\nD\u2111,qht(Y\u00b7h) = \u2111(Y\u00b7h)\u22a4Var[\u2111( \u02dcVt)]\u2111(Y\u00b7h) + Qqht,\n(43c)\nExcluding the zeros in (42) yields D\u211c,1ht(Yh) = \u211c(z)\u22a4R\u211c,1ht\u211c(z), and similarly for DQ,qht(Yh) with any Q \u2208\n{\u211c, \u211c\u2111, \u2111} and q \u2208{1, 2}.\nMoreover, \u03a6\u22a4\nhN+t\u03a6hN+t also has the same sparsity pattern as the R matrices. Therefore, for any k, the kth n-by-n\nblock of FMLE is only nonzero for the terms of the sum (20) where h = k. This means that\nVar\n\u0014 \u02c6Y \u211c\nh\n\u02c6Y \u2111\nh\n\u0015\n=\n\uf8eb\n\uf8ed\n2,N\nX\nq,t=1\n\u211c(\u03a6hN+t)\u22a4\u211c(\u03a6hN+t)\nD\u211c,qht(Y\u00b7h)\n\u211c(\u03a6hN+t)\u22a4\u2111(\u03a6hN+t)\nD\u211c\u2111,qht(Y\u00b7h)\n\u22c6\n\u2111(\u03a6hN+t)\u22a4\u2111(\u03a6hN+t)\nD\u2111,qht(Y\u00b7h)\n\uf8f6\n\uf8f8\n\u22121\n.\n(44)\nEquations (41) and (44) respectively show not only that the columns ( \u02c6Y \u22a4)h of \u02c6YMLE are statistically independent, but\nalso that their variance does not depend on the exact values of one another, which \ufb01nishes the proof.\nB.2\nProof of Corollary 5\nLet Y 1\n\u00b7h and Y 2\n\u00b7h be such that |\u211c(Y 2\n\u00b7h)| \u2265|\u211c(Y 1\n\u00b7h)| and |\u2111(Y 2\n\u00b7h)| \u2265|\u2111(Y 1\n\u00b7h)| element-wise. From (43) and assuming\nCov[\u211c(Vt), \u2111(Vt)] = 0, it follows that, for all q and t,\nD\u211c,qht(Y 2\n\u00b7h) \u2265D\u211c,pit(Y 1\n\u00b7h),\n(45a)\nD\u211c\u2111,qht(Y 2\n\u00b7h) = D\u211c\u2111,qht(Y 1\n\u00b7h),\n(45b)\nD\u2111,qht(Y 2\n\u00b7h) \u2265D\u2111,qht(Y 1\n\u00b7h),\n(45c)\nbecause Var[\u211c( \u02dcVt)] and Var[\u2111( \u02dcVt)] are positive diagonal matrices. Using (44), we then write\n\u0012\nVar\n\u0014 \u02c6Y 2,\u211c\nh\n\u02c6Y 2,\u2111\nh\n\u0015\u0013\u22121\n\u2212\n\u0012\nVar\n\u0014 \u02c6Y 1,\u211c\nh\n\u02c6Y 1,\u2111\nh\n\u0015\u0013\u22121\n=\n(46)\n2,N\nX\nq,t=1\n\u0012\n\u03b1\u211c\u211c(\u03a6hN+t)\u22a4\u211c(\u03a6hN+t)\n0\n0\n\u03b1\u2111\u2111(\u03a6hN+t)\u22a4\u2111(\u03a6hN+t)\n\u0013\n,\nwith \u03b1\u211c= D\u211c,qht(Y 2\n\u00b7h)\u22121\u2212D\u211c,qht(Y 1\n\u00b7h)\u22121 and \u03b1\u2111= D\u2111,qht(Y 2\n\u00b7h)\u22121\u2212D\u2111,pit(Y 1\n\u00b7h)\u22121. The inequalities in (45) show\nthat both \u03b1\u211cand \u03b1\u2111are non-positive. Hence, the blocks of (46) are the product of a negative scalar and a quadratic\nform and are therefore negative semi-de\ufb01nite. From this observation, it follows that\nVar\n\u0014 \u02c6Y 2,\u211c\nh\n\u02c6Y 2,\u2111\nh\n\u0015\n\u2ab0Var\n\u0014 \u02c6Y 1,\u211c\nh\n\u02c6Y 1,\u2111\nh\n\u0015\n,\n(47)\nwhich \ufb01nishes the proof.\nC\nNon diagonal Bayesian prior\nThe goal of the regularization (37) of diagonal elements of \u02c6Y is to reduce the bias from a sparsity-promoting prior.\nThis means that the diagonal elements should keep a value close to the one estimated with MLE. For all h and with\nthe structural prior described in Section 7, this means\n\u02c6YMLE,hh = 1\n21\u22a4\nn2 vec\n\uf8eb\n\uf8ec\n\uf8ed\n0\n\u02c6Y \u22a4\nh,:h\u22121\n0\n\u02c6Yh,:h\u22121\n0\n\u02c6Yh,h+1:\n0\n\u02c6Y \u22a4\nh,h+1:\n0\n\uf8f6\n\uf8f7\n\uf8f8,\n= [1, j] \u2297\n\u0000ve(eh1\u22a4\nn + 1ne\u22a4\nh )\n\u0001\u22a4\u02c6yr,\nwhere [ \u02c6Yh,:h\u22121, \u02c6Yhh, \u02c6Yh,h+1:] = \u02c6Yh is the hth row of \u02c6Y . The non-diagonal prior is then given by \u00b5nd = 1n and Lnd\nde\ufb01ned as:\nLnd = \u03bb\u2032\n\u03bb\n\" \n\u211c( \u02c6Y \u22121\nMLE,hh)\n0\n0\n\u2111( \u02c6Y \u22121\nMLE,hh)\n!\n\u2297ve(eh1\u22a4\nn + 1ne\u22a4\nh )\n#n\nh=1\n.\n18\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\nD\nNoise bias in Cartesian coordinates\nIn Section 4 we stated that the noise bias Eq. (7) is negligible with realistic levels of accuracy. In order to show this,\nwe adapt the procedure in [25, Sec III.A]. By using the \ufb01rst-order Taylor expansion of Eq. (7) about \u03c3\u03b4 = 0, we get:\nE[\u2206c|\u02dcv, \u02dc\u03b8] \u2243\u2212(\u02dcv\u03c32\n\u03b4/2) cos \u02dc\u03b8,\n(48a)\nE[\u2206d|\u02dcv, \u02dc\u03b8] \u2243\u2212(\u02dcv\u03c32\n\u03b4/2) sin \u02dc\u03b8.\n(48b)\nThen,\n\u2225[E[\u2206c|\u02dcv, \u02dc\u03b8], E[\u2206d|\u02dcv, \u02dc\u03b8]]\u2225= \u02dcv\u03c32\n\u03b4/2.\n(49)\nMoreover, the minimum eigenvalue \u03bbmin of the covariance matrix (8) is min(\u03c32\n\u03f5 , \u02dcv2\u03c32\n\u03b4): therefore, the minimum\nstandard deviation in the covariance matrix is \u03c3min = \u221a\u03bbmin =\np\nmin(\u03c32\u03f5 , \u02dcv2\u03c32\n\u03b4).\nThe bias can be considered non-signi\ufb01cant if \u2225[E[\u2206c|\u02dcv, \u02dc\u03b8], E[\u2206d|\u02dcv, \u02dc\u03b8]]\u2225/\u03c3min is small. Adopting the per-unit system,\nand using realistic \u00b5PMU accuracy speci\ufb01cations (Table 1), we obtain:\n\u2225[E[\u2206c|\u02dcv, \u02dc\u03b8], E[\u2206d|\u02dcv, \u02dc\u03b8]]\u2225\n\u03c3min\n\u22438.72 \u00b7 10\u22125.\n(50)\nThus, the bias is four orders of magnitude smaller than the smallest standard deviation from the noise covariance\nmatrix and therefore can be safely neglected.\nReferences\n[1] Y. Weng, Y. Liao, and R. Rajagopal. Distributed energy resources topology identi\ufb01cation via graphical modeling.\nIEEE Transactions on Power Systems, 32(4):2682\u20132694, 2017.\n[2] L. Schenato, G. Barchi, D. Macii, R. Arghandeh, K. Poolla, and A. Von Meier. Bayesian linear state estimation\nusing smart meters and pmus measurements in distribution grids. In 2014 IEEE International Conference on\nSmart Grid Communications (SmartGridComm), pages 572\u2013577, 2014.\n[3] A. La Bella, S. Negri, R. Scattolini, and E. Tironi. A two-layer control architecture for islanded ac microgrids\nwith storage devices. In 2018 IEEE Conference on Control Technology and Applications (CCTA), pages 1421\u2013\n1426, 2018.\n[4] Alessio Iovine, Tristan Rigaut, Gilney Damm, Elena De Santis, and Maria Domenica Di Benedetto. Power\nmanagement for a dc microgrid integrating renewables and storages. Control Engineering Practice, 85:59\u201379,\n2019.\n[5] Alessandra Parisio, Evangelos Rikos, and Luigi Glielmo.\nStochastic model predictive control for eco-\nnomic/environmental operation management of microgrids: An experimental case study. Journal of Process\nControl, 43:24\u201337, 2016.\n[6] Deepa S Kumar, JS Savier, and SS Biju. Micro-synchrophasor based special protection scheme for distribution\nsystem automation in a smart city. Protection and Control of Modern Power Systems, 5(1):1\u201314, 2020.\n[7] Guido Cavraro and Vassilis Kekatos. Graph algorithms for topology identi\ufb01cation using power grid probing.\nIEEE control systems letters, 2(4):689\u2013694, 2018.\n[8] Guido Cavraro and Vassilis Kekatos. Inverter probing for power distribution network topology processing. IEEE\nTransactions on Control of Network Systems, 6(3):980\u2013992, 2019.\n[9] Xu Du, Alexander Engelmann, Yuning Jiang, Timm Faulwasser, and Boris Houska. Optimal experiment design\nfor ac power systems admittance estimation. arXiv preprint arXiv:1912.09017, 2019.\n[10] M. Angjelichinoski, C. Stefanovi\u00b4c, P. Popovski, A. Scaglione, and F. Blaabjerg. Topology identi\ufb01cation for\nmultiple-bus dc microgrids via primary control perturbations. In 2017 IEEE Second International Conference on\nDC Microgrids (ICDCM), pages 202\u2013206, June 2017.\n[11] Ye Yuan, Steven Low, Omid Ardakanian, and Claire Tomlin.\nInverse power \ufb02ow problem.\narXiv preprint\narXiv:1610.06631, 2016.\n[12] Emanuele Fabbiani, Pulkit Nahata, Giuseppe De Nicolao, and Giancarlo Ferrari-Trecate. Identi\ufb01cation of ac\nnetworks via online learning. IEEE Transactions on Control Systems Technology, to appear.\n[13] O. Ardakanian, V. W. S. Wong, R. Dobbe, S. H. Low, A. von Meier, C. J. Tomlin, and Y. Yuan. On identi\ufb01cation\nof distribution grids. IEEE Transactions on Control of Network Systems, 6(3):950\u2013960, 2019.\n19\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\n[14] Bogdan Pinte, Michael Quinlan, and Karl Reinhard. Low voltage micro-phasor measurement unit (\u00b5pmu). In\n2015 IEEE Power and Energy Conference at Illinois (PECI), pages 1\u20134. IEEE, 2015.\n[15] Styliani Sarri. Methods and performance assessment of PMU-based real-time state estimation of active distribu-\ntion networks. PhD thesis, EPFL, 2016.\n[16] Jiafan Yu, Yang Weng, and Ram Rajagopal. Patopa: A data-driven parameter and topology joint estimation\nframework in distribution grids. IEEE Transactions on Power Systems, PP, 05 2017.\n[17] Antoine Wehenkel, Arpan Mukhopadhyay, Jean-Yves Le Boudec, and Mario Paolone. Parameter estimation\nof three-phase untransposed short transmission lines from synchrophasor measurements. IEEE Transactions on\nInstrumentation and Measurement, 69(9):6143\u20136154, 2020.\n[18] Allen J Wood, Bruce F Wollenberg, and Gerald B Shebl\u00e9. Power generation, operation, and control. John Wiley\n& Sons, 2013.\n[19] P. Kundur. Power System Stability and Control. CRC Press New York, NY, USA, 2007.\n[20] Florian D\u00f6r\ufb02er, John W Simpson-Porco, and Francesco Bullo. Electrical networks and algebraic graph theory:\nModels, properties, and applications. Proceedings of the IEEE, 106(5):977\u20131005, 2018.\n[21] Alexandra Von Meier, Emma Stewart, Alex McEachern, Michael Andersen, and Laura Mehrmanesh. Precision\nmicro-synchrophasors for distribution systems: A summary of applications. IEEE Transactions on Smart Grid,\n8(6):2926\u20132936, 2017.\n[22] Simon J Julier and Jeffrey K Uhlmann. Unscented \ufb01ltering and nonlinear estimation. Proceedings of the IEEE,\n92(3):401\u2013422, 2004.\n[23] Mo Longbin, Song Xiaoquan, Zhou Yiyu, Sun Zhong Kang, and Yaakov Bar-Shalom. Unbiased converted\nmeasurements for tracking. IEEE Transactions on Aerospace and Electronic Systems, 34(3):1023\u20131027, 1998.\n[24] Zhansheng Duan, Chongzhao Han, and X Rong Li. Comments on \"unbiased converted measurements for track-\ning\". IEEE transactions on aerospace and electronic systems, 40(4):1374, 2004.\n[25] Don Lerro and Yaakov Bar-Shalom. Tracking with debiased consistent converted measurements versus ekf.\nIEEE transactions on aerospace and electronic systems, 29(3):1015\u20131022, 1993.\n[26] Ivan Markovsky and Sabine Van Huffel.\nOverview of total least-squares methods.\nSignal Processing,\n87(10):2283\u20132302, 2007. Special Section: Total Least Squares and Errors-in-Variables Modeling.\n[27] Sabine Van Huffel and Vandewalle Joos. The Total Least Squares Problem: Computational Aspects and Analysis,\nchapter 8, pages 227\u2013250. Society for Industial and Applied Mathematics, 1991.\n[28] Giuseppina Casella and R. Berger. Statistical Inference. Duxbury Press, 01 2002.\n[29] Robert Engle and Daniel McFadden, editors. Handbook of Econometrics, volume 4. Elsevier, 1 edition, 1986.\n[30] John Crassidis and Yang Cheng. Error-covariance analysis of the total least squares problem. Journal of Guid-\nance, Control, and Dynamics, 37, 07 2014.\n[31] Xing Fang, Bofeng Li, Hamza Alkhatib, Wenxian Zeng, and Yibin Yao. Bayesian inference for the errors-in-\nvariables model. Studia Geophysica et Geodaetica, 61:35\u201352, 02 2017.\n[32] Paul Holland. Weighted ridge regression: Combining ridge and robust regression methods. 10 1973.\n[33] Trevor Park and George Casella.\nThe bayesian lasso.\nJournal of the American Statistical Association,\n103(482):681\u2013686, 2008.\n[34] Dimitris Bertsimas and Martin S. Copenhaver. Characterization of the equivalence of robusti\ufb01cation and regu-\nlarization in linear and matrix regression. European Journal of Operational Research, 270(3):931\u2013942, 2018.\n[35] Minjung Kyung, Jeff Gill, Malay Ghosh, and George Casella. Penalized regression, standard errors, and bayesian\nlassos. Bayesian Analysis, 5:369\u2013412, 06 2010.\n[36] Michael Osborne, Brett Presnell, and Berwin Turlach. On the lasso and its dual. Journal of Computational and\nGraphical Statistics, 9:319\u2013337, 07 2000.\n[37] Samuel Kotz, Tomaz J. Kozubowski, and Krzysztof Podg\u00f3rski. Asymmetric Laplace Distributions, pages 133\u2013\n178. Birkh\u00e4user Boston, Boston, MA, 2001.\n[38] Hui Zou.\nThe adaptive lasso and its oracle properties.\nJournal of the American Statistical Association,\n101(476):1418\u20131429, 2006.\n[39] M. Taleb, M. J. Ditto, and T. Bouthiba. Performance of short transmission lines models. In 2006 IEEE GCC\nConference (GCC), pages 1\u20137, March 2006.\n20\nJ.S. Brouillon et al, Bayesian Error-in-Variable Models\nTECHNICAL REPORT\n[40] Florian D\u00f6r\ufb02er and Francesco Bullo. Kron reduction of graphs with applications to electrical networks. Com-\nputing Research Repository - CORR, 60, 02 2011.\n[41] Emile Dusabimana and Sung-Guk Yoon. A survey on the micro-phasor measurement unit in distribution net-\nworks. Electronics, 9(2), 2020.\n[42] Hao Zhu, Geert Leus, and G.B. Giannakis. Sparsity-cognizant total least-squares for perturbed compressive\nsampling. Signal Processing, IEEE Transactions on, 59:2002 \u2013 2016, 06 2011.\n[43] Linlin Dai, Kani Chen, Zhihua Sun, Zhenqiu Liu, and Gang Li. Broken adaptive ridge regression and its asymp-\ntotic properties. Journal of Multivariate Analysis, 168, 08 2018.\n[44] Florian Frommlet and Gregory Nuel. An adaptive ridge procedure for l0 regularization. PloS one, 11, 05 2015.\n[45] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statis-\ntical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning,\n3:1\u2013122, 01 2011.\n[46] Amir Beck.\nFirst-Order Methods in Optimization.\nSIAM-Society for Industrial and Applied Mathematics,\nPhiladelphia, PA, USA, 2017.\n[47] Nicholas G. Polson, James G. Scott, and Brandon T. Willard. Proximal algorithms in statistics and machine\nlearning. arXiv preprint arXiv:1502.03175, 2015.\n[48] Masayuki Tanaka and Masatoshi Okutomi. Uni\ufb01ed optimization framework for L2, L1, and/or L0 constrained\nimage reconstruction. In Computational Imaging II, volume 10222, pages 99 \u2013 108. SPIE, 2017.\n[49] S. Bolognani, R. Carli, G. Cavraro, and S. Zampieri. On the need for communication for voltage regulation of\npower distribution grids. IEEE Transactions on Control of Network Systems, 6(3):1111\u20131123, 2019.\n[50] Jean-Sebastien Brouillon, Emanuele Fabbiani, and Pulkit Nahata. Simulation and identi\ufb01cation software for\ndistribution grids. https://github.com/donlelef/eiv-grid-id, doi:10.5281/zenodo.5725215.\n[51] G. Hoogsteen, A. Molderink, J. L. Hurink, and G. J. M. Smit. Generation of \ufb02exible domestic load pro\ufb01les to\nevaluate demand side management approaches. In 2016 IEEE International Energy Conference (ENERGYCON),\npages 1\u20136, 2016.\n[52] L. Thurner, A. Scheidler, F. Schafer, J. H. Menke, J. Dollichon, F. Meier, S. Meinecke, and M. Braun. pandapower\n- an open source python tool for convenient modeling, analysis and optimization of electric power systems. IEEE\nTransactions on Power Systems, 2018.\n[53] C.R. Bayliss and B.J. Hardy. Chapter 28 - fundamentals. In C.R. Bayliss and B.J. Hardy, editors, Transmission\nand Distribution Electrical Engineering (Fourth Edition), pages 1075\u20131132. Newnes, Oxford, fourth edition\nedition, 2012.\n[54] Manas Mukherjee and Biman Kumar Saha Roy. Optimal micro pmu placement in practical distribution network:\nA graph theoretic approach. In 2020 IEEE First International Conference on Smart Technologies for Power,\nEnergy and Control (STPEC), pages 1\u20136, 2020.\n21\n"}