{"text": "An Adaptive-to-Model Test for Parametric Single-Index\nErrors-in-Variables Models\nHira L. Koul,\nChuanlong Xie,\nLixing Zhu1\nMichigan State University, USA\nHong Kong Baptist University, Hong Kong, China\nAbstract\nThis paper provides some useful tests for \ufb01tting a parametric single-index regression\nmodel when covariates are measured with error and validation data is available. We\npropose two tests whose consistency rates do not depend on the dimension of the\ncovariate vector when an adaptive-to-model strategy is applied. One of these tests has\na bias term that becomes arbitrarily large with increasing sample size but its asymptotic\nvariance is smaller, and the other is asymptotically unbiased with larger asymptotic\nvariance.\nCompared with the existing local smoothing tests, the new tests behave\nlike a classical local smoothing test with only one covariate, and still are omnibus\nagainst general alternatives. This avoids the di\ufb03culty associated with the curse of\ndimensionality. Further, a systematic study is conducted to give an insight on the\ne\ufb00ect of the values of the ratio between the sample size and the size of validation data\non the asymptotic behavior of these tests. Simulations are conducted to examine the\nperformance in several \ufb01nite sample scenarios.\nKey words: Dimension reduction; error in variable model; model check; adaptive test.\n1\nIntroduction\nConsider the nonparametric regression model with measurement error where the response\nvariable Y , a p-dimensional unobservable predicting covariate X and its observable cohort\nvector W are related to each other by the relations\nY = \u00b5(X) + \u03b5,\nW = X + U.\n(1.1)\nHere p is assumed to be known, and the variables \u03b5, U, and X are assumed to be mutually\nindependent with E(\u03b5) = 0 = E(U). Hence \u00b5(x) = E(Y |X = x) is the usual regression\nfunction. This is the so called nonparametric errors in variables (EIVs) regression model. The\nmonographs of Fuller (1987), Cheng and Van Ness (1999), and Carroll, Ruppert, Stefansky\nand Crainiceanu (2006) contain a vast number of real data examples where this model is\nnaturally applicable.\n1The corresponding author. Email: lzhu@hkbu.edu.hk. The research described here was supported by\na grant from the Research Council of Hong Kong, and a grant from Hong Kong Baptist University, Hong\nKong. This is a part of the PHD thesis of the second author.\n1\narXiv:1604.08294v1  [stat.ME]  28 Apr 2016\nThe problem of interest here is to \ufb01t a parametric single-index regression model to the\nregression function, i.e., for a known real valued link function g we wish to test the hypothesis\nH0 : \u00b5(x) = g(\u03b2\u22a4x),\nfor all x \u2208Rp and for some \u03b2 \u2208Rp, versus\nH1 : H0 is not true.\nThroughout this paper, a\u22a4denotes transpose of the vector a \u2208Rp. The model is called\nparametric single index although it is also often called generalized linear model. This is\nbecause it is in e\ufb00ect slightly di\ufb00erent from the generalized linear model that has its special\nde\ufb01nition in the literature. A motivation for considering the above testing problem is that in\npractice model checking is necessary to prevent possible wrong conclusions when an improper\nmodel is used. Moreover, e\ufb03cient and accurate inference is possible in a parametric model\nthan in a nonparametric or semiparametric model.\nHart (1997) described numerous tests for lack-of-\ufb01t of a parametric regression model in\nthe classical regression set up where X is observable. Since the mid 1990\u2019s, there has been an\nexplosion of activities in this area as is summarized in the recent review by Gonzlez-Manteiga\nand Crujeiras (2013).\nIt is well known that the naive application of the inference procedures valid for the classi-\ncal regression set up, where one replaces X by W, often yields ine\ufb03cient inference procedures\nfor the EIV models, see, e.g. Fuller (1987) and Carroll et al. (2006). An alternative approach\nadopted in the literature is that of calibration, where the original regression relationship is\ntransferred to the regression E(Y |W) relationship between the response Y and the cohort\nW. Zhu, Cui and Ng (2004) established a su\ufb03cient and necessary condition for the linearity\nof E[Y |W] with respect to W when g(\u03b2\u22a4x) = \u03b1 + \u03b2\u22a4x. A score-type lack-of-\ufb01tness test was\nproposed based on this fact. This testing procedure has been extended to polynomial EIVs\nmodels by Cheng and Kukush (2004) and Zhu, Song and Cui (2003) independently, without\nthe normality restriction on the covariates. Hall and Ma (2007) proposed a test based on\ndeconvolution methods assuming that the distribution of the covariate errors is known. Zhu\nand Cui (2005) proposed a test for \ufb01tting a general linear model \u03b1 + \u03b2\u22a4h(x), where h is a\nvector of known functions. Song (2008) proposed a test for \ufb01tting \u03b2\u22a4h(x) to \u00b5(x), without\nrequiring the knowledge of the density of X. He used the deconvolution kernel density es-\ntimator. Koul and Song (2009) developed an analog of the minimum distance tests of Koul\nand Ni (2004) to \ufb01t a parametric form to the regression function for the Berkson measure-\nment error models. Koul and Song (2010) developed tests for \ufb01tting a parametric function\nto the nonparametric part in a partial linear regression model under a similar condition.\nThese latter \ufb01ve references assume that density of the measurement error U is known. All\nof these authors employ the calibrated methodology and test for \ufb01tting the parameter form\nof the regression function E[Y |W] implied by H0.\nThere is no valid test in the literature for \ufb01tting a parametric model under general\nconditions where the distributions of both X and U may not be known. Some of the main\n2\nreasons for this are the di\ufb03culties associated with the estimation of the calibrated regression\nfunction and some of the other underlying functions involved in the construction of a test\nstatistic.\nHowever, it is possible to circumvent some of these di\ufb03culties when there are\nvalidation data available. Stute, Xue and Zhu (2007) used validation data and empirical\nlikelihood methodology to develop con\ufb01dence regions for some underlying parameters. Song\n(2009) developed a test for general EIVs models with the assistance of validation data without\nassuming any knowledge of the distributions of X or U, under somewhat restrictive conditions\non the kernel function and bandwidth. Dai, Sun and Wang (2010) constructed a test with\nvalidation data for the same model as in Zhu and Cui (2005). They used speci\ufb01c models\nand relaxed some conditions in Song (2009). Xu and Zhu (2014) considered a nonparametric\ntest for partial linear EIVs models with validation data. All of these tests are based on local\nsmoothing methodology.\nIn the classical regression setup, it is known that a common property of lack-of-\ufb01t tests\nfor \ufb01tting a parametric regression model based on nonparametric smoothing methodology is\nthat the rate of consistency of the test statistics is 1/\n\u221a\nnhp/2. That is, the null distribution\nof a suitably centered and scaled test statistic multiplied by\n\u221a\nnhp/2 has a weak limit, and\nthese tests can detect local alternatives distinct from the null only at this rate. When p\nis even 2 or larger, this rate can be very slow. Consequently, for moderate sample sizes,\nlocal smoothing tests cannot maintain the signi\ufb01cance level well and have low power even\nfor p = 2 or 3. See, e.g., Zheng (1996), Koul and Ni (2002), and several other cited references\nfor this phenomena. It is expected that the same fact will continue to hold for various local\nsmoothing tests in the EIVs setup.\nThe main goal of the present paper is to propose tests of dimension reduction nature\nwhen validation data is available, which do not su\ufb00er from the above slow rate of consis-\ntency. Speci\ufb01cally, the tests do not su\ufb00er severely from the curse of dimensionality and can\nwell maintain the signi\ufb01cance level with good power performance for moderate \ufb01nite sample\nsizes. Towards this goal we proceed as follows. First, we discuss su\ufb03cient dimension reduc-\ntion (SDR) technique as illustrated in Cook (1998), Li and Yin (2007), and Carroll and Li\n(1992). The goal is to have a technique such that the dimension of X can be reduced to\none-dimensional projection \u03b2\u22a4X under the null hypothesis, where \u03b2 is just the projection\ndirection in the model (1.1) and to B\u22a4X automatically under the alternative, where B is a\np \u00d7 q orthonormal matrix with q \u2264p to be speci\ufb01ed. Second, based on dimension reduc-\ntion, we can then construct a test with the consistency rate of 1/\n\u221a\nnh1/2 (or 1/(nh1/2) when\na quadratic form is used) when the size N of validation data is proportional to or larger\nthan the sample size n. When N is much smaller than n, the consistency rate can be slower.\nTherefore, the third issue is to investigate the relationship between the asymptotic behaviour\nof the tests and the size of validation data set. In Section 3, a systematic study is performed\nto analyze the three di\ufb00erent scenarios: N/n \u2192\u03bb, as min(n, N) \u2192\u221e, where \u03bb = 0, \u221e, or\n0 < \u03bb < \u221e. Another interesting issue is raised during the construction procedure. When\n3\nvalidation data are used to de\ufb01ne the nonparametric kernel estimate of E(Y |W) such that\nthe residuals can be derived, the resulting test would have a bias term going to in\ufb01nity as\nn \u2192\u221e. It motivates us to consider a bias correction.\nTo e\ufb03ciently employ su\ufb03cient dimension reduction theory (SDR) of Cook (1998) or CMS\nof Cook and Li (2002), we consider the alternatives \u02dcH1 : \u00b5(x) = G(B\u22a4x), for all x \u2208Rp,\nand for some p \u00d7 q orthonormal matrix B with an unknown q \u2264p and for some real valued\nfunction G. When there are no measurement errors in covariates, Guo, Wang and Zhu (2015)\nproposed a dimension-reduction model-adaptive approach to circumvent the dimensionality\nproblem. To implement this methodology one needs to estimate the matrix B. There are\na number of proposals available in the literature for this purpose. Examples include sliced\ninverse regression (SIR) of Li (1991), sliced average variance estimation (SAVE) of Cook and\nWeisberg (1991), contour regression (CR) of Li et al. (2005), directional regression (DR) of\nLi and Wang (2007), discretization-expectation estimation (DEE) of Zhu et al. (2010a), and\nthe average partial mean estimation (APME) of Zhu et al. (2010b).\nIn this paper, we construct an adaptive-to-model test in the current set up. The proposed\ntest is based on the Zheng\u2019s test (1996). To this end, we consider a di\ufb00erent kind of calibration\nwhere instead of conditioning on W we condition on \u03b2\u22a4W under the null hypothesis and\non B\u22a4W under the alternatives, and then constructs a test for this testing problem. Thus,\nour strategy is sketched as follows: 1). Use the data (w1, y1), \u00b7 \u00b7 \u00b7 , (wn, yn) to estimate \u03b2\nunder the null hypothesis and automatically the matrix B by a q \u00d7 q orthogonal matrix C\nunder the alternative; 2). Use the validation data to estimate the conditional expectation\nE[g(\u03b2\u22a4X)|\u03b2\u22a4W]. 3). Compute the test statistic using these regression function estimates.\nAs mentioned above, the test statistic is asymptotically biased.\nIt is because of the\ndependence among the residuals when we use all the validation data to obtain the estimators\nin Step 2. To reduce the bias, we propose a bias correction method to construct another\ntest. In the simulation studies, we will compare their performance.\nThe paper is organized as follows: Section 2 contains a brief description of the test statistic\nconstruction. Since the estimation the matrix B plays a key role in having the dimension\nreduction property of the test, we review a widely used dimension reduction method in this\nsection. The needed assumptions are also stated in this section. The asymptotic properties\nof the test statistic under the null and alternative hypotheses are described in Section 3.\nParticularly, a systematic study is conducted on the asymptotic behaviors of the tests under\nthe three scenarios where the ratio N/n of the validation data N and the sample size n\nis small, moderate and large. Section 4 presents the simulation results. The proofs are\npostponed to Appendix.\nBefore closing this section, we describe some notation used in the sequel. The sample\nis denoted by {(yi, wi), i = 1, \u00b7 \u00b7 \u00b7 , n} and the validation data is dented by {( \u02dcws, \u02dcxs), s =\n1, \u00b7 \u00b7 \u00b7 , N}. The two data sets are assumed to independent of each other. Further, in various\nexpressions below, i and j often represent the indices of primary data, while s and t those\n4\nof validation data. Throughout this paper, \u2192p denotes the convergence in probability and\n\u201d\u2192D\u201d stands for the convergence in distribution. All limits are taken as n \u2227N \u2192\u221e, unless\nspeci\ufb01ed otherwise.\nThe normal distribution with mean a and variance b is denoted by\nN(a, b).\n2\nMethodology development\n2.1\nTest construction: a dimension-reduction adaptive-to-model\nstrategy\nIn this subsection, we describe the details of test statistics construction. It consists of three\ncomponents as follows.\n1). Model adaptation. To proceed further, let r(w, \u03b2) = E[g(\u03b2\u22a4X)|W = w], w \u2208Rp, denote\nthe new regression function under the null hypothesis. In order to avoid the above mentioned\nhigh dimensionality problem of nonparametric estimators of r(\u00b7, \u00b7) due to the dimension of\nW, we adopt the following dimension reduction adaptive-to-model strategy (DREAM). Re-\ncall that W = X + U. Note that under H0, the regression function g(\u03b2\u22a4X) depends on X\nonly through the linear combination \u03b2\u22a4X. It is then natural to consider the situation where\nthe calibrated regression function E(Y |W) depends on W only through a linear combina-\ntion of the components of W, i.e., when E(Y |W) = E[g(\u03b2\u22a4X)|W] = E[g(\u03b2\u22a4X)|\u03b2\u22a4W] :=\nr(\u03b2\u22a4W, \u03b2).\nSimilarly, under the alternative, we assume that E(Y |W) = E(Y |B\u22a4W) =\nE(G(B\u22a4X)|B\u22a4W). Thus the transferred hypotheses become as follows:\nH0 : P{E(Y |W) = r(\u03b2\u22a4W, \u03b2)} = 1,\nfor some \u03b2 \u2208Rp,\n(2.1)\nversus the transferred alternative hypothesis:\nH1 : P{E(Y |W) = E(Y |B\u22a4W) \u0338= r(\u03b2\u22a4W, \u03b2)} = 1,\nfor all \u03b2 \u2208Rp .\n(2.2)\nGenerally the two hypotheses H0 and H0 are not exactly equivalent. But, as in Song (2008),\nwhen the family densities f\u03b2\u22a4U(\u03b2\u22a4w \u2212\u00b7) is a complete family over the parameter \u03b2\u22a4w \u2208R,\nthe equivalence can hold.\n2). Test statistic construction. Let e = Y \u2212r(\u03b2\u22a4W, \u03b2). To unify the null and alternatives,\nlet B = \u03b2c under H0 where c is a constant, hence E[e|\u03b2\u22a4W] = E[e|B\u22a4W] = 0. Moreover,\nfollowing Zheng (1996),\nE[eE[e|\u03b2\u22a4W]f(\u03b2\u22a4W)] = E[E2(e|\u03b2\u22a4W)f(\u03b2\u22a4W)] = E[E2(e|B\u22a4W)f(B\u22a4W)] = 0,\nand under H1, E[E2(e|B\u22a4W)f(B\u22a4W)] > 0. To obtain residuals for the construction of the\ntest statistics, we assume the availability of validation data (ws, xs), s = 1, \u00b7 \u00b7 \u00b7 , N, which\nis used to estimate the function r. Note that r is an unknown function of \u03b2\u22a4W. In order\n5\nto construct an estimator r(\u03b2\u22a4W, \u03b2), let M(\u00b7) be a kernel function, vN be a bandwidth\nsequence, and set MvN(\u00b7) = v\u22121\nN M(\u00b7/vN). Then an estimator of r(\u03b2\u22a4W, \u03b2) is\n\u02c6r(\u02c6\u03b2\u22a4w, \u02c6\u03b2) =\nPN\ns=1 MvN(\u02c6\u03b2\u22a4w \u2212\u02c6\u03b2\u22a4\u02dcws)g(\u02c6\u03b2\u22a4\u02dcxs)\nPN\ns=1 MvN(\u02c6\u03b2\u22a4w \u2212\u02c6\u03b2\u22a4\u02dcws)\n,\n(2.3)\nwhere \u02c6\u03b2 is a consistent estimate of \u03b2 based on primary data. De\ufb01ne the residuals\nei = yi \u2212r(\u03b2\u22a4wi, \u03b2),\n\u02c6ei = yi \u2212\u02c6r(\u02c6\u03b2\u22a4wi, \u02c6\u03b2),\ni = 1, \u00b7 \u00b7 \u00b7 , n.\n(2.4)\nTo estimate the conditional expectation of the error e, given B\u22a4W, we also need an esti-\nmator \u02c6B(\u02c6q) of B that is consistent to \u03b2/\u2225\u03b2\u2225under the null, and to B under the alternative.\nThis model adaptation property of \u02c6B(\u02c6q) can enable the test statistic to adapt to model and\nthen to alleviate the curse of dimensionality. This estimator will be speci\ufb01ed later. For the\nmoment assume the existence of such an estimator.\nTo proceed further, let K be another kernel function and h \u2261hn another bandwidth.\nThen an estimator of the product E[e|B\u22a4W]f(B\u22a4W) at \u02c6B\u22a4wi is given by\n\u02c6E[ei| \u02c6B(\u02c6q)\u22a4wi] \u02c6f( \u02c6B(\u02c6q)\u22a4wi)\n=\n1\nn \u22121\nn\nX\nj\u0338=i\nKh( \u02c6B(\u02c6q)\u22a4wj \u2212\u02c6B(\u02c6q)\u22a4wi)\u02c6ej.\nThe analog of the Zheng\u2019s test statistic in the current set up is based on an estimator of\nE[eE[e|W]f(W)], given by\n\u02dcVn =\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\n\u02c6eiKh( \u02c6B(\u02c6q)\u22a4(wi \u2212wj))\u02c6ej.\n(2.5)\n3). Bias correction. From the technical details in Appendix, we can see that the test statistic\nin (2.5) has non-negligible asymptotic bias and thus its limiting null distribution has a mean\ntending to in\ufb01nity unless n/(Nh1/2) \u21920, which makes the bias term vanish. The main\nreason is the dependence between the residuals \u02c6ei and \u02c6ej for i \u0338= j when all validation data\nare used to estimate the function r. There are two ways to correct for this bias. One is to\ncenter the test statistic at a suitable estimator of this bias. This is a traditional method, and\nhas been used. Alternately, we propose a block-wise estimation approach to asymptotically\neliminate the bias as follows. Assume N is a positive even integer. We halve the whole\nvalidation data set, use the two halves to construct two estimators of the regression function\nr, which results in the two sets of residuals as follows. Let\n\u02c6r(1)(\u02c6\u03b2\u22a4w, \u02c6\u03b2) =\nPN/2\ns=1 MvN(\u02c6\u03b2\u22a4w \u2212\u02c6\u03b2\u22a4\u02dcws)g(\u02c6\u03b2\u22a4\u02dcxs)\nPN/2\ns=1 MvN(\u02c6\u03b2\u22a4w \u2212\u02c6\u03b2\u22a4\u02dcws)\n,\n(2.6)\n\u02c6r(2)(\u02c6\u03b2\u22a4w, \u02c6\u03b2) =\nPN\ns=N/2+1 MvN(\u02c6\u03b2\u22a4w \u2212\u02c6\u03b2\u22a4\u02dcws)g(\u02c6\u03b2\u22a4\u02dcxs)\nPN\ns=N/2+1 MvN(\u02c6\u03b2\u22a4w \u2212\u02c6\u03b2\u22a4\u02dcws)\n,\n\u02c6ei(1) := yi \u2212\u02c6r(1)(\u02c6\u03b2\u22a4wi, \u02c6\u03b2),\n\u02c6ei(2) = yi \u2212\u02c6r(2)(\u02c6\u03b2\u22a4wi, \u02c6\u03b2),\ni = 1, \u00b7 \u00b7 \u00b7 , n.\n6\nUse these residuals to de\ufb01ne the test statistic\nVn =\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\n\u02c6ei(1)Kh( \u02c6B(\u02c6q)\u22a4wi \u2212\u02c6B(\u02c6q)\u22a4wj)\u02c6ej(2)\n(2.7)\nto perform the test. We shall prove that the asymptotic bias of Vn vanishes, but its asymp-\ntotic variance gets larger than that of \u02dcVn. Note that \u02dcVn and Vn are non-standardized, the\nstandardizing constants will be speci\ufb01ed in Section 3. Here, we mention a signi\ufb01cant fea-\nture of both of these statistics, which is that their asymptotic behavior is like that of a test\nstatistic with one-dimensional covariate X, i.e., their consistency rate is 1/\n\u221a\nnh1/2, which in\nturn greatly alleviates the dimensionality issue.\nFrom the above construction, it is obvious that estimating adaptively the matrix B under\nthe null and alternative hypothesis plays a crucial role for dimension reduction. The next\nsubsection is devoted to this issue.\n2.2\nEstimation of B and \u03b2\nTo achieve the adaptation property of the estimators of B and \u03b2 mentioned above, the key\nis to derive an estimator of B up to an q \u00d7 q orthonormal matrix C without depending on\nthe assumed models under the null and alternative hypotheses. With measurement errors,\nCarroll and Li (1992) extended sliced inverse regression (SIR, Li 1991) to errors-in-variables\nregression models.\nLue (2004) extended the principal Hessian directions (pHd, Li 1992)\nmethod to the surrogate problem. Li and Yin (2007) established a general invariance law\nbetween the surrogate and the original dimension reduction spaces when X and U are jointly\nmultivariate normal. If X or U is not normally distributed, they suggested an approximation\nbased on the results of Hall and Li (1993). See also Zhang, Zhu and Zhu (2014).\nAs the discretization-expectation estimation method (DEE) of Zhu et al. (2010a) is simple\nto implement without selecting the number of slices, we adopt it to errors-in-variables models\nwhen SIR is used. Write SY |X as the central subspace that is the intersection of all column\nspaces spanned by the columns of B that makes Y conditionally independent of X, given\nB\u22a4X, i.e., Y \u22a5\u22a5X|B\u22a4X. This means that identifying SY |X is equivalent to identifying a base\nmatrix \u02dcB that is equal to BC\u22a4for a q \u00d7 q orthogonal matrix C. Note that the function\nG is unknown in the alternative. We can rewrite G(B\u22a4X) as \u02dcG( \u02dcB\u22a4X). In other words,\nidentifying \u02dcB is enough for model identi\ufb01cation. Without notational confusion, we write\n\u02dcB = B throughout the rest of this paper.\nTo extend the DEE method to the setting with measurement errors, we \ufb01rst give a\nvery brief review. Assume that Cov(X) is the identity matrix. As is known, SIR is fully\ndependent on the reverse regression function E(X|Y ) such that we can consider the eigen-\ndecomposition of its covariance matrix Cov(E(X|Y )). The eigen vectors associated with\nnonzero eigen values of this matrix form the base matrix B. SIR-based DEE uses the matrix\n7\n\u039b = E{Cov(E(X|\u02dcY (T)))} as the target matrix, where \u02dcY (t) = I(Y \u2264t), t \u2208R and T is\nan independent copy of Y . Because the measurement error U is independent of Y , and\nthus, when X is replaced by W, at the population level, nothing is changed about eigen-\ndecomposition and eigen vectors.\nWe use surrogate predictors Cov(X, W)\u03a3\u22121\nW W, which\nforms the least squares prediction of X when W is given. Carroll and Li (1992) pointed\nout that sliced inverse regression (SIR) with the surrogate predictors can produce consistent\nestimators of SY |X. In other words, all steps of estimation are exactly the same as those in\nthe without measurement errors set up. The reader can refer to Zhu et al. (2010a) for more\ndetails.\nWhen we use data to construct an estimate \u039bn of \u039b, we can then obtain an estimate \u02c6B(\u02c6q)\nof B, which consists of the \u02c6q eigenvectors of \u039bn with non-zero eigenvalues, where \u02c6q is de\ufb01ned\nas follows, using the BIC type criterion proposed by Zhu et al. (2006). Let \u02c6\u03bb1 \u2265\u02c6\u03bb2 \u2265\u00b7 \u00b7 \u00b7 \u2265\u02c6\u03bbp\nbe the eigen values of the matrix \u039bn in descending order. An estimate \u02c6q of q is given by\n\u02c6q = arg max\nl=1,\u00b7\u00b7\u00b7 ,p\n(\nn\n2 \u00d7\nPl\ni=1{log(\u02c6\u03bbi + 1) \u2212\u02c6\u03bbi}\nPp\ni=1{log(\u02c6\u03bbi + 1) \u2212\u02c6\u03bbi}\n\u22122 \u00d7 Dn \u00d7 l(l + 1)\n2p\n)\n,\n(2.8)\nwhere Dn is a sequence of constants not depending on the data. Here we take Dn = n1/2.\nThe following consistency results can be obtained from Zhu et al. (2010a).\nProposition 2.1 Suppose the assumptions in Zhu et al. (2010a) hold and N/n \u2192\u03bb. Then\nthe following hold.\n(1). Under H0, P(\u02c6q = 1) \u21921, and B is a vector proportional to \u03b2. Moreover,\n\u02c6B(\u02c6q) \u2212B\n=\nOp(1/\u221an),\n0 < \u03bb \u2264\u221e,\n(2.9)\n=\nOp(1/\n\u221a\nN),\n\u03bb = 0.\n(2). Under H1, P(\u02c6q = q) \u21921, B is a p \u00d7 q orthonormal matrix and \u02c6B(\u02c6q) satis\ufb01es (2.9).\nThere are various estimators of \u03b2 for EIVs models available in the literature. Here we\nshall focus on the estimators proposed by Lee and Sepanski (1995) for linear and nonlinear\nEIVs regression models. Their estimator under the null hypothesis is\n\u02c6\u03b2 = arg min\n\u03b2 (Y \u2212D(D\u22a4\nv Dv)\u22121Dvg(Xv\u03b2))\u22a4(Y \u2212D(D\u22a4\nv Dv)\u22121Dvg(Xv\u03b2))\nwhere Xv is the N \u00d7 p matrix whose sth row is \u02dcxT\ns , s = 1, \u00b7 \u00b7 \u00b7 , N, Y is a n \u00d7 1 vector,\nand g(Xv\u03b2) represents N \u00d7 1 vector [g(\u03b2\u22a4\u02dcx1), \u00b7 \u00b7 \u00b7 , g(\u03b2\u22a4\u02dcxN)]\u22a4. The matrices D and Dv are\ndesign matrices according to g(\u00b7). More precisely, D is the n \u00d7 k matrix whose i-th row\ndenoted by \u00afw\n\u2032\ni, is a vector consisting of polynomials of wi, while Dv is the corresponding\nmatrix of validation data, whose s-th row \u00afws is a vector consisting of polynomials of \u02dcws.\nFor linear model, \u00afwi = wi and \u00afws = \u02dcws. For nonlinear model, we let \u00afwi( \u00afws) be the vector\n8\nconsisting of a constant and the \ufb01rst two order polynomials of wi( \u02dcws). Lee and Sepanski\n(1995) assume that lim\np\nn/N exists. They show that if this limit is non-negative and \ufb01nite\nthen \u02c6\u03b2 is root-n consistent for \u03b2, and if lim\np\nn/N = \u221e, then \u02c6\u03b2 is a root-N consistent for\n\u03b2. More precisely, we have the following proposition.\nProposition 2.2 Suppose the assumptions for Proposition 2.2 in Lee and Sepanski (1995)\nhold.\n(1). Suppose in addition H0 holds and N/n \u2192\u03bb. Then for 0 < \u03bb \u2264\u221e, \u221an(\u02c6\u03b2 \u2212\u03b2) = Op(1),\nwhile for \u03bb = 0,\n\u221a\nN(\u02c6\u03b2 \u2212\u03b2) = Op(1).\n(2). In addition, suppose the following sequence of local alternatives holds, where Cn \u21920.\nH1n : \u00b5(x) = g(\u03b2\u22a4x) + CnG(x).\nThen\n\u02c6\u03b2 \u2212\u03b20\n=\nCn\n\b\nE[g\u2032(\u03b2\u22a4X)X \u00afW \u22a4]E\u22121[ \u00afW \u00afW \u22a4]E[g\u2032(\u03b2\u22a4X) \u00afWX\u22a4]\n\t\u22121\n\u00d7E[g\u2032(\u03b2\u22a4X)X \u00afW \u22a4]E\u22121[ \u00afW \u00afW \u22a4]E[ \u00afWG(X)](1 + op(1))\n+Op(1/\u221an) + Op(1/\n\u221a\nN).\nwhere \u00afW is a vector consist of polynomials of W and g\u2032(t) is the derivative of g(t) with\nrespect to t.\n3\nAsymptotic distributions\n3.1\nLimiting null distribution\nIn this section, we will establish the asymptotic null distribution of the proposed test statistics\n\u02dcVn in (2.5) and Vn in (2.7). De\ufb01ne\nZ = B\u22a4W,\n\u03c32(Z) = E[e2|Z],\n\u2206(Z) = E[G(B\u22a4X)|Z],\n(3.1)\n\u03b7 = g(\u03b2\u22a4X) \u2212r(\u03b2\u22a4W, \u03b2),\n\u03be2(Z) = E[\u03b72|Z],\nwhere e is de\ufb01ned in (2.4). Write Z as \u02dcZ, when W is replaced by validation data \u02dcW.\nTo proceed further we now state the assumptions needed here.\nAssumptions:\n(f). The support C of Z is a compact subset of the support of \u02dcZ and bounded away from\nthe boundary of the support of \u02dcZ. The density f of Z has bounded partial derivatives up\nto order \u2113\u22651 and satis\ufb01es\n0 < inf\nz\u2208C f(z) \u2264sup\nz\u2208C\nf(z) < \u221e.\n9\n(g). g(\u03b2\u22a4x) is a measurable function of x for each \u03b2 and is di\ufb00erentiable in \u03b2 up to order\n\u2113+ 1, and E\n\r\r \u2202g(\u03b2\u22a4\n0 X)\n\u2202\u03b2\n\r\r2 < \u221e.\n(r). The function r(\u03b2\u22a4w, \u03b2) has bounded partial derivatives with respect to \u03b2Tw up to order\n\u2113+ 1, and E[r2(\u03b2\u22a4W, \u03b2)] < \u221e, \u03b2 \u2208Rp.\n(G). E[\u22062(Z)] < \u221e, E[(G(B\u22a4X)\u2212\u2206(Z))4] < \u221e, and \u2206(z) has bounded partial derivatives\nup to order \u2113.\n(W). max1\u2264k\u2264p E[W 2\n(k)|Z] < \u221e, W(k) represents the k-th coordinate of W, k = 1, \u00b7 \u00b7 \u00b7 , p.\n(e). E[(\u03c32(Z))2] < \u221e, E[(\u03be2(Z))2] < \u221e, and \u03c32(z) and \u03be2(z) are uniformly continuous\nfunctions.\n(K). K is a spherically symmetric and continuous kernel function with bounded support and\nof order \u2113, having all derivatives bounded.\n(M). M is a symmetric and continuous kernel function with bounded support and of order\n\u2113, having all derivatives bounded.\n(h1). h \u21920, vN \u21920, vN/h \u21920.\n(h2). h \u21920, vN \u21920, h4/v5\nN \u21920.\n(h3). nh2 \u2192\u221e, Nv2\nN \u2192\u221e, nv2\u2113\nN \u21920 and nhvN/N \u21920.\n(h4). nh5/2 \u2192\u221e, Nv2\nN \u2192\u221e, nv2\u2113\nN \u21920 and nhvN/N \u21920.\n(h5). nh \u2192\u221e, Nh2 \u2192\u221e, Nv1/2\nN /(nh1/2) \u21920 and Nv1/2+2\u2113\nN\n\u21920.\n(h6). nhq \u2192\u221e, NvN \u2192\u221e.\nThe positive integer \u2113in all of the above assumptions is the same as in the assumption\n(f). For the consistency of \u02c6\u03b2 and \u02c6B(\u02c6q), some additional conditions are also needed. The\nreader can refer to Lee and Sepanski (1995) and Zhu et al. (2010a) for more details.\nRemark 3.1 Conditions (g), (r), (W), (e) are very common for the asymptotic normality\nof the proposed test statistics. The lower bound assumption on f is typically designed for\nthe nonparametric estimation of the corresponding regression function r(\u03b2\u22a4W, \u03b2) and the\nconditional mean E[e|Z]. This is a commonly used condition. In assumption (h6), nhq \u2192\u221e\nis to ensure the consistency in quadratic mean of kernel density estimator under some global\nalternative. If vN/h \u21920, some convolution of kernel functions can be approximated by\nkernel function. If N/n \u2192\u221eor a \ufb01nite constant, this condition is easily satis\ufb01ed. We choose\nvN = O((N/2)\u22122/5) in the simulation studies later. But when N/n \u21920, the condition is\nchanged to h/vN \u21920.\nTo proceed further, we need some more notation as follows:\nzi = B\u22a4wi,\ngi = g(\u03b2\u22a4xi),\nri = r(\u03b2\u22a4wi, \u03b2),\n\u03b7i = gi \u2212ri.\n(3.2)\n10\nWrite \u02dczs, \u02dcgs, \u02dcrs and \u02dc\u03b7s for the entities in (3.2) when wi is replaced by validation data \u02dcws in\nthere. When \u03b2 and B are respectively replaced by their estimators \u02c6\u03b2 and \u02c6B(\u02c6q) in the above\nde\ufb01nitions, write the respective \u02c6zi, \u02c6gi, \u02c6ri and \u02c6\u03b7i for zi, gi, ri and \u03b7i, and similarly write the\nrespective \u02c6\u02dczs, \u02c6\u02dcgi, \u02c6\u02dcri and \u02c6\u02dc\u03b7i for \u02dczi, \u02dcgi, \u02dcri and \u02dc\u03b7i.\nTo state the next theorem we need to de\ufb01ne\n\u00b5 = K(0)E[\u03be2(z)]/(Nh),\n\u03c41 = 2\nZ\nK2(u)du\nZ\n(\u03c32(z))2f 2(z)dz,\n(3.3)\n\u03c42 =\nZ\nK2(u)du\nZ\n\u03c32(z)\u03be2(z)f 2(z)dz,\n\u03c43 = 2\nZ\nK2(u)du\nZ\n(\u03be2(z))2f 2(z)dz.\nwhere \u03c32(\u00b7) and \u03be2(\u00b7) are de\ufb01ned in (3.1) and f is the density of Z = B\u22a4W. Consistent\nestimates of \u03a3i, i = 1, 2, 3 under H0 are given by\n\u02c6\u03c41 =\n2\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\n1\nh\u02c6q K2( \u02c6zi \u2212\u02c6zj\nh\n)\u02c6e2\ni \u02c6e2\nj,\n\u02c6\u03c42 =\n1\nnN\nn\nX\ni=1\nN\nX\ns=1\n1\nh\u02c6q K2( \u02c6zi \u2212\u02c6\u02dczs\nh\n)\u02c6e2\ni \u02c6\u02dc\u03b72\ns(3.4)\n\u02c6\u03c43 =\n2\nN(N \u22121)\nN\nX\ns=1\nN\nX\ns\u2032\u0338=s\n1\nh\u02c6q K2(\n\u02c6\u02dczs \u2212\u02c6\u02dczs\u2032\nh\n)\u02c6\u02dc\u03b72\ns \u02c6\u02dc\u03b72\ns\u2032.\nWe are now ready to state\nTheorem 3.1 Suppose H0 and the conditions (f), (g), (r), (W), (e), (K), (M), (h1) and\n(h3) hold, and that N/n \u2192\u03bb, 0 < \u03bb \u2264\u221e. Then nh1/2\u0000\u02dcVn \u2212\u00b5\n\u0001\n\u2192D N(0, \u02dc\u03c4), where\n\u02dc\u03c4\n=\n\u03c41 + 2\n\u03bb\u03c42 + 1\n\u03bb2\u03c43,\n0 < \u03bb < \u221e,\n=\n\u03c41,\n\u03bb = \u221e.\nHere, consistent estimators of \u00b5 and \u03c4 under H0 are given by\n\u02c6\u00b5 =\n1\nN 2hK(0)\nN\nX\ns=1\n\u02c6\u02dc\u03b72\ns,\n\u02c6\u02dc\u03c4 = \u02c6\u03c41 + 2\n\u03bb \u02c6\u03c42 + 1\n\u03bb2 \u02c6\u03c43,\n0 < \u03bb < \u221e,\nwith \u02c6\u03c4i\u2019s as in (3.4). The \u02dcVn test rejects H0 whenever \u02dcVn > \u02c6\u02dc\u03c4 1/2(nh1/2)\u22121z\u03b1 + \u02c6\u00b5, where z\u03b1 is\nthe upper 100(1 \u2212\u03b1)% quantile of the standard normal distribution.\nThe above theorem shows that the asymptotic variance of \u02dcVn consists of the three parts\nwhen 0 < \u03bb < \u221e. The part \u03c41 re\ufb02ects the variation in the regression model, \u03c43 is the variation\ncaused by the measurement error while the part \u03c42 is the intersection of the variation due to\nthe regression model and measurement error.\nThe next result gives the asymptotic null distribution of the Vn statistic of (2.7). As can\nbe seen from this result, Vn does not have any asymptotic bias.\n11\nTheorem 3.2 Under the conditions of Theorem 3.1, nh1/2Vn \u2192D N(0, \u03c4), where\n\u03c4\n=\n\u03c41 + 4\n\u03bb\u03c42 + 2\n\u03bb2\u03c43,\n0 < \u03bb < \u221e,\n=\n\u03c41,\n\u03bb = \u221e,\nwhere \u03c4i, i = 1, 2, 3, are as in (3.3).\nTo studentize Vn, we use the following consistent estimate of \u03c4 in the case 0 < \u03bb < \u221e.\n\u02c6\u03c4 =\n2\nn(n\u22121)\nPn\ni=1\nPn\nj\u0338=i\n1\nh\u02c6q K2( \u02c6zi\u2212\u02c6zj\nh )\u02c6e2\ni(1)\u02c6e2\nj(2) +\n4\n\u03bbnN\nPn\ni=1\nPN\ns=N/2+1\n1\nh\u02c6q K2( \u02c6zi\u2212\u02c6\u02dczs\nh\n)\u02c6e2\ni(1)\u02c6\u02dc\u03b72\ns\n+\n4\n\u03bbnN\nPn\ni=1\nPN/2\nt=1\n1\nh\u02c6q K2( \u02c6zi\u2212\u02c6\u02dczt\nh )\u02c6e2\ni(2)\u02c6\u02dc\u03b72\nt +\n16\n\u03bb2N2\nPN/2\nt=1\nPN\ns=N/2+1\n1\nh\u02c6q K2(\n\u02c6\u02dczs\u2212\u02c6\u02dczt\nh\n)\u02c6\u02dc\u03b72\ns \u02c6\u02dc\u03b72\nt ,\nwhere s and t are indices of the two sets of validation data respectively, \u02c6\u03b7t or \u02c6\u03b7s is estimated\nby the other half of validation data. That is, \u02c6\u02dc\u03b7t = g(\u02c6\u03b2\u22a4\u02dcxt) \u2212\u02c6r(2)(\u02c6\u03b2\u22a4\u02dcwt, \u02c6\u03b2), t = 1, \u00b7 \u00b7 \u00b7 , N/2\nand \u02c6\u02dc\u03b7s = g(\u02c6\u03b2\u22a4\u02dcxs) \u2212\u02c6r(1)(\u02c6\u03b2\u22a4\u02dcws, \u02c6\u03b2), s = N/2 + 1, \u00b7 \u00b7 \u00b7 , N, where \u02c6r(1) and \u02c6r(2) are de\ufb01ned in\n(2.6). The standardized test statistic is\nTn\n=\n\u02c6\u03c4 \u22121/2nh1/2Vn,\n0 < \u03bb < \u221e,\n=\n\u02c6\u03c4 \u22121/2\n1\nnh1/2Vn,\n\u03bb = \u221e,\nwhere \u02c6\u03c41 is as in (3.4). According to the Slusky theorem, Tn is asymptotically standard\nnormal. At the signi\ufb01cance level \u03b1, the null hypothesis is rejected when Tn > z\u03b1. For large\n\u03bb, the terms about \u03c42 and \u03c43 vanish in the asymptotic variance, and thus, the estimated\nvariance \u02c6\u03c4 is replaced by \u02c6\u03c41.\nRemark 3.2 A signi\ufb01cant feature of this test is that we only need to use the standardizing\nsequence nh1/2, which is the same as the one used in the classical local smoothing tests when\nX is one-dimensional. This shows that the test statistic has a much faster convergence rate\nto its limit compared to some of the classical tests that have the rate of order nhp/2. This\ngreatly assists in maintaining the signi\ufb01cance level of this test in \ufb01nite samples when its\nasymptotic null distribution is used to determine the critical values for its implementation.\nWhen N/n \u2192\u03bb = 0, the standardizing constant will be di\ufb00erent because of the plug-in\nestimate \u02c6r(\u00b7) of the function r(\u00b7), as is evidenced by the following theorem.\nTheorem 3.3 Suppose H0 and the above conditions (f), (g), (r), (W), (e), (K), (M), (h2),\n(h5) hold and that N/n \u21920. Then Nv1/2\nN {\u02dcVn \u2212\u03bd} \u2192D N(0, \u02dc\u03c4),\nNv1/2\nN Vn \u2192D N(0, \u03c4),\nwhere \u03bd = \u2225\u03b2\u2225(vNN)\u22121 R\nM 2(u)du E[\u03be2(Z)],\n\u03c4 := 2\u02dc\u03c4, and\n\u02dc\u03c4 = 2\u2225\u03b2\u2225\nZ \u0010 Z\nM(u)M(u + v)du\n\u00112\ndv\nZ\n(\u03be2(z))2f 2(z)dz.\n12\n3.2\nAsymptotic Power\nIn this section, we assume N/n \u2192\u03bb, \u03bb a positive constant and investigate the asymp-\ntotic properties of the test statistic Vn under global and local alternatives. This is because\nthe asymptotic properties can be much more easily derived than those for \u02dcVn. Consider a\nsequence of alternatives\nH1n : \u00b5(x) = g(\u03b2\u22a4x) + CnG(B\u22a4x),\nx \u2208Rp,\n(3.5)\nwhere G(\u00b7) satis\ufb01es E(G2(B\u22a4X)) < \u221eand \u03b2 is a column of B. When Cn is a \ufb01xed constant,\nthe alternative is a global alternative and when Cn = n\u22121/2h\u22121/4 tends to zero, H1n specify\nthe local alternatives of interest here. Note that the asymptotic properties of the estimates\n\u02c6B(\u02c6q) and \u02c6\u03b2 will a\ufb00ect the behavior of the test statistic Vn. The asymptotic results of \u02c6\u03b2 have\nbeen illustrated in Proposition 2.2. Thus, we discuss the result about the consistency of \u02c6q\nhere. Under the local alternatives, it is no longer consistent for the dimension q.\nTheorem 3.4 Suppose the conditions in Zhu et. al (2010a) hold. Under H1n of (3.5) with\nCn = n\u22121/2h\u22121/4 \u21920, P(\u02c6q = 1) \u21921.\nHowever, this inconsistency does not hurt the power performance of the test. We will see\nbelow in a \ufb01nite sample simulation study that the test can be much more powerful than the\nclassical local smoothing tests in the literature.\nTheorem 3.5 Under the alternatives of (3.5), the following results are hold:\n(i)Suppose (f), (g), (r), (G), (W), (e), (K), (M), (h1) and (h6) hold. Under the global\nalternative with \ufb01xed Cn,\nVn/\u02c6\u03c4 \u2192V > 0.\n(3.6)\n(ii) Suppose (f), (g), (r), (G), (W), (e), (K), (M), (h1) and (h4) hold. Then, under the\nlocal alternatives H1n with Cn = n\u22121/2h\u22121/4, nh1/2Vn \u2192D N(\u2206, \u03c4), where \u03c4 is given in\nTheorem 3.2 and \u2206= E\n\u0002\n{\u2206(Z) \u2212E[g\u2032(\u03b2\u22a4\n0 X)X\u22a4|Z]H(\u03b20)}2f(Z)\n\u0003\n.\nRemark 3.3 The result (3.6) implies the consistency of the Tn test gainst the class of the\nabove \ufb01xed alternative. It also implies that under the global alternatives, the test statistic\ncan diverge to in\ufb01nity at a much faster rate than the existing local smoothing tests in the\nliterature can achieve such as Zheng\u2019s test (1996), which has the consistency rate of the order\n1/(nhp/2). The test can also detect the local alternatives distinct from the null at the rate\nof order 1/\n\u221a\nnh1/2 while the classical ones can only detect those alternatives converging to\nthe null at the rate of order 1/\n\u221a\nnhp/2.\n13\n4\nNumerical studies\nThis section presents four simulation studies to examine the performance of the proposed\ntest (Tn). To compare with existing tests, we consider Zheng\u2019s (1996) test (T Zh\nn ) adapted to\nthe errors-in-variables settings and Song\u2019s (2009) test (T S\nn ) as the competitors. The adapted\nZheng\u2019s test is the same as our test except that B\u22a4W is replaced by the original W. This is a\ntypical local smoothing test. Song\u2019s test is a score type test and is designed for EIVs models\nwith validation data. Consider the linear regression models under the null hypothesis. In\nthe simulation study 1 below, the matrix B is equal to \u03b2 and thus, the model is a parametric\nsingle index. The dimension of X is respectively p = 2 and 8. Note that our test fully\nuses the information under the null hypothesis that only relates to a single index \u03b2. In\naddition, we run simulation studies of the test \u02dcTn based on the statistic \u02dcVn of Theorem 3.1\nwhen 0 < \u03bb < \u221e, and illustrate its weakness. The purpose of Study 2 is to con\ufb01rm that\nthe proposed test Tn is not a directional test by assuming B = (\u03b21, \u03b22) with q = 2 under the\nalternative hypothesis. Study 3 is designed to examine the \ufb01nite sample performance when\nN < n and N > n. Study 4 considers four nonlinear models. All simulations are based on\n2000 replications.\nRecall that the tests Tn and T Zh\nn\nare based on the estimates of the quantities that are zero\nunder the null and positive under the alternative. Because of the asymptotic normality, the\nrejection regions of \u02dcVn, Tn and T Zh\nn\nare one-sided: {\u02dcVn > \u02c6\u02dc\u03c4 1/2(nh1/2)\u221211.65+ \u02c6\u00b5}, {Tn > 1.65}\nand {T Zh\nn\n> 1.65} at the 0.05 level of signi\ufb01cance. The reported size and power are computed\nby #{Tn > 1.65}/2000. For T S\nn , the rejection region is two sided and the reported size and\npower are computed by #{|T S\nn | > 1.96}/2000. Throughout the simulation studies, X is\ntaken to be multivariate normal with mean zero and covariance matrices \u03a31 = Ip\u00d7p and\n\u03a32 = (0.3|i\u2212j|)p\u00d7p. The regression model error \u03b5 follows standard normal distribution, while\nthe measurement error U \u223cN(0, 0.5). The kernel function is K(u) = 15\n16(1 \u2212u2)2I(|u| \u22641)\nwhich is a second-order symmetric kernel and M(u) = K(u).\nBandwidth selection.\nAs the tests involve bandwidth selection in the kernel estimation,\nwe run a simulation to empirically select the bandwidths for the three tests in the comparison.\nBecause the signi\ufb01cance level maintainance is important, we then select bandwidths such\nthat the tests can have empirical sizes close to the signi\ufb01cance level and retain the use under\nother models. To this end, we use a simple model to select them and to check whether they\ncan be used in general. In our test, there are two bandwidths. As is well known, the optimal\nbandwidth in hypothesis testing is still an outstanding problem, but the optimal rate of the\nbandwidth in kernel estimation is n\u22121/(4+q) where n is the sample size. We then adopt its\nrate with a search for the constant c1 in h = c1n\u22121/(4+\u02c6q). Similarly, for the kernel estimator of\nthe function r(\u03b2\u22a4W, \u03b2), we choose the window width vN = c2(N/2)\u22122/5, because we halved\nthe validation data set of size N. For \u02dcTn, vN is c2N \u22122/5. To select proper bandwidths, we\ntried di\ufb00erent bandwidths to investigate their impact on the empirical size. To reduce the\n14\ncomputational burden, we consider c1 = c2 = c to see whether such selections can o\ufb00er\nbandwidths for general use. The selection is based on hypothetical models as the primary\ntarget is to maintain the signi\ufb01cance level. Thus, we compute the empirical size at every\nequal gird point c = (i \u22121)/10 for i = 1, \u00b7 \u00b7 \u00b7 21. In Figure 1, we report the empirical sizes\nassociated with di\ufb00erent bandwidths when the regression model is \u00b5(x) = \u03b2\u22a4x and p = 2, 8,\nn = 100, 200, N = 4 \u00d7 n, and the covariance matrix of X is \u03a31. We can see that the test is\nnot very sensitive to the bandwidth and a value of c = 1.6 may be a good choice for both Tn\nand \u02dcTn. For the adapted Zheng\u2019s test, there are also two bandwidths to be selected. As the\noptimal rate for the kernel estimation is h = c1n\u22121/(4+p), we then also consider c1 = c2 = c.\nWe found that to maintain the signi\ufb01cance level, the bandwidths must be with larger c.\nThe initial selection provides us an idea to choose a good bandwidth within the equal grid\npoints as c = 2.5 + (i \u22121)/10 for i = 1, \u00b7 \u00b7 \u00b7 21. The results are also reported in Figure 1.\nAs for Song\u2019s score test, only one bandwidth is required. We also found a larger bandwidth\nis required. Set the bandwidth as vN = cN \u22121/(4+p) and search for the proper c within the\nequal grid points as c = 1 + (i \u22121)/10 for i = 1, \u00b7 \u00b7 \u00b7 21. The reported curves are in Figure 1.\nFigure 1. about here\nWe can see that the empirical sizes of Tn are not sensitively a\ufb00ected by the bandwidths\nselected. The curves of empirical size under p = 2 and p = 8 are almost coincident. While\nthe empirical size of \u02dcTn is slightly e\ufb00ected by dimensionality, but it is still more robust than\nthat of T Zh\nn\nand T S\nn . A value of c = 1.6 is worthy of recommendation for both, Tn and \u02dcTn.\nHowever, the empirical sizes of T Zh\nn\nand T S\nn associated with the bandwidths are not as robust\nas that of Tn. The empirical sizes show the e\ufb03cient bandwidth changes as p increase. When\np is small, a small h can keep the theoretical size. As p increase, a larger h is necessary.\nThis phenomenon is particularly serious for T Zh\nn . For the bandwidths of T Zh\nn , c = 3.9 is\nappropriate. Finally, c = 2.2 seems to be proper for T S\nn .\nStudy 1. The data are generated from the following model:\nH11 : \u00b5(x) = \u03b2\u22a4x + a (\u03b2\u22a4x)2,\nH12 : \u00b5(x) = \u03b2\u22a4x + a exp(\u2212(\u03b2\u22a4x)2/2),\nH13 : \u00b5(x) = \u03b2\u22a4x + 2a cos(0.6\u03c0\u03b2\u22a4x).\nThe case of a = 0 corresponds to the null hypothesis and a \u0338= 0 to the alternatives. In other\nwords, both the hypothetical and alternative models have a single index B = c\u03b2. Models\nunder H11 and H12 represent low frequency alternatives while H13 is an example of high\nfrequency alternative. In H11 and H12, the alternative parts (\u03b2\u22a4x)2 and exp(\u2212(\u03b2\u22a4x)2/2\nalways exist for any nonzero a. While for H13, the alternative part cos(0.6\u03c0\u03b2\u22a4x) appears\nand disappears periodically for a \u0338= 0, which makes the bandwidth selection process even\nmore challenging. Because a large bandwidth selected to maintain signi\ufb01cance level may\n15\nmake the test obtuse to high frequency alternatives. The dimension p equals 2 and 8 such\nthat we can check the impact from the dimensionality. Let \u03b2 = (1, 1, \u00b7 \u00b7 \u00b7 , 1)\u22a4/\u221ap. The\nnumber of validation data is N = 4n. The simulation results are presented in Tables 1, 2\nand 3.\nTables 1-3 about here\nFrom these tables we see that when p = 2, T S\nn performs very well. This is expected\nwhen the dimension is low or moderate, because the consistency rate of this test is 1/\u221an.\nAlso, when p is small, T Zh\nn\nis comparable to Tn as both are local smoothing tests. When\nthe dimension increases, T Zh\nn\nand T S\nn are however severely impacted by the dimensionality.\nThe test T Zh\nn\nbehaves much worse. Especially, when p = 8, it breaks down for n = 100 and\nregains its power as n increase. The test T S\nn is also a\ufb00ected by the dimensionality because\nthe residuals contain nonparametric estimation by local smoothing technique. Its powers\ndecrease both for small and large sample size. On the other hand, the dimension-reduction\nadaptive-to-model test Tn does not su\ufb00er from the curse of dimensionality in the limited\nsimulation studies presented here. When p is large, Tn performs better than T S\nn . The \ufb01nite\nsample power of the T S\nn test is poor against the alternatives H13 for both the cases p = 2 and\np = 8. This may be due to the fact that T S\nn is a directional test. We illustrate this problem\nin the next study.\nThe comparison between Tn and \u02dcTn is another purpose of this study. We \ufb01nd that the\nempirical power of \u02dcTn is slightly higher than that of Tn, but the size of \u02dcTn also tends to be\nslightly larger, even when n = 200 and p = 2. Although \u02dcTn has bias, but each residual in \u02dcTn\nis estimated by all validation data which is more precise with smaller variance than that of\nTn derived by half validation dat. We can then conclude, based on this limited simulation,\nthe test \u02dcTn is slightly more liberal than the bias-corrected test Tn, but also slightly more\npowerful. These two tests are competitive. Therefore, in the following simulation studies,\nwe only report the results about Tn to save space.\nStudy 2. In this study, we aim to design a simulation study to check that the dimension-\nreduction model-adaptive test Tn is not a directional test, while Song\u2019s test T S\nn is. The data\nare generated from the following model:\nH14 : \u00b5(x) = \u03b2\u22a4\n1 x + a(\u03b2\u22a4\n2 x)2,\nH15 : \u00b5(x) = 2\u03b2\u22a4\n1 x + a(2\u03b2\u22a4\n2 x)3.\nHere also, a = 0 corresponds to the null hypothesis and a \u0338= 0 to the alternatives. The matrix\nB = (\u03b21, \u03b22) and then the structural dimension q under the alternative is 2. Let p = 4,\n\u03b21 = (1, 1, 0, 0)\u22a4/2 and \u03b22 = (0, 0, 1, 1)\u22a4/2. The number of validation data is N = 4 \u00d7 n.\nThe simulation results are presented in Table 4. From these results, we \ufb01rst observe that\nT S\nn has good performance under H14, which coincides with that in Study 1. However, the\npoor performance under H15 shows that T S\nn is a directional test as this alternative cannot be\ndetected by it at all. At population level, we can see that the conditional expectation of the\n16\nresidual is equal to zero under this alternative. In this case, Tn still works well. This lends\nsupport to the claim that Tn is an omnibus test.\nTables 4 about here\nStudy 3. In this study, we aim to explore the impact of the estimation of r(\u00b7) on the\nperformance of the proposed tests. Small \u03bb = lim(N/n) means that there are not many\nvalidation data available and large \u03bb means the estimator \u02c6r(\u00b7) is very close to the true\nfunction r(\u00b7). For this purpose, consider N/n = 0.1, 0.5, 4, 8. We only choose these ratios\nbecause if \u03bb is either too small or too large, we need to have too large sample size or too large\nsize of validation data. These are practically not possible. From Theorem 3.3, we know that\nwhen \u03bb is small, we can have a test with simpler limiting variance. Write the related test as\nT (1)\nn . From Theorem 3.2, \u03bb = \u221ecase, we can also have a test for large N/n. Write it as T (2)\nn .\nTo examine whether these two variants of the test Tn work or not, we generate data from\nthe model H11 in Study 1. When the size of validation data is such that N/n = 0.1, 0.5,\nT (1)\nn\nis used, and when N/n = 4, 8, T (2)\nn\nis applied. As T (1)\nn\nis a test with very di\ufb00erent\nconvergence rate, we then also need to choose bandwidths suitable for it. Similarly as the\nabove, we also search for the bandwidths at the rates vN = c1(N/2)\u22121/3 and h = c2n\u22121/(2+\u02c6q).\nLet c1 = c2 = c. We found that c = 2 is a good choice. For T (2)\nn , only the asymptotic\nvariance changes, we then still use the same bandwidths as before. When \u03bb = 0.1, 0.5, we\nthen use larger sample size of validation data N = 100, 200, otherwise, N is too small to\nmake the tests well performed. The simulation results are presented in Table 5.\nTable 5 about here\nFrom Table 5, we have the following two observations. First, for \u03bb = 0.1, Tn is more\nconservative with lower power than T (1)\nn .\nThis seems to say, Tn is less sensitive to the\nalternative model than T (1)\nn . This phenomenon would come from the improper selection of\nbandwidths for Tn because Conditions (h1) and (h2) assure that the consistency of Tn and\nT (1)\nn\nrequire di\ufb00erent ratios of h and vN. Thus, when N/n is very small, T (1)\nn\nseems to be\na better choice than Tn. But when \u03bb is closed to 1, T (1)\nn\ncannot maintain the signi\ufb01cance\nlevel well. Secondly, T (2)\nn\nhas very slightly higher empirical size and power than Tn. Overall,\nthe performances of T (2)\nn\nis very similar to that of Tn. Therefore, when the size of validation\ndata N is reasonably large, and the ratio N/n is large, T (2)\nn\nwould be applicable. Also, from\nthe simulations we see that although T (1)\nn\ncan be used, it does not maintain the \ufb01nite sample\nsigni\ufb01cance level as well as the Tn test does. Thus, when the ratio N/n is not too small, we\nrecommend the test Tn, rather than T (1)\nn , for practical use.\nStudy 4. In this study, a nonlinear single-index null model is considered. We try four\nalternatives with di\ufb00erent structural dimension as follows:\nH16 : Y = (\u03b2\u22a4X)3 + a|\u03b2\u22a4X| + \u03f5\n17\nH17 : Y = (\u03b2\u22a4X)3 + aX2\n3 + \u03f5\nH18 : Y = (\u03b2\u22a4X)3 + a(X2/4 + |X2\n3| + cos(\u03c0X4)) + \u03f5\nH19 : Y = (\u03b2\u22a4X)3 + a(X2/2 + X2\n3 + cos(\u03c0X4) + X5 exp(X6/2) + X8X7) + \u03f5\nLet p = 4 for H16, H17, H18 and p = 8 for H19. \u03b2 = [1, 0, \u00b7 \u00b7 \u00b7 , 0]\u22a4. \u03a3 = \u03a31, \u03c3u = 0.5. a is\ndesigned to be 0, 0.2, 0.4, 0.6, 0.8, 1.0. In these cases, q is always 1 for the null but di\ufb00erent\nfor alternatives. For H16, q = 1 for any nonzero a. The structure dimension under H17 is 2,\nand under H18, p = q = 4. For H19, p = q = 8. The test Tn uses the same bandwidths as\nchosen for linear model above. For T Zh\nn , we adjust bandwidths to keep its performance. Set\nc = 2.7 for H16, H17, H18 and c = 3 for H19. The results are presented in Figure 2.\nFigure 2. about here\nWe have the following observations. First, the model-adaptive method Tn has greater\nempirical power than T Zh\nn\nfor all chosen alternatives. Under H18 and H19, though convergence\nrate of the two teats are same, Tn is still more powerful than T Zh\nn . Because Tn is constructed\nby nh1/2Vn/\n\u221a\n\u03a3 = h(1\u2212q/2) \u00d7 nhq/2Vn/\n\u221a\n\u03a3. Secondly, the power of T Zh\nn\ndecreases quickly as\np increases while that of Tn does not.\n18\n5\nAppendix. Proofs\nThis section is organized as follows. In Section 5.1, Proposition 2.2 is proved. The proof of\nTheorem 3.4 appears in Section 5.2. Based on the asymptotic behavior of \u02c6\u03b2 and \u02c6B under\nthe local alternatives, the proof of Theorem 3.5 is included in Section 5.3. As Theorem 3.2\nis a special case of Theorem 3.5 when Cn = 0, its proof is omitted. In Section 5.4, we only\nsketch the proof of Theorem 3.1 as it is similar to that of Theorem 3.5. Section 5.5 shows a\nsketch of the proof for Theorem 3.3.\n5.1\nProof of Proposition 2.2\nThe claim (1) has been proved in Lee and Sepanski (1995). We now prove the claim (2).\nRecall some notation: X is n \u00d7 p matrix whose ith row is x\u22a4\ni , i = 1, \u00b7 \u00b7 \u00b7 , n, Xv is the\nN \u00d7 p matrix whose sth row is \u02dcx\u22a4\ns , s = 1, \u00b7 \u00b7 \u00b7 , N, and Y is a n \u00d7 1 vector, while g(Xv\u03b2)\nrepresents the N \u00d7 1 vector and equals to [g(\u03b2\u22a4\u02dcx1), \u00b7 \u00b7 \u00b7 , g(\u03b2\u22a4\u02dcxN)]\u22a4. The matrix D is the\nn \u00d7 k matrix whose i-th row \u00afw\u22a4\ni is a 1 \u00d7 k vector consist of polynomials of wi. The matrix\nDv is the corresponding matrix of validation data, whose s-th row \u00afw\u22a4\ns is a vector consist of\npolynomials of \u02dcws. For linear model, \u00afwi = wi and \u00afws = \u02dcws. For nonlinear model, we let \u00afwi\nbe a vector consisting of a constant and the \ufb01rst two order polynomials of wi.\nLet\nQn(\u03b2) = 1\nn\n\u0010\nY \u2212D(D\u22a4\nv Dv)\u22121D\u22a4\nv g(Xv\u03b2)\n\u0011\u22a4\u0010\nY \u2212D(D\u22a4\nv Dv)\u22121D\u22a4\nv g(Xv\u03b2)\n\u0011\n.\nThe estimator \u02c6\u03b2 satis\ufb01es the \ufb01rst order condition: \u2202Qn(\u02c6\u03b2)/\u2202\u03b2 = 0. By Taylor expansion\nand the mean value theorem:\nh\u2202g\u22a4(Xv\u03b20)\n\u2202\u03b2\nDv\ni\n(D\u22a4\nv Dv)\u22121D\u22a4(Y \u2212D(D\u22a4\nv Dv)\u22121D\u22a4\nv g(Xv\u03b20))\n=\nnh\u22022g\u22a4(Xv \u00af\u03b2)\n\u2202\u03b2\u2202\u03b2\u22a4\nDv\ni\n(D\u22a4\nv Dv)\u22121D\u22a4(Y \u2212D(D\u22a4\nv Dv)\u22121D\u22a4\nv g(Xv \u00af\u03b2))\n\u2212\nh\u2202g\u22a4(Xv \u00af\u03b2)\n\u2202\u03b2\nDv\ni\n(D\u22a4\nv Dv)\u22121(D\u22a4D)(D\u22a4\nv Dv)\u22121[\u2202g\u22a4(Xv \u00af\u03b2)\n\u2202\u03b2\nDv]\no\n(\u03b20 \u2212\u02c6\u03b2)\nwhere \u00af\u03b2 is a vector satisfying \u2225\u00af\u03b2 \u2212\u03b2\u2225\u2264\u2225\u02c6\u03b2 \u2212\u03b20\u2225, and\n[\u22022g\u22a4(Xv \u00af\u03b2)\n\u2202\u03b2\u2202\u03b2\u22a4\nDv] = [\u22022g\u22a4(Xv \u00af\u03b2)\n\u2202\u03b2\u2202\u03b21\nDv, \u00b7 \u00b7 \u00b7 , \u22022g\u22a4(Xv \u00af\u03b2)\n\u2202\u03b2\u2202\u03b2p\nDv].\nLet g\u2032, g\u2032\u2032 denote the \ufb01rst and second derivatives of g, respectively. By the LLNs,\n1\nN\n\u2202g\u22a4(Xv\u03b2)\n\u2202\u03b2\nDv = 1\nN\nN\nX\ns=1\ng\u2032(\u03b2\u22a4\u02dcxs)\u02dcxs \u00afw\u22a4\ns \u2192p E[g\u2032(\u03b2\u22a4X)X \u00afW \u22a4],\n1\nN\n\u22022g\u22a4(Xv \u00af\u03b2)\n\u2202\u03b2\u2202\u03b2l\nDv \u2192p E[g\u2032\u2032(\u03b2\u22a4X)X(l)X \u00afW \u22a4],\n19\nand\n1\nnD\u22a4(Y \u2212D(D\u22a4\nv Dv)\u22121D\u22a4\nv g(Xv \u00af\u03b2))\n=\nCnE[ \u00afWG(X)] + (E[ \u00afWg(\u03b2\u22a4\n0 X)] \u2212E( \u00afW \u00afW \u22a4)\u03b30) + op(1)\n=\nop(1),\nwhere \u03b30 = E\u22121( \u00afW \u00afW \u22a4)E[ \u00afWg(\u03b2\u22a4\n0 X)]. Hence\n\u02c6\u03b2 \u2212\u03b20\n=\n\u001a\n[\u22022g\u22a4(Xv \u00af\u03b2)\n\u2202\u03b2\u2202\u03b2\u22a4\nDv](D\u22a4\nv Dv)\u22121D\u22a4(Y \u2212D(D\u22a4\nv Dv)\u22121D\u22a4\nv g(Xv \u00af\u03b2))\n\u2212[\u2202g\u22a4(Xv \u00af\u03b2)\n\u2202\u03b2\nDv](D\u22a4\nv Dv)\u22121(D\u22a4D)(D\u22a4\nv Dv)\u22121[\u2202g\u22a4(Xv \u00af\u03b2)\n\u2202\u03b2\nDv]\n\u001b\u22121\n\u00d7[\u2202g\u22a4(Xv\u03b20)\n\u2202\u03b2\nDv](D\u22a4\nv Dv)\u22121D\u22a4(Y \u2212D(D\u22a4\nv Dv)\u22121D\u22a4\nv g(Xv\u03b20))\n=\n\b\nE[g\u2032(\u03b2\u22a4X)X \u00afW \u22a4]E\u22121[ \u00afW \u00afW \u22a4]E[g\u2032(\u03b2\u22a4X) \u00afWX\u22a4] + Op(Cn)\n\t\u22121\n\u00d7\n\b\nE[g\u2032(\u03b2\u22a4X)X \u00afW \u22a4]E\u22121[ \u00afW \u00afW \u22a4]\n\t 1\nnD\u22a4(Y \u2212D(D\u22a4\nv Dv)\u22121D\u22a4\nv g(Xv\u03b2)).\nOn the other hand,\n1\nnD\u22a4(Y \u2212D(D\u22a4\nv Dv)\u22121D\u22a4\nv g(Xv\u03b2))\n=\n1\nnD\u22a4CnG(X) + 1\nnD\u22a4(g(X\u03b2) + \u03b5 \u2212D(D\u22a4\nv Dv)\u22121D\u22a4\nv g(Xv\u03b2))\n=\nCn\nn\nn\nX\ni=1\n\u02dcwiG(xi) + 1\nnD\u22a4(g(X\u03b2) + \u03b5 \u2212DE\u22121[ \u00afW \u00afW \u22a4]E[ \u00afW \u22a4g(\u03b2\u22a4X)])\n\u2212\n\u00121\nnD\u22a4D\n\u0013 \u0014 1\nN D\u22a4\nv Dv\n\u0015\u22121 1\nN (D\u22a4\nv g(Xv\u03b2) \u2212D\u22a4\nv DvE\u22121[ \u00afW \u22a4\u00afW]E[ \u00afW \u22a4g(\u03b2\u22a4X)])\n=\nCnE[ \u00afWG(x)] + Op(1/\u221an) + Op(1/\n\u221a\nN).\nThis completes the proof of part (2) of Proposition 2.2.\n5.2\nProof of Theorem 3.4\nDenote \u03b6 = Cov(X, W)\u03a3\u22121\nW W. In the discretization step, we construct new samples (\u03b6i, I(yi \u2264\nyj)). For each yj, we estimate \u039b(yj) which spans SI(Y \u2264yj)|\u03b6 by using SIR and denote the es-\ntimate by \u039bn(yj). In the expectation step, we estimate \u039b = E[\u039b(t)], which spans SY |\u03b6, by\n\u039bn,n = n\u22121 Pn\nj=1 \u039bn(yj). Let \u03bb1 > \u03bb2 > \u00b7 \u00b7 \u00b7 > \u03bbq > \u03bbq+1 = 0 = \u00b7 \u00b7 \u00b7 = \u03bbp be the descending\nsequence of eigenvalues of the matrix \u039b and \u02c6\u03bb1 \u2265\u02c6\u03bb2 \u2265\u00b7 \u00b7 \u00b7 \u2265\u02c6\u03bbp be the descending sequence\nof eigenvalues of the matrix \u039bn,n. Recall the Dn in \u02c6q of (2.8) was selected as \u221an. De\ufb01ne the\nobjective function in (2.8) as\nG(l) = n\n2 \u00d7\nPl\ni=1{log(\u02c6\u03bbi + 1) \u2212\u02c6\u03bbi}\nPp\ni=1{log(\u02c6\u03bbi + 1) \u2212\u02c6\u03bbi}\n\u22122 \u00d7 n1/2 \u00d7 l(l + 1)\n2p\n.\n20\nNow we prove that for any l > 1, P(G(1) > G(l)) \u21921, i.e., P(\u02c6q = 1) \u21921.\nG(1) \u2212G(l) = n1/2 \u00d7 l(l + 1) \u22122\np\n\u2212n\n2 \u00d7\nPl\ni=2{log(\u02c6\u03bbi + 1) \u2212\u02c6\u03bbi}\nPp\ni=1{log(\u02c6\u03bbi + 1) \u2212\u02c6\u03bbi}\nIf \u039bn,n \u2212\u039b = Op(Cn), then \u02c6\u03bbi \u2212\u03bbi = Op(Cn). By the second order Taylor Expansion,\nwe have log(\u02c6\u03bbi + 1) \u2212\u02c6\u03bbi = \u2212\u02c6\u03bb2\ni + op(\u02c6\u03bb2\ni ). Thus, Pl\ni=2{log(\u02c6\u03bbi + 1) \u2212\u02c6\u03bbi} = Op(C2\nn) and\nPp\ni=1{log(\u02c6\u03bbi + 1) \u2212\u02c6\u03bbi} converge to a negative constant in probability. Since nC2\nn/n1/2 \u21920\nand l(l + 1) > 2, P(G(1) > G(l)) \u21921.\nNow we check the condition of \u039bn,n \u2212\u039b = Op(Cn). First, we investigate the convergence\nrate of \u039bn(t) \u2212\u039b(t) for any \ufb01xed t. We have\n\u039b(t) = \u03a3\u22121\n\u03b6 Var(E[\u03b6|\u02dcY (t)])p(1 \u2212p) = \u03a3\u22121\nX \u03a3W\u03a3\u22121\nX Var(E[\u03b6|\u02dcY (t)])p(1 \u2212p).\nIt is easy to see that\nVar(E[\u03b6|\u02dcY (t)]) = (u1 \u2212u0)(u1 \u2212u0)\u22a4p(1 \u2212p)\nwhere p = P(Y \u2264t) = E(I(Y \u2264t)), ui = E[\u03b6|\u02dcY (t) = i], i = 0, 1. Further, u1 \u2212u0 can be\nrewritten as\nu1 \u2212u0 = {E[\u03b6I(Y \u2264t)] \u2212E[\u03b6]E[I(Y \u2264t)]} /(p(1 \u2212p)).\nWe can use the matrix\n\u039b(t) = \u03a3\u22121\nX \u03a3W\u03a3\u22121\nX [E{(\u03b6 \u2212E(\u03b6))I(Y \u2264t)}] [E{(\u03b6 \u2212E(\u03b6))I(Y \u2264t)}]\u22a4\nto identify the central subspace we want. Denote m(t) = E[(\u03b6 \u2212E(\u03b6))I(Y \u2264t)]. The sample\nversion of m(t) is\n\u02c6m(t) = 1\nn\nn\nX\ni=1\n(\u03b6i \u2212\u00af\u03b6)I(yi \u2264t),\nwhere \u03b6i =\n\u02c6\nCov(X, W)\u02c6\u03a3\u22121\nW wi and \u00af\u03b6 = (1/n) Pn\ni=1 \u03b6i. Let Ya be the response under the local\nalternative, then\n\u02c6m(t) \u2212m(t)\n=\n1\nn\nn\nX\ni=1\n(\u03b6i \u2212\u00af\u03b6)I(yi \u2264t) \u2212E{(\u03b6 \u2212E(\u03b6))I(Y \u2264t)}\n=\n1\nn\nn\nX\ni=1\n(\u03b6i \u2212\u00af\u03b6)I(yi \u2264t) \u2212E{(\u03b6 \u2212E(\u03b6))I(Ya \u2264t)}\n+E{(\u03b6 \u2212E(\u03b6))I(Ya \u2264t)} \u2212E{(\u03b6 \u2212E(\u03b6))I(Y \u2264t)}.\nThe convergence rate of the \ufb01rst term in the right hand side is Op(\u221an). For simplicity, we\nassume E(\u03b6) = 0. The second term is\nE[\u03b6I(Ya \u2264t)] \u2212E[\u03b6I(Y \u2264t)]\n=\nE {\u03b6[P(Ya \u2264t|\u03b6) \u2212P(Y \u2264t|\u03b6)]}\n21\nSince \u03b6 = \u03a3X\u03a3\u22121\nW W,\nP(Ya \u2264t|\u03b6) \u2212P(Y \u2264t|\u03b6)\n=\nP(Ya \u2264t|W) \u2212P(Y \u2264t|W) = FY |W(t \u2212CnE[G(B\u22a4X)|B\u22a4W]) \u2212FY |W(t)\n=\n\u2212CnE[G(B\u22a4X)|B\u22a4W]fY |W(t) + Op(C2\nn).\nThus, we have E{(\u03b6 \u2212E(\u03b6))I(Ya \u2264t)} \u2212E{(\u03b6 \u2212E(\u03b6))I(Y \u2264t)} = Op(Cn). Altogether,\n\u039bn(t) \u2212\u039b(t) = Op(Cn), for each t \u2208R. Finally, similar to the proof for Theorem 3.2 of Li et\nal. (2008) the condition \u039bn,n \u2212\u039b = Op(Cn) holds.\n5.3\nProof of Theorem 3.5\nIn this subsection, we \ufb01rst prove (ii) which is the large sample property of Vn under the local\nalternatives and then give a sketch of the proof of (i). For the local alternatives in (3.5),\naccording to Theorem 3.4, \u02c6q = 1 with a probability going to 1. Thus, we can only work on\nthe event that \u02c6q = 1. Note that \u02c6B(\u02c6q) converges to \u03b2/\u2225\u03b2\u2225in probability rather than the\np \u00d7 q matrix B that is the dimension reduction base matrix of the central mean subspace.\nIn other words, \u02c6B is not a consistent estimate of B. However, in this proof, we still use B\nto write the limit of \u02c6B for notation simplicity. By Proposition 2.2, we have\n\u02c6\u03b2 \u2212\u03b2 =\nCnH(\u03b2)(1 + op(1)).\n(5.1)\nwhere\nH(\u03b2)\n=\n\b\nE[g\u2032(\u03b2\u22a4X)X \u00afW \u22a4]E\u22121[ \u00afW \u00afW \u22a4]E[g\u2032(\u03b2\u22a4X) \u00afWX\u22a4]\n\t\u22121\n\u00d7E[g\u2032(\u03b2\u22a4X)X \u00afW \u22a4]E\u22121[ \u00afW \u00afW \u22a4]E[ \u00afWG(B\u22a4X)].\nLet Gi = G(zi) and \u2206i = \u2206(zi), where zi = B\u22a4wi, G is as in (3.5), and \u2206as in (3.1). Recall\nthe notation from (2.3) and (3.2). Rewrite\n\u02c6ei = gi + CnGi + \u03b5i \u2212\u02c6ri = ri \u2212\u02c6ri + CnGi + ei.\nRecalling \u02c6zi = \u02c6B\u22a4wi, we obtain the following decomposition for Vn.\nVn\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(\u02c6zi \u2212\u02c6zj)(ei + CnGi)(ej + CnGj)\n(5.2)\n+\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(\u02c6zi \u2212\u02c6zj)(ei + CnGi)(rj \u2212\u02c6rj(2))\n+\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(\u02c6zi \u2212\u02c6zj)(ri \u2212\u02c6ri(1))(ej + CnGj)\n+\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(\u02c6zi \u2212\u02c6zj)(ri \u2212\u02c6ri(1))(rj \u2212\u02c6rj(2))\n=:\nVn1 + Vn2 + Vn3 + Vn4,\nsay.\n22\nWe now deal with Vni\u2019s in the following steps.\nStep 5.1 nh1/2Vn1 \u2192D N(\u03bd1, \u03c41), where \u03c41 is as in (3.3) and\n\u03bd1 = E[\u22062(Z)f(Z)].\n(5.3)\nProof: It follows from (5.2) that\nVn1\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(\u02c6zi \u2212\u02c6zj)eiej + 2Cn\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(\u02c6zi \u2212\u02c6zj)eiGj (5.4)\n+C2\nn\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(\u02c6zi \u2212\u02c6zj)GiGj\n=:\nI1 + 2CnI2 + C2\nnI3.\nStep 5.1.1. Deal with I1. Rewrite I1 = I1,1 + I1,2, where\nI1,1\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)eiej,\nI1,2\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\n(Kh(\u02c6zi \u2212\u02c6zj) \u2212Kh(zi \u2212zj))eiej.\nFollowing Lemma 3.3a of Zheng (1996) we obtain nh1/2I1,1 \u2192D N(0, \u03c41), where\n\u03c41 = 2\nZ\n(\u03c32(z))2f 2(z)dz\nZ\nK2(u)du.\nThe Taylor expansion yields that\nI1,2 = ( \u02c6B \u2212B)\u22a4\nh\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nK\u2032(zi \u2212zj\nh\n)wi \u2212wj\nh\neiej(1 + op(1)).\nLet\nI\u2217\n1,2 =\n1\n(n \u22121)n\nn\nX\ni=1\nn\nX\nj\u0338=i\nK\u2032(zi \u2212zj\nh\n)wi \u2212wj\nh\neiej.\nSimilarly as I1,1, I\u2217\n1,2 is a degenerate U-statistic with kernel\nHn((yi, wi), (yj, wj)) = K\u2032(zi \u2212zj\nh\n)wi \u2212wj\nh\neiej.\nCombining \u2225\u02c6B \u2212B\u22252 = Op(Cn) and nh5/2 \u2192\u221e, we obtain nh1/2I12 = op(1).\nHence\nnh1/2I1 \u2192D N(0, \u03c41).\n23\nStep 5.1.2. Next, consider I2. Rewrite I2 = I2,1 + I2,2, where\nI2,1\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)eiGj,\nI22\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\n(Kh(\u02c6zi \u2212\u02c6zj) \u2212Kh(zi \u2212zj))eiGj.\nBy computing the second order moment, we know I2,1 = Op(1/\u221an). As to I2,2,\nI2,2 =\n\u02c6B \u2212B\nh\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nK\u2032(zi \u2212zj\nh\n)wi \u2212wj\nh\neiGj(1 + op(1)).\nLet\nI\u2217\n2,2 =\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nK\u2032(zi \u2212zj\nh\n)wi \u2212wj\nh\neiGj.\nSince the kernel function K(\u00b7) is symmetric, I\u2217\n2,2 can be rewritten as a non-degenerate U-\nstatistic. Thus I\u2217\n2,2 = Op(1/\u221an). Combining the convergence rates of I2,1 and I2,2, we know\nthat nh1/2CnI2 = op(1).\nStep 5.1.3. Consider I3. It is easy to see that I3 \u2192p E[\u22062(Z)f(Z)], where Z = B\u22a4W.\nSummarizing the above results for I1, I2 and I3, we have that if Cn = n\u22121/2h\u22121/4,\nnh1/2Vn1 \u2192D N(\u03bd1, \u03c41), thereby completing the proof of Step 5.1.\nStep 5.2 nh1/2Vn2 \u2192D N (\u03bd2, 2\u03bb\u22121\u03c42) , where \u03c42 is de\ufb01ned in (3.3) and\n\u03bd2 = \u2212E{\u2206(Z)E[g\u2032(\u03b2\u22a4X)X\u22a4|Z]f(Z)}H(\u03b20).\n(5.5)\nProof: Rewrite Vn2 as\nVn2\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(\u02c6zi \u2212\u02c6zj)ei(rj \u2212\u02c6rj(2))\n(5.6)\n+\nCn\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(\u02c6zi \u2212\u02c6zj)Gi(rj \u2212\u02c6rj(2))\n=:\nVn2,1 + CnVn2,2,\nsay.\nStep 5.2.1. Deal with the term Vn2,1. It can be decomposed as\nVn2,1\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)ei(rj \u2212\u02c6rj(2))\n+\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\n(Kh(\u02c6zi \u2212\u02c6zj) \u2212Kh(zi \u2212zj))ei(rj \u2212\u02c6rj(2)).\n24\nRecalling the de\ufb01nition of the estimator of r(2)(\u03b2\u22a4w, \u03b2) in (2.3), we have\nrj \u2212\u02c6rj(2) = 2\nN\nN\nX\ns=N/2+1\nMvN(\u02c6\u03b2\u22a4wj \u2212\u02c6\u03b2\u22a4\u02dcws)(rj \u2212\u02c6\u02dcgs)/ 2\nN\nN\nX\ns=N/2+1\nMvN(\u02c6\u03b2\u22a4wj \u2212\u02c6\u03b2\u22a4\u02dcws),\n(5.7)\nwhere \u02c6\u02dcgs is de\ufb01ned in (3.2). In order to analyze rj \u2212\u02c6rj(2) further, we need the following\nentities. Let\n\u00affN(2)(x) = 2\nN\nN\nX\ns=N/2+1\nMvN(x \u2212\u03b2\u22a4\u02dcws), \u02c6\u00affN(2)(x) = 2\nN\nN\nX\ns=N/2+1\nMvN(x \u2212\u02c6\u03b2\u22a4\u02dcws), (5.8)\nQ1(2)(\u03b2\u22a4wj) = 2\nN\nN\nX\ns=N/2+1\nMvN(\u03b2\u22a4wj \u2212\u03b2\u22a4\u02dcws)(rj \u2212\u02dcrs),\n(5.9)\nQ2(2)(\u03b2\u22a4wj) = 2\nN\nN\nX\ns=N/2+1\nMvN(\u03b2\u22a4wj \u2212\u03b2\u22a4\u02dcws)(\u02dcrs \u2212\u02dcgs),\nQ3(2)(\u03b2\u22a4wj) = 2\nN\nN\nX\ns=N/2+1\nMvN(\u03b2\u22a4wj \u2212\u03b2\u22a4\u02dcws)(\u02dcgs \u2212\u02c6\u02dcgs).\nThe kernel function MvN(\u02c6\u03b2\u22a4wj \u2212\u02c6\u03b2\u22a4\u02dcws) in the numerator of (5.7) can be rewritten as\nMvN(\u03b2\u22a4wj \u2212\u03b2\u22a4ws) + [MvN(\u02c6\u03b2\u22a4wj \u2212\u02c6\u03b2\u22a4ws) \u2212MvN(\u03b2\u22a4wj \u2212\u03b2\u22a4ws)],\nand the denominator can be decomposed as\n1\n\u00affN(2)(\u03b2\u22a4wj) + [\n1\n\u02c6\u00affN(2)(\u02c6\u03b2\u22a4wj)\n\u2212\n1\n\u00affN(2)(\u03b2\u22a4wj)].\nFurther, write\nrj \u2212\u02c6\u02dcgs = [rj \u2212\u02dcrs] + [\u02dcrs \u2212\u02dcgs] + [\u02dcgs \u2212\u02c6\u02dcgs].\nCombining the above decompositions into (5.7), rj \u2212\u02c6rj(2) can be decomposed into 12 terms,\nand then Vn2,1 can be decomposed into 24 terms.\nWe only consider the following three\nterms that make non-negligible contribution.\nThe remaining terms can be shown to be\nasymptotically negligible, in probability. Accordingly, consider\nI4\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)eiQ1(2)(\u03b2\u22a4wj)/ \u00affN(2)(\u03b2\u22a4wj),\n(5.10)\nI5\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)eiQ2(2)(\u03b2\u22a4wj)/ \u00affN(2)(\u03b2\u22a4wj),\nI6\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)eiQ3(2)(\u03b2\u22a4wj)/ \u00affN(2)(\u03b2\u22a4wj)\n25\nwhere \u00affN(2)(\u03b2\u22a4wj) is de\ufb01ned in (5.8), and Q1(2)(\u00b7), Q2(2)(\u00b7), Q3(2)(\u00b7) are in (5.9). Let \u00aff denote\nthe density of \u03b2\u22a4W.\nWe \ufb01rst prove that nh1/2I4 = op(1). Rewrite I4 = n\u22121 Pn\nj=1 I41(zj) \u00d7 I42(\u03b2\u22a4wj), where\nI41(zj) =\n1\n(n \u22121)\nn\nX\ni\u0338=j\nKh(zi \u2212zj)ei,\nI42(\u03b2\u22a4wj) = Q1(2)(\u03b2\u22a4wj)\n\u00affN(2)(\u03b2\u22a4wj).\nThus, the application of Cauchy - Schwarz inequality yields that |I4| \u2264\nq\n(1/n) Pn\nj=1 I2\n41(zj)\u00d7\nq\n(1/n) Pn\nj=1 I2\n42(\u03b2\u22a4wj). We only need to bound the conditional expectations E[I2\n41(zj)] and\nE[I2\n42(\u03b2\u22a4wj)] when zj, \u03b2\u22a4wj are given. For I41(zj),\nE[I2\n41(zj)] =\n1\n(n\u22121)2E[(Pn\ni\u0338=j Kh(zi \u2212zj)ei)2] =\n1\n(n\u22121)h2E[K2( zi\u2212zj\nh )e2\ni ] = O( 1\nnh).\nFor I42, we can obtain that given \u03b2\u22a4wj,\n|I42(\u03b2\u22a4wj)| \u2264\n\f\f\f\f\nQ1(2)(\u03b2\u22a4wj)\n\u00aff(\u03b2\u22a4wj)\n\f\f\f\f sup\n\u03b2\u22a4wj\n\f\f\f\f\n\u00aff(\u03b2\u22a4wj)\n\u00affN(2)(\u03b2\u22a4wj)\n\f\f\f\f .\nSince\nsup\n\u03b2\u22a4wj\n| \u00affN(2)(\u03b2\u22a4wj) \u2212\u00aff(\u03b2\u22a4wj)| = op(1), sup\n\u03b2\u22a4wj\n\f\f\f\f\n\u00affN(2)(\u03b2\u22a4wj)\n\u00aff(\u03b2\u22a4wj)\n\u22121\n\f\f\f\f = op(1),\nand \u00aff(\u03b2\u22a4wj) is uniformly bounded below, we only need to bound Q2\n1(2)(\u03b2\u22a4wj) in the numer-\nators. But\nE[Q2\n1(2)(\u03b2\u22a4wj)] =N(N \u22122)\nN 2v2\nN\nE[M(\u03b2\u22a4wj \u2212\u03b2\u22a4\u02dcws\nvN\n)(rj \u2212\u02dcrs)M(\u03b2\u22a4wj \u2212\u03b2\u22a4\u02dcws\u2032\nvN\n)(rj \u2212\u02dcrs\u2032)]\n+\n2\nNv2\nN\nE[M 2(\u03b2\u22a4wj \u2212\u03b2\u22a4\u02dcws\nvN\n)(rj \u2212\u02dcrs)2]\n\u2264C1v2\u2113\nN + N \u22121C2vN,\nwhere C1 and C2 are two constants. The last inequality is obtained by Conditions (f),(r)\nand (M). Thus E[I2\n42(\u03b2\u22a4wj)] is bounded from the above by C1v2\u2113\nN + C2vN/N, in probability.\nSummarizing the results of E[I2\n41] and E[I2\n42], we have |nh1/2I4| \u2264nh1/2Op(\n1\n\u221a\nnh\nq\nv2\u2113\nN + vN\nN ) =\nop(1).\nConsider I5. Rewrite it as I5 = I51 + I52, where\nI51 = E[I5|\u02dc\u03b7s, \u02dczs, zi, ei],\nI52 = (I5 \u2212E[I5|\u02dc\u03b7s, \u02dczs, zi, ei]).\n(5.11)\nNote that\nI51\n=\n2\nnN\nn\nX\nj=1\nN\nX\ns=N/2+1\nei\u02dc\u03b7s\nZ 1\nhK(zi \u2212zj\nh\n) 1\nvN\nM(\u03b2\u22a4wj \u2212\u03b2\u22a4\u02dcws\nvN\n)d(\u03b2\u22a4wj)\n26\n=\n2\nnN\nn\nX\nj=1\nN\nX\ns=N/2+1\nei\u02dc\u03b7s\nZ 1\nhK(zi \u2212\u02dczs \u2212vNu/\u2225\u03b2\u2225\nh\n) 1\nvN\nM(u)d(\u03b2\u22a4\u02dcws + vNu).\nThe second equation holds because zj = B\u22a4wj = \u03b2\u22a4wj/\u2225\u03b2\u2225. Further,\nZ 1\nhK(zi \u2212\u02dczs \u2212vNu/\u2225\u03b2\u2225\nh\n)M(u)du = 1\nhK(zi \u2212\u02dczs\nh\n) + 1\nhK\n\u2032\u2032(zi \u2212\u02dczs\nh\n)v2\nN\u2225\u03b2\u22252\nh2\n.\nThus, I51 =\n2\nnN\nPn\ni=1\nPN\ns=N/2+1 ei\u02dc\u03b7sKh(zi \u2212\u02dczs)(1 + op(1)). By Central Limit Theorem we\nhave\nr\nnN\n2 h1/2I5,1 \u2192D N(0,\nZ\nK2(u)du\nZ\n\u03c32(z)\u03be2(z)f 2(z)dz),\nwhere \u03c32(Z) and \u03be2(Z) are de\ufb01ned in (3.1). By some elementary calculations, we can derive\nthat E[(I52)2] = Op(1/(n2NhvN)). Chebyshev\u2019s inequality yields that nh1/2I52 = op(1).\nHence\nnh1/2I5 \u2192D N\n\u0010\n0, 2\u03bb\u22121\nZ\nK2(u)du\nZ\n\u03c32(z)\u03be2(z)f 2(z)dz\n\u0011\n.\n(5.12)\nNow consider I6. Recall the de\ufb01nition of Q3(2) in (5.9) and the de\ufb01nition of \u02dcg below (3.2).\nTaylor expansion of the function \u02dcg yields that I6 = I\u2217\n6(\u03b2 \u2212\u02c6\u03b2)(1 + op(1)), where\nI\u2217\n6\n=\n2\nNn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)ei\n\u00affN(2)(\u03b2\u22a4wj)\nN\nX\ns=N/2+1\nMvN(\u03b2\u22a4wj \u2212\u03b2\u22a4\u02dcws)g\u2032(\u03b2\u22a4\u02dcxs)\u02dcx\u22a4\ns\n=:\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\ni\u0338=j\nKh(zi \u2212zj)eiI62(\u03b2\u22a4wj),\nsay.\nIt is easy to see that for any given \u03b2\u22a4wj, E[I62(\u03b2\u22a4wj)] = E[g\u2032(\u03b2\u22a4x)x\u22a4|\u03b2\u22a4wj] by noticing\nthat \u02dcx has the same distribution as that of x. By Lemma 2 of Guo et al. (2015),\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\ni\u0338=j\nKh(zi \u2212zj)eiE[g\u2032(\u03b2\u22a4x)x\u22a4|\u03b2\u22a4wj] = Op( 1\n\u221an).\nSimilarly, as in the proof for I4, we can also derive that as N \u2192\u221e, sup\u03b2\u22a4w |I62(\u03b2\u22a4w) \u2212\nE[I62(\u03b2\u22a4w)]| \u2264O(v2\nN + log(N)/\u221aNvN) and then\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\ni\u0338=j\nKh(zi \u2212zj)ei(I62(\u03b2\u22a4wj) \u2212E[g\u2032(\u03b2\u22a4x)x\u22a4|\u03b2\u22a4wj]) = op( 1\n\u221an).\nHence nh1/2I6 = op(1).\nCombining the above results for I4, I5 and I6 with the fact that the remaining 21 terms\ntend to zero, in probability, we obtain that nh1/2Vn2,1 \u2192D N(0, 2\u03bb\u22121\u03c42), where \u03c42 is in (3.3).\n27\nStep 5.2.2. Next, consider the second term Vn2,2 of the decomposition (5.6). Rewrite\nVn2,2\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)Gi(rj \u2212\u02c6rj(2))\n+\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\n(Kh(\u02c6zi \u2212\u02c6zj) \u2212Kh(zi \u2212zj))Gi(rj \u2212\u02c6rj(2)).\nSimilarly as the decomposition in (5.7), Vn2,2 can also be decomposed into 24 terms. Again,\nwe only give the detail about how to treat the three leading terms. Again, the remaining 21\nterms tend to zero, in probability. The three leading terms are:\nI7\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)GiQ1(2)(\u03b2\u22a4wj)/ \u00affN(2)(\u03b2\u22a4wj),\nI8\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)GiQ2(2)(\u03b2\u22a4wj)/ \u00affN(2)(\u03b2\u22a4wj),\nI9\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)GiQ3(2)(\u03b2\u22a4wj)/ \u00affN(2)(\u03b2\u22a4wj),\nwhere Q1(2)(\u03b2\u22a4wj), Q2(2)(\u03b2\u22a4wj), Q3(2)(\u03b2\u22a4wj) and \u00affN(2)(\u03b2\u22a4wj) are de\ufb01ned in (5.9) and (5.8).\nRecall that Cn = n\u22121/2h\u22121/4 and E[Q2\n1(2)(\u03b2\u22a4wj)] \u2264C1v2\u2113\nN +C2vN/N, which was proved when\nwe handled I4. By the Cauchy\u2013Schwarz inequality,\n|nh1/2CnI7| \u2264Op\n\u0010\nn1/2h1/4\nq\nC1v2\u2113\nN + C2vN/N\n\u0011\n= op(1).\nTo deal with I8, decompose I8 = I81 + I82, with\nI81 =\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)GiQ2(2)(\u03b2\u22a4wj)/ \u00aff(\u03b2\u22a4wj),\nI82 =\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)GiQ2(2)(\u03b2\u22a4wj)[\n1\n\u00affN(2)(\u03b2\u22a4wj) \u2212\n1\n\u00aff(\u03b2\u22a4wj)],\nwhere \u00aff(\u03b2\u22a4w) is the density of \u03b2\u22a4w. By some elementary calculations, one can verify that\nE[I2\n81] = Op(1/N). This implies nh1/2CnI81 = op(1) by recalling the de\ufb01nition of Cn.\nNext, consider I82. By the Cauchy\u2013Schwarz inequality, I2\n82 is bounded above by a product\nof Pn\nj=1 I2\n821(zj)/n and Pn\nj=1 I2\n822(wj)/n, where\nI821(zj) = 1\nn\nX\ni\u0338=j\nKh(zi \u2212zj)Gi,\nI822(wj) = Q2(2)(\u03b2\u22a4wj)[\n1\n\u00affN(2)(\u03b2\u22a4wj) \u2212\n1\n\u00aff(\u03b2\u22a4wj)].\nNow we bound E[I2\n821(zj)] and E[I2\n822(wj)]. Clearly, conditional on zj, E[I2\n821(zj)] = O(1),\nwhich in turn implies that E\n\b Pn\nj=1 I2\n821(zj)/n\n\t\n= O(1).\n28\nNext, note that\n1\nn\nn\nX\nj=1\nI2\n822(wj)\n\u2264\n1\nn\nn\nX\nj=1\nQ2\n2(2)(\u03b2\u22a4wj) sup\nw |\n1\n\u00affN(2)(\u03b2\u22a4w) \u2212\n1\n\u00aff(\u03b2\u22a4w)|2\n\u2264\nOp(v2\nN + log(N)/\np\nNvN)1\nn\nn\nX\nj=1\nQ2\n2(2)(\u03b2\u22a4wj).\nThe second inequality is from the fact that \u00aff(\u03b2\u22a4w) is bounded below and supw | \u00affN(2)(\u03b2\u22a4w)\u2212\n\u00aff(\u03b2\u22a4w)| = Op(v2\nN+log(N)/\u221aNvN). By E[(\u02dcrs\u2212\u02dcgs)|\u03b2\u22a4\u02dcws] = 0, E[Q2\n2(2)(\u03b2\u22a4wj)] \u2264O(1/(NvN))\nfor any \ufb01xed \u03b2\u22a4wj.\nIn other words, E\n\b Pn\nj=1 Q2\n2(2)(\u03b2\u22a4wj)/n\n\t\n\u2264O(1/(NvN)). By the\nMarkov inequality, Pn\nj=1 I2\n822(wj)/n is bounded by Op(1/NvN)Op(v2\nN + log(N)/\u221aNvN) =\nop(1/(nh1/2Cn)2). Combining these results, we obtain that\n\f\fnh1/2CnI82\n\f\f \u2264nh1/2Cnop(1/(nh1/2Cn)) = op(1).\nThe above results about I81 and I82 in turn yield that nh1/2CnI8 = op(1).\nNow we analyze I9. Recall the de\ufb01nitions that Gi = G(B\u22a4xi) and \u2206i = E[G(B\u22a4X)|Z =\nzi]. Write I9 = I91 + I92, where\nI91 =\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)\u2206iQ3(2)(\u03b2\u22a4wj)/ \u00affN(2)(\u03b2\u22a4wj)\nI92 =\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)(Gi \u2212\u2206i)Q3(2)(\u03b2\u22a4wj)/ \u00affN(2)(\u03b2\u22a4wj).\nFor I92, E[Gi \u2212\u2206i|Zi] = 0. Thus, nh1/2I92 = op(1), at the same rate as I6. So nh1/2CnI92 =\nop(1).\nNext, we deal with I91. Similar to I8, rewrite I91 = I911 + I912, where\nI911 =\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)\u2206iQ3(2)(\u03b2\u22a4wj)/ \u00aff(\u03b2\u22a4wj),\nI912 =\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)\u2206iQ3(2)(\u03b2\u22a4wj)[\n1\n\u00affN(2)(\u03b2\u22a4wj) \u2212\n1\n\u00aff(\u03b2\u22a4wj)].\nSimilar to the proof of I82, we have nh1/2I912 = op(1), because E[Q2\n3(2)(\u03b2\u22a4wj)] = Op(C2\nn).\nNext, consider I911. De\ufb01ne\nI\u2217\n911 := E[I911|zi, \u02dczs, \u02dcxs] =\n2\nnN\nn\nX\ni=1\nN\nX\ns=N/2+1\nKh(zi \u2212\u02dczs)\u2206i(\u02dcgs \u2212\u02c6\u02dcgs).\n29\nBy the \ufb01rst order Taylor expansion,\nI\u2217\n911 =\n2\nnN\nn\nX\ni=1\nN\nX\ns=N/2+1\nKh(zi \u2212\u02dczs)\u2206ig\u2032(\u03b2\u22a4\n0 \u02dcxs)\u02dcx\u22a4\ns (\u03b20 \u2212\u02c6\u03b2)(1 + op(1))\nCombining the result of (5.1),\nnh1/2CnI\u2217\n911 \u2192p\n\u03bd2 = \u2212E{\u2206(Z)E[g\u2032(\u03b2\u22a4\n0 X)X\u22a4|Z]f(Z)}H(\u03b20).\nBy computing the second moment of I911 \u2212I\u2217\n911 and using the Markov inequality, one can\nverify nh1/2Cn(I911 \u2212I\u2217\n911) = op(1). Hence nh1/2CnI9 \u2192\u03bd2. These results about I7, I8 and\nI9 imply that nh1/2CnVn2,2 \u2192p \u03bd2. Hence Step 5.2 is \ufb01nished.\nStep 5.3 nh1/2Vn3 \u2192D N (\u03bd2, 2\u03bb\u22121\u03c42) , where \u03bd2 and \u03c42 are as in (5.5) and (3.3).\nProof: The proof is similar to that pertaining to Vn2 in STEP 5.2. The only di\ufb00erence is\nthat instead of the representation (5.7) we now use\nri \u2212\u02c6ri(1) = 2\nN\nN/2\nX\nt=1\nMvN(\u02c6\u03b2\u22a4wi \u2212\u02c6\u03b2\u22a4\u02dcwt)(ri \u2212\u02c6\u02dcgt)/ 2\nN\nN/2\nX\nt=1\nMvN(\u02c6\u03b2\u22a4wi \u2212\u02c6\u03b2\u22a4\u02dcwt).\n(5.13)\nFurther the de\ufb01nitions in (5.8) and (5.9) are changed into\n\u00affN(1)(x) = 2\nN\nN/2\nX\nt=1\nMvN(x \u2212\u03b2\u22a4\u02dcwt), \u02c6\u00affN(1)(x) = 2\nN\nN/2\nX\nt=1\nMvN(x \u2212\u02c6\u03b2\u22a4\u02dcwt),\n(5.14)\nand\nQ1(1)(\u03b2\u22a4wi) = 2\nN\nN/2\nX\nt=1\nMvN(\u03b2\u22a4wi \u2212\u03b2\u22a4\u02dcwt)(ri \u2212\u02dcrt),\n(5.15)\nQ2(1)(\u03b2\u22a4wi) = 2\nN\nN/2\nX\nt=1\nMvN(\u03b2\u22a4wi \u2212\u03b2\u22a4\u02dcwt)(\u02dcrt \u2212\u02dcgt),\nQ3(1)(\u03b2\u22a4wi) = 2\nN\nN/2\nX\nt=1\nMvN(\u03b2\u22a4wi \u2212\u03b2\u22a4\u02dcwt)(\u02dcgt \u2212\u02c6\u02dcgt).\nWe omit the details here.\nStep 5.4 nh1/2Vn4 \u2192D N(\u03bd3, 2\u03bb\u22122\u03c43), where \u03c43 is as in (3.3) and\n\u03bd3 = H\u22a4(\u03b20)E{E[g\u2032(\u03b2\u22a4\n0 X)X|Z]E[g\u2032(\u03b2\u22a4\n0 X)\u22a4X\u22a4|Z]f(Z)}H(\u03b20).\n(5.16)\n30\nProof: By the same decompositions in (5.7) and (5.13), Vn4 can be decomposed to 9 dominant\nterms, and seven of those are of order op(1/nh1/2). We investigate the other two terms as\nfollows:\nI10\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)Q2(1)(\u03b2\u22a4wi)Q2(2)(\u03b2\u22a4wj)/ \u00affN(1)(\u03b2\u22a4wi) \u00affN(2)(\u03b2\u22a4wj),\nI11\n=\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)Q3(1)(\u03b2\u22a4wi)Q3(2)(\u03b2\u22a4wj)/ \u00affN(1)(\u03b2\u22a4wi) \u00affN(2)(\u03b2\u22a4wj).\nSimilar to the proof of I5, we have Nh1/2I10 \u2192D N(0, 2\u03c43), where \u03c43 is de\ufb01ned in (3.3).\nSimilarly as I91, I11 can be rewritten as\nI11\n=\n4\nN 2\nN/2\nX\nt=1\nN\nX\ns=N/2+1\nKh(\u02dczt \u2212\u02dczs)(\u02dcgs \u2212\u02c6\u02dcgs)(\u02dcgt \u2212\u02c6\u02dcgt)(1 + op(1))\n=\n(\u03b20 \u2212\u02c6\u03b2)\u22a4\n\uf8ee\n\uf8f04\nN 2\nN/2\nX\ns=1\nN\nX\nt=N/2+1\nKh(\u02dczt \u2212\u02dczs)g\u2032(\u03b2\u22a4\n0 \u02dcxs)g\u2032(\u03b2\u22a4\n0 \u02dcxt)\u02dcxs\u02dcx\u22a4\nt\n\uf8f9\n\uf8fb(\u03b20 \u2212\u02c6\u03b2).\nCombining the result of (5.1), nh1/2I11 converges to \u03bd3 in probability. Hence Step 5.4 is\ncompleted.\nAltogether, Steps 5.1\u2013 5.4 conclude the proof of (ii) in Theorem 3.5.\nNext, we give a sketch of the proof of (i), which describes the asymptotic power perfor-\nmance of the test under the global alternative with \ufb01xed Cn \u2261C. Let\n\u02dc\u03b2 = arg min\n\u03b2\nE\n\b\nY \u2212\u00afWE\u22121[ \u00afW \u00afW \u22a4]E[ \u00afWg(\u03b2\u22a4X)]\n\t2\nwhich is di\ufb00erent from the true parameter \u03b20. Here \u00afW is a vector consisting of polynomials\nof W. Then, for \ufb01xed Cn \u2261C,\n\u02c6e\n=\ne + C(G(B\u22a4W) \u2212E[G(B\u22a4W)|\u02dc\u03b2\u22a4W]) + CE[G(B\u22a4W)|\u02dc\u03b2\u22a4W]\n+(E[g(\u03b2\u22a4\n0 X)|\u02dc\u03b2\u22a4W] \u2212E[g(\u02dc\u03b2\u22a4X)|\u02dc\u03b2\u22a4W]) + (E[g(\u02dc\u03b2\u22a4X)|\u02dc\u03b2\u22a4W] \u2212E[g(\u02c6\u03b2\u22a4X)|\u02c6\u03b2\u22a4W]).\nWe can obtain that Vn tends, in probability, to a positive constant since the third term\nin the right hand side of the above equation is not 0. Similarly, we can also prove that\n\u02c6\u03c4 converges to a positive constant. We then have that Vn/\u02c6\u03c4 converges in probability to a\npositive constant. That is, the test statistic nh1/2Vn goes to in\ufb01nity at the rate of order\nnh1/2. The proof is \ufb01nished.\n5.4\nProof of Theorem 3.1\nAs the arguments used for proving Theorem 3.5 with Cn = 0, the results \u2225\u02c6B \u2212B\u2225=\nOp(1/\u221an) and \u02c6\u03b2 \u2212\u03b2 = Op(1/\u221an) are applicable for proving this theorem, we then omit\n31\nmost of the details, but focus on the bias term. The terms \u00affN(j)(x), Qk(j)(\u00b7), k = 1, 2, 3 and\nj = 1, 2 in the proof of Theorem 3.5 are replaced by\n\u00affN(x) = 1\nN\nN\nX\ns=1\nMvN(x \u2212\u03b2\u22a4\u02dcws), \u02c6\u00affN(x) = 1\nN\nN\nX\ns=1\nMvN(x \u2212\u02c6\u03b2\u22a4\u02dcws)\n(5.17)\nand\nQ1(\u03b2\u22a4wi) = 1\nN\nN\nX\ns=1\nMvN(\u03b2\u22a4wi \u2212\u03b2\u22a4\u02dcws)(ri \u2212\u02dcrs),\n(5.18)\nQ2(\u03b2\u22a4wi) = 1\nN\nN\nX\ns=1\nMvN(\u03b2\u22a4wi \u2212\u03b2\u22a4\u02dcws)(\u02dcrs \u2212\u02dcgs),\nQ3(\u03b2\u22a4wi) = 1\nN\nN\nX\ns=1\nMvN(\u03b2\u22a4wi \u2212\u03b2\u22a4\u02dcws)(\u02dcgs \u2212\u02c6\u02dcgs).\nUsing the same decomposition as in the proof of Step 5.4, we also have a term similar to I10\nwith the conditional expectation as\nI10 =\n1\nn(n \u22121)\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)Q2(\u03b2\u22a4wi)Q2(\u03b2\u22a4wj)/ \u00affN(\u03b2\u22a4wi) \u00affN(\u03b2\u22a4wj)\nand\nE[I10|\u02dc\u03b7s, \u02dczs, \u02dc\u03b7t, \u02dczt]\n=\n1\nN 2\nN\nX\ns=1\nN\nX\nt=1\n1\nhK( \u02dczs \u2212\u02dczt\nh\n)\u02dc\u03b7s\u02dc\u03b7t(1 + op(1)).\nSeparate the summands with s \u0338= t and s = t to write the leading term in the above\nexpression as the sum of the following two terms.\nI\u2217\n101 = 1\nN 2\nN\nX\ns=1\nN\nX\nt\u0338=s\n1\nhK( \u02dczs \u2212\u02dczt\nh\n)\u02dc\u03b7s\u02dc\u03b7t,\nI\u2217\n102 = 1\nN 2\nN\nX\ns=1\n1\nhK(0)\u02dc\u03b72\ns.\nSince K is symmetric, I\u2217\n101 can be written as an U-statistic with the kernel\nHn((\u02dczs, \u02dc\u03b7s), (\u02dczt, \u02dc\u03b7t)) = 1\nhK( \u02dczs \u2212\u02dczt\nh\n)\u02dc\u03b7s\u02dc\u03b7t.\nFurther,\nE[Hn((\u02dczs, \u02dc\u03b7s), (\u02dczt, \u02dc\u03b7t))|(\u02dczs, \u02dc\u03b7s)] = 1\nh \u02dc\u03b7sE{K( \u02dczs \u2212\u02dczt\nh\n) \u00d7 E[\u02dc\u03b7t|\u02dczt]} = 0.\nThus the U-statistic I\u2217\n101 is degenerate. By Central Limit Theorem for degenerate U-statistic\n(see, Hall 1984),\nNh1/2I\u2217\n101 \u2192D N(0, 2\nZ\nK2(u)du\nZ\n(\u03be2(z))2f 2(z)dz).\nHence nh1/2I\u2217\n101 \u2192D N(0, \u03bb\u22122\u03c43), where \u03c43 is de\ufb01ned in (3.3).\nFurther, the fact that\nNhEI\u2217\n102 = K(0)E[\u03be2(Z)] implies that nh1/2EI\u2217\n102 \u2192\u221e, which results in the asymptotic\nbias in \u02dcVn.\n32\n5.5\nProof of Theorem 3.3\nWhen N/n \u21920, \u02c6\u03b2 and \u02c6B are\n\u221a\nN consistent estimates of \u03b2 and B, respectively. Again as\nthe decompositions used in the proof of Theorem 3.5 are applicable for proving this theorem,\nwe give only a sketch of the proof of (i) here. Put Cn = 0 in the proof of Theorem 3.5. We\nonly consider I1, Vn2,1, and I10. As (Nv1/2\nN )/(nh1/2) \u21920, Nv1/2\nN I1,1 in Step 5.1 is op(1). In\naddition, Nh2 \u2192\u221eleads to Nv1/2\nN I1,2 = op(1). Thus Nv1/2\nN I1 = op(1). For Vn2,1, following\nthe proof of Step 5.2, we obtain that Nv1/2\nN I4 = op(1), Nv1/2\nN I5 = op(1), Nv1/2\nN I6 = op(1).\nThese imply that Nv1/2\nN Vn2 = op(1). Recalling the notation in (3.1), (3.2), (5.17) and (5.18),\nI10 can be written as\nI10 =\n1\nn(n \u22121)N 2\nn\nX\ni=1\nn\nX\nj\u0338=i\nKh(zi \u2212zj)Q2(\u03b2\u22a4wi)Q2(\u03b2\u22a4wj)/ \u00affN(\u03b2\u22a4wi) \u00affN(\u03b2\u22a4wj).\nAgain de\ufb01ne its conditional expectation as\nI\u2217\n10\n=\nE[I10|\u02dczs, \u02dc\u03b7s, \u02dczt, \u02dc\u03b7t]\n=\n1\nN 2\nN\nX\ns=1\nN\nX\nt=1\n\u02dc\u03b7s\u02dc\u03b7t\nZ Z 1\nhK(zi \u2212zj\nh\n) 1\nvN\nM(\u03b2\u22a4wi \u2212\u03b2\u22a4\u02dcws\nvN\n)\n\u00d7 1\nvN\nM(\u03b2\u22a4wj \u2212\u03b2\u22a4\u02dcwt\nvN\n)d(\u03b2\u22a4wi)d(\u03b2\u22a4wj).\nNote that \u03b2\u22a4w = \u2225\u03b2\u2225z. Thus,\nZ Z 1\nhK(zi \u2212zj\nh\n) 1\nvN\nM(\u03b2\u22a4wi \u2212\u03b2\u22a4\u02dcws\nvN\n) 1\nvN\nM(\u03b2\u22a4wj \u2212\u03b2\u22a4wt\nvN\n)d(\u03b2\u22a4wi)d(\u03b2\u22a4wj)\n=\nZ Z 1\nhK(zi \u2212zj\nh\n)\u2225\u03b2\u2225\nvN\nM( zi \u2212\u02dczs\nvN/\u2225\u03b2\u2225)\u2225\u03b2\u2225\nvN\nM( zj \u2212\u02dczt\nvN/\u2225\u03b2\u2225)dzidzj\n=\nZ Z 1\nhK(u)\u2225\u03b2\u2225\nvN\nM(hu + zj \u2212\u02dczs\nvN/\u2225\u03b2\u2225\n)\u2225\u03b2\u2225\nvN\nM( zj \u2212\u02dczt\nvN/\u2225\u03b2\u2225)d(zj + uh)dzj\n=\nZ \u2225\u03b2\u2225\nvN\nM( zj \u2212\u02dczs\nvN/\u2225\u03b2\u2225)\u2225\u03b2\u2225\nvN\nM( zj \u2212\u02dczt\nvN/\u2225\u03b2\u2225)dzj\n+\nZ \u2225\u03b2\u2225\nvN\nM\n\u2032\u2032( zj \u2212\u02dczs\nvN/\u2225\u03b2\u2225)\u2225\u03b2\u22252h2\nv2\nN\n\u2225\u03b2\u2225\nvN\nM( zj \u2212\u02dczt\nvN/\u2225\u03b2\u2225)dzj.\n33\nLet \u02dcvN = vN/\u2225\u03b2\u2225. Then we have\nI\u2217\n10 = 1\nN 2\nN\nX\ns=1\nN\nX\nt=1\n\u02dc\u03b7s\u02dc\u03b7t\nZ\n1\n\u02dcvN\nM(zj \u2212\u02dczs\n\u02dcvN\n) 1\n\u02dcvN\nM(zj \u2212\u02dczt\n\u02dcvN\n)dzj\n= 1\nN 2\nN\nX\ns=1\nN\nX\nt\u0338=s\n\u02dc\u03b7s\u02dc\u03b7t\nZ\n1\n\u02dcvN\nM(zj \u2212\u02dczs\n\u02dcvN\n) 1\n\u02dcvN\nM(zj \u2212\u02dczt\n\u02dcvN\n)dzj\n+ 1\nN 2\nN\nX\ns=1\n\u02dc\u03b72\ns\nZ\n1\n\u02dcvN\nM(zj \u2212\u02dczs\nvN\n) 1\n\u02dcvN\nM(zj \u2212\u02dczs\n\u02dcvN\n)dzj\n+ 1\nN 2\nN\nX\ns=1\nN\nX\nt\u0338=s\n\u02dc\u03b7s\u02dc\u03b7t\nZ\n1\n\u02dcvN\nM\n\u2032\u2032(zj \u2212\u02dczs\n\u02dcvN\n) h2\n\u02dcv2\nN\n1\n\u02dcvN\nM(zj \u2212\u02dczt\n\u02dcvN\n)dzj\n+ 1\nN 2\nN\nX\ns=1\n\u02dc\u03b72\ns\nZ\n1\n\u02dcvN\nM\n\u2032\u2032(zj \u2212\u02dczs\n\u02dcvN\n) h2\n\u02dcv2\nN\n1\n\u02dcvN\nM(zj \u2212\u02dczs\n\u02dcvN\n)dzj\n= : I101 + I102 + I103 + I104.\nRewrite I101 as\n2\nN\nX\ns=2\nN\nX\nt<s\n\u02dc\u03b7s\u02dc\u03b7t\n1\nN 2\nZ\n1\n\u02dcvN\nM(zj \u2212\u02dczs\n\u02dcvN\n) 1\n\u02dcvN\nM(zj \u2212\u02dczt\n\u02dcvN\n)dzj.\nBy Theorem 1 of Hall (1984), Nv1/2\nN I101 \u2192D N(0, \u02dc\u03c4), where\n\u02dc\u03c4 = 2\u2225\u03b2\u2225\nZ \u0010 Z\nM(u)M(u + v)du)2dv\nZ\n(\u03be2(z)\n\u00112\nf 2(z)dz,\n\u03be2(z) = E[\u03b72|Z = z].\nWe also have in probability\nN\u02dcvNI102 \u2192p E[\nZ\n1\n\u02dcvN\nM(zj \u2212\u02dczs\n\u02dcvN\n)M(zj \u2212\u02dczs\n\u02dcvN\n)dzj\u02dc\u03b72\ns] =\nZ\nM 2(u)duE[\u03be2(z)].\nFurther it can be proved that\nE[I2\n103]\n=\nOp( h4\n\u02dcv4\nN\n1\nN 2\u02dcvN\n) = op(\n1\nN 2vN\n),\nE[I2\n104]\n=\nOp( h4\n\u02dcv4\nN\n1\nN 2\u02dcv2\nN\n) + Op( h4\n\u02dcv4\nN\n1\nN 3\u02dcv3\nN\n) = op(\n1\nN 2vN\n).\nThen the Markov inequality implies that both I103 and I104 converge in probability to zero\nat the faster rate than 1/(Nv1/2\nN ). We have Nv1/2\nN {I\u2217\n10 \u2212\u03bd} \u2192D N(0, \u02dc\u03c4). We can further\nprove that\nE[(I10 \u2212I\u2217\n10)2] = Op(\n1\nN 2nvN\n) = op(\n1\nN 2vN\n).\nHence Nv1/2\nN {I10 \u2212\u03bd} \u2192D N(0, \u02dc\u03c4). This completes the proof of Theorem 3.3.\n34\nReferences\n[1] Carroll, R.J., and Li, K.C. (1992). Measurement Error Regression With Unknown Link:\nDimension Reduction and Data Visualization. Journal of the American Statistical Asso-\nciation, 87, 1040-1050.\n[2] Carroll, R. J., Ruppert, D., Stefanski, L. A., and Crainiceanu, C. M. (2006). Measurement\nerror in nonlinear models: a modern perspective. CRC press.\n[3] Cheng C.L., and Kukush A.G. (2004). A Goodness-of-Fit Test for a Polynomial Errors-\nin-Variables Model. Ukrainian Mathematical Journal, 56, 527\u2013543.\n[4] Cook, R. D. (1998). Regression Graphics: Ideas for Studying Regressions through Graph-\nics. Wiley, New York.\n[5] Cook, R. D. and Li, B. (2002). Dimension Reduction for Conditional Mean in Regression.\nThe Annals of Statistics, 30, 455\u2013474.\n[6] Cook, R. D. and Weisberg, S. (1991). Sliced Inverse Regression for Dimension Reduction:\nComment. Journal of the American Statistical Association, 86, 328\u2013332.\n[7] Dai, P., Sun, Z., and Wang, P. (2010). Model Checking for General Linear Error-in-\nCovariables Model with Validation Data. Journal of Systems Science and Complexity, 23,\n1153\u20131166.\n[8] Fuller, W.A., 1987. Measurement Error Models. Wiley, New York.\n[9] Gonzlez-Manteiga, W., and Crujeiras, R. M. (2013). An updated review of Goodness-of-\nFit tests for regression models. Test, 22, 361-411.\n[10] Guo, X. Wang, T. and Zhu, L.X. (2015). Model Checking for Generalized Linear Mod-\nels: a Dimension-Reduction Model-Adaptive Approach. Journal of the Royal Statistical\nSociety: Series B, forthcoming.\n[11] Hall, P. (1984). Central Limit Theorem for Integrated Square Error of Multivariate\nNonparametric Density Estimators. Journal of multivariate analysis, 14, 1\u201316.\n[12] Hall, P., and Li, K.C. (1993). On almost Linearity of Low Dimensional Projections from\nHigh Dimensional Data. The Annals of Statistics, 21, 867\u2013889.\n[13] Hall, P., and Ma, Y. (2007). Testing the Suitability of Polynomial Models in Errors-in-\nVariables Problems. The Annals of Statistics, 35, 2620\u20132638.\n[14] Hart, J. (1997). Nonparametric smoothing and lack-of-\ufb01t tests. Springer Series in Statis-\ntics. Springer-Verlag, New York, 1997.\n[15] Koul, H. L. and Ni, P. P. (2004). Minimum distance regression model checking. Journal\nof Statistical Planning and Inference, 119, 109-141.\n[16] Koul, H.L., and Song, W. (2009). Minimum Distance Regression Model Checking with\nBerkson Measurement Errors. The Annals of Statistics, 37, 132\u2013156.\n[17] Koul, H. L., and Song, W. (2010). Model Checking in Partial Linear Regression Models\nwith Berkson Measurement Errors. Statistica Sinica, 20, 1551\u20131579.\n35\n[18] Li, B. and Wang, S.L. (2007). On Directional Regression for Dimension Deduction.\nJournal of the American Statistical Association, 102, 997\u20131008.\n[19] Li, B., Wen, S., and Zhu, L.X. (2008). On a projective resampling method for dimension\nreduction with multivariate responses. Journal of the American Statistical Association,\n103, 1177\u20131186.\n[20] Li, B. and Yin, X.R. (2007). On Surrogate Dimension Reduction for Measurement Error\nRegression: an Invaraince Law. The Annals of Statistics 35, 2143\u20132172.\n[21] Li, B., Zha, H.Y. and Chiaromonte, F. (2005). Contour Regression: a General Approach\nto Dimension Reduction. The Annals of Statistics, 33, 1580-1616.\n[22] Li, K.C. (1991). Sliced Inverse Regression for Dimension Reduction. Journal of the\nAmerican Statistical Association, 86, 316\u2013342.\n[23] Li, K.C. (1992). On Principal Hessian Directions for Data Visualization and Dimension\nReduction: Another Application of Stein\u2019s Lemma. Journal of the American Statistical\nAssociation, 87, 1025-1039.\n[24] Lee, L.F. and Sepanski, J.H. (1995). Estimation of Linear and Nonlinear Errors-in-\nVariables Models using Validation Data. Journal of the American Statistical Association,\n90 , 130\u2013140.\n[25] Lue, H.H. (2004). Principal Hessian Directions for Regression with Measurement Error.\nBiometrika, 91, 409\u2013423.\n[26] Ser\ufb02ing, R.J. (1980). Approximation Theorems of Mathematical Statistics. John Wiley,\nNew York.\n[27] Song, W.X. (2008). Model Checking in Errors-in-Variables Regression. Journal of Mul-\ntivariate Analysis, 99, 2406\u2013443.\n[28] Song, W.X. (2009). Lack-of-\ufb01t Testing in Errors-in-Variables Regression Model with\nValidation Data. Statistical and Probability Letters, 79, 765\u2013733.\n[29] Stute, W. (1997). Nonparametric Model Checks for Regression. The Annals of Statistics,\n25, 613\u2013641.\n[30] Stute, W., Thies, G. and Zhu, L. X. (1998). Model Checks for Regression: An Innovation\nApproach.The Annals of Statistics,26, 1916\u20131934.\n[31] Stute, W., Xue, L.G., and Zhu,L.X. (2007). Empirical Likelihood Inference in Nonlinear\nErrors-in-Covariables Models with Validation Data. Journal of the American Statistical\nAssociation, 102, 332\u2013346.\n[32] Xu, W., and Zhu, L. (2014). Nonparametric Check for Partial Linear Errors-in-\nCovariables Models with Validation Data. Annals of the Institute of Statistical Mathe-\nmatics, 1\u201323.\n[33] Zhang, J., Zhu, L.P., and Zhu, L.X. (2014). Surrogate Dimension Reduction in Mea-\nsurement Error Regressions. Statistica Sinica, 24, 1341\u20131363.\n36\n[34] Zheng, J.X. (1996). A Consistent Test of Functional Form via Nonparametric Estimation\nTechnique. Journal of Econometrics, 75, 263\u2013289.\n[35] Zhu, L.X., Cui, H.J., and Ng, K.W. (2004). Some Properties of A Lack-of-Fit Test for\na Linear Errors in Variables Model. Acta Mathematicae Applicatae Sinica, 20, 533\u2013540.\n[36] Zhu, L.X., and Cui, H.J. (2005). Testing the Adequacy for a General Linear Errors-in-\nVariables Model. Statistica Sinica, 15, 1049\u20131068.\n[37] Zhu, L. X., Miao, B. Q. and Peng, H. (2006). On Sliced Inverse Regression With High-\nDimensional Covariates. Journal of the American Statistical Association, 100, 630\u2013643.\n[38] Zhu L.X., Song W.X., and Cui H.J. (2003). Testing lack-of-\ufb01t for a polynomial errors-\nin-variables model. Acta Mathematicae Applicatae Sinica, 19, 353\u2013362.\n[39] Zhu, L.P., Wang, T., Zhu, L.X., and Ferr, L. (2010a). Su\ufb03cient Dimension Reduction\nthrough Discretization-Expectation Estimation. Biometrika, 97, 295\u2013304.\n[40] Zhu, L.P., Zhu, L.X. and Feng, Z.H. (2010b). Dimension Reduction in Regressions\nthrough Cumulative Slicing Estimation. Journal of the American Statistical Association,\n105, 1455\u20131466.\n37\n0\n0.5\n1\n1.5\n2\n0\n0.05\n0.1\nTn, n=100\nc\nEmpirical Size\n0\n0.5\n1\n1.5\n2\n0\n0.05\n0.1\nTn, n=200\nc\nEmpirical Size\n0\n0.5\n1\n1.5\n2\n0\n0.05\n0.1\n\u02dcTn, n=100\nc\nEmpirical Size\n0\n0.5\n1\n1.5\n2\n0\n0.05\n0.1\n\u02dcTn, n=200\nc\nEmpirical Size\n2.5\n3\n3.5\n4\n4.5\n0\n0.05\n0.1\nT Zh\nn ,n=100\nc\nEmpirical Size\n2.5\n3\n3.5\n4\n4.5\n0\n0.05\n0.1\nT Zh\nn , n=200\nc\nEmpirical Size\n1\n1.5\n2\n2.5\n3\n0\n0.05\n0.1\nT S\nn , n=100\nc\nEmpirical Size\n1\n1.5\n2\n2.5\n3\n0\n0.05\n0.1\nT S\nn , n=200\nc\nEmpirical Size\nFigure 1: Plots for the empirical size curve against di\ufb00erent values of c in the bandwidths\nh = cn\u22121/(4+q), vN = c(N/2)\u22122/5. For model Y = \u03b2\u22a4X + \u03f5, the solid lines are with p = 2,\nq = 1 and the dash-dotted lines are with p = 8, q = 1.\n38\nTable 1. Empirical sizes and powers of Tn, T b\nn, T Zh\nn\nand T S\nn of H0 vs. H11 in Study 1.\nH11\na\np=2\np=8\np=2\np=8\n\u03bb = 4\n\u03a3 = \u03a31\n\u03a3 = \u03a31\n\u03a3 = \u03a32\n\u03a3 = \u03a32\nn=100\nn=200\nn=100\nn=200\nn=100\nn=200\nn=100\nn=200\nTn\n0\n0.0455\n0.0430\n0.0420\n0.0410\n0.0495\n0.0525\n0.0505\n0.0535\n0.1\n0.0700\n0.0860\n0.0715\n0.0835\n0.0720\n0.1155\n0.0825\n0.1580\n0.2\n0.1275\n0.2190\n0.1185\n0.2145\n0.1970\n0.4005\n0.2720\n0.6260\n0.3\n0.2360\n0.4985\n0.2185\n0.4865\n0.4245\n0.7840\n0.5630\n0.9510\n0.4\n0.4265\n0.8050\n0.3940\n0.7840\n0.6695\n0.9670\n0.8180\n0.9965\n0.5\n0.6315\n0.9570\n0.5670\n0.9295\n0.8385\n0.9975\n0.9305\n1.0000\n\u02dcTn\n0\n0.0485\n0.0520\n0.0440\n0.0525\n0.0440\n0.0510\n0.0485\n0.0460\n0.1\n0.0645\n0.0760\n0.0505\n0.0865\n0.0790\n0.1300\n0.1070\n0.1615\n0.2\n0.1130\n0.2335\n0.1230\n0.2210\n0.2010\n0.4135\n0.2720\n0.6240\n0.3\n0.2530\n0.5205\n0.2245\n0.4975\n0.4110\n0.7900\n0.5845\n0.9500\n0.4\n0.4365\n0.8055\n0.3800\n0.7980\n0.6945\n0.9720\n0.8125\n0.9930\n0.5\n0.6475\n0.9495\n0.5715\n0.9360\n0.8545\n0.9995\n0.9280\n1.0000\nT Zh\nn\n0\n0.0360\n0.0335\n0.0285\n0.0410\n0.0400\n0.0385\n0.0350\n0.0405\n0.1\n0.0525\n0.0940\n0.0420\n0.0525\n0.0735\n0.1060\n0.0615\n0.0925\n0.2\n0.1410\n0.2475\n0.0690\n0.1045\n0.2295\n0.4280\n0.1405\n0.2710\n0.3\n0.3015\n0.5780\n0.1165\n0.2230\n0.4970\n0.8385\n0.2740\n0.5715\n0.4\n0.5200\n0.8395\n0.1770\n0.3740\n0.7655\n0.9800\n0.4675\n0.8270\n0.5\n0.7105\n0.9690\n0.2875\n0.5500\n0.9065\n0.9985\n0.6190\n0.9420\nT S\nn\n0\n0.0495\n0.0570\n0.0440\n0.0340\n0.0655\n0.0595\n0.0430\n0.0425\n0.1\n0.1460\n0.2060\n0.0785\n0.1125\n0.2010\n0.3020\n0.1450\n0.2250\n0.2\n0.3615\n0.6110\n0.2030\n0.3400\n0.4895\n0.8150\n0.4015\n0.7160\n0.3\n0.6235\n0.9145\n0.3665\n0.6625\n0.8045\n0.9860\n0.7030\n0.9650\n0.4\n0.8580\n0.9870\n0.5555\n0.8820\n0.9610\n0.9990\n0.8895\n0.9975\n0.5\n0.9550\n0.9999\n0.7305\n0.9705\n0.9895\n1.0000\n0.9715\n1.0000\n39\nTable 2. Empirical sizes and powers of Tn, \u02dcTn, T Zh\nn\nand T S\nn of H0 vs. H12 in Study 1.\nH12\na\np=2\np=8\np=2\np=8\n\u03bb = 4\n\u03a3 = \u03a31\n\u03a3 = \u03a31\n\u03a3 = \u03a32\n\u03a3 = \u03a32\nn=100\nn=200\nn=100\nn=200\nn=100\nn=200\nn=100\nn=200\nTn\n0\n0.0480\n0.0555\n0.0410\n0.0440\n0.0525\n0.0465\n0.0475\n0.0410\n0.1\n0.0520\n0.1020\n0.0595\n0.0885\n0.0625\n0.0990\n0.0495\n0.0675\n0.2\n0.1315\n0.2350\n0.1258\n0.2140\n0.1340\n0.2080\n0.1075\n0.1835\n0.3\n0.2465\n0.4935\n0.2245\n0.4545\n0.2375\n0.4580\n0.1875\n0.3755\n0.4\n0.4260\n0.7585\n0.3660\n0.7250\n0.3970\n0.7020\n0.2980\n0.6045\n0.5\n0.6310\n0.9220\n0.5685\n0.9105\n0.5815\n0.8840\n0.4665\n0.8155\n\u02dcTn\n0\n0.0445\n0.0490\n0.0500\n0.0515\n0.0555\n0.0480\n0.0475\n0.0410\n0.1\n0.0705\n0.0825\n0.0625\n0.0790\n0.0635\n0.0855\n0.0695\n0.0820\n0.2\n0.1375\n0.2280\n0.1130\n0.2245\n0.1425\n0.2235\n0.1055\n0.1880\n0.3\n0.2805\n0.4830\n0.2280\n0.4630\n0.2545\n0.4335\n0.1995\n0.3615\n0.4\n0.4415\n0.7750\n0.3700\n0.7410\n0.4165\n0.7050\n0.3120\n0.6335\n0.5\n0.6315\n0.9250\n0.5875\n0.9165\n0.5705\n0.8935\n0.4650\n0.8275\nT Zh\nn\n0\n0.0330\n0.0425\n0.0300\n0.0400\n0.0390\n0.0495\n0.0420\n0.0405\n0.1\n0.0670\n0.0995\n0.0400\n0.0500\n0.0585\n0.0930\n0.0445\n0.0640\n0.2\n0.1535\n0.2520\n0.0615\n0.1065\n0.1425\n0.2340\n0.0655\n0.0975\n0.3\n0.3005\n0.5330\n0.1215\n0.2320\n0.2620\n0.4795\n0.0990\n0.1845\n0.4\n0.5000\n0.7975\n0.2040\n0.3825\n0.4590\n0.7525\n0.1630\n0.3225\n0.5\n0.7060\n0.9445\n0.3060\n0.5900\n0.6620\n0.9115\n0.2500\n0.4865\nT S\nn\n0\n0.0530\n0.0510\n0.0460\n0.0365\n0.0505\n0.0475\n0.0450\n0.0365\n0.1\n0.0100\n0.1390\n0.0715\n0.0805\n0.0855\n0.1335\n0.0580\n0.0805\n0.2\n0.2135\n0.3790\n0.1470\n0.2305\n0.1985\n0.3290\n0.1240\n0.1765\n0.3\n0.4385\n0.6930\n0.2625\n0.4995\n0.3695\n0.6185\n0.2005\n0.3680\n0.4\n0.6710\n0.9050\n0.4420\n0.7505\n0.5720\n0.8685\n0.3130\n0.5885\n0.5\n0.8375\n0.9825\n0.6265\n0.9170\n0.7670\n0.9645\n0.4890\n0.8050\n40\nTable 3. Empirical sizes and powers of Tn, \u02dcTn, T Zh\nn\nand T S\nn of H0 vs. H13 in Study 1.\nH13\na\np=2\np=8\np=2\np=8\n\u03bb = 4\n\u03a3 = \u03a31\n\u03a3 = \u03a31\n\u03a3 = \u03a32\n\u03a3 = \u03a32\nn=100\nn=200\nn=100\nn=200\nn=100\nn=200\nn=100\nn=200\nTn\n0\n0.0415\n0.0505\n0.0565\n0.0455\n0.0500\n0.0420\n0.0460\n0.0495\n0.1\n0.0770\n0.0900\n0.0725\n0.0860\n0.0665\n0.0735\n0.0595\n0.0705\n0.2\n0.1370\n0.2470\n0.1125\n0.2115\n0.1165\n0.1885\n0.0865\n0.1550\n0.3\n0.2530\n0.4430\n0.2105\n0.4130\n0.2235\n0.3920\n0.1390\n0.2980\n0.4\n0.3980\n0.6965\n0.3480\n0.6470\n0.3185\n0.6220\n0.1980\n0.4410\n0.5\n0.5395\n0.8715\n0.4515\n0.8205\n0.4425\n0.7815\n0.2810\n0.6075\n\u02dcTn\n0\n0.0455\n0.0530\n0.0585\n0.0455\n0.0475\n0.0565\n0.0500\n0.0485\n0.1\n0.0605\n0.0910\n0.0665\n0.0805\n0.0765\n0.0965\n0.0590\n0.0725\n0.2\n0.1360\n0.2420\n0.1100\n0.2240\n0.1100\n0.1980\n0.0880\n0.1570\n0.3\n0.2680\n0.4595\n0.2090\n0.4440\n0.2120\n0.4065\n0.1335\n0.2905\n0.4\n0.3750\n0.6920\n0.3365\n0.6405\n0.3375\n0.6135\n0.1910\n0.4665\n0.5\n0.5520\n0.8730\n0.4400\n0.8375\n0.4605\n0.7775\n0.2685\n0.5910\nT Zh\nn\n0\n0.0350\n0.0450\n0.0250\n0.0450\n0.0365\n0.0505\n0.0355\n0.0415\n0.1\n0.0560\n0.0875\n0.0350\n0.0410\n0.0510\n0.0610\n0.0365\n0.0445\n0.2\n0.1130\n0.2250\n0.0525\n0.0875\n0.0985\n0.1650\n0.0400\n0.0600\n0.3\n0.2215\n0.4460\n0.0795\n0.1380\n0.1705\n0.3570\n0.0580\n0.0860\n0.4\n0.3700\n0.6760\n0.1135\n0.2265\n0.3120\n0.5650\n0.0665\n0.1295\n0.5\n0.5075\n0.8410\n0.1610\n0.3225\n0.4010\n0.7330\n0.0780\n0.1650\nT S\nn\n0\n0.0570\n0.0410\n0.0405\n0.0420\n0.0560\n0.0565\n0.0440\n0.0400\n0.1\n0.0560\n0.0695\n0.0505\n0.0390\n0.0500\n0.0650\n0.0555\n0.0300\n0.2\n0.0945\n0.1305\n0.0750\n0.0945\n0.0640\n0.0840\n0.0610\n0.0380\n0.3\n0.1455\n0.2065\n0.1150\n0.1550\n0.0870\n0.0990\n0.0520\n0.0615\n0.4\n0.2030\n0.3225\n0.1550\n0.2560\n0.1120\n0.1400\n0.0625\n0.0665\n0.5\n0.2540\n0.4255\n0.1895\n0.3600\n0.1350\n0.1840\n0.0660\n0.0600\n41\nTable 4. Empirical sizes and powers of Tn and T S\nn of H0 vs. H14 and H15 in Study 2.\na\nH14\nH15\n\u03bb = 4\n\u03a3 = \u03a31\n\u03a3 = \u03a32\n\u03a3 = \u03a31\n\u03a3 = \u03a32\nn=100\nn=200\nn=100\nn=200\nn=100\nn=200\nn=100\nn=200\nTn\n0\n0.0525\n0.0470\n0.0460\n0.0485\n0.0440\n0.0450\n0.0395\n0.0460\n0.1\n0.0530\n0.0720\n0.0650\n0.0805\n0.0455\n0.0430\n0.0515\n0.0710\n0.2\n0.0780\n0.1245\n0.1130\n0.1720\n0.0700\n0.0700\n0.1175\n0.2020\n0.3\n0.1390\n0.2385\n0.1905\n0.3865\n0.0905\n0.1455\n0.1890\n0.3920\n0.4\n0.2065\n0.3660\n0.2885\n0.5860\n0.1175\n0.2490\n0.2285\n0.5200\n0.5\n0.3060\n0.5560\n0.4405\n0.7890\n0.1485\n0.3130\n0.2690\n0.6105\nT S\nn\n0\n0.0525\n0.0605\n0.0605\n0.0540\n0.0450\n0.0515\n0.0540\n0.0535\n0.1\n0.0830\n0.0970\n0.0915\n0.1155\n0.0620\n0.0545\n0.0525\n0.0490\n0.2\n0.1375\n0.2190\n0.1755\n0.3390\n0.0575\n0.0555\n0.0450\n0.0525\n0.3\n0.2310\n0.4245\n0.3575\n0.6170\n0.0485\n0.0465\n0.0590\n0.0570\n0.4\n0.3615\n0.6375\n0.5205\n0.8340\n0.0530\n0.0540\n0.0550\n0.0590\n0.5\n0.5020\n0.8040\n0.6935\n0.9410\n0.0590\n0.0515\n0.0505\n0.0410\n0\n0.5\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nH16, p=4,q=1\na\nEmpirical Size and Power\n0\n0.5\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nH17, p=4,q=2\na\nEmpirical Size and Power\n0\n0.5\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nH18, p=4,q=4\na\nEmpirical Size and Power\n0\n0.5\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nH19, p=8,q=8\na\nEmpirical Size and Power\nFigure 2: Plots of power curves over a under H16 \u2212H19 in Study 4. The solid lines are for\nTn and the dash-dotted lines are for T Zh\nn .\n42\nTable 5. Empirical sizes and powers of Tn and T (1)\nn (with small \u03bb), T (2)\nn (with large \u03bb) of H0\nvs. H11 in Study 3.\nH11\np=2\np=8\np=2\np=8\n\u03bb = 0.1\n\u03bb = 0.1\n\u03bb = 0.5\n\u03bb = 0.5\na\nN=100\nN=200\nN=100\nN=200\nN=100\nN=200\nN=100\nN=200\nTn\n0\n0.0160\n0.0255\n0.0080\n0.0120\n0.0330\n0.0420\n0.0235\n0.0295\n0.1\n0.0380\n0.0865\n0.0280\n0.0535\n0.0535\n0.0725\n0.0425\n0.0685\n0.2\n0.1710\n0.4420\n0.1305\n0.4305\n0.1245\n0.2400\n0.0970\n0.2265\n0.3\n0.4695\n0.8920\n0.4465\n0.8835\n0.2720\n0.6005\n0.2370\n0.5905\n0.4\n0.7775\n0.9935\n0.7980\n0.9930\n0.4955\n0.8990\n0.4445\n0.8765\n0.5\n0.9465\n1.0000\n0.9360\n1.0000\n0.7270\n0.9860\n0.6390\n0.9805\nT (1)\nn\n0\n0.0610\n0.0555\n0.0400\n0.0475\n0.1690\n0.1720\n0.1190\n0.1490\n0.1\n0.1135\n0.1745\n0.0885\n0.1635\n0.2175\n0.2470\n0.1745\n0.2600\n0.2\n0.3705\n0.6415\n0.3095\n0.6200\n0.3470\n0.5370\n0.3135\n0.5295\n0.3\n0.7100\n0.9680\n0.6550\n0.9595\n0.5695\n0.8410\n0.5100\n0.8165\n0.4\n0.9255\n0.9995\n0.9145\n0.9995\n0.7765\n0.9715\n0.7300\n0.9605\n0.5\n0.9865\n1.0000\n0.9860\n1.0000\n0.9115\n0.9975\n0.8625\n0.9985\n\u03bb = 4\n\u03bb = 4\n\u03bb = 8\n\u03bb = 8\na\nn=100\nn=200\nn=100\nn=200\nn=100\nn=200\nn=100\nn=200\nTn\n0\n0.0525\n0.0545\n0.0480\n0.0405\n0.0485\n0.0385\n0.0430\n0.0545\n0.1\n0.0590\n0.0960\n0.0530\n0.0925\n0.0705\n0.0850\n0.0615\n0.0780\n0.2\n0.1270\n0.2335\n0.1110\n0.2290\n0.1325\n0.2560\n0.1340\n0.2530\n0.3\n0.2645\n0.5715\n0.2525\n0.5445\n0.3045\n0.5815\n0.2550\n0.5605\n0.4\n0.4390\n0.8310\n0.4175\n0.8260\n0.5030\n0.8675\n0.4445\n0.8350\n0.5\n0.6705\n0.9700\n0.6295\n0.9665\n0.6885\n0.9690\n0.6620\n0.9690\nT (2)\nn\n0\n0.0610\n0.0620\n0.0575\n0.0495\n0.0530\n0.0420\n0.0445\n0.0575\n0.1\n0.0660\n0.1075\n0.0685\n0.1085\n0.0755\n0.0890\n0.0690\n0.0840\n0.2\n0.1410\n0.2560\n0.1310\n0.2505\n0.1450\n0.2670\n0.1430\n0.2685\n0.3\n0.2910\n0.5985\n0.2845\n0.5775\n0.3145\n0.5960\n0.2735\n0.5760\n0.4\n0.4720\n0.8510\n0.4490\n0.8445\n0.5175\n0.8760\n0.4620\n0.8415\n0.5\n0.6880\n0.9735\n0.6580\n0.9720\n0.6950\n0.9700\n0.6745\n0.9715\n43\n"}