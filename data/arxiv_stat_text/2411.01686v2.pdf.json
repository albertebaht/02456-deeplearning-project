{"text": "arXiv:2411.01686v2  [stat.ME]  5 Nov 2024\nFRODO: A novel approach to micro-macro\nmultilevel regression\nNovember 6, 2024\nShaun McDonald (Corresponding Author)1\nAlexandre Leblanc2\nSaman Muthukumarana2\nDavid Campbell3\nAbstract\nWithin the \ufb01eld of hierarchical modelling, little attention is paid to\nmicro-macro models: those in which group-level outcomes are dependent\non covariates measured at the level of individuals within groups. Although\nsuch models are perhaps underrepresented in the literature, they have ap-\nplications in economics, epidemiology, and the social sciences.\nDespite\nthe strong mathematical similarities between micro-macro and measure-\nment error models, few e\ufb00orts have been made to apply the much better-\ndeveloped methodology of the latter to the former. Here, we present a\nnew empirical Bayesian technique for micro-macro data, called FRODO\n(Functional Regression On Densities of Observations). The method jointly\ninfers group-speci\ufb01c densities for multilevel covariates and uses them as\nfunctional predictors in a functional linear regression, resulting in a model\nthat is analogous to a generalized additive model (GAM). In doing so,\nit achieves a level of generality comparable to more sophisticated meth-\nods developed for errors-in-variables models, while further leveraging the\nlarger group sizes characteristic of multilevel data to provide richer infor-\nmation about the within-group covariate distributions. After explaining\nthe hierarchical structure of FRODO, its power and versatility are demon-\nstrated on several simulated datasets, showcasing its ability to accommo-\ndate a wide variety of covariate distributions and regression models.\n1Department of Statistics and Actuarial Science, Simon Fraser Univesity, 8888 University\nDrive, Burnaby, V5A 1S6, British Columbia, Canada. drshaunmcdonald at gmail dot com\n2Department of Statistics, University of Manitoba, 66 Chancellors Circle, Winnipeg, R3T\n2N2, Manitoba, Canada. alex.leblanc at umanitoba dot ca, saman.muthukumarana at uman-\nitoba dot ca\n3School of Mathematics and Statistics, Carleton University, 1125 Colonel By Drive, Ot-\ntawa, K1S 5B6, Ontario, Canada. davecampbell at math dot carleton dot ca\n1\n1\nIntroduction\nHierarchically structured data is quite common in statistics, with a litany\nof resources and methodology available for almost every imaginable con\ufb01g-\nuration. Books such as [18] provide comprehensive reviews on the subject\nof multilevel data. For the purposes of this manuscript, it will su\ufb03ce to\nconsider data organized in a two-level hierarchy. Data will be observed\nfrom \u201cgroups\u201d, each of which is comprised of multiple \u201cindividuals\u201d, with\nvariables measured at either the group level (i.e. one measurement per\ngroup) or individual level (i.e. one measurement for each individual within\neach group).\nMultilevel data structures can be broadly categorized into two types:\nmacro-micro, in which an individual-level outcome is predicted from group-\nlevel covariates; and micro-macro, which is the opposite [39]. Although\nsubstantial attention has been given to the former structure (random\ne\ufb00ects models being one example of the macro-micro framework), the\nmicro-macro paradigm is the subject of much less discussion [14], despite\nthe occurrence of such datasets in health sciences [12], sociology [3], and\neconomics [2]. Among the relatively few papers on the subject is the one\nby Croon and van Veldhoven [11], one of the earliest papers to devise a\nmethod speci\ufb01cally for micro-macro regression. The data structure they\nconsidered (hereafter described as \u201cclassical\u201d) is as follows. Letting sub-\nscripts i and ij denote, respectively, the ith group and the jth individual\nwithin that group, the basic structure is\nYi = \u03b1 + \u03b2\u03bei + \u03b2ZZi + \u01ebi,\n(1)\nXij = \u03bei + \u03bdij.\n(2)\nAssuming group i contains ni individuals, the observed data correspond-\ning to that group is {Yi, Zi, Xi1, . . . , Xini}. In words, Yi is a group-level\nresponse variable (with regression error \u01ebi), Zi is a group-level scalar co-\nvariate, and the Xij\u2019s are individual-level measurements of some \u201clatent\u201d\nunobserved covariate \u03bei with errors \u03bdij. One can think of the model as\ntwo \u201cparts\u201d: a regression part speci\ufb01ed by (1), and a covariate observa-\ntion part speci\ufb01ed by (2). The linearity of the regression and additivity\nof the covariate error justify the \u201cclassical\u201d moniker for this structure.\nAlthough micro-macro modelling literature is relatively scarce, the\nstructure implied by (1\u20132) is essentially equivalent to (a version of) the\nmuch better-studied classical measurement error model [chapter 1 of 9,\nand references therein]. The main di\ufb00erence is conceptual: in a micro-\nmacro model, replicate covariate measurements correspond to distinct in-\ndividuals within a group; while in a measurement error model, they are\nmerely repeated noise-corrupted observations of some true explanatory\nvariable for the ith observational unit. There is another practical di\ufb00er-\nence: most measurement error literature assumes smaller ni\u2019s (the number\nof covariate measurements per group) than one tends to encounter in a\n\u201ctrue\u201d micro-macro setting.\nThe simplest approach to modelling such data is the \u201cnaive\u201d one: sim-\nply using the sample means \u00afXi = n\u22121\ni\nP\nj Xij as proxies for the latent \u03bei\u2019s.\nHowever, it is well-known [e.g. chapter 3 of 9, and references therein] that\n2\nsuch a failure to account for the uncertainty in the Xij\u2019s biases estimates\nof the regression parameters. Most notably, it creates attenuation in the\nestimate of \u03b2: letting \u02c6\u03b2 denote such an estimate, we will have |b\u03b2| < |\u03b2|,\neven as the number of groups grows asymptotically. In intuitive terms,\nthis attenuation happens because the noise in the covariates stretches the\nregression line on the horizontal axis. Thus, a plethora of both frequen-\ntist and Bayesian methods have been proposed to account for covariate\nuncertainty in a way that produces less biased estimation and inference\nfor the regression part of the model. A comprehensive review of measure-\nment error methodology is beyond the scope of this manuscript, but the\ninterested reader may refer to books such as [7, 9] or the review paper of\nSchennach [36].\nMany real-world datasets do not obey the \u201cclassical\u201d framework of\n(1\u20132) [e.g. Section 6.4 of 7, and references therein], and there are two\nways to transcend it: by replacing the linear terms \u03b2\u03bei and \u03b2ZZi in (1)\nwith arbitrary regression functions, or by generalizing the additive covari-\nate structure in (2). There are few micro-macro modelling papers with\ngeneralizations of either type, aside from the discrete variable methods of\nBennink et al. [3, 4]. Thus, we focus our attention here on the measure-\nment error literature instead. Beyond the comprehensive review sources\nmentioned above, the most generalized framework which is relevant to\nthis manuscript is that of Hu and Schennach [22]. They assumed each ob-\nservational unit i only has a single covariate measurement Xi \u223cfX|\u03be=\u03bei,\nbut also has a single replicate measurement or instrumental variable Wi,\nassumed to provide further information about \u03bei. They also allowed a\nvery general form for the regression function in which Y only depends on\nthe unobserved \u03be, with only some technical assumptions on the distribu-\ntions of Y | \u03be, X | \u03be, and \u03be | W . Their assumptions on the covariate\nstructure were very broad, requiring only that there exists a functional\nM such that M\n\u0002\nfX|\u03be (\u00b7 | \u03be)\n\u0003\n\u2261\u03be for all \u03be. Examples of such functionals\ninclude the mode, as well as any quantile or moment. With this frame-\nwork, the authors proposed a sieve likelihood estimator for the regression\nparameters and the densities of X | \u03be and \u03be | W . To our knowledge, there\nare no established Bayesian methods that accommodate this level of gen-\nerality. Sarkar et al. [35] proposed a Bayesian model which used Dirichlet\nProcess mixtures to achieve a great deal of \ufb02exibility in modelling the re-\ngression function, latent covariates, and error terms; but it still assumed\nan additive error structure of the form (2).\nNeither of the aforementioned papers (or, indeed, any measurement er-\nror literature we have seen) gives much consideration to the \u201cunit-speci\ufb01c\u201d\ncovariate distributions fX|\u03be=\u03bei \u2014 speci\ufb01cally, to any di\ufb00erences between\nthem across units.\nThis is understandable, as most errors-in-variables\nproblems have no more than a single-digit number of covariate measure-\nments available per unit, making any such di\ufb00erences irrelevant. How-\never, in an explicitly multilevel setting, there are typically many more\nindividuals per group [e.g. 11, 2], and it may be of interest to explicitly\nconsider the group-speci\ufb01c covariate densities in inference.\nWe believe\nthat the Bayesian paradigm (or, at the very least, the empirical Bayesian\nparadigm) is the most natural setting in which to achieve this.\nWith all of the above considerations in mind, our goals in this paper are\n3\nthreefold. First, we seek to develop a(n empirical) Bayesian model with\ngenerality comparable to that of Hu and Schennach [22]. Second, we wish\nto apply this model in the micro-macro multilevel setting, providing an\nability to accommodate \u201cnon-classical\u201d data structures which we believe is\nsorely missing in that literature. Our \ufb01nal goal is to leverage the data sizes\ncharacteristic of micro-macro situations in order to focus our inference\nnot only on the regression part of the model, but also the distributions of\n\u201cindividual-level\u201d covariates within each group.\nTo achieve these goals, we propose FRODO (Functional Regression\nOn Densities of Observations), a method which uni\ufb01es density estima-\ntion and functional regression in a joint empirical Bayesian model. Al-\nthough the core idea of FRODO is a fairly straightforward combination of\nwell-established methods in principle, it allows for a remarkable degree of\ngenerality in data structures, and its design proves to be far from trivial.\nBefore describing FRODO, we \ufb01rst give an overview of necessary func-\ntional data analysis concepts in Section 2. We then give a general overview\nof the FRODO model and its assumed data structure in Section 3, fol-\nlowed by a detailed description of its prior and likelihood components, as\nwell as its practical implementation. In Sections 4 and 5, we show several\nsimulation studies which demonstrate the potential generality of FRODO\nin both the regression and covariate observation parts of a micro-macro\nmodel.\n2\nA brief review of key functional data\nanalysis concepts\nBroadly speaking, functional data analysis (FDA) is a \ufb01eld of statistics in\nwhich the fundamental units of interest are (almost everywhere) smooth\nfunctions. A detailed overview of the \ufb01eld is beyond the scope of this\nmanuscript, but the interested reader may \ufb01nd one in the excellent book\nby Ramsay and Siverman [32]. Here we discuss only the concepts neces-\nsary to establish notation and motivation for FRODO.\n2.1\nScalar-on-function functional regression\nAs the name implies, scalar-on-function regression concerns the modelling\nof a real-valued univariate (or \u201cscalar\u201d) response variable with predictors\nthat are functions [32, Section 12.3]. This is achieved by using integrals\nin place of the sums which de\ufb01ne scalar regression models. For example,\nconsider a simple case in which our data are pairs {Yi, f \u2217\ni }, i = 1, . . . , N,\nwhere Yi is a real-valued (continuous) scalar response and f \u2217\ni is an almost\neverywhere continuous function on [0, 1]. For this data, a functional linear\nmodel would be of the form\nYi = \u03b1 +\nZ 1\n0\n\u03b2\u2217(x)f \u2217\ni (x)dx + \u01ebi,\n(3)\nwith i.i.d. errors \u01ebi \u223cN (0, \u03c3Y ). The coe\ufb03cient function \u03b2\u2217weighting the\nintegral is analogous to regression coe\ufb03cients in a fully scalar regression\nmodel.\n4\n2.2\nBasis function expansions\nBecause function spaces are in\ufb01nite-dimensional, a core component of\nFDA is the representation of functions of interest in \ufb01nite-dimensional\nspaces [32]. Typically, this is achieved by modelling functions as linear\ncombinations of \ufb01nitely many basis functions [32, Section 3.3]. Through-\nout this manuscript, we will use f \u2217to denote a function of interest, and\nremove the asterisk to denote a relevant basis function approximation f.\nSeveral types of functional bases exist, including those based on func-\ntional principal components, Fourier series, and splines [32]. Attention\nhere is restricted to the latter, and in particular the P-splines of Eilers\nand Marx [13]. For our purposes, it su\ufb03ces to know that a P-spline rep-\nresentation of a function f \u2217on a compact interval [a, b] has the form\nf(x) =\nK\nX\nk=1\nckBk(x).\n(4)\nHere, the basis functions Bk are splines: piecewise polynomials with sup-\nports de\ufb01ned by a set of equally-spaced \u201cknots\u201d in [a, b]. More detailed\nexplanations of splines can be found in, for example, [32] and Eilers and\nMarx [13]. In the frequentist setting, the coe\ufb03cients c = (c1, . . . , cK) can\nbe \ufb01t with a penalized likelihood method. Common penalties force f to\nadhere to desirable shapes by penalizing \u201croughness\u201d, as measured with a\nsuitable linear di\ufb00erential operator [see 32, Chapter 5]. Eilers and Marx\n[13] modi\ufb01ed this idea by instead using a penalty based on \ufb01nite di\ufb00er-\nences between coe\ufb03cients. Their penalty de\ufb01nes the notion of P-splines\nand is of the form\n\u03bb\nK\nX\nk=r+1\n\u0002\n(\u2206rc)k\u2212r\n\u00032 ,\n(5)\nfor a positive integer r, where \u2206r denotes the rth-order \ufb01nite di\ufb00erence\noperator and (\u2206rc)k\u2212r denotes the (k \u2212r)th element of the (K \u2212r)-\ndimensional vector (\u2206rc). For instance,\n\u0000\u22061c\n\u0001\n1 = c2 \u2212c1,\n\u0000\u22062c\n\u0001\n1 = c3 \u22122c2 + c1, and\n\u0000\u22063c\n\u0001\n1 = c4 \u22123c3 + 3c2 \u2212c1.\nWhen the smoothing parameter \u03bb > 0 is large, (5) dominates the penalized\nlikelihood. Eilers and Marx noted that the sum in this penalty is a good\napproximation to the rth derivative of f when the knots de\ufb01ning the spline\nbasis are equally spaced, especially for large dimensionality K. Thus, for\nlarge \u03bb the estimated f is forced to take the approximate shape of a\npolynomial of degree r \u22121.\nLang and Brezger [24] devised a Bayesian version of P-splines, based on\nthe notion that a penalized likelihood function is analogous to a posterior\ndistribution on the log scale, with the penalty term assuming the role of\nthe prior.\nThe penalty (5) is the log density of an rth-order Gaussian\n5\nrandom walk:\n(\u2206rc)k\u2212r \u223cN\n\u0012\n0,\n1\n\u221a\n2\u03bb\n\u0013\n(6)\nfor k = r, r + 1, . . . , K. Lang and Brezger [24] gave the \ufb01rst r components\nof c (which we call \u201cfree parameters\u201d in contrast with the last K \u2212r\ncomponents, whose behaviour is restricted by (6)) \ufb02at priors. However,\nwe adopt the philosophy that such priors are unreasonable because they\ngive equal weight to all values, no matter how extreme [e.g. the case\nstudy of 5], and we have also found such priors to result in extremely\npoor MCMC sampling behaviour in our models. Our priors on the free\nparameters in the various P-spline components of FRODO are described\nin Sections 3.2\u20133.3.\nAs noted by Eilers and Marx [13], one can use P-splines to model a\ndensity f \u2217by replacing f with log f in (4). The imposition of a polynomial\nshape on log f then leads to a density estimate which is close to the\nexponentiation of the corresponding polynomial. For instance, a penalty\nof order r = 3 (in either the frequentist or Bayesian setting) forces log f\ntowards a quadratic shape, and therefore the resulting density estimate\nwill be similar in shape to a Gaussian.\n3\nThe FRODO model\n3.1\nGeneral overview\nHaving reviewed the necessary functional data analysis concepts, we are\nnow ready to describe the FRODO approach to micro-macro modelling.\nAssume the data is organized into N groups, with the ith group contain-\ning ni individuals.\nIn the simplest case (assumed in the remainder of\nthis section for ease of exposition), data in the ith group consists of a\ngroup-level response variable Yi, and individual-level observations of a co-\nvariate X, (Xi1, . . . , Xini). Although we assume real-valued Gaussian Yi\u2019s\nthroughout this paper for the sake of simplicity, in principle the following\nmethodology could be extended to any response type for which general-\nized linear modelling is possible. As in Section 1, the model is comprised\nof both a regression part and a covariate observation part, but we assume\na much greater level of generality than in (1\u20132). Our only assumption for\nthe covariate density part is that, for the ith group, Xi := (Xi1, . . . , Xini)\n(where an omitted subscript means the collection of all elements across\nthat subscript) is an i.i.d. sample from an unobserved or \u201clatent\u201d group-\nspeci\ufb01c covariate density f \u2217\ni . The regression part of the model de\ufb01nes\nthe \u201cnovel\u201d idea at the core of FRODO: the use of these densities (tech-\nnically, basis expansion estimators thereof) as predictors in a functional\nlinear regression. In mathematical terms, the regression part of the model\n6\nis\nYi = \u03b1 +\nZ\n\u03b2\u2217(x)f \u2217\ni (x)dx + \u01ebi\n(7)\n= \u03b1 + E\u2217\ni [\u03b2\u2217(X)] + \u01ebi,\n(8)\n\u01ebi\ni.i.d\n\u223cN (0, \u03c3Y ) ,\nwhere E\u2217\ni [\u03b2\u2217(X)] denotes the expectation of \u03b2\u2217(X) with respect to the\ndensity f \u2217\ni . The equivalence between (7) and (8) is the key to FRODO\u2019s\nutility: by simply using densities as predictors in a functional linear regres-\nsion, the resulting model is essentially a GAM. Thus, FRODO allows for\na fully nonparametric approach to both regression functions and covariate\nstructures.\nIt must be noted that the regressor in (8), E\u2217\ni [\u03b2\u2217(X)], is the \u201cex-\npectation of the regression function\u201d.\nIn general, this is not equal to\n\u03b2\u2217(E\u2217\ni [X]) \u2014 the \u201cregression on the expectations\u201d \u2014 unless \u03b2\u2217is linear.\nUse of the latter is perhaps more \u201cstandard\u201d in the measurement error\nliterature, where it is typically assumed that the Xij\u2019s within each unit\ni are noise-corrupted versions of some \u201ctrue\u201d covariate \u03bei [see 9, or any\nstandard reference on measurement error]. Although it is not always as-\nsumed that E\u2217\ni [X] = \u03bei (e.g. the general linear error structures described\nin Section 6.4 of [7], and references therein), typically the target is estima-\ntion of \u03b2\u2217(\u03bei), possibly marginalized over an estimate of the \u201cposterior\u201d\nf\u03be|Xi [e.g. 21, 25]. We are not aware of any literature which explicitly uses\n\u201cexpectations of the regression\u201d in the way that FRODO does.\nIn the next two subsections, we detail the priors and likelihoods com-\nprising FRODO. Recall that we approximate \u03b2\u2217and the f \u2217\ni \u2019s with basis\nfunction expansions, use of which will be denoted without asterisks. In a\nslight abuse of notation, we consider the model\nYi = \u03b1 +\nZ\n\u03b2(x)fi(x)dx + \u01ebi\n(9)\n= \u03b1 + Ei [\u03b2 (X)] + \u01ebi,\n(10)\nas a proxy to (7\u20138), where \u03b2 and fi are the basis function approximations\nto their \u201ctrue\u201d counterparts (\u03b2\u2217and f \u2217\ni , respectively), and Ei denotes\nexpectation w.r.t. fi.\nTo formally justify this \u201cproxy model\u201d in terms of the \u201ctrue\u201d model\n(7), it would be necessary to replace the error terms in (9) with terms\nthat account for both the random regression error and the approximation\nerror inherent to such \ufb01nite-dimensional approximations [e.g. 19]. Such\nterms would be of the form\nei = \u01ebi +\nZ\n[\u03b2\u2217(x)f \u2217\ni (x) \u2212\u03b2(x)f(x)] dx.\n(11)\nHowever, in practice we e\ufb00ectively assume that the second term in (11)\nis negligible and do not concern ourselves with the distinction between \u01ebi\nand ei.\nBefore exploring the details of FRODO, some \ufb01nal technical and no-\ntational points are in order. We recommend standardizing the data so\n7\nthat default prior choices are weakly informative [17, Sections 2.9 and\n16.3]. Keeping with our convention of using omitted subscripts to mean\nthe collection of all elements across that subscript, let Y = (Y1, . . . , YN)\nand X = {X1, . . . , XN}, where Xi was de\ufb01ned above. In what follows, we\nwill assume that Y and X have both been standardized to have zero mean\nand unit variance. Note that for X, this standardization is \u201cmarginal\u201d,\nmeaning that it is done across groups and individuals within groups. We\nwill overload notation and use f \u2217\ni and fi to refer to, respectively, the true\ndensity and its basis function approximation for the standardized version\nof Xi. For technical reasons, it is necessary to assume that \u03b2 and the fi\u2019s\nare all de\ufb01ned on a common compact interval. This will be denoted by\n[a, b] on the standardized scale, and when it is necessary to speak about\nthe domain of the covariates on the original (unstandardized) scale, it\nwill be denoted by [a\u2032, b\u2032].\nAssuming X has been standardized as rec-\nommended above, we have a =\n\u0000a\u2032 \u2212\u00afX\n\u0001\n/\u03c3(X) and b =\n\u0000b\u2032 \u2212\u00af\nX\n\u0001\n/\u03c3(X),\nand [a\u2032, b\u2032] can be chosen so that its endpoints are (nearly) equal to the\nunscaled extrema of the covariates.\n3.2\nThe density model\nFor computational convenience \u2014 and because it su\ufb03ces for the ordinal\ncovariates which are common in real micro-macro datasets [e.g. 11, 2, 12]\n\u2014 the fi\u2019s are modelled as histograms. In practical terms, this means\nthat they are linear combinations of constant basis functions:\nfi(x) =\nK\nX\nk=1\n\u03c6ik\n1Ik(x),\n(12)\nwhere Ik is the kth equal-width subinterval [a + (k \u22121)h, a + kh) of [a, b],\n1Ik is the indicator function of Ik, and h = (b\u2212a)/K is the bin width. The\ndensity coe\ufb03cients \u03c6ik are scaled \u201csoftmax\u201d transformations of Gaussian\nrandom variables \u03b8ik, i = 1, . . . , N, k = 1, . . . , K:\n\u03c6ik =\ne\u03b8ik\nh PK\nj=1 e\u03b8ij ,\n(13)\nwhere, for all i, \u03b8i1 \u22610 to ensure identi\ufb01ability. Equivalently, we may\nsay that the \u03c6i\u2019s are (up to the scaling factor h) logistic normal random\nvectors [1].\nThe priors for the \u03b8\u2019s are chosen in order to impose useful constraints\non the behaviour of the densities. In particular, for some positive inte-\nger r we will impose an rth-order Gaussian random walk prior on \u03b8i =\n(\u03b8i1, . . . , \u03b8iK) for all i. Since the logarithms of the fi\u2019s are also piecewise\nconstant, this structure means that log fi is a Bayesian P-spline of degree\nzero, with rth-order penalty, for all i. Recall from Section 2.2 that an\nrth-order random walk prior on \u03b8i,\n(\u2206r\u03b8i)k\u2212r \u223cN (0, \u03c4i) , k \u2265r + 1\n(14)\n8\nforces log fi towards the approximate shape of a (r \u22121)th-degree polyno-\nmial when the smoothing parameter \u03c4i is small1.\nNote that (14) completely determines the conditional distributions of\n\u03b8ik for k > r given \u03b8ik for k \u2264r.\nIn the case r > 1, it remains to\nset the priors on the \u201cfree parameters\u201d \u03b8ik for 2 \u2264k \u2264r: the \u201cinitial\nvalues\u201d of the random walk. A seemingly sensible and simple choice would\nbe di\ufb00use, mean-zero, independent Gaussian priors. Unfortunately, this\nturns out not to be entirely suitable for FRODO. For r > 1, imposing fully\nindependent priors on the densities2 causes bias in the posterior mean\ncoe\ufb03cient function, b\u03b2. For instance, if the true \u03b2 is a linear function,\nthe magnitude of the slope of b\u03b2 will be biased downward, just as in the\n\u201cnaive\u201d approach to modelling described in Section 1. In the Bayesian\nhierarchical setting, this \u201cattenuation\u201d problem can be solved by putting\npriors on the covariates which introduce dependence between them and\n\u201cpool\u201d each group\u2019s measurements towards a latent group-level variable.\nThe solution here is similar.\nTo expand on this, \ufb01rst note that with \u03b8i1 \u22610 for all i, we have\n\u03b8ik = log\n\uf8eb\n\uf8ec\n\uf8edh\u22121\na+kh\nZ\na+(k\u22121)h\nfi(x)dx\n\uf8f6\n\uf8f7\n\uf8f8\u2212log\n\uf8eb\n\uf8edh\u22121\na+h\nZ\na\nfi(x)dx\n\uf8f6\n\uf8f8\n(15)\n\u2248log f \u2217\ni\n\u0012\na + h\n\u0012\nk \u22121\n2\n\u0013\u0013\n\u2212log f \u2217\ni\n\u0012\na + h\n2\n\u0013\n,\n(16)\nrecalling that f \u2217\ni is the \u201ctrue\u201d density for group i.\nSuppose f \u2217\ni is that of a N (\u03bei, \u03c3i) random variable3. This corresponds\nto the limiting case for r = 3 as \u03c4i \u21920, and it can be shown that (16) in\nthis case reduces to\n\u03b8ik \u2248h(k \u22121)\n\u03c32\ni\n\u0012\n\u03bei \u2212\n\u0012\na + kh\n2\n\u0013\u0013\n(17)\nFor r = 3, this approximation motivates our choice of priors for the\n\u201cfree parameters\u201d. For each i and k = 2, 3, we take them to be Gaussian\nwith mean given by the right side of (17) and standard deviation \u03c4i. Thus,\n\u03c4i controls fi\u2019s adherence to the limiting Gaussian shape in two respects:\nby controlling the free parameters\u2019 deviations from their means, and by\nscaling the random walk behaviour in (14).\nWe now set priors on \u03bei and \u03c3i. When the true covariate densities are\nGaussian, the structure of the data is analogous to that of the \u201cclassical\u201d\nmicro-macro model, with \u03bei being a \u201clatent group-level covariate\u201d and \u03c3i\ncontrolling the level of Gaussian noise for each group\u2019s individual-level co-\nvariate measurements. In keeping with natural choices for that setting, we\n1Henceforth, the phrase \u201csmoothing parameter\u201d will refer to the standard deviation of the\nrandom walk prior (\u03c4), instead of its precision as in Section 2.2 (where it was denoted by\n\u03bb = \u03c4 \u22122/2).\n2When discussing the model itself, we will typically write \u201cthe densities\u201d to refer to the\nhistograms fi which are actually part of the model. When it is necessary to invoke the f\u2217\ni \u2019s,\nwe will specify them as the \u201ctrue densities\u201d.\n3Assuming the covariates have been standardized as recommended in Section 3.1, most of\nf\u2217\ni \u2019s mass presumably lies in [a, b], and a < \u03bei < b.\n9\n\ufb01rst assign the \u03bei\u2019s a N (\u00b5\u03be, \u03c3\u03be) prior. Recalling that [a\u2032, b\u2032] denotes the as-\nsumed domain of the covariate densities on the original (unstandardized)\nscale, the mean \u00b5\u03be is given a N\n\u0000(a\u2032b \u2212b\u2032a)/(a\u2032 \u2212b\u2032), 15/K2\u0001\nhyperprior.\nThis corresponds to a mean-zero hyperprior on the original covariate scale,\nwith the empirically-determined standard deviation 15/K2 accounting for\nthe discretization error from approximation (16). The scale \u03c3\u03be is given\na standard half-normal prior, which will be fairly uninformative if the\nXij\u2019s have been scaled to have unit marginal variance. It will often be\nreasonable that the covariate densities are homoscedastic: \u03c3i \u2261\u03c3X for\nall i. A standard half-normal prior is a sensible choice in this case. If\none wishes to explicitly model heterogeneity, then each \u03c3i can be given its\nown half-normal prior, perhaps sharing a common scale parameter with\nits own hyperprior.\nNow, suppose f \u2217\ni is instead a (shifted) Exponential (\u03bbi) density. This\ncorresponds to the limiting case for the random walk with r = 2, and here\n(16) reduces to\n\u03b8ik = \u2212\u03bbi (k \u22121) h.\n(18)\nNote that for an exponential density, there is no discretization error, so\n(15) and (16) are equal. Thus, analogously to the r = 3 case described\nabove, when r = 2 we assume the \u201cfree parameters\u201d \u03b8i2 are Gaussian\nwith mean given by the right side of (18) and standard deviation \u03c4i. A\nnatural choice of prior for the \u201clatent rates\u201d \u03bbi is Gamma (\u03b1\u03bb, \u03b1\u03bb/\u00b5\u03bb).\nThe mean \u00b5\u03bb is given a standard half-normal prior (which should be only\nweakly informative if the covariates have been standardized), while the\nshape parameter \u03b1\u03bb is given a more di\ufb00use half-normal prior with scale\n10.\nNote that this parameterization of the Gamma in terms of shape\nand mean, rather than the more conventional shape and rate, proved\ncomputationally advantageous.\nBy de\ufb01ning the \u201cfree parameters\u201d in terms of latent group-level vari-\nables with their own hyperpriors, we introduce the necessary dependence\nand \u201cpooling\u201d to prevent bias in the regression part of the model, just as\none might do in the scalar case. For any order r, the density model is\ncompleted with priors on the smoothing parameters \u03c4i, which we take to\nbe exponentials with rates \u03b4\u22121\ni\n. The scales are assumed to be \ufb01xed data,\nchosen empirically based on heuristics and the properties of the Xij\u2019s in\nthe absence of more meaningful prior information.\nSuch choices place\nFRODO in the category of \u201cempirical Bayesian\u201d methods, but we have\nfound that sampling behaviour and posterior results can become poor\nwhen the \u03b4i\u2019s are not chosen carefully. If group sizes are moderate (ni\u2019s\nroughly between 20 and 60) and one doesn\u2019t expect any of the covariate\ndensities to deviate too seriously from the shape implied by the rth-order\nrandom walk prior, \u03b4i = 0.1 for all i seems to be a good default choice\nbased on preliminary empirical results. Smaller groups tend to require\nsmaller \u03b4i\u2019s, and it may also be advantageous to shrink them when the\nbasis dimension K is very large, especially relative to the ni\u2019s.\nFinally note that, because the densities are piecewise constant, the like-\nlihood Xi \u223cfi is equivalent to mi := (mi1, . . . , miK) \u223cMultinomial (ni, \u03c6i),\nwhere mik is the bin count |{j : Xij \u2208Ik}|. In summary, the model for the\n10\ndensities, assuming an rth-order random walk prior structure (for r \u22643),\nis\nmi \u223cMultinomial (ni, \u03c6i)\n\u03c6ik =\ne\u03b8ik\nh PK\nj=1 e\u03b8ij\n\u03b8i1 \u22610\n\u03b8i2 \u223cN (\u2212\u03bbih, \u03c4i)\n\u03bbi \u223cGamma\n\u0012\n\u03b1\u03bb, \u03b1\u03bb\n\u00b5\u03bb\n\u0013\n\u03b1\u03bb \u223cHalf-Normal(0, 10)\n\u00b5\u03bb \u223cHalf-Normal(0, 1)\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\nr = 2\n\u03b8ik \u223cN\n\u0012h(k \u22121)\n\u03c32\ni\n\u0012\n\u03bei \u2212\n\u0012\na + kh\n2\n\u0013\u0013\n, \u03c4i\n\u0013\n(k = 2, 3)\n\u03bei \u223cN (\u00b5\u03be, \u03c3\u03be)\n\u00b5\u03be \u223cN\n\u0012a\u2032b \u2212b\u2032a\na\u2032 \u2212b\u2032 , 15\nK2\n\u0013\n\u03c3\u03be \u223cHalf-Normal(0, 1)\n\u03c3X \u223cHalf-Normal(0, 1)\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\nr = 3\n(\u2206r\u03b8i)k\u2212r \u223cN (0, \u03c4i) , k > r\n\u03c4i \u223cExp(\u03b4\u22121\ni\n)\n3.3\nThe regression model\nHere we detail priors for the regression part of FRODO, the likelihood for\nwhich is de\ufb01ned by (9\u201310). Recall that we have restricted our attention\nin this manuscript to continuous real-valued responses Yi with i.i.d. errors\n\u01ebi \u223cN (0, \u03c3Y ), The following priors on \u03b1 and \u03b2 would require only minor\nchanges to accommodate more general response types (e.g. di\ufb00erent scal-\ning may be in order to ensure plausible e\ufb00ect sizes in a logistic regression;\nsee Section 16.3 of Gelman et al. [17]), and the prior on the dispersion\nparameter could easily be changed as necessary.\nThe error scale \u03c3Y is given a half-T prior with 4 degrees of freedom and\nscale 1/\n\u221a\n2, so that \u03c3Y has a prior mean of 1/\n\u221a\n2. Recalling the assump-\ntion from Section 3.1 that Y has been standardized to have unit variance,\nthis scale (in informal terms) loosely corresponds to a prior expectation\nthat roughly half of the variation in the response values is due to regres-\nsion error (assuming that the errors and regressors are independent, which\nwe do here). This seems to be a sensible approach for a \u201cdefault\u201d prior,\nunless one has prior domain knowledge which would allow for context-\nspeci\ufb01c prior beliefs about the regression error.\n11\nBoth \u03b1 and \u03b2 are given hierarchical priors with scales proportional to\n\u03c3Y . This can be shown to ensure unimodality in some penalized Bayesian\nregression models [30], and we also found that it improved sampling be-\nhaviour. The intercept \u03b1 is given a di\ufb00use N (0, 20\u03c3Y ) prior.\nWe take the coe\ufb03cient function \u03b2 to be piecewise constant, with the\nsame dimensionality K as the densities. This is quite computationally\nconvenient, as the integral in (9) then reduces to the inner product be-\ntween the coe\ufb03cients of \u03b2 and fi, scaled by the bin width h. Because\nthe functional predictors all have unit integral, adding a constant shift to\n\u03b2 does not change the model: for any c \u2208R, the model is identical if \u03b2\nand \u03b1 are replaced by \u03b2 + c and \u03b1 \u2212c, respectively. Thus, we impose the\nidenti\ufb01ability constraint E [\u03b2 (X)] :=\nR b\na bfCent(x)\u03b2(x)dx = 0, where bfCent\nis the empirical central density:\nbfCent(x) :=\nK\nX\nk=1\nPN\ni=1 mik\nPK\nl=1\nPN\ni=1 mil\n1Ik(x).\n(19)\nEssentially, bfCent is the \u201cmarginal histogram\u201d of all covariate data across\ngroups.\nPresumably, the total number of covariate observations P\ni ni\nwill be large enough in most data sets to ensure that bfCent is reasonably\n\u201csmooth\u201d, so that it is a good approximation to the \u201cmarginal\u201d covariate\ndensity (i.e. marginalized across groups) for large K. Note that we use\nthe empirical central density mainly for computational convenience: an\n\u201cinferred central density\u201d like N \u22121 P\ni fi would certainly be \u201csmoother\u201d,\nbut this would add needless complexity to the gradients used in NUTS\nwhen the empirical version is su\ufb03cient to ensure identi\ufb01ability.\nThis constraint amounts to centering the inferred regressors Ei [\u03b2(X)].\nIn practice, the constraint is achieved by de\ufb01ning a piecewise constant\nfunction\n\u03b20(x) :=\nK\nX\nk=1\n\u03b20\nk\n1Ik(x)\n(20)\nand taking \u03b2 = \u03b20 \u2212\nR bfCent\u03b20. In keeping with Bayesian functional re-\ngression approaches such as [10], we put a second-order random walk prior\non the coe\ufb03cients of \u03b20, with the \ufb01rst coe\ufb03cient set to 0 for identi\ufb01ability:\n\u03b20\n1 \u22610,\n\u03b20\n2 \u223cN (0, 20h\u03c3Y ) ,\n\u0000\u22062\u03b20\u0001\nk\u22122 \u223cN (0, \u03c4\u03b2\u03c3Y ) .\nThe smoothing parameter \u03c4\u03b2 controls the extent to which \u03b2 deviates from\nthe random-walk behaviour. As \u03c4\u03b2 \u21920, \u03b2 is forced towards a stepwise\napproximation to a straight line, and the regression model (9) is therefore\nforced towards a linear regression. In this limiting case, the \u201cslope\u201d of \u03b2,\nh\u22121\u03b20\n2, is equivalent to the regression coe\ufb03cient in a scalar linear model.\nThus, using a scale factor of 20\u03c3Y h in \u03b20\n2\u2019s prior can be considered roughly\nanalogous to placing a N (0, 20\u03c3Y ) prior on the coe\ufb03cient in the scalar\ncase, which should be reasonably di\ufb00use if the covariates have been scaled\n12\nas recommended above [e.g 40, Section 25.12 of User\u2019s Guide]. Finally,\n\u03c4\u03b2 is given an exponential prior with rate 2 (equivalently, scale 0.5). In\ncontrast to the smoothing parameters for the densities, we found that \u03c4\u03b2\ndid not require a careful selection of prior scale in order to ensure good\nmodel performance.\n3.4\nImplementation\nThe FRODO model is implemented in the Stan programming language [8],\nwhich provides exceptional power, \ufb02exibility, and e\ufb03ciency through its use\nof the No-U-Turns Sampling (NUTS) variant of Hamiltonian Monte Carlo\n[20]. For each of the below simulation studies, four parallel chains were\nrun with fairly di\ufb00use starting values, with su\ufb03ciently many sampling\niterations to ensure e\ufb00ective sample sizes of at least 450 for all parameters\n[see 17, Section 11.5]. All model runs were devoid of divergent transitions\n[40], and the overwhelming majority of parameters in all simulations had\n\u02c6R values (where \u02c6R is a diagnostic which helps to assess model convergence,\nsee Vehtari et al. [44]) below 1.01, with only a single parameter in each\nof the models of Sections 4.2 and 4.5 having a value very slightly above\nthis threshold. All of the simulation studies below were conducted using\nR [31], interfacing with Stan via the RStan package [41]. More details are\ngiven in Appendix A.\n4\nSimulation studies\nAs discussed in previous sections, FRODO is uniquely powerful in theory\nbecause it is \u201cdoubly nonparametric\u201d: it can capture arbitrary unknown\nstructures in both the covariate densities and the regression model. In\nthe following subsections, we put this to the test with a wide variety of\nsimulated datasets. We will assess FRODO\u2019s ability to harness location,\nscale, and shape information from covariate densities and use it to recover\ntrue regression relationships. In each study, FRODO will be compared to\ntwo simpler models:\n1. a \u201cnaive\u201d scalar regression model using only the sample means of the\ncovariate measurements (or of some suitable transformation thereof,\nwhere applicable); and\n2. a \u201chierarchical\u201d scalar regression model, where the form of the re-\ngression function and covariate distributions are assumed known,\nwith only the actual parameter values unknown.\nMore detail will be provided in the following subsections.\nBecause FRODO does not assume any parametric form for either the\nregression or covariate parts of the model, all that is required are choices of\nan appropriate random walk order r, dimensionality K, (unstandardized)\ndensity domain [a\u2032, b\u2032], and set of density scaling factors \u03b4 = (\u03b41, . . . , \u03b4N).\nThese choices must be made assuming that the true data-generating mech-\nanisms are not known a priori. One could use subject-speci\ufb01c domain\nknowledge if it is available. Otherwise, an \u201cempirical Bayesian\u201d approach\nbased on informal inspections of the data is acceptable, and this is the\n13\napproach we will use for all simulation studies in this manuscript. Vi-\nsual inspection of default histograms or KDE\u2019s su\ufb03ces to this end. From\na strictly Bayesian perspective on inference, one could argue that this\ndata dependence in the prior is not philosophically sound. However, an\nempirical Bayesian approach to nonparametric modelling is certainly not\nwithout precedent [e.g. 34, 42]. Serra and Krivobokova [37] devised an\nempirical Bayesian method for determining both the smoothing parame-\nter and penalty order in spline \ufb01tting; our strategy could be viewed as a\ncrude, heuristic approximation of such a method.\n4.1\nGaussian covariate densities, linear regression\nmodel\nWe begin with the \u201cclassical\u201d structure from Section 1, where the individual-\nlevel measurements within groups are Gaussian deviations from a latent\ngroup-level covariate, itself Gaussian:\n\u03bei \u223cN (0, \u03c3\u03be) ,\n(21)\nXij \u223cN (\u03bei, \u03c3X) .\n(22)\nThe regression model is also linear:\nYi = \u03b1 + \u02dc\u03b2\u03bei + \u01ebi,\n(23)\n= \u03b1 + Ei\nh\n\u02dc\u03b2X\ni\n+ \u01ebi,\n\u01ebi \u223cN (0, \u03c3Y ) .\n(24)\nNote that the second line explicitly restates the regression model in the\nform of (8), with the true regression function \u03b2\u2217(x) = \u02dc\u03b2x being a line with\nslope \u02dc\u03b2. Some clari\ufb01cation on notation is in order here. Throughout Sec-\ntions 4\u20135, \u02dc\u03b2 \u2208R will denote a scalar which determines the magnitude and\nsign of the true regression function \u03b2\u2217. In turn, recall that the (piecewise\nconstant) basis function approximation to \u03b2\u2217is denoted as \u03b2.\nThe true parameter values before standardizing4 the data as described\nin Section 3.2 are \u03c3\u03be = 2, \u03c3X = 3, \u03b1 = 0.3, \u02dc\u03b2 = 0.4, and \u03c3Y = 0.5. The\nresult is a dataset with moderate amounts of noise in both the regression\nand the covariate measurements. The number of groups is N = 275 and\neach group contains covariate samples for n = 20 individuals.\nUpon inspecting the data as recommended in the introduction to this\nsection (not shown), we \ufb01nd that an assumption of roughly Gaussian\ndensity shape (corresponding to r = 3) is reasonable for these data. Be-\ncause the densities are moderately wide but relatively close together (as\nthe between-density variability \u03c3\u03be, is somewhat smaller than the within-\ndensity variability, \u03c3X), a modest basis of size K = 10 should su\ufb03ce\nwithout substantial loss of information. For this simulated data we have\nmini,j Xij = \u221213.54922, maxi,j Xij = 10.87845, so we extend this range\nslightly by the same amount in each direction to arrive at an assumed\n4Throughout this section, all parameter values and results will be presented on the original\n(unstandardized) scale of the given data. The standardization only occurs \u201cinternally\u201d, during\nthe \ufb01tting of the FRODO model.\n14\n\u221210\n\u22125\n0\n5\n10\n\u22126\n\u22124\n\u22122\n0\n2\n4\nRegression function\nx\n\u03b2(x)\nFRODO post. mean\nFRODO P.W. 95% C.I.\nTrue function\nHierarchical post. mean\nNaive post. mean\n\u22121\n0\n1\n2\n\u22122\n\u22121\n0\n1\n2\n3\nResponse values\nPredicted responses\nActual responses\n95% P.I.\u2019s\nFigure 1: Results of FRODO applied to data with Gaussian covariates and a\nlinear regression structure. Left: the regression function estimated by FRODO,\nalongside its pointwise 95% credible region, the true function, and posterior\nmean estimates from hierarchical and naive scalar models. Right: responses \u02c6Yi\npredicted by FRODO (along with 95% prediction intervals) vs. true responses.\ndensity domain5 of [a\u2032, b\u2032] = [\u221213.67077, 11]. Finally, the default choice of\n\u03b4i = 0.1 for all i recommended in Section 3.2 is used here.\nAs stated at the beginning of this section, we compare FRODO to\ntwo simpler models. The \ufb01rst is simply a standard Bayesian linear re-\ngression, with (21) omitted and the group-level sample covariate means\n\u00af\nXi treated as the \u201ctrue\u201d covariates. The second is a scalar micro-macro\nBayesian regression, implemented in the \u201cobvious\u201d way: namely, (21)\u2013\n(24) are assumed to be the known form of the model, with all parameters\n(including the latent \u03bei\u2019s) unknown and inferred. Recall that the estimate\nof \u02dc\u03b2 from the \u201cnaive\u201d model will be smaller in magnitude than the \u201ctrue\u201d\nvalue, which the hierarchical scalar model will presumably recover more\ne\ufb00ectively.\nFigure 1 shows results for the regression part of the model. In the left\nplot, the stepwise estimator of the regression function is shown with its\npointwise (P.W.) 95% credible interval (C.I.). Superimposed on the plot\nare the true regression function, as well as the posterior means from the\nhierarchical and naive scalar models (both of which assume a known linear\nform for the regression unlike FRODO, which only controls adherence to\na linear regression through \u03c4\u03b2). Because the within-group variability is\nnot too much larger than the across-group variability and the sample sizes\nare reasonable, only a small amount of attenuation is caused by using the\nnaive model, so the estimated regression functions for both scalar models\nare entirely within the pointwise C.I. from FRODO. However, the \u201cslope\u201d\n5Henceforth, the \u201cassumed domain\u201d will be stated on the unstandardized scale of the\noriginal data (i.e. [a\u2032, b\u2032]), with the standardization to [a, b] left unstated.\n15\n\u221210\n\u22125\n0\n5\n10\n0.00\n0.05\n0.10\n0.15\nf10\nx\nf(x)\nPost. mean\nP.W. 95% C.I.\nTrue density\nXij\n\u221210\n\u22125\n0\n5\n10\n0.00\n0.05\n0.10\n0.15\nf 192\nx\n\u221210\n\u22125\n0\n5\n10\n0.00\n0.05\n0.10\n0.15\nf 73\nx\nFigure 2: For a selection of groups (from the data with Gaussian covariates and a\nlinear regression structure), the FRODO estimate of the group-speci\ufb01c covariate\ndensity, alongside its pointwise 95% credible regions. The true densities are\nsuperimposed as red lines, and the actual covariate samples are shown as rug\nplots.\nof the mean regression function from FRODO seems to be closer to those\nof the true function and the hierarchical scalar estimate, rather than that\nof the naive estimate. We can formalize this observation by considering\nthe secant line to the FRODO regression function which intersects it at\nthe midpoints of the \ufb01rst and last bins.\nThe slope of this line (which\nis roughly analogous to a notion of \u201cslope\u201d for the FRODO regression\nfunction) is 0.4002, whereas the slopes of the true, hierarchical scalar,\nand naive scalar regressions are 0.4, 0.4172, and 0.3678, respectively.\nAnother way to assess FRODO\u2019s ability to infer the \u201ctrue\u201d regression\n(rather than the incorrect one implied by the naive model) is by checking\nthe posterior for the regression error scale, \u03c3Y . Because of the additional\nnoise in the individual-level covariate measurements, the naive model\u2019s\nestimate for \u03c3Y will be biased upward [e.g. 9, Section 3.2.1]. Indeed, the\nposterior mean for this parameter from the naive scalar model is 0.5556\n(95% C.I. (0.5107, 0.6027)), while the posterior means from FRODO and\nthe hierarchical scalar model are 0.4930 (95% C.I. (0.4421, 0.5473)) and\n0.4904 (95% C.I. (0.4384, 0.5453)), respectively. Because the FRODO es-\ntimate is much closer to the true value of 0.5 than it is to the \u201cnaive\nestimate\u201d, we are satis\ufb01ed that we have avoided the attenuation prob-\nlem inherent in the naive model. Table 2 contains summaries of the \u03c3Y\nposteriors for every simulation study in this manuscript.\nOn the right of Figure 1, we have plotted the posterior mean predicted\nresponses \u02c6Yi against the observed responses. The shaded region is a visual\nrepresentation of 95% posterior prediction intervals (P.I.\u2019s) for each group.\nFigure 2 shows the estimated fi\u2019s, along with their pointwise 95%\nC.I.\u2019s, for the group with the smallest (left) and largest (right) \u03bei\u2019s, as well\nas the group whose \u03bei is closest to the sample mean (middle). The middle\nand right \ufb01ts are satisfactory, with the inference e\ufb00ectively capturing the\n16\ntrue covariate densities (shown in red). The left plot shows that there\nis something of a mismatch between the inferred and true densities for\nthe group with the lowest \u03bei, with the former shifted slightly too far to\nthe right.\nGiven that the model appears to perform well in all other\nrespects, this is not a signi\ufb01cant concern, especially since the rug plot\nsuggests consistency with the data. We did not observe this problem in\nother datasets generated with the same parameter values (not shown),\nand therefore assume it is simply an unfortunate quirk of this particular\ndata.\n4.2\nGaussian covariate densities, nonlinear regres-\nsion model\nHere, we test FRODO\u2019s ability to handle nonlinear regression functions.\nThe covariates adhere to the same Gaussian structure as in Section 4.1,\nbut the regression model is now quadratic:\nYi = \u03b1 + \u02dc\u03b2\n\u0000\u03be2\ni + \u03c32\nX\n\u0001\n+ \u01ebi\n= \u03b1 + Ei\nh\n\u02dc\u03b2X2i\n+ \u01ebi.\nBecause the true covariate densities all have common variance \u03c32\nX, the\ndi\ufb00erence between Ei\n\u0002\nX2\u0003\nand (Ei [X])2 is constant and can therefore be\nabsorbed into the intercept. Here, the regression function is \u03b2\u2217(x) = \u02dc\u03b2x2.\nThe same parameter values\n\u0010\n\u03c3\u03be, \u03c3X, \u03b1, \u02dc\u03b2, \u03c3Y\n\u0011\n= (2, 3, 0.3, 0.4, 0.5) and\nnumber of groups N = 275 are used as in Section 4.1 although the data is\nnot strictly the same as we used a di\ufb00erent seed for pseudorandom number\ngeneration in this study. Because the values of \u03be2 span a wider interval\nthan those of \u03be, the \u201crelative\u201d level of regression error is lower than in\nSection 4.1, since the \u201csignal\u201d is larger in scale than the \u201cnoise\u201d.\nAs before, we compare FRODO to two scalar models, one hierarchical\nand one naive. Here, however, it is assumed known in the hierarchical\nmodel that the regression is quadratic in the latent covariates \u03be, with no\nlinear term. The naive scalar model here is a GAM rather than a linear\nmodel, with the covariates taken to be the group-level sample means and\nthe unknown regression function modelled as a cubic P-spline with second-\norder penalty.\nBecause the regression function is not one-to-one, an interesting di\ufb03-\nculty arises in this framework when the group sizes ni are too small. On\nthe regression side, the distributions are unchanged if \u03bei is replaced with\n\u2212\u03bei in a given group. When ni is small and the true \u03bei is close to zero, the\navailable measurements Xi may not be informative enough to distinguish\nbetween these possibilities6. This creates multimodality in the posterior\n(for the hierarchical scalar model, and for FRODO to a somewhat lesser\nextent) with all of its associated di\ufb03culties, including poor HMC sam-\npling behaviour and posterior mean estimates that are not particularly\nmeaningful. Thus, larger group sizes are required if one wants meaningful\ninference on the covariate parameters as well as the regression parameters.\nHere, we increase the group size in the simulated data from the n = 20\nused in Section 4.1 to n = 50 for all i. The Xij\u2019s range from -13.76074 to\n17\n\u221210\n\u22125\n0\n5\n10\n15\n0\n10\n20\n30\n40\n50\nRegression function\nx\n\u03b2(x)\nFRODO post. mean\nFRODO P.W. 95% C.I.\nTrue function\nHierarchical post. mean\nNaive post. mean\n0\n5\n10\n15\n0\n5\n10\n15\nResponse values\nPredicted responses\nActual responses\n95% P.I.\u2019s\nFigure 3: Results of FRODO applied to data with Gaussian covariates and\na quadratic regression structure.\nLeft: the regression function estimated by\nFRODO, alongside its pointwise 95% credible region, the true function, and\nposterior mean estimates from hierarchical and naive scalar models.\nRight:\nresponses \u02c6Yi predicted by FRODO (along with 95% prediction intervals) vs. the\ntrue response values.\n14.0043, and we expand this range by a small amount in each direction\nfor an assumed density domain of [\u221213.80644, 14.05]. As before, we \ufb01nd\nK = 10 and \u03b4i = 0.1 \u2200i to be suitable choices here.\nResults for the regression part of the model are shown in Figure 3.\nAt \ufb01rst glance, it may appear as though the FRODO estimate of the\nregression function is too attenuated, as it is closer to the estimate from\nthe naive scalar model at the endpoints than it is to the true function and\nthe hierarchical scalar estimate. Note, however, that over 95% of the Xij\u2019s\nlie within the middle six bins, and over 95% of the true latent \u03bei\u2019s within\nthe middle four.\nIn those regions, the FRODO estimate is quite close\nto the true quadratic regression function. Towards the endpoints where\nthe Xij\u2019s are very sparse, there is much less information with which to\nestimate value of the regression function. This edge e\ufb00ect is readily seen in\nseveral examples in this manuscript by observing that the pointwise C.I.\u2019s\nfor \u03b2 are wider in regions with few covariate estimates.\nIn the linear\nexample of Section 4.1, this did not create noticeable bias in the actual\nposterior mean for \u03b2 near the endpoints. Presumably this is because \u2014\nin somewhat informal terms \u2014 the covariates in the middle of the domain\nwere su\ufb03ciently informative to constrain the posterior for \u03b2 to a linear\nshape with high probability, which results in the smoothing parameter \u03c4\u03b2\nbeing small with high probability, which, in turn, enforces a linear shape\n6This appears to also depend on the amount of covariate variability withing the group\nrelative to the size of its regression error, although it is not currently clear exactly how this\ndependence works.\n18\n\u221210\n\u22125\n0\n5\n10\n15\n0.00\n0.05\n0.10\n0.15\nf 175\nx\nf(x)\nPost. mean\nP.W. 95% C.I.\nTrue density\nXij\n\u221210\n\u22125\n0\n5\n10\n15\n0.00\n0.05\n0.10\n0.15\nf 191\nx\n\u221210\n\u22125\n0\n5\n10\n15\n0.00\n0.05\n0.10\n0.15\nf 112\nx\nFigure 4: For a selection of groups (from the data with Gaussian covariates and\na quadratic regression structure), the FRODO estimate of the group-speci\ufb01c\ncovariate density, alongside its pointwise 95% credible region. The true densities\nare superimposed as red lines, and the actual covariate samples are shown as\nrug plots.\nin \u03b2 with fairly high probability throughout the rest of the domain. In this\nexample, we do not penalize \u03b2 towards a quadratic shape \u2014 only away\nfrom a linear shape. As such, it is not surprising that the posterior for \u03b2\nis biased away from the truth near the endpoints, as neither the prior nor\nthe likelihood are very informative there. In principle, one could specify\na third-order random walk prior for \u03b2 in order to ensure a more genuinely\nquadratic shape, provided one had su\ufb03cient reason a priori to assume\nthis was an appropriate choice. However, we argue that the second-order\nrandom walk prior used here is more intuitive, as it is formulated in terms\nof deviations from a linear model. At any rate, the heightened bias and\nuncertainty in the FRODO regression function near the endpoints does\nnot create any seriously adverse consequences for the rest of the inference.\nIn particular, the FRODO posterior mean for \u03c3Y is 0.4734 (95% C.I.\n(0.3849, 0.5643)), much closer to the true value of 0.5 than the estimate\nfrom the naive model (0.8863, 95% C.I. (0.8152, 0.9669)), suggesting that\nFRODO is successfully recovering the true regression model and not the\nbiased naive version. The plot of estimate vs. true responses on the right\nof Figure 3 shows an overall good \ufb01t, although there is a small amount of\nbias in the estimates of the lowest responses.\nFigure 4 shows a sample of covariate densities, once again for the\ngroup with the smallest and largest \u03bei\u2019s, and the \u03bei closest to the sample\nmean. With larger group sizes, FRODO successfully approximates the\ntrue densities for each group shown here.\n19\n4.3\nExponential covariate densities, linear regres-\nsion model\nAlthough it is useful to model arbitrary regression functions, doing so with\nGaussian covariate distributions is a capability shared by many methods.\nIn fact, authors such as Sarkar et al. [35] have developed Bayesian methods\nwhich allow for even more general structures of the form Xij = \u03bei + \u03bdij.\nThe true advantage of FRODO lies in its ability to handle covariates that\nare not based on any kind of additive error structure. To demonstrate\nthis, here we use an exponential covariate structure:\n\u03bbi \u223cGamma (10, 10)\nXij \u223cExponential (\u03bbi) ;\nand a linear regression model\nYi = \u03b1 + \u02dc\u03b2\u03bb\u22121\ni\n+ \u01ebi\n= \u03b1 + Ei\nh\n\u02dc\u03b2X\ni\n+ \u01ebi,\nwhere we have not restated the distribution for the error variance since it\nis identical to (24) for all subsequent studies.\nIt is worth contrasting this framework with that of Section 4.1. There,\nthe true covariate densities were Gaussians with equal variances, so the\ngroup-level responses depended on their locations. With exponential co-\nvariate distributions, the linear regression model implies responses that\ninstead vary with the scales of the densities. This turns out to be a some-\nwhat challenging type of model for FRODO, due to its treatment of \u03b2 and\nthe fi\u2019s as piecewise constant functions on bins of equal width. When the\ntrue densities are exponential, for any group i it is highly probable that\nmost of the Xij\u2019s will be near 0, with a few very large measurements in\nthe groups with small rates \u03bbi. If the dimensionality (equivalently, the\nnumber of bins) K is taken too small, then the groups with large rates will\nall have estimated fi\u2019s with probability mass near one in the \ufb01rst bin, and\nmass near zero in the rest. Thus, it is necessary to use a fairly large K\nin order to capture the di\ufb00erences between these densities. However, this\nintroduces an opposing challenge due to the sparsity of large Xij\u2019s: near\nthe right end of the domain, many of the bins will not contain any covari-\nate measurements, so there is little information with which to estimate\nthe densities \u2014 and therefore, the regression function \u2014 in that region.\nIn summary, when the density scales di\ufb00er to this extent, the \u201cresolution\u201d\nof the data varies throughout the domain.\nThe use of unequal-width bins would perhaps mitigate this problem,\nbut recall from Section 2.2 that the P-spline constructions used here\nare predicated on an assumption of equally-spaced \u201cknots\u201d (which, with\nsplines of degree zero, are simply the bin endpoints). Without these, the\nunaltered \ufb01nite-di\ufb00erence penalties on the coe\ufb03cients no longer serve as\napproximations to derivatives of a suitable order. It then becomes non-\ntrivial to penalize the fi\u2019s towards some predetermined \u201csmooth\u201d shape,\nalthough Li and Cao [26] proposed a method of modifying the P-spline\npenalty in the presence of uneven knots.\nWe do not pursue this here,\n20\n0\n5\n10\n15\n\u221215\n\u221210\n\u22125\n0\nRegression function\nx\n\u03b2(x)\nFRODO post. mean\nFRODO P.W. 95% C.I.\nTrue function\nHierarchical post. mean\nNaive post. mean\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n\u22122.5\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\nResponse values\nPredicted responses\nActual responses\n95% P.I.\u2019s\nFigure 5: Results of FRODO applied to data with exponential covariates and a\nlinear regression structure. Left: the regression function estimated by FRODO,\nalongside its pointwise 95% credible region, the true function, and posterior\nmean estimates from hierarchical and naive scalar models. Right: responses \u02c6Yi\npredicted by FRODO (along with 95% prediction intervals) vs. the true response\nvalues.\nacknowledging that FRODO in its current state has slightly more di\ufb03-\nculty using scale information in the covariate densities than it does using\nlocation or shape information.\nFor this dataset (N = 200 groups, each of size n = 50), we use param-\neter values\n\u0010\n\u03b1, \u02dc\u03b2, \u03c3Y\n\u0011\n= (0.1, \u22120.9, 0.1). A preliminary visual inspection\nof KDE\u2019s or histograms (not shown) of the covariate data \u2014 and the ob-\nservation that they are all strictly positive and highly concentrated near\nzero \u2014 justi\ufb01es a random walk prior of order r = 2 on the densities. In\norder to capture the \u201chigh-resolution\u201d di\ufb00erences between covariate mea-\nsurements near zero as described above, we use a moderately large basis\nof size K = 20. With no reason to suspect severe deviations from this\nshape we once again set \u03b4i = 0.1 for all groups. The observed covariates\nrange from 1.3232 \u00d7 10\u22124 to 16.3810. Zero is a natural choice for the left\nendpoint of the assumed domain, and because there are so few large val-\nues, we simply take the right endpoint to be the overall sample maximum\n16.3810.\nThe regression results in the left plot of Figure 5 represent the most\nsigni\ufb01cant example of the phenomenon discussed in Section 4.2; namely,\nthe heightened uncertainty in the regression function in regions where\ncovariate measurements are sparse. Here, 99.73% of the observed Xij\u2019s\nlie in the left half of the domain, while all of the latent \u03bb\u22121\ni\n\u2019s lie within\nthe \ufb01rst 3 bins. Thus, the pointwise 95% credible interval for \u03b2 is quite\nnarrow near zero \u2014 where most of the covariates are concentrated \u2014\nand becomes signi\ufb01cantly wider moving from left to right. Once again,\n21\n0\n5\n10\n15\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nf 186\nx\nf(x)\nPost. mean\nP.W. 95% C.I.\nTrue density\nXij\n0\n5\n10\n15\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nf 106\nx\n0\n5\n10\n15\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nf 28\nx\nFigure 6: For a selection of groups (from the data with exponential covariates\nand a linear regression structure), the FRODO estimate of the group-speci\ufb01c\ncovariate density, alongside its pointwise 95% credible region. The true densities\nare superimposed as red lines, and the actual covariate samples are shown as\nrug plots.\nwe compare FRODO to two scalar models: a naive linear regression using\nthe of the \u00af\nXi\u2019s as \ufb01xed covariates, and a hierarchical linear model in which\nthe latent \u03bbi\u2019s are jointly inferred with the regression parameters. Once\nagain, the estimated regression function from the hierarchical model is\nvery close to the true function, and the FRODO estimate approximates it\nquite well. Some attenuation bias occurs in the right half of the domain,\nbut because all of the covariate densities have such small mass in this\nregion, this does not seem to adversely a\ufb00ect the regression inference in\nany other signi\ufb01cant way. Indeed, the right plot of Figure 5 shows that\nthe predicted responses closely align with the true Yi\u2019s.\nAs in previous studies, we compare inferred and true covariate densities\nfor multiple groups in Figure 6. FRODO appears to do a good job of\ncapturing the true densities for small, moderate, and large \u03bbi\u2019s, although\nwith no real deviations from the shape imposed by the random walk prior,\nthis is perhaps not surprising.\n4.4\nBeta covariate densities, linear regression model\nIn the following two sections, we demonstrate FRODO\u2019s ability to cap-\nture regression relationships that are encapsulated in the shapes of the\ncovariate densities, rather than their locations or scales.\nWhereas the\ncovariate densities in preceding examples were governed by group-level\nlatent parameters which were random themselves, here those parameters\nare deterministic, allowing us to better control the range of shapes we see.\nIn particular, for this section we take \u03be = (\u03be1, . . . , \u03beN) to be a mesh of\nequally-spaced points from 1/10 to 9/10, and\nXij \u223cBeta (\u03bei, 1 \u2212\u03bei) .\n22\nThe regression model is\nYi = \u03b1 + \u02dc\u03b2\u03bei + \u01ebi\n= \u03b1 + Ei\nh\n\u02dc\u03b2X\ni\n+ \u01ebi.\nThe true densities f \u2217\ni are bimodal for all i, with peaks at 0 and 1 and\nminima at 1/2. For small i with \u03bei < 1 \u2212\u03bei, the peak on the left is wider\nthan the one on the right, so fi is skewed towards 0 and Ei [X] < 1/2. The\nopposite is true for large i, and for i near N/2 the densities are roughly\nsymmetric.\nFor this simulation, we use N = 250 groups. Because the beta densities\nhave relatively low variance (for the parameter values used here, all of\nthem have variance below 1/8), we use relatively small groups of size\nni = 15 for all i, so that the di\ufb00erence between the \u201ctrue\u201d and \u201cnaive\u201d\nregression functions is more pronounced7. The true regression parameters\nare\n\u0010\n\u03b1, \u02dc\u03b2, \u03c3Y\n\u0011\n= (0.2, 1, 0.05).\nUpon inspection of the available covariate data, one would see that\nall covariate measurements are constrained to the unit interval, with the\nminimum and maximum measurements being extremely close to 0 and\n1, respectively. Thus, [a\u2032, b\u2032] = [0, 1] is a sensible choice for the assumed\ndomain. Quick visual assessment of KDE or histogram estimates for the\ngroup-speci\ufb01c covariate densities reveals that they are neither Gaussian\nnor exponential.\nThis observation, combined with the strong evidence\nthat the densities are supported only on the unit interval, may lead one\nto believe that the covariates within each group are, indeed, roughly Beta-\ndistributed. This justi\ufb01es a random walk prior of order r = 1 on the densi-\nties, for which the limiting shape is a uniform distribution. Note, however,\nthat unlike the examples above for which we used second- and third-order\nrandom walk priors, here the limiting behaviour is unique, in the sense\nthat there is only one uniform density on the chosen domain. Thus, if all\ngroups had small smoothing parameter scales \u03b4i (corresponding to a prior\nassumption that no severe deviations from the limiting shape occurred),\nthe FRODO estimates of the covariate densities all would be nearly identi-\ncal, thereby suppressing the di\ufb00erences between groups and compromising\nthe model\u2019s ability to extract meaningful regression information. With\nan assumed \ufb01rst-order random walk prior, one should therefore expect\nthat the covariate densities will exhibit larger deviations from the limit-\ning shape than they would in a situation where r > 1 was appropriate\n(especially since a bimodal shape will be apparent for at least some of the\ngroups upon preliminary visual inspection). Thus, rather than the default\n\u03b4i = 0.1 used in previous examples, here we take \u03b4i = 1 for all groups.\nFinally, since several groups have most of their covariate measurements\nnear the endpoints (necessitating bins which are narrow enough to cap-\nture di\ufb00erences in densities within these regions), we use K = 12 bins:\nmore than the 10 used in the Gaussian examples, but less than the 20\nused in Section 4.3 since we do not have enough covariate measurements\n7With large groups, the \u201cnaive\u201d regression with group-level covariate sample means would\nbe quite close to the true model, making it di\ufb03cult to tell which one FRODO was capturing.\n23\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nRegression function\nx\n\u03b2(x)\nFRODO post. mean\nFRODO P.W. 95% C.I.\nTrue function\nHierarchical post. mean\nNaive post. mean\n0.4\n0.6\n0.8\n1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nResponse values\nPredicted responses\nActual responses\n95% P.I.\u2019s\nFigure 7: Results of FRODO applied to data with beta-distributed covariates\nand a linear regression structure. Left: the regression function estimated by\nFRODO, alongside its pointwise 95% credible region, the true function, and\nposterior mean estimates from hierarchical and naive scalar models.\nRight:\nresponses \u02c6Yi predicted by FRODO (along with 95% prediction intervals) vs. the\ntrue response values.\nper group to support such a large number of bins (especially since \u201crough-\nness\u201d, or deviation from the random walk shape, is penalized less severely\nhere).\nOnce again, the regression component of the model is visualized in\nFigure 7, alongside posterior mean estimates from naive and hierarchi-\ncal scalar models In contrast to previous datasets, here there are more\ncovariate measurements at each endpoint of the domain than there are\nin the middle, leading to a slight \u201cbulge\u201d in the pointwise 95% credi-\nble interval around 0.5.\nHowever, each bin is relatively well-populated\nwith observations, compared to the large di\ufb00erences in concentration seen\nin previous examples. It is visually obvious that FRODO captures the\ntrue regression function and not the naive one. The plot of predicted vs.\ntrue responses on the right of Figure 7 provides further con\ufb01rmation that\nFRODO\u2019s regression inference is satisfactory here.\nFigure 8 shows that FRODO has more di\ufb03culty inferring the true den-\nsities here than for previous examples. Although the asymmetrical shapes\nfor \u03bei\u2019s near 0.1 or 0.9 are captured, the steep curvature of the true den-\nsities near the endpoints in these cases results in them being near the\nedges of the model\u2019s pointwise 95% credible intervals \u2014 if not excluded\naltogether \u2014 in these regions.\nFrom the middle plot, we see that the\nmodel imposes a somewhat excessive degree of uniformity on the nearly-\nsymmetric densities for which \u03bei is near 0.5.\nThese di\ufb03culties are not\nsurprising: given the small group sizes and the fairly large values used for\nK and the \u03b4i\u2019s, neither the prior nor the likelihood make very strong im-\n24\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n2\n4\n6\n8\n10\nf1\nx\nf(x)\nPost. mean\nP.W. 95% C.I.\nTrue density\nXij\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n2\n4\n6\n8\n10\nf 125\nx\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n2\n4\n6\n8\n10\nf 250\nx\nFigure 8: For a selection of groups (from the data with beta-distributed covari-\nates and linear regression structure), the FRODO estimate of the group-speci\ufb01c\ncovariate density, alongside its pointwise 95% credible region. The true densities\nare superimposed as red lines, and the actual covariate samples are shown as\nrug plots.\nplications about the density shapes. Aside from collecting more covariate\nmeasurements for each group (i.e. strengthening the likelihood), the only\nother possible mitigation for this would be to strengthen the prior: either\nby using smaller \u03b4i\u2019s to more strictly enforce the uniform shape, or by\nusing a smaller K to reduce the dimensionality of the problem. However,\nas discussed above, both of these options would result in an obfuscation\nof any information that does exist in the available covariate data. Thus,\nthe most prudent choice seems to be accepting that FRODO\u2019s density in-\nference in this example is necessarily limited to some degree. Fortunately,\nthis limitation does not adversely a\ufb00ect any of the inference on the regres-\nsion side of the model. Furthermore, despite the relative \u201croughness\u201d of\nthe FRODO density estimates8, they are certainly improvements over, say,\n\u201craw\u201d histograms (corresponding to \u03b4i \u2192\u221e), for which the low amount\nof covariate data would result in even less interpretable shapes.\n4.5\nBeta covariate densities, nonlinear regression\nmodel\nAlthough the previous example shows that FRODO can extract relation-\nships based on the shapes of covariate densities, the regression model itself\nstill ultimately depended only on the means of the covariate measure-\nments. The non-additive structure of the Xij\u2019s would pose a challenge\nfor many established multilevel methods, but it is conceivable that one\ncould devise a nonparametric, hierarchical Bayesian method which jointly\n8Note that this is an inherent di\ufb03culty in any dataset for which the \ufb01rst-order random\nwalk prior is justi\ufb01ed, because imposing smoothness in this case is inseparable from forcing\nall of the densities towards being identical.\n25\ninferred the Ei [X]\u2019s while using them to recover the correct regression\nparameters, subverting the need for full functional regression on the den-\nsities. When the regression is not linear, this may not be the case. Thus,\nin this section we combine a nonadditive covariate structure with a nonlin-\near regression model to demonstrate the full generality of FRODO. Once\nagain \u03be is a mesh of equally-spaced points, this time from 1/10 to 2, and\nXij \u223cBeta (\u03bei, \u03bei) ,\nYi = \u03b1 + \u02dc\u03b2\n\u0012\n1 +\n1\n2\u03bei + 1\n\u0013\n+ \u01ebi\n= \u03b1 + Ei\n\"\n4\u02dc\u03b2\n\u0012\nX \u22121\n2\n\u00132#\n+ \u01ebi.\n(25)\nHere, the regression function is \u03b2\u2217(x) = 4\u02dc\u03b2(x \u22121/2)2. The f \u2217\ni \u2019s are all\nsymmetric: bimodal and U-shaped for i near 1, roughly uniform for i\nnear N/2, and peaked at 1/2 for i near N. For positive \u02dc\u03b2, the expected\nresponse Ei [Y ] is higher for \u201cmore bimodal\u201d covariate densities and lower\nfor \u201cmore unimodal\u201d ones. The regression is therefore entirely dependent\non the shapes of the densities, not their locations or scales. Furthermore,\nbecause the densities are all symmetric it holds that E\u2217\ni [X] = 1/2 for all\ni. Thus, any modelling approach targeting \u03b2 (Ei [X]) (\u201cregression on the\nexpectaton\u201d) will be unsuitable here9, as opposed to FRODO with its use\nof \u201cthe expectation of the regression\u201d, Ei [\u03b2 (X)]. In every aspect, this\nparticular data structure is decidedly \u201cnon-classical\u201d, and FRODO seems\nuniquely well-suited to handle such a structure.\nBecause the true covariate densities all have expectation equal to 1/2,\nthe regression function is actually not unique: indeed, when the f \u2217\ni \u2019s are\nall symmetric Beta densities, (25) is equivalent to \u03b1 + E\nh\n4\u02dc\u03b2X2i\n+ \u01ebi, up\nto a term which is constant with respect to i.\nThis does not seem to\nbe a problem in practice, however: even when HMC chains are explicitly\ninitialized such that \u03b2 is close to the latter form, they converge to a\nposterior which is consistent with (25). We conjecture that the FRODO\nposterior concentrates around the form of the regression function with\n\u201clowest error\u201d: empirically, we observed that the within-group sample\nmeans of (Xij \u22121/2)2 values provide much more accurate estimates of\ntheir population analogues than the within-group sample means of the\nX2\nij\u2019s.\nFor this example, we simulated a dataset with N = 250 groups, each\ncontaining n = 60 covariate measurements. The true regression parame-\nters were\n\u0010\n\u03b1, \u02dc\u03b2, \u03c3Y\n\u0011\n= (0.7, 1, 0.1). As in Section 4.4, the observed range\nof the covariate measurements provides strong evidence that [0, 1] is a\ngood choice for the assumed density domain. Here, the range of shapes\n9In theory, one could invoke a measurement error method with more general assumptions\non the covariate structure. Recall from 1 that the frequentist approach of Hu and Schennach\n[22] described in Section 1 assumed a general functional mapping the f\u2217\ni \u2019s to the \u03bei\u2019s. Although\nhigher-order moments should be permissible under their assumptions, the authors required\na known functional. Thus, even with their level of generality it would still be necessary to\nassume quadratic regression a priori.\n26\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n\u22120.5\n0.0\n0.5\n1.0\nRegression function\nx\n\u03b2(x)\nFRODO post. mean\nFRODO P.W. 95% C.I.\nTrue function\nHierarchical post. mean\nNaive post. mean\n2.0\n2.2\n2.4\n2.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\nResponse values\nPredicted responses\nActual responses\n95% P.I.\u2019s\nFigure 9: Results of FRODO applied to data with beta-distributed covariate\ndata and a quadratic regression structure. Left: the regression function esti-\nmated by FRODO, alongside its pointwise 95% credible region, the true func-\ntion, and posterior mean estimates from hierarchical and naive scalar models.\nRight: responses \u02c6Yi predicted by FRODO (along with 95% prediction intervals)\nvs. the true response values.\nin preliminary histograms or KDE\u2019s (from bimodal, to roughly uniform,\nto unimodal) gives further justi\ufb01cation for a random walk prior of order\nr = 1. As in the previous section, we take \u03b4i = 1 for all i to allow a\ngreater degree of deviation from the limiting (uniform) shape of the prior.\nBecause the data is highly concentrated near the endpoints for the groups\nwhose \u03be-values are low (even moreso than in Section 4.4\u2019s dataset), we use\na basis of size K = 15.\nDue to the aforementioned uselessness of methods involving \u201cregres-\nsion on expectations\u201d here, constructing scalar models to compare with\nFRODO is nontrivial. We cannot use a \u201cnaive GAM\u201d as we did for the\nGaussian quadratic model in Section 4.2. There, Ei\n\u0002\nX2\u0003\nand (Ei [X])2\ndi\ufb00ered by a constant, but this is not the case here. Thus, the naive scalar\nmodel we use for comparisons is somewhat contrived: a linear regression\nmodel, using the within-group sample means of the (Xij \u22121/2)2 values\nas covariates. As always, the hierarchical scalar model assumes the true\nforms of the regression function and covariate densities are all known,\njointly inferring the \u03bei\u2019s and all regression parameters.\nBecause of the relatively large group sizes, and the fact that the\nquadratic form of the regression function was assumed known in both\nscalar models, the naive model does not su\ufb00er from any appreciable at-\ntenuation bias.\nAs shown on the left of Figure 9, both it and the hi-\nerarchical scalar model approximate the true regression function almost\nperfectly. Some bias is apparent in the FRODO estimate, particularly\nnear the vertex at 1/2, but its pointwise 95% credible interval almost\n27\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\n6\nf1\nx\nf(x)\nPost. mean\nP.W. 95% C.I.\nTrue density\nXij\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\n6\nf 126\nx\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\n6\nf 250\nx\nFigure 10: For a selection of groups (from the data with beta-distributed co-\nvariates and quadratic regression structure), the FRODO estimate of the group-\nspeci\ufb01c covariate density, alongside its pointwise 95% credible region. The true\ndensities are superimposed as red lines, and the actual covariate samples are\nshown as rug plots.\ncompletely captures the true function. On the right side of Figure 9, we\nsee a moderate \u201cclumping\u201d of predicted responses just over 2.0, where the\nvariability in the actual Yi\u2019s exceeds that of the mean predictions from\nFRODO. These values correspond to groups with \u03be-values near 1 (i.e.\nthose whose true covariate densities f \u2217\ni are close to uniform). For this\ndataset, it appears that FRODO has a small amount of di\ufb03culty captur-\ning small shape di\ufb00erences between nearly-uniform densities. Note also\nthat a few groups have posterior 95% prediction intervals which exclude\ntheir observed responses, although it seems reasonable to attribute this to\nmere random chance given the large number of groups. In any case, the\noverall \ufb01t appears largely satisfactory, especially considering that the true\nforms of the regression function and covariate densities are not known a\npriori.\nFigure 10 shows that FRODO roughly captures all three types of den-\nsity shapes present in this data, although some excess noise and bias is\nevident in the posterior estimates.\nThis is particularly evident for the\nunimodal density in the right plot.\nAlthough the true density is fully\ncontained in the pointwise 95% credible interval, the posterior mean is\nperhaps somewhat too \ufb02at. The true unimodal densities in this dataset\ncertainly di\ufb00er more subtly from the uniform shape than the bimodal ones\n(contrast the true density in the left plot of Figure 10 with that on the\nright) \u2014 since the prior on densities here is structured only in terms of\n\u201cdeviations from uniformity\u201d, this slight de\ufb01ciency is not entirely unex-\npected. As in Section 4.4, some of the excess noise in the density inference\nis an unavoidable consequence of the larger values of K and \u03b4 necessary\nto capture the shapes and \ufb01ne structure of the true densities with the\n\ufb01rst-order prior.\n28\n5\nExtended simulation study:\nFRODO\nwith varying group sizes and a group-level\ncovariate\nAs a \ufb01nal \u201capplication\u201d of FRODO, we recreate the simulated data con-\nsidered by Croon and van Veldhoven [11]. This is very much a \u201cclassi-\ncal\u201d model, with Gaussian covariate data and a linear regression function\nmuch like the one considered in Section 4.1.\nHowever, there are three\nunique features here which were absent from the \u201ctoy\u201d examples explored\nabove. First (recalling the notation of (21\u201324)), the parameter values are\n\u0010\n\u03c3\u03be, \u03c3X, \u03b1, \u02dc\u03b2, \u03c3Y\n\u0011\n=\n\u00001, 3, 0.3, 0.3,\n\u221a\n0.35\n\u0001\n: not only is the within-group\nvariability of the Xij\u2019s much greater than the between-group variability\nof the true \u03bei\u2019s, but the regression error is also quite high, accounting\nfor just under 65% of the variability in the Yi\u2019s.\nOverall, the amount\nof \u201csignal\u201d in the data \u2014 at both the covariate and regression levels \u2014\nis low relative to the amount of noise. Second, there are varying group\nsizes, some of which are quite small: out of N = 100 groups, roughly 50%\n(randomly selected with probability 1/2) contain ni = 10 covariate mea-\nsurements, and the rest contain ni = 40. Finally, the actual regression\nmodel is altered from the basic FRODO form considered thus far, with\nthe inclusion of a \u201cscalar\u201d group-level covariate Z as in (1):\nYi = \u03b1 + \u02dc\u03b2\u03bei + \u03b2ZZi + \u01ebi.\n(26)\nThe covariate values Zi are generated from a standard Normal distribu-\ntion, independently of \u03be, and are treated as \ufb01xed observations.\nIt is straightforward to extend FRODO to accommodate for Z by\nputting a N (0, 20\u03c3Y ) prior on \u03b2Z, conditionally independent from the\nprior for \u03b2 (which still denotes the regression function corresponding to the\ngroup-speci\ufb01c densities of the Xij\u2019s). We use a third-order random walk\nprior on the fi\u2019s with K = 10 bins as in Section 4.1, since the available\ndata gives no reason to suspect that \ufb01ner structures need to be captured.\nDue to the relatively small amount of covariate measurements, we simply\ntake the assumed domain [a\u2032, b\u2032] to be the range of observed Xij-values,\nwhich in this case is [\u221212.0365, 11.2258]. For the groups of size ni = 40,\nthe default smoothing prior scale choice \u03b4i is appropriate, but with only\nni = 10 observations in the smaller groups, a tighter prior is necessary to\nensure posterior density estimates with useful shape information. Thus,\nwe set \u03b4i = 0.05 for the small groups.\nThe actual method proposed by Croon and van Veldhoven [11] for\nmicro-macro modelling is frequentist and involves a stepwise estimation\nprocedure. An R implementation exists [27], but here we are only inter-\nested in comparing FRODO to analogous scalar Bayesian methods. Thus,\nas in the studies of Section 4 we compare it to both a naive and hierar-\nchical scalar model, trivially extended to accommodate Z and place a\nprior on \u03b2Z. These results are shown in the left plot of 11. Note how\nmuch wider the pointwise 95% credible interval is \u2014 particularly near\nthe endpoints \u2014 than the one in the similar model of Figure 1, owing\nto the higher noise and smaller amount of available covariate data here.\n29\n\u221210\n\u22125\n0\n5\n10\n\u22126\n\u22124\n\u22122\n0\n2\n4\nRegression function\nx\n\u03b2(x)\nFRODO post. mean\nFRODO P.W. 95% C.I.\nTrue function\nHierarchical post. mean\nNaive post. mean\n\u22120.5\n0.0\n0.5\n1.0\n\u22122\n\u22121\n0\n1\n2\nResponse values\nPredicted responses\nActual responses\n95% P.I.\u2019s\nFigure 11: Results of FRODO applied to data with Gaussian covariates, a linear\nregression structure, and an additional group-level scalar covariate. Left: the\nregression function for the multilevel covariate estimated by FRODO, alongside\nits pointwise 95% credible region, the true function, and posterior mean esti-\nmates from hierarchical and naive scalar models. Right: responses \u02c6Yi predicted\nby FRODO (along with 95% prediction intervals) vs. the true response values.\nIt appears that the posterior for FRODO has concentrated somewhere\nin between the true and naive regressions. Indeed, FRODO\u2019s posterior\nmean for \u03c3Y is 0.5987 (95% C.I. (0.5182, 0.6972)), in contrast with 0.5857\nfrom the hierarchical scalar model (95% C.I. (0.5053, 0.6834)) and 0.6130\nfrom the naive scalar model (95% C.I. (0.5370, 0.70234)). Given that the\ndataset is fairly small and high in noise, it is perhaps unsurprising that\nFRODO struggles more than it did in previous studies. However, this\nseems to be a problem of variability, not of bias: other simulated datasets\nwith the exact same parameters, group sizes, and number of groups re-\nsulted in FRODO estimates with di\ufb00ering amounts of attenuation (not\nshown). Even the scalar hierarchical model proved quite variable with\nother datasets, as its estimate of the regression function did not always\nalign as closely with the true function as it does here. Although the high\ndegree of noise in the right plot of Figure 11 may appear troubling, this\nis re\ufb02ective of the actual amount of noise in the data: a plot of predicted\nvs. actual responses from a frequentist multiple linear regression using the\ntrue \u03bei\u2019s appears similar.\nThe usual density plots are shown in Figure 12. Note that the group\nin the left plot contains 40 individuals, and the other two contain only\n10.\nIt is intuitive that the smaller groups would have wider pointwise\ncredible intervals for their densities (on further inspection, this pattern\nalso seemed to hold for other groups not shown here), although it is some-\nwhat noteworthy that the smaller \u03b4i-values for these groups do not seem\nto neutralize this e\ufb00ect. Some bias in the model is evident, particularly\n30\n\u221210\n\u22125\n0\n5\n10\n0.00\n0.05\n0.10\n0.15\nf61\nx\nf(x)\nPost. mean\nP.W. 95% C.I.\nTrue density\nXij\n\u221210\n\u22125\n0\n5\n10\n0.00\n0.05\n0.10\n0.15\nf 5\nx\n\u221210\n\u22125\n0\n5\n10\n0.00\n0.05\n0.10\n0.15\nf 56\nx\nFigure 12: For a selection of groups (from the data with Gaussian covariate\ndata, a linear regression structure, and an additional group-level covariate), the\nFRODO estimate of the group-speci\ufb01c covariate density, alongside its pointwise\n95% credible region. The true densities are superimposed as red lines, and the\nactual covariate samples are shown as rug plots.\nin the middle plot, but overall the inference provided by FRODO seems\nreasonable.\n6\nDiscussion\nIn this paper, we have presented a new approach for micro-macro mod-\nelling which combines density estimation and functional data analysis into\na uni\ufb01ed hierarchical Bayesian framework. Although FRODO is relatively\nsimple in principle due to its use of step functions and only linear func-\ntional regression terms, it is deceptively powerful in its ability to use these\nelements for approximation of generalized additive models. Beyond the\ngenerality of the regression component of the model, FRODO is also quite\n\ufb02exible in terms of the individual-level covariate structures it can accom-\nmodate. Whereas many Bayesian methods for GAM\u2019s with measurement\nerror or micro-macro structure assume a Gaussian \u2014 or at the very least,\nadditive \u2014 error structure in the Xij\u2019s, FRODO has no such limitation,\nallowing for covariate densities which in\ufb02uence the group-level regression\nresponses through their locations, scales, or shapes. All that is required\nis the selection of a suitable prior structure for the densities, based on\neither prior domain knowledge, or \u2014 if this is not possible and an empir-\nical Bayesian approach is required \u2014 a preliminary heuristic examination\nof the data. Although FRODO\u2019s inference on the covariate densities is\ngenerally more accurate when the true densities adhere to the speci\ufb01ed\n\u201csmooth shape\u201d encoded in the prior, this is not a strict requirement pro-\nvided hyperparameters are chosen carefully.\nThe simulation studies conducted above show that the power and gen-\nerality of FRODO translate from theory to practice, providing reasonable\n31\ninference for a variety of data structures. However, the potential for im-\nprovements and extensions to the model is vast.\nThe most immediate\npotential for this is in the density part of the model, as described in\nSection 3.2. Here we have not considered rth-order random walk priors\nfor any integer r > 3. These would result in densities being penalized\ntowards exponentiated polynomials of higher degree: with an rth-order\nrandom walk prior, log fi(x) is close to a polynomial of degree r \u22121 when\nthe smoothing parameter \u03c4i is small. Such limiting smooth shapes corre-\nspond to generalized error distributions [43] (or folded versions thereof)\nwith shape parameter r \u22121, of which the Normal, Laplace, and uniform\ndistributions are special cases. For r > 2, the generalized error distribu-\ntion has lighter tails than a Gaussian. It is not certain how useful such\nhigher-order random walk priors would be in practice (i.e. how often one\nmight expect covariate densities to be similar to, say, an exponentiated\nquartic), but one challenge in implementing these would be determining\nsuitable distributions for the \u201cfree parameters\u201d \u03b8ik, 2 \u2264k \u2264r. Equivalent\nderivations of the type carried out for r = 2 and 3 in Section 3.2 would\nbe much more complex.\nThere is even room for generalization within the con\ufb01nes of the third-\norder (resp. second-order) random walk priors considered here. Although\nthe construction in Section 3.2 was explicitly tailored in terms of Gaussian\n(resp. exponential) distributions, in principle it could be adapted for any\ndensities whose logarithms are roughly quadratic (resp. linear) in shape.\nFolded or truncated Normal distributions may be a useful shape to ac-\ncommodate with a third-order random walk prior; one could even modify\nit to allow for densities f such that log f is approximately quadratic with\npositive leading coe\ufb03cient, not negative as for a Gaussian. This may be\nuseful for modelling \u201cU-shaped\u201d densities, such as the Beta distributions\nconsidered in Section 4.4.\nSimilarly, the second-order structure could\nbe generalized to allow for positively-sloped densities (i.e. \u201creversed\u201d ex-\nponentials), or Laplace densities whose logarithms are piecewise linear.\nFurthermore, it may be useful to combine di\ufb00ering random walk orders\nwithin the same model. For instance, the example in Section 4.5 might\nhave bene\ufb01ted if we used a third-order random walk prior for the uni-\nmodal densities (since symmetric Beta densities are close to Gaussians in\nshape for large parameter values), a \ufb01rst-order R.W. prior for the \ufb02at-\nter densities, and perhaps an \u201cinverted\u201d third-order R.W. prior for the\nU-shaped densities as suggested above.\nFurther investigation of the relationships between n, r, K, and \u03b4 would\nalso be useful, particularly how best to set the latter two in terms of the\nformer two.\nAlthough the empirical heuristic methods employed here\nworked well in practice, a more formal approach might result in bet-\nter performance and generalization. Appeals to asymptotics could guide\nderivation of mathematical relationships between the hyperparameters:\nfor instance, an expression for an \u201coptimal\u201d \u03b4i in terms of r, K, and ni,\nbased on the \u201cbig-O\u201d relationships shown by Silverman [38] to guarantee\nconvergence of penalized density estimators in the frequentist setting. The\nchoice of the assumed domain for the densities may also have an e\ufb00ect on\nany such expressions.\nOn the subject of \u201cbig-O\u201d considerations, recall that the di\ufb00erence be-\n32\ntween the \u201ctrue\u201d and \u201cproxy\u201d models ((7) and (9), respectively) was swept\nunder the rug, with a passing acknowledgement that a truly formally valid\ntreatment would consider a combined error (11) accounting for the use of\n\ufb01nite-dimensional approximation in this nonparametric setting. Further\nstudy of this approximation error \u2013 including its large-sample behaviour\n\u2013 would be of interest.\nThere is also signi\ufb01cant potential for generalizations on the regression\nside of the model. The most immediate of these is the realization of our\nproposed extension to non-Gaussian responses such as count or categorical\ndata. Just as the regression part of FRODO for the Gaussian responses\nconsidered here is nothing more than a functional linear model, allow-\ning for other response types is simply a matter of using functional GLM\nmachinery.\nPerhaps the most useful immediate extension to FRODO would be\nthe incorporation of multiple multilevel covariates.\nIndeed, many real-\nworld micro-macro datasets include several covariates measured at the\nindividual level within groups [e.g 11, 2, 12]. Of course, this would increase\nthe computational complexity of FRODO, as the number of parameters\nto infer grows roughly linearly in the number of multilevel covariates.\nNote, however, that real-world micro-macro datasets commonly include\nordinal covariates with a small number of levels [e.g. 2, 12]. Modelling the\ndistributions for these covariates requires only as many basis functions as\nthere are levels, which would mitigate computational di\ufb03culty to some\nextent in practice.\nA powerful yet challenging improvement would be modelling more\ncomplex relationships amongst covariates. For instance, Croon and van\nVeldhoven [11] considered a version of the simulation study replicated in\nSection 5 where the latent and observed group-level covariates (\u03be and Z,\nrespectively) were correlated [see also measurement error literature such\nas 33]. Accounting for dependence between multilevel and \u201cscalar\u201d co-\nvariates in FRODO will be highly nontrivial, especially if one wishes to\nmaintain \ufb02exibility in the shapes of the inferred densities. For instance, if\nthe multilevel data is Gaussian as in Section 5, the most obvious way to\naccount for correlation between \u03be and Z is to explicitly include it in the\nprior for the \u03bei\u2019s (see Section 3.2). However, we have found in practice\nthat the \u03bei\u2019s inferred by FRODO are often poor approximations for the\nactual latent group means of the Xij\u2019s, unless a Gaussian shape is heav-\nily enforced on the fi\u2019s by deliberately taking very small \u03b4i\u2019s. This was\nnot a problem for the examples in Sections 4.1 and 4.2, as the posterior\ndensity estimates ended up being close enough to the true Gaussians that\nthere were no major di\ufb03culties in the inference. If such latent density\nparameters are required more explicitly to model correlations with scalar\ncovariates, this inaccuracy may become problematic. The potential for\ndependence between distinct multilevel covariates is arguably even more\ninteresting. Presumably this would require regression on multiple integrals\nover their joint densities. However, even with the degree-zero splines con-\nsidered here, this would result in a substantial increase in computational\ncomplexity. Indeed, the number of coe\ufb03cients required to model the joint\ndensity of d multilevel covariates for a single group in this way is exponen-\ntial in d. Therefore, some type of simpli\ufb01cation would likely be required\n33\nto make interactions between multilevel covariates viable. See Lambert\nand Eilers [23] for a discussion of multivariate density estimation with\nsplines in the case of a single density.\nWe conclude by acknowledging potential shortcomings in FRODO for\nwhich there are likely no solutions, either due to the inherent properties of\nthe model or the excessive computational di\ufb03culty that would be required\nto solve them. First, one may question the use of piecewise constant basis\nfunctions, since higher-order splines would certainly result in smoother\nand better-behaved density estimates. However, recall from Section 3.3\nthat this choice was made partially for computational convenience: it\nensures that the integral of \u03b2 \u00b7 fi is simply the inner product of the two\nfunctions\u2019 coe\ufb03cients. This is no longer the case with higher-order splines,\nfor which the integrals are more complicated expressions involving prod-\nucts between neighbouring coe\ufb03cients. Beyond the heightened complex-\nity, we also found in preliminary experiments that the resulting posterior\ngeometry was extremely di\ufb03cult to navigate with NUTS. Note that these\nexperiments modelled the densities themselves with higher-degree splines,\nrequiring (among other things) a potentially costly softmax transforma-\ntion of each \u03b8i vector. The other possibility is modelling the logarithms\nof the densities with splines [e.g. 29]. These approaches are equivalent\nfor degree-zero splines, but with higher degrees the logarithmic approach\nrequires approximate numerical integration to normalize the fi\u2019s, which\nare exponentiated piecewise polynomials. These numerical integrals, in\nturn, depend on the spline coe\ufb03cients in complex ways which would likely\ncomplicate the posterior geometry even further. Thus, unless a radically\ndi\ufb00erent approach is used to \ufb01t the model, higher-order splines do not\nseem to be worth the e\ufb00ort, given the satisfactory results obtained with\npiecewise constant functions and the prevalence of ordinal covariates in\nreal-world micro-macro data.\nIn earlier experiments (not shown), we found problems with bias and\nsampling e\ufb03ciency when the within-group covariate noise was large rel-\native to either the regression noise or between-group covariate scale. In\nthe notation of the Gaussian model, problems occurred when the ni\u2019s\nwere small and \u03c3X was large relative to either \u03c3\u03be or \u03c3Y , especially when\nthe magnitude of the e\ufb00ect size \u02dc\u03b2 was large. This problem also a\ufb00ected\nhierarchical scalar models \u2014 suggesting that there is innate di\ufb03culty in\nthe posteriors induced by such datasets \u2014 but FRODO did seem slightly\nmore sensitive to it, in the sense that some parameter combinations were\nproblematic for FRODO but not for a scalar model. These problems could\nbe mitigated with di\ufb00erent prior choices such as a zero-avoiding prior for\n\u03c3Y , but these can create bias [15]. Fortunately, we suspect that the rel-\native noise levels which tend to create problems are unlikely to occur in\npractice, as they imply either extremely low-error regression models or\nhigh-error covariate groups.\nFinally, it bears repeating that FRODO only models responses in\nterms of expectations of functions of covariates: any regression relation-\nship that cannot be expressed in the form (8), or some multivariate ex-\ntension thereof, is incompatible with this methodology.\nIn particular,\nresponses which depend on the medians or modes of densities cannot be\nmodelled with FRODO, requiring other methods speci\ufb01cally suited for\n34\nthose purposes [e.g. 22]. Its current inability to model functions of expec-\ntations may also be a shortcoming. For instance, if the data in Section\n4.2 was modi\ufb01ed so that the covariate densities had unequal variances and\nthe group-level responses were proportional to these variances, FRODO\nwould not be usable due to the nonconstant (Ei [X])2 term in the regres-\nsion. One could potentially augment (8) with an \u201couter function\u201d, using\nterms of the form g (Ei [\u03b2 (X)]) with some unknown function g to be mod-\nelled with a basis function expansion. However, this would likely create a\nlitany of problems with unidenti\ufb01ability.\nDespite these challenges, we believe that FRODO\u2019s power and \ufb02ex-\nibility make it a strong addition to the \ufb01eld of micro-macro regression\nmodelling, especially as improvements and extensions are developed to\nhandle an even broader variety of data structures.\nAcknowledgements\nShaun McDonald wishes to thank the defense committee who reviewed\nthe Ph.D. thesis in which this work originally appeared.\nDeclarations\nThis manuscript is largely an adaptation of work originally included in\nShaun McDonald\u2019s doctoral thesis [28].\nCon\ufb02ict of interest\nThe authors have no con\ufb02icts of interest to report.\nFunding\nDuring the time of this work, Shaun McDonald was a\ufb03liated with Si-\nmon Fraser University and supported by an NSERC Alexander Graham\nBell Canada Graduate Scholarship. Alexandre Leblanc, Saman Muthuku-\nmarana, and David Campbell are also supported by NSERC.\nAppendix A\nDetails of the implementa-\ntion of FRODO in Stan\nThis appendix expands on the brief discussion in Section 3.4 regarding the\nStan implementation of FRODO. We explain our method of initializing\nHMC chains, detail the parameter values used in the NUTS sampler, and\nassess the sampling behaviour of the simulation studies in Sections 4\u20135.\nThe reader may also refer to our source code at https://github.com/ShaunMcDonald1021/FRODO.\nThis appendix will assume the reader is familiar with Stan, and the\nterminology associated with implementation and assessment of models\ntherein. However, references to relevant Stan documentation are included\nwhere appropriate.\n35\nA.1\nReparameterizations\nIt is known that Stan\u2019s sampling behaviour can su\ufb00er in the presence of\ndi\ufb03cult posterior geometries: for instance, when the posterior has heavy\ntails or nonlinear correlations between parameters [40, Section 25.7 of the\nUser\u2019s Guide and references therein]. Following standard advice [ibid.],\nwe use non-centered parameterizations for various parameters.\nBrie\ufb02y,\nthis means restating the target distribution (i.e. the posterior) in terms of\nparameters which do not have the same hierarchical dependence structures\nas in the original parameterization, thereby inducing a posterior geometry\nmore amenable to HMC. The parameters of interest (see Sections 3.2\u2013\n3.3) are then recovered as deterministic functions of the ones actually\nsampled. Additionally, the error variance \u03c3Y is expressed as the ratio of\na half-normal random variable and a Gamma random variable with shape\nparameter 2, neither of which have the type of heavy tails which are often\nproblematic in NUTS [40].\nThe full details of the reparameterizations\nused are described in the comments of the source code referenced above.\nA.2\nInitialization of chains\nBy default, Stan initializes all parameters uniformly in the range [\u22122, 2]\n(for positive parameters, this is done on the logarithmic scale) [8]. This\nproved to be a problem for the densities: the default scheme, in conjunc-\ntion with the reparameterizations discussed in Section A.1, almost always\nresulted in initial density estimates for which the logarithm of the poste-\nrior was in\ufb01nite. It is not known how often these were \u201cgenuine\u201d in\ufb01nities\nas opposed to mere numerical over\ufb02ow, but in either case the result is an\ninability to obtain posterior samples.\nThe problem appears to be related to the random walk structure of\nthe \u03b8i\u2019s, which are encoded into the Stan model through a linear trans-\nformation of \u201cnon-centered\u201d parameters.\nThis transformation tends to\n\u201cmagnify\u201d the variability in the default initial values to the extent that\nthe initial \u03c6i\u2019s are severely mismatched with the likelihood of their cor-\nresponding covariate data (see Section 3.2).\nThus, we use a modi\ufb01ed\ninitialization strategy based on preliminary frequentist estimates for the\nfi\u2019s. These are obtained using P-splines and Poisson regression models\nfor the bin counts in each group, as proposed by Eilers and Marx [13,\nSection 8]. These are then \u201cinverse-transformed\u201d to obtain initial values\nfor the parameterization used in Stan. A modest amount of randomness\n\u2014 Gaussian noise for the \u03b8i\u2019s, and Gamma-distributed initial values for\nthe \u03c4i\u2019s and scale components for the \u201cfree parameter\u201d means de\ufb01ned in\nSection 3.2 \u2014 is injected into the initialization to ensure that the starting\npoints of the HMC chains are reasonably di\ufb00use [16].\nA.3\nParameters of NUTS samplers\nSampling in Stan depends on several \u201cparameters10\u201d which govern the be-\nhaviour of the NUTS algorithm. Section 15.2 of the Stan Reference Man-\nual [40] explains these parameters, and further details on their implica-\ntions for sampling performance are discussed in the vignette at https://mc-stan.org/misc/warnings.html.\n36\nStudy\nMax. warmup time\nMax. sampling time\nMin. nE\ufb00\nMax. \u02c6R\n4.1\n654.280\n486.304\n572.618\n1.004\n4.2\n1350.34\n2168.11\n996.3261\n1.007\n4.3\n719.058\n1108.260\n1036.961\n1.004\n4.4\n447.584\n831.499\n857.6613\n1.007\n4.5\n509.821\n702.896\n1022.321\n1.006\n5\n99.554\n89.056\n622.911\n1.009\nTable 1: Various quantities quantifying the performance and sampling behaviour\nof FRODO, for each of the simulated datasets in Section 4.\nDue to the complexity of FRODO\u2019s posterior geometry, we found it\nnecessary to use maximum tree depths and target Metropolis acceptance\nrates which were higher than the defaults (10 and 0.8, respectively). In\nall of the simulation studies shown in Sections 4\u20135, we used a maximum\ntree depth of 12. The target acceptance rate was set to 0.99, except in\nthe studies with Gaussian covariate data, where it was set to 0.985. For\neach study, we ran four NUTS chains in parallel. Each chain was run for\n750 warmup iterations, then 1250 sampling iterations.\nA.4\nBehaviour of simulation runs\nIn Table 1, we summarize the performance of the samplers for each of the\nsix simulation studies in Section 4. Each study is denoted by the section\nin which it appears, and the following information is included for each\none.\n1. The maximum warmup time (in seconds) for any of the four chains,\n2. the maximum sampling time (in seconds) for any of the four chains,\n3. the smallest estimated [44] e\ufb00ective sample size (nE\ufb00) for any pa-\nrameter in the model, and\n4. the maximum split \u02c6R value for any parameter in the model [44].\nNote that the reported nE\ufb00(resp. \u02c6R) is the minimum (resp. maximum)\nover the actual sampled parameters and the \u201ctrue\u201d model parameters\nobtained with transformations (see Section A.1). All simulations were run\non an Acer laptop with 16 GB of RAM and four Intel i5-9300H 2.40GHz\nCPU cores.\nIn every simulation study, all parameters had e\ufb00ective sample sizes\nexceeding 450. Vehtari et al. [44] recommend a threshold of at least 550\ne\ufb00ective samples per parameter, so we are con\ufb01dent that ours are large\nenough for inference to be reasonably accurate. Vehtari et al. [44] recom-\nmends considering split \u02c6R values above a threshold of 1.01 to be indica-\ntive of convergence problems, and no values in our studies exceeded this\nthreshold.\n10Not to be confused with the \u201cparameters\u201d whose posterior is the target of inference.\nIn Section A.3, the word \u201cparameters\u201d refers only to the \u201csampling parameters\u201d discussed\ntherein.\n37\nStudy\nTrue\nFRODO\nHierarchical scalar model\nNaive scalar model\n4.1\n0.5\n0.493 (0.442, 0.547)\n0.490 (0.438, 0.545)\n0.555 (0.511, 0.603)\n4.2\n0.5\n0.473 (0.385, 0.564)\n0.479 (0.416, 0.550)\n0.886 (0.815, 0.967)\n4.3\n0.1\n0.100 (0.073, 0.128)\n0.097 (0.070, 0.123)\n0.165 (0.150, 0.182)\n4.4\n0.05\n0.065 (0.054, 0.077)\n0.055 (0.046, 0.065)\n0.089 (0.082, 0.098)\n4.5\n0.1\n0.094 (0.084, 0.105)\n0.099 (0.090, 0.109)\n0.101 (0.093, 0.111)\n5\n0.592\n0.599 (0.518, 0.697)\n0.586 (0.505, 0.684)\n0.613 (0.537, 0.704)\nTable 2: Posterior inference for \u03c3Y (the regression error) from FRODO and the\nscalar models within each simulation study. For each model, the posterior mean\nis reported, as is a 95% credible interval in parentheses. The second column\nfrom the left shows the true \u03c3Y .\nAs one would expect given FRODO\u2019s complexity, warmup and sam-\npling are several times slower than they are for the corresponding scalar\nmodels used in the simulation studies (not shown). The only study whose\ncomputation time we would consider problematic is the one from Section\n4.2, with Gaussian covariate data and a quadratic regression structure.\nIncluding warmup and sampling, the Stan model for this study took over\nan hour to run. Most of the sampling iterations for this study had larger\ntree depths than in the other studies, meaning that the number of gra-\ndient evaluations involved in sampling was roughly higher by a factor of\n2 or more [40, Section 15.2 of Reference Manual]. This is likely a conse-\nquence of posterior geometry, and the way in which the samplers adapt\nto it during warmup. However, it should be noted that we deliberately\nused a liberal number of warmup iterations, and chains appeared to have\nconverged to the \u201ctypical set\u201d [6] well before sampling began (not shown).\nNote also that the smallest e\ufb00ective sample size is over twice as large as\nthe threshold of 400 recommended by Vehtari et al. [44] Therefore, rea-\nsonable posterior inference with acceptable computation time could likely\nbe achieved by reducing the number of warmup and sampling iterations,\nprovided the latter did not induce problematic \u02c6R values.\nFinally, recall from Section 4 that estimates of the regression variance,\n\u03c3Y , are biased upward in \u201cnaive\u201d regression models, and this fact can\nbe used to check whether or not FRODO is recovering \u201ctrue\u201d regression\nrelationships. For each simulation study, Table 2 shows the true value of\n\u03c3Y , as well as the posterior mean and 95% credible interval for this pa-\nrameter from FRODO, the hierarchical scalar model, and the naive scalar\nmodel (see the beginning of Section 4). The endpoints posterior intervals\nare simply 0.025- and 0.975-quantiles from the HMC samples. For almost\nevery simulation study, the FRODO estimate for \u03c3Y is much closer to the\ntrue value than the estimate from the naive model. The sole exception is\nthe example of Section 4.5, with Beta-distributed covariates and a nonlin-\near regression structure, for which all models produce accurate estimates\nof \u03c3Y . Recall, however, that the large group sizes in this example rendered\nbias in the naive model negligible.\n38\nReferences\n[1] J. Aitchison and S. M. Shen. Logistic-normal distributions: Some\nproperties and uses. Biometrika, 67(2):261\u2013272, 1980. doi: 10.2307/\n2335470.\n[2] Adalgiso Amendola, Cristian Barra, and Roberto Zotti. Does gradu-\nate human capital production increase local economic development?\nAn instrumental variable approach. Journal of Regional Science, 60\n(5):959\u2013994, 2020. doi: 10.1111/jors.12490.\n[3] Margot Bennink, Marcel A. Croon, and Jeroen K. Vermunt. Micro-\nMacro Multilevel Analysis for Discrete Data:\nA Latent Variable\nApproach and an Application on Personal Network Data.\nSoci-\nological Methods & Research, 42(4):431\u2013457, 2013.\ndoi: 10.1177/\n0049124113500479.\n[4] Margot Bennink, Marcel A. Croon, Brigitte Kroon, and Jeroen K.\nVermunt. Micro-macro multilevel latent class models with multiple\ndiscrete individual-level variables.\nAdvances in Data Analysis and\nClassi\ufb01cation, 10:139\u2013154, 2016. doi: 10.1007/s11634-016-0234-1.\n[5] Michael\nBetancourt.\nHow\nthe\nshape\nof\na\nweakly\ninformative\nprior\na\ufb00ects\ninferences,\n2017.\nURL\nhttps://mc-stan.org/users/documentation/case-studies/weakly_informative_shapes.\n[6] Michael\nBetancourt.\nA\nConceptual\nintroduction\nto\nHamiltonian\nMonte\nCarlo,\n2017.\nPreprint\nat\nhttps://arxiv.org/abs/1701.02434v2.\n[7] John P Buonaccorsi. Measurement error: models, methods, and ap-\nplications. Chapman and Hall/CRC, Boca Raton, 2010.\n[8] Bob Carpenter, Andrew Gelman, Matthew D. Ho\ufb00man, Daniel Lee,\nBen Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo,\nPeter Li, and Allen Riddell.\nStan: A probabilistic programming\nlanguage. Journal of statistical software, 76(1), 2017. doi: 10.18637/\njss.v076.i01.\n[9] Raymond J. Carroll, David Ruppert, Leonard A. Stefanski, and\nCiprian M. Crainiceanu. Measurement Error in Nonlinear Models.\nChapman and Hall/CRC, London, 2006.\n[10] Ciprian M. Crainiceanu and A. Je\ufb00rey Goldsmith. Bayesian Func-\ntional Data Analysis Using WinBUGS. Journal Of Statistical Soft-\nware, 32(11):195, 2010. doi: 10.1103/PhysRevLett.106.107404.\n[11] Marcel A. Croon and Marc J.P.M. van Veldhoven. Predicting group-\nlevel outcome variables from variables measured at the individual\nlevel: A latent variable multilevel model. Psychological Methods, 12\n(1):45\u201357, 2007. doi: 10.1037/1082-989X.12.1.45.\n39\n[12] Oumou Salama Daouda, Mounia N. Hocine, and Laura Temime. De-\nterminants of healthcare worker turnover in intensive care units: A\nmicro-macro multilevel analysis. Plos One, 16(5):1\u201313, 2021. doi:\n10.1371/journal.pone.0251779.\n[13] Paul H. C. Eilers and Brian D. Marx. Flexible Smoothing with B-\nsplines and Penalties. Statistical Science, 11(2):89\u2013121, 1996. doi:\n10.1214/ss/1038425655.\n[14] Lynn Foster-Johnson and Je\ufb00rey D. Kromrey. Predicting group-level\noutcome variables: An empirical comparison of analysis strategies.\nBehavior Research Methods, 50(6):2461\u20132479, 2018. doi: 10.3758/\nS13428-018-1025-8.\n[15] Andrew Gelman.\nPrior distributions for variance parameters in\nhierarchical models.\nBayesian Analysis, 1(3):515\u2013533, 2006.\ndoi:\n10.1214/06-BA117A.\n[16] Andrew Gelman and Donald B. Rubin. Inference from Iterative Sim-\nulation Using Multiple Sequences. Statistical Science, 7(4):457 \u2013 472,\n1992. doi: 10.1214/ss/1177011136.\n[17] Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki\nVehtari, and Donald B. Rubin. Bayesian Data Analysis. Chapman\nand Hall/CRC, Boca Raton, third edition, 2013.\n[18] Harvey Goldstein. Multilevel statistical models. John Wiley & Sons,\nChichester, fourth edition, 2010. ISBN 9780470973400.\n[19] Bruce E. Hansen. 5Nonparametric Sieve Regression: Least Squares,\nAveraging Least Squares, and Cross-Validation.\nIn The Oxford\nHandbook of Applied Nonparametric and Semiparametric Econo-\nmetrics and Statistics. Oxford University Press, 02 2014.\nISBN\n9780199857944. doi: 10.1093/oxfordhb/9780199857944.013.008.\n[20] Matthew D. Ho\ufb00man and Andrew Gelman. The No-U-Turn Sam-\npler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.\nJournal of Machine Learning Research, 15(1):1593\u20131623, 2014.\n[21] Cheng Hsiao.\nConsistent estimation for some nonlinear errors-in-\nvariables models. Journal of Econometrics, 41(1):159\u2013185, 1989. doi:\n10.1016/0304-4076(89)90047-X.\n[22] Yingyao Hu and Susanne M. Schennach.\nInstrumental Variable\nTreatment of Nonclassical Measurement Error Models. Economet-\nrica, 76(1):195\u2013216, 2008. doi: 10.1111/j.0012-9682.2008.00823.x.\n[23] Philippe Lambert and Paul H. C. Eilers. Bayesian multi-dimensional\ndensity estimation with P-splines. In John Hinde, Jochen Einbeck,\nand John Newell, editors, Proceedings of the 21st International Work-\nshop on Statistical Modelling, pages 313\u2013320, 2006.\n40\n[24] Stefan Lang and Andreas Brezger. Bayesian P-Splines. Journal of\nComputational and Graphical Statistics, 13(1):183\u2013212, 2004.\ndoi:\n10.1198/1061860043010.\n[25] Tong Li.\nRobust and consistent estimation of nonlinear errors-in-\nvariables models. Journal of Econometrics, 110(1):1\u201326, 2002. doi:\n0.1016/S0304-4076(02)00120-3.\n[26] Zheyuan Li and Jiguo Cao. General P-Splines for Non-Uniform B-\nSplines, 2022. Preprint at https://arxiv.org/abs/2201.06808v2.\n[27] Jackson G Lu, Elizabeth Page-Gould, and Nancy R Xu.\nMicro-\nMacroMultilevel: Micro-Macro Multilevel Modeling, 2017. R package\nversion 0.4.0.\n[28] Shaun McDonald. Novel approaches to uncertainty quanti\ufb01cation in\nnonparametric settings. PhD thesis, 2022.\n[29] Finbarr O\u2019Sullivan.\nFast Computation of Fully Automated Log-\nDensity and Log-Hazard Estimators. SIAM Journal on Scienti\ufb01c and\nStatistical Computing, 9(2):363\u2013379, 1988. doi: 10.1137/0909024.\n[30] Trevor Park and George Casella. The Bayesian Lasso. Journal of\nthe American Statistical Association, 103(482):681\u2013686, 2008. doi:\n10.1198/016214508000000337.\n[31] R Core Team. R: A Language and Environment for Statistical Com-\nputing. R Foundation for Statistical Computing, Vienna, Austria,\n2020.\n[32] James O. Ramsay and Bernard W. Siverman. Functional Data Anal-\nysis. Springer Series in Statistics. Springer, New York, 2005.\n[33] Sylvia Richardson and Walter R. Gilks.\nConditional Indepen-\ndence Models for Epidemiological Studies with Covariate Measure-\nment Error.\nStatistics in Medicine, 12(18):1703\u20131722, 1993.\ndoi:\n10.1002/sim.4780121806.\n[34] Judith Rousseau.\nOn the Frequentist Properties of Bayesian\nNonparametric\nMethods.\nThe\nAnnual\nReview\nof\nStatis-\ntics\nand\nIts\nApplications,\n3:211\u2013231,\n2016.\ndoi:\n10.1146/\nannurev-statistics-041715-033523.\n[35] Abhra Sarkar, Bani K. Mallick, and Raymond J. Carroll. Bayesian\nSemiparametric Regression in the Presence of Conditionally Het-\neroscedastic Measurement and Regression Errors. Biometrics, 70(4):\n823\u2013834, 2014. doi: 10.1111/biom.12197.\n[36] Susanne M. Schennach. Recent Advances in the Measurement Error\nLiterature. Annual Review of Economics, 8:341\u2013377, 2016. doi: 10.\n1146/annurev-economics-080315-015058.\n41\n[37] Paulo Serra and Tatyana Krivobokova. Adaptive Empirical Bayesian\nSmoothing Splines. Bayesian Analysis, 12(1):219 \u2013 238, 2017. doi:\n10.1214/16-BA997.\n[38] B.W. Silverman. On the Estimation of a Probability Density Func-\ntion by the Maximum Penalized Likelihood Method. The Annals of\nStatistics, 10(3):795\u2013810, 1982. doi: 10.1214/aos/1176345872.\n[39] Tom A. B. Snijders and Roel J. Bosker.\nMultilevel analysis: An\nintroduction to basic and advanced multilevel modeling. SAGE Pub-\nlications Ltd., Los Angeles, 2011.\n[40] Stan Development Team. Stan modeling language users guide and\nreference manual, version 2.30.0, 2018.\n[41] Stan Development Team. RStan: the R interface to Stan, 2021. R\npackage version 2.21.3.\n[42] Mark A. van de Wiel, Dennis E. Te Beest, and Magnus M. M\u00a8unch.\nLearning from a lot: Empirical Bayes for high-dimensional model-\nbased prediction.\nScandinavian Journal of Statistics, 46(1):2\u201325,\n2019. doi: 10.1111/sjos.12335.\n[43] Mahesh K. Varanasi and Behnaam Aazhang. Parametric generalized\ngaussian density estimation. The Journal of the Acoustical Society of\nAmerica, 86(4):1404\u20131415, 1989. doi: 10.1121/1.398700.\n[44] Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter,\nand Paul-Christian B\u00a8urkner. Rank-normalization, folding, and lo-\ncalization:\nan improved\n\u02c6R for assessing convergence of MCMC\n(with discussion).\nBayesian analysis, 16(2):667\u2013718, 2021.\ndoi:\n10.1214/20-BA1221.\n42\n"}