{"text": "1 \n \nRCR: Robust Compound Regression for Robust \nEstimation of Errors-in-Variables Model \n \n \nHao Han and Wei Zhu \nDepartment of Applied Mathematics and Statistics, State University of New York, Stony \nBrook, NY 11794, USA \nCorresponding Email: hanhao224@gmail.com \n \n \nAbstract \nThe errors-in-variables (EIV) regression model, being more realistic by accounting for \nmeasurement errors in both the dependent and the independent variables, is widely \nadopted in applied sciences. The traditional EIV model estimators, however, can be \nhighly biased by outliers and other departures from the underlying assumptions. In this \npaper, we develop a novel nonparametric regression approach - the robust compound \nregression (RCR) analysis method for the robust estimation of EIV models. We first \nintroduce a robust and efficient estimator called least sine squares (LSS). Taking full \nadvantage of both the new LSS method and the compound regression analysis method \ndeveloped in our own group, we subsequently propose the RCR approach as a \ngeneralization of those two, which provides a robust counterpart of the entire class of the \nmaximum likelihood estimation (MLE) solutions of the EIV model, in a 1-1 mapping. \nTechnically, our approach gives users the flexibility to select from a class of RCR \nestimates the optimal one with a predefined regression efficiency criterion satisfied. \nSimulation studies and real-life examples are provided to illustrate the effectiveness of \nthe RCR approach. \n \nKey Words: Errors-in-variables, robust regression, nonparametric regression, least sine \nsquares, robust compound regression, regression efficiency \n \n \n1. Introduction \n \nConsider the linear errors-in-variables (EIV) model \n                                                            (1) \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n2 \n \nwhere the unobservable random vector        and the linearly related random variable \n        are subject to measurement errors, and we only observe X and Y instead. To \nensure model identifiability, the random errors   and each component of   are assumed to \nbe independent and follow the same distribution. The model is called functional when x is \ndeterministic, and it is called structural when x is random such that the x,   and   are \nindependent. In the present paper, we focus on the structural EIV model which is more \ngeneral. A general treatment of linear EIV models is given in Fuller (1987), and more \nrecent developments and applications are summarized in Van Huffel and Lemmerling \n(2002).  \nThe popular and widely used ordinary least squares (OLS) regression is known to be \nbiased and inconsistent in EIV models, while the orthogonal regression (OR) and \ngeometric mean regression (GMR) are better in that case. Unfortunately, these traditional \nestimators can behave poorly in the presence of outliers or violations of distributional \nassumptions (Zamar 1989, Cheng and Vanness 1992) and therefore some robust \nalternatives are needed. To overcome the drawback of orthogonal regression, Brown \n(1982) proposed robust w-estimator which was shown to be robust to outlier \ncontaminations in EIV models, especially in the small sample case. Ketellapper and \nRonner (1984) showed robust estimation procedures, especially bounded-influence \nestimators, are also applicable to the EIV model with or without contaminated \nobservational errors. Zamar (1989) proposed robust orthogonal regression M-estimators \n(ORM) and showed that it outperforms the robust ordinary regressions. Since the \nnormality assumption cannot be guaranteed in real-life problems, He and Liang (2000) \nprovided a quantile regression (QR) approach that is robust to heavier-tailed errors \ndistribution than the Gaussian. More recently, Fekri and Ruiz-Gazen (2004 and 2006) \nderived a class of bounded influence robust estimators of the parameters of the EIV \nmodel from reweighted multivariate estimators of location and scatter, and extended the \nproposed estimators to usual error variances assumptions for the simple EIV model. \nIn this paper, we present a new class of robust estimators, called the robust compound \nregression (RCR) estimators, for the linear EIV model. This class of estimators naturally \nextends the least sine squares (LSS) estimator systematically studied by Han 2011. \nUnlike the robust regression techniques discussed in the previous paragraph and other \nregression M-estimators (He et al. 1990), the presented estimators not only allow for \nerrors in both variables and departures from normality but also are robust to outliers in \neither response or explanatory variables. The rest of the paper is organized as follows. In \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n3 \n \nSection 2, we first briefly introduce the novel LSS estimator from a motivational point of \nview, which serves as a prototype of the RCR-estimators. Subsequently, we define the \nRCR-estimators and show some of its unique properties. In Section 3, by calibrating our \nRCR-estimators, we compare the new robust estimators with the usual nonrobust \nestimators on simulated data sets as well as real-life examples. Finally, conclusions are \ndrawn in Section 4 and some proofs are provided in Appendix. \n \n2. Methodology \n \n2.1 The Least Sine Squares Estimator \nThe RCR-estimators is actually a generalization of the LSS estimator we firstly introduce \nhere. The LSS estimator after its name is defined to minimize the sum of squared sine \nresiduals as follows \n      \u2211\n      \n \n   \n \u2211\n    \n \n  \n \n \n   \n                                    (2) \nwhere    denotes the angle formed by the fitted hyper-plane and the line connecting each \ncase    \n      to the dataset mean   \u0305   \u0305 . Here      \n|     \n    |\n\u221a     \n represents the \northogonal residual of the ith observation, and the Euclidean distance from the \nobservation to the mean is    \u221a     \u0305       \u0305       \u0305  . Geometrically, the \nweighted orthogonal residual as of         is exactly the sine residual as of      . \nIt can be seen that the LSS estimator is simply defined through a weighted total least-\nsquares criterion. But different from the traditional total least-squares (i.e. OR) estimator, \nthe LSS estimator as its robust counterpart not only accounts for measurement errors on \nboth dependent and independent variables, but also down-weighs observations with large \northogonal residuals. More importantly, from the insight of a close relationship between \nthe total least-squares regression and the principle component analysis (PCA) approach \n(Jackson and Dunlevy 1988), we have the following proposition. \nProposition 1. Consider a data set with dependent variable Y and independent variables \n              , the LSS estimator is equivalent to the principle component associated \nto the smallest eigenvalue of the robust weighted sample covariance matrix: \n \u0303  \n[\n \n \n \n  \u0303      \n \n \u0303      \n \u0303    \n \n \n \n \n \u0303      \n \n \u0303      \n \u0303    \n \u0303    \n \n \u0303    \n \u0303  ]\n \n \n \n \n             \n                                (3) \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n4 \n \nwhere  \u0303       \u2211\n        \n\u0305\u0305\u0305\u0305         \n\u0305\u0305\u0305\u0305\u0305 \n  \n \n \n   \n for j, k = 1, \u2026, P, and  \u0303     \u2211\n        \n\u0305\u0305\u0305\u0305      \u0305 \n  \n \n \n   \n \nfor p = 1, \u2026, P. Consequently, the LSS estimator in simple regression case yields \n \u0302    \n \u0303    \u0303   \u221a  \u0303    \u0303       \u0303  \n \n  \u0303  \n     \u0302     \u0305   \u0302    \u0305                    (4) \nThe proof of Proposition 1 is given in the Appendix. Of note, a closed-form solution of \nthe LSS estimator in multivariate case can be easily obtained by the use of singular value \ndecomposition (SVD), see Golub and Van Loan (1996). \n \n2.2 The Robust Compound Regression Estimator \nThe RCR-estimators, as a natural extension of the LSS estimator, is defined through a \nweighted compound least-squares criterion by minimizing \n      \u2211\n \n  \n         \u0302    \u2211\n        \u0302    \n \n   \n \n \n   \n                 (5) \nsubject to the constraints of \u2211\n  \n \n   \n   and     . Moreover, the generalized RCR can \nbe easily defined by replacing   \n  with   \n  in (5), where the power k can be any \nnonnegative integer. When k = 0 it becomes the objective function of ordinary compound \nregression, and further investigations are needed to seek an adequate power k for the \nrobustness-efficiency tradeoff. It is worth to mentioning that, in our previous work (Han \n2011, Han et al. 2012), we have shown that the RCR-estimator provides a robust \ncounterpart of the entire class of the MLE solutions of the EIV model, in a 1-1 mapping.  \n \n2.2.1 Parameter estimate and its asymptotic variance \nSince the objective function (5) can be further simplified as \n         \u2211\n     \u0302   \n  \n \n \n   \n \u2211\n  \n   \n \n   \n\u2211\n     \u0302   \n  \n \n \n   \n \n             (   \u2211\n  \n   \n \n   \n) \u2211\n[    \u0305 \u2211\n  (       \n\u0305\u0305\u0305\u0305)\n    \n]\n \n  \n \n \n   \n \n             (   \u2211\n  \n   \n \n   \n)   \u0303   \u2211\n\u2211\n     \u0303      \n \n   \n \n   \n  \u2211\n   \u0303    \n \n   \n                   (6) \nwe can obtain the slope vector estimate by solving the system of P equations \n      \n   \n  , \nfor p = 1, 2, \u2026, P simultaneously. Specifically, for any l in p = 1, \u2026, P, we have \n      \n   \n  \n   \n  \n ( \u0303   \u2211\n\u2211\n     \u0303      \n \n   \n \n   \n  \u2211\n   \u0303    \n \n   \n)    \n               (   \u2211\n  \n   \n \n   \n) ( \u2211\n   \u0303      \n \n   \n   \n     \u0303         \u0303    )                            (7)        \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n5 \n \nSolutions can be obtained via any standard numerical software such as MATLAB, from \nwhich we have the slope estimate  \u0302    with some regulatory conditions satisfied. \nMeanwhile, the intercept estimate can be readily calculated by  \u0302     \u0305   \u0305  \u0302   . \nObtaining asymptotic theory for the RCR-estimators in linear EIV models is a very \nchallenging task. Hence, we use a bootstrap estimator for the asymptotic covariance \nmatrix of the slope estimate  \u0302   . The bootstrap estimator can be written by \n     \u2211\n  \u0302      \u0302\u0305\n      \u0302      \u0302\u0305\n     \n \n   \n                             (8) \nwhere  \u0302     is estimated from the bth bootstrapping sample and  \u0302\u0305\n    \n \n \u2211\n \u0302    \n \n   \n. \nThe asymptotic variance of  \u0302    can be calculated in a similar manner. \n \n2.2.2 Two special members of the RCR-estimators \nIn the simple linear regression situation, the objective function (5) can be simplified as \n       \u2211           \n  \n \n \n      \u2211\n    \n    \n   \n  \n \n \n                            (9) \nwhere      . Straight-forward derivations show that the slope estimate would satisfy \n                            \n\u0303        \n\u0303            \n\u0303           \n\u0303                            (10) \nObviously at two extremes of \u03b3, we have  \u0302    \n   \n\u0303\n   \n\u0303  and  \u0302    \n   \n\u0303\n   \n\u0303 respectively. \nProposition 2. The compound parameter \u03b3 is a monotonic function of the slope estimate \n \u0302 , when the range of  \u0302  is between  \u0302    and  \u0302   . \nProof   Equation (10) can be rewritten as \n     \n \n    \n   \n\u0303   \u0302    \n\u0303\n \u0302    \n\u0303    \n\u0303\n \n \u0302   \n \u0302     \u0302 \n \u0302   \u0302     \n \n \u0302                                   (11) \nNote that      is an increasing function of        . The Cauchy-Schwarz inequality \nensures that  \u0303  \n   \u0303   \u0303  , hence, if  \u0303     we have  \u0302     \u0302   \u0302     , and thus \n      as  \u0302  , i.e. \u03b3 is a decreasing function of  \u0302 ; otherwise, if  \u0303     we have \n \u0302     \u0302   \u0302     , and thus       as  \u0302  , i.e. \u03b3 is an increasing function of  \u0302 . \nAnalogous to the definition of GMR in simple linear regression, the robust geometric \nmean (RGM) estimator is defined to minimize  \n \n        \u2211     \u0302       \u0302  \n  \n \n \n, and we can \neasily obtain its corresponding slope estimate  \u0302           \u0303   \u221a\n \u0303  \n \u0303  .  \nProposition 3. In the simple linear regression case, the LSS and the RGM estimators are \nboth special members of the RCR-estimators.  \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n6 \n \nProof   Based on the Cauchy-Schwarz inequality, it is easy to show that \n \u0303  \n \u0303    \u0302    \n \u0303    \u0303   \u221a  \u0303    \u0303       \u0303  \n \n  \u0303  \n \n \u0303  \n \u0303   given  \u0303    , and \n \u0303  \n \u0303    \u0302    \n \u0303  \n \u0303   given  \u0303    . \nSimilarly, we have \n \u0303  \n \u0303    \u0302    \u221a\n \u0303  \n \u0303   \n \u0303  \n \u0303   when  \u0303    , and \n \u0303  \n \u0303    \u0302    \n \u221a\n \u0303  \n \u0303   \n \u0303  \n \u0303   when  \u0303    . Since both  \u0302    and  \u0302    are in between  \u0302    and  \u0302   , \nand by Proposition 2 there exists a monotonic relationship between \u03b3 and  \u0302 , we can \nconclude that both      and      are between 0 and 1. Therefore, the RCR-estimators \ninclude the LSS and the RGM estimators as special members.  \n \n2.2.3 Robust regression efficiency \nThe robust regression efficiency with respect to each regression variable is defined as the \nratio of the minimized to the observed weighted squared-residuals along the \ncorresponding coordinate direction. Mathematically, the regression efficiency with \nrespect to the dependent variable Y and each independent variable     for p = 1, \u2026, P are \nformulated as follows. \n   \n   \u2211\n     \u0302   \n  \n \n \n   \n\u2211\n     \u0302   \n  \n \n \n   \n \n       \u0302     \n\u2211\n     \u0302   \n  \n \n \n   \n                                     (12) \n     \n   \u2211\n      \u0302    \n  \n \n \n   \n\u2211\n      \u0302    \n  \n \n \n   \n \n       \u0302     \n\u2211\n      \u0302    \n  \n \n \n   \n                                 (13) \nProposition 4. The robust geometric mean estimator will always yield the equal    and \n  , and furthermore the maximum sum of robust regression efficiencies      . \nThe proof is given in the Appendix. We hereby point out that the definition of regression \nefficiency is not only designed for the proposed robust compound regression analysis, but \nalso can be utilized to compare the performance of different regression estimators given a \nreal-data set. There are two ways to build up the corresponding goodness-of-fit criterion. \nOne is the Frequentist approach, where all the regression variables are considered equally \nimportant due to the lack of prior knowledge, and our suggestion is the higher the sum of \nregression efficiencies    \u2211\n    \n \n   \n the better the regression estimate is. The other is \nthe Bayesian approach. For example in simple regression, if one has the prior information \nof keeping the prediction accuracy of Y above a certain threshold, the best estimate will \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n7 \n \nmaximize    subject to      given        . Symmetrically, we can reverse the order \nof the importance for X and Y and obtain the best fit for Y subject to     . In addition, \nusers have the flexibility to impose constraints on more than one variable and optimize \nthe rest in the multivariate scenario. \n \n3. Simulation and Example \n \n3.1 Simulation Studies \nIn this section, we assess the performance of our proposals in two steps. We firstly \ncalibrate the whole class of RCR-estimators via the regression efficiency plot diagnosis, \nand then we focus on the comparison of the two special members \u2013 the LSS and the \nRGM estimators with the traditional nonrobust estimators. Simulation studies are \nconducted on a simple linear EIV model Y = 1 + y + \u03b5 and X = x + \u03b4 with \u03be ~ N(0, 100), \u03b4 \n~ N(0, \u03b6\u03b4\n2) and \u03b5 ~ N(0, \u03b6\u03b5\n2). In addition, we denote \u03bb = \u03b62\n\u03b5/\u03b62\n\u03b4 as the ratio of the error \nvariances. \n \n3.1.1 Calibration of the RCR-estimators by regression efficiency plot \nThe robust regression efficiencies of the class of RCR-estimates can be graphically \nsummarized through the RCR efficiency plot as shown in Figure 1. In the following \nscenarios, without loss of generality, we assume the error variances \u03b62\n\u03b4 = \u03b62\n\u03b5 = 10 to be \nequal that is \u03bb = 1. The model characteristics and corresponding results are summarized \nas follows: \n(a): There are 5% outliers in the Y direction with contaminated errors \u03b5c ~ N(50, \u03b62\n\u03b5). \nAs can be seen from Figure 1(a), if let both eY and eX to be no less than 0.75, we \nhave                 with  \u0302 varies from 0.98 to 1.05, where the cross point \ncorresponds to the RGM estimate  \u0302 = 1.002. The LSS estimate  \u0302 = 1.004 with \ncorresponding \u03b3 = 0.496 falls inside the selected \u03b3 interval. \n(b): There are 5% leverage points in the X direction with contaminated errors \u03b4c ~ \nN(50, \u03b62\n\u03b4). From Figure 1(b), we can choose                 to make both eY \nand eX to be at least 0.775, and the cross point corresponds to the RGM estimate \n \u0302 = 1.012. The LSS estimate  \u0302 = 1.020 with corresponding \u03b3 = 0.481 is still \ninside the selected interval. \n(c): Let \u03be follows U(0, 100) distribution, and \u03b5 and \u03b4 also follow uniform distributions \nwith mean 0 such that \u03b62\n\u03b5/\u03b62\n\u03b7 = \u03b62\n\u03b4/\u03b62\n\u03be = 10%. In Figure 1(c), the range   \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n8 \n \n              ensures both eY and eX to be at least 0.875, and it includes the LSS \nestimate  \u0302 = 1.009 with corresponding \u03b3 = 0.491. The cross point is the RGM \nestimate  \u0302       . \n(d): Let \u03be follows \u221a      distribution, and \u03b5 and \u03b4 both follow the student\u2019s t(3) \ndistribution such that \u03b62\n\u03b5/\u03b62\n\u03b7 = \u03b62\n\u03b4/\u03b62\n\u03be = 20%. By setting both eY and eX above a \nthreshold of 0.725 as shown in Figure 1(d), we find                 and the \nRGM estimate  \u0302 = 1.027. Again, the LSS estimate  \u0302 = 1.055 with corresponding \n\u03b3 = 0.447 are within the selected \u03b3 interval. \nWe further apply the bootstrap resampling technique with 1000 replicates to obtain the \n95% confidence interval (C.I.) of the RCR estimate  \u0302. As can be seen from Figure 2, the \ntrue slope \u03b2 = 1 always lies in the 95% C.I. of the selected RCR slope estimates.  \n(Insert Figure 1 and Figure 2) \n \n3.1.2 Comparison of different estimators \nSince the LSS and the RGM estimators are most representative among the whole class of \nRCR-estimators, a more extensive simulation study is necessary to systematically \ncompare them with their nonrobust counterparts \u2013 the OR and GMR estimators. We keep \nthe above settings of the linear EIV model unchanged, and draw 10000 samples of size \nn = 200 from that model. Since all the fitted regression lines can be expressed by the \npoint-slope form    \u0305   \u0302    \u0305 , we therefore expect that the results of simulation \nfor the intercept are similar to those of the slope. \nTo measure performance, we use the bias, the standard deviation (std) as well as the root \nmean squared error (RMSE) of the  \u0302. Since the ratio of the error variances \u03bb is known, \nthe MLE solution of structural model is treated as the ground truth. Of note, the MLEs \nare calculated after excluding the outliers for the contaminated EIV data studies. The \nresults of our simulation study are given in Tables 1-4. For the uncontaminated EIV data \nin the first two tables, our estimators are comparable with the traditional EIV model \nestimator. More importantly, the last two tables show good performance of our new \nestimators in the presence of outliers, while the OR and GMR estimators are badly \ninfluenced by such contaminations.  \n(Insert Tables 1-4) \n \n3.2 Real-life Examples \n \n3.2.1 Serum kanamycin data \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n9 \n \nTo further illustrate and motivate our proposals, let us consider a simple example which \nis given by (Kelly 1984) and reanalyzed by (Zamar 1989) in the context of EIV models. \nThe data consists of simultaneous pairs of measurements of serum kanamycin levels in \nblood samples drawn from twenty babies. One of the measurements was obtained by a \nheelstick method (X), the other by using an umbilical catheter (Y). The question was \nwhether the two methods are systematically different and so that one could be substituted \nfor the other after correction for bias. It seems reasonable to assume that both methods \nare subject to measurement errors with equal variances (Kelly 1984). To better test the \nrobustness of different estimator, we change the original value (33.2, 26.0) of case 2 to \n(39.2, 32.0) as in the numerical example given by (Zamar 1989). \nUsing the usual OR and the GMR estimators lead to about 0.97 for the slope while using \nthe LSS estimator we propose leads to 1.52 for the slope, which is very similar to the \nslope (1.39) of Zamar\u2019s orthogonal regression M-estimators (ORM). We apply the RCR-\nestimators on this data set as well, and as can be seen from Figure 4(a), if we set both eY \nand eX be no less than 0.825, the satisfied \u03b3 interval is [0.344, 0.379] and the \ncorresponding slope ranges from 1.32 to 1.34. Of note, the results by previous authors \nshow that observations 2, 16 are potential influential, and furthermore observation 2 has \nmuch greater influence on the slope compared to case 16. Figure 3(b) depicts the residual \nplot from the RGM estimate with maximum regression efficiency. It is clear that \nobservation 2 is very influential and observation 16 has little influence.  \n(Insert Figure 3) \n \n3.2.2 Brain versus body weights data \nAnother numerical example is provided to illustrate the effectiveness of the RCR-\nestimators on the data set given in Rousseeuw and Leroy (1987). The data has been \nanalyzed by He and Liang (2000) and Fekri and Ruiz-Gazen (2006). The data consists of \nbrain and body weights of 28 animals. We also take the view that both weights are \nassumed to be measured with error and some observations are outlying (Fekri and Ruiz-\nGazen 2006). The question we are interested in is whether a larger brain is required to \ngovern a heavier body, or from another perspective, whether the brain weight increases \nlinearly as the body weight increases. \nWe obtain the usual OR estimates  \u0302         \u0302        and the GMR estimates \n \u0302          \u0302        . Compared to both, the LSS estimates  \u0302          \u0302    \n     are more close to the slope estimate  \u0302        from He and Liang\u2019s 50% quantile \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n10 \n \nregression (QR). In addition, we also apply the RCR-estimators on this data set, and as \ncan be seen from Figure 5(a), if we set both eY and eX to be no less than 0.75, the satisfied \n\u03b3 interval is [0.560, 0.645], and the corresponding slope estimate varies from 0.79 to 0.83. \nCases 6, 16 and 25 are detected by the RGM estimator in Figure 4(b), because they as \ndinosaurs are distinct from the other 25 cases of mammals. \n(Insert Figure 4) \n \n \n4. Summary and Conclusion \n \nIn the present paper, we propose a novel robust estimator for the multivariate linear EIV \nmodel. We not only show the LSS estimator is a special case of the RCR-estimators, but \nalso prove the optimality of the RGM in the respect of maximizing the sum of regression \nefficiencies in simple linear regression. The generalized RCR-estimators is defined for \nfurther investigations, and the sum of regression efficiencies as a general goodness-of-fit \ncriterion is proposed to compare the estimates from different regression approaches in \nreal data analysis. \nThe advantages of the new RCR-estimators lie in its intuitive geometric representation, \nits distribution free nonparametric nature being a direct generalization of the \nnonparametric compound regression analysis method, its operational independence to the \nratio of the error variances, and more importantly its robustness to outliers and other \ndepartures from the underlying assumptions. Nevertheless, the disadvantage of our \nproposals is that the fitted hyper-plane must pass through the center of the target dataset \nwhich will to some extent sacrifice its robustness performance. A robust location \nestimator from the data depth point of view (Liu et al. 1999) could be a remedy to further \nenhance the robustness of the newly proposed RCR-estimators. \n \nAppendix \n \nA.1. Proof of Proposition 1 \nWithout loss of generality, we form the multivariate regression model as \u2211\n    \n \n   \n   \nor in matrix form X\u03b2 = 0 for the centered data, where X = [X1, X2, \u2026, XP] is a n by P \nmatrix of observations, and \u03b2 is a P by 1 column vector of regression coefficients. This \nlinear relationship is uniquely specified by imposing the constraint \u03b2T\u03b2 = 1.  \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n11 \n \nWe first define  \u0303    \u0303   \u0303     \u0303   \n[\n \n \n \n   \n  \n \n   \n  \n \n \n \n   \n  \n \n   \n  ]\n \n \n \n     \n as the n by P transformed \nmatrix of observations, where    \u221a\u2211\n   \n \n \n   \n is the distance from the ith observation to \nthe origin. In this context, the LSS is defined to minimize  \n      ( \u0303 )\n ( \u0303 )     \u0303  \u0303      \u0303  \u0303      \u0303  \nwhere  \u0303 is the P by P robust sample covariance matrix \n \u0303  [\n \u0303 \n  \u0303 \n \n \u0303 \n  \u0303 \n \n \n \n \u0303 \n  \u0303 \n \n \u0303 \n  \u0303 \n]\n     \n \nWe define the eigenvectors of  \u0303 as           in the order of descending eigenvalues \n         . Assume  \u0303 is non-singular, the eigenvectors can expand the P-dimensional \nspace, and then the slope estimate \u03b2 can be expressed as a linear combination       \n        subject to \u2211\n  \n \n   \n  . Hence, the minimization of SSLSS is equivalent to the \nminimization of \n               \u0303              \nSince as we know that the eigenvectors are orthogonal and     \u0303     , the problem \nbecomes the minimization of \u2211\n    \n \n   \n. Under the constraints \u2211\n  \n \n   \n   and    \n    , the minimum is achieved when we set      and thus     . That is the \neigenvector corresponding to    - the smallest eigenvalue of  \u0303. \n \nA.2. Proof of Propostion 4 \n(1) The RGM estimator aims to minimize \n       \n \n        \u2211\n     \u0302       \u0302  \n  \n \n \n  \n \n        \u2211\n      \u0305   \n      \u0305        \u0305        \u0305  \n  \n \n \n  \nBy solving \n      \n  \n  , we obtain the slope estimate  \u0302        \u0303   \u221a \u0303   \u0303  \n\u2044\n.  \nGiven the following conditions  \n\u2211\n     \u0302   \n \n   \n  \u0303    \u0302  \u0303     \u0302 \u0303    \n\u2211\n     \u0302   \n \n   \n \n \n \u0302 \u2211\n     \u0302   \n \n   \n \n \n \u0302  \u0303    \u0303   \n \n \u0302  \u0303   ,  \nthe regression efficiencies    and    can be further expressed as \n   \n   \u2211\n     \u0302   \n \n   \n\u2211\n     \u0302   \n \n   \n  \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n12 \n \n \n\u2211\n     \u0302   \n \n   \n  \u0302  \u0303   \u0303  \n\u2044\n\u2211\n     \u0302   \n \n   \n \n \u0302     ( \u0303  )\u221a \u0303   \u0303  \n\u2044\n  \n \n \u0303   \u0303    \u0303  \n  \u0303   \u0303      \u0303   \u221a \u0303   \u0303    \nand \n   \n   \u2211\n     \u0302   \n \n   \n\u2211\n     \u0302   \n \n   \n  \n \n \n \u0302 \u2211\n     \u0302   \n \n   \n  \u0302  \u0303   \u0303  \n\u2044\n \n \u0302 \u2211\n     \u0302   \n \n   \n \n \u0302     ( \u0303  )\u221a \u0303   \u0303  \n\u2044\n  \n \n \u0303   \u0303    \u0303  \n  \u0303   \u0303      \u0303   \u221a \u0303   \u0303   . \nHence, we have shown the equality of regression efficiencies    and   . \n(2) For the slope estimate  \u0302 of any estimator, we have \n        \n \n\u2211\n     \u0302   \n \n   \n  \u0302  \u0303   \u0303  \n\u2044\n\u2211\n     \u0302   \n \n   \n \n \n \u0302 \u2211\n     \u0302   \n \n   \n  \u0302  \u0303   \u0303  \n\u2044\n \n \u0302 \u2211\n     \u0302   \n \n   \n  \n \n  \u0303    \u0303   \u0303  \n\u2044\n      \u0303    \u0303   \u0303  \n\u2044\n \n \u0303      \u0303      \u0303  \n . \nLet \n  \n  \u0302   , we have  \u0302  \n \u0303  \n \u0303  , which means         is unimodal and maximized at \n \u0302  \u221a \u0303  \n \u0303       ( \u0303  )\u221a \u0303  \n \u0303    \u0302    when  \u0303    , and it is unimodal and maximized \nat  \u0302   \u221a \u0303  \n \u0303         \u0303   \u221a \u0303  \n \u0303    \u0302    when  \u0303    .   \nTherefore, we have proven that the RGM estimator has the maximum      . \n \nReferences \n \nBrown, M. L. (1982), \u201cRobust Line Estimation with Errors in Both Variables,\u201d Journal of \nthe American Statistical Association, 77, 71-79. \nCheng, C. L., and Vanness, J. W. (1992), \u201cGeneralized M-Estimators for Errors-in-\nVariables Regression,\u201d Annals of Statistics, 20, 385-397. \nFekri, M., and Ruiz-Gazen, A. (2004), \u201cRobust Weighted Orthogonal Regression in the \nErrors-in-Variables Model,\u201d Journal of Multivariate Analysis, 88, 89-108. \nFekri, M., and Ruiz-Gazen, A. (2006), \u201cRobust Estimation in the Simple Errors-in-\nvariables Model,\u201d Statistics & Probability Letters, 76, 1741-1747. \nFuller, W. A. (1987), Measurement Error Models, New York: Wiley. \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n13 \n \nGolub, G. H., and Van Loan, C. F. v. (1996), Matrix Computations (3rd ed.), The Johns \nHopkins University Press. \nHan, H. (2011), Least Sine Squares and Robust Compound Regression Analysis, Stony \nBrook Theses & Dissertations, The Graduete School, Stony Brook University: Stony \nBrook, NY. \nHan, H., Ma, Y., Jiao, X., Leng, L., Liang, Z., and Zhu, W. (2012), \u201cRobust Compound \nRegression: A New Approach for Robust Estimation of Errors-in-Variables Models,\u201d \nIn JSM Proceedings, Nonparametric Statistics Section. Alexandria, VA: American \nStatistical Association. \nHe, X. M., and Liang, H. (2000), \u201cQuantile Regression Estimates for a Class of Linear \nand Partially Linear Errors-in-Variables Models,\u201d Statistica Sinica, 10, 129-140. \nJackson, J. D., and Dunlevy, J. A. (1988), \u201cOrthogonal Least Squares and the \nInterchangeability of Alternative Proxy Variables in the Social Sciences,\u201d The \nStatistician, 37, 7-14. \nKelly, G. (1984), \u201cThe Influence Function in the Errors in Variables Problem,\u201d THe \nAnnals of Statistics, 12, 87-100. \nKetellapper, R., and Ronner, A. (1984), \u201cAre Robust Estimation Methods Useful in the \nStructural Errors-in-Variables Model?,\u201d Metrika, 31, 33-41. \nLiu, R. Y., Parelius, J. M., and Singh, K. (1999), \u201cMultivariate Analysis by Data Depth: \nDescriptive Statistics, Graphics and Inference,\u201d Annals of Statistics, 27, 783-840. \nRousseeuw, P. J., and Leroy, A. M. (1987), Robust Regression and Outlier Detection, \nNew York: Wiley. \nVan Huffel, S., and Lemmerling, P. (2002), Total Least Squares and Errors-in-Variables \nModeling: Analysis, Algorithms and Applications, Dordrecht: Kluwer Academic \nPublishers. \nZamar, R. H. (1989), \u201cRobust Estimation in the Errors-in-Variables Model,\u201d Biometrika, \n76, 149-160. \n \n \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n14 \n \n \n(a)                                                              (b) \n \n(c)                                                             (d) \n \nFigure 1. The RCR efficiency plots of (a) EIV data with 5% outliers, (b) with 5% \nleverage points, (c) with uniformly distributed errors, and (d) with student\u2019s t distributed \nerrors. \n \n \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n15 \n \n \n                      (a)                                                                  (b) \n \n                                   (c)                                                                  (d) \n \nFigure 2. The 95% C.I. of RCR slope estimates of (a) EIV data with 5% outliers, (b) with \n5% leverage points, (c) with uniformly distributed errors, and (d) with student\u2019s t \ndistributed errors. \n \n \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n16 \n \n \n(a) \n \n(b) \n \nFigure 3. (a) The RCR efficiency plot and (b) the RGM residual diagnosis plot on the \nserum kanamycin data set \n \n \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n17 \n \n \n(a) \n \n(b) \n \nFigure 4. (a) The RCR efficiency plot and (b) the RGM residual diagnosis plot on the \nbrain versus body weights data set \n \n \nRCR: Robust Compound Regression \nHao Han and Wei Zhu \n \n18 \n \n \n \n \n \n \n \nTable 1: Comparison of MSE, bias, std from different estimators on uncontaminated EIV \ndata with different \u03bb and the X direction noise-to-signal ratio is .2 \n \n\u03bb \nRMSE( \u0302) \nbias( \u0302) \nstd( \u0302) \nOR \nLSS \nGMR \nRGM \nOR \nLSS \nGMR \nRGM \nOR \nLSS \nGMR \nRGM \n0 \n.097 \n.106 \n.089 \n.072 \n-.094 \n-.093 \n-.087 \n-.062 \n.029 \n.054 \n.026 \n.036 \n.25 \n.074 \n.088 \n.066 \n.058 \n-.071 \n-.070 \n-.064 \n-.044 \n.034 \n.060 \n.031 \n.038 \n.5 \n.050 \n.072 \n.044 \n.047 \n-.048 \n-.046 \n-.042 \n-.028 \n.038 \n.066 \n.033 \n.039 \n.75 \n.025 \n.064 \n.023 \n.041 \n-.024 \n-.023 \n-.021 \n-.014 \n.043 \n.074 \n.036 \n.041 \n1 \n0 \n.068 \n.008 \n.040 \n.001 \n.003 \n.001 \n.001 \n.048 \n.080 \n.040 \n.043 \nTable 2:  Comparison of MSE, bias, std from different estimators on uncontaminated \nEIV data with different \u03bb and the X direction noise-to-signal ratio is .05 \n \n\u03bb \nRMSE( \u0302) \nbias( \u0302) \nstd( \u0302) \nOR \nLSS \nGMR \nRGM \nOR \nLSS \nGMR \nRGM \nOR \nLSS \nGMR \nRGM \n1 \n0 \n.042 \n.001 \n.031 \n.000 \n.001 \n.000 \n.001 \n.023 \n.047 \n.022 \n.034 \n2 \n.026 \n.055 \n.024 \n.038 \n.026 \n.028 \n.024 \n.018 \n.029 \n.056 \n.027 \n.038 \n3 \n.052 \n.076 \n.047 \n.049 \n.051 \n.053 \n.047 \n.034 \n.034 \n.062 \n.031 \n.039 \n4 \n.079 \n.100 \n.070 \n.061 \n.078 \n.081 \n.070 \n.048 \n.039 \n.071 \n.035 \n.042 \n5 \n.107 \n.126 \n.093 \n.072 \n.107 \n.108 \n.092 \n.061 \n.044 \n.077 \n.038 \n.043 \nTable 3:   Comparison of MSE, bias, std from different estimators on contaminated EIV \ndata with different \u03bb and 5% outliers in the X direction \n \n\u03bb \nRMSE( \u0302) \nbias( \u0302) \nstd( \u0302) \nOR \nLSS \nGMR \nRGM \nOR \nLSS \nGMR \nRGM \nOR \nLSS \nGMR \nRGM \n0 \n.459 \n.149 \n.343 \n.098 \n-.458 \n-.142 \n-.341 \n-.093 \n.036 \n.048 \n.032 \n.032 \n.5 \n.445 \n.133 \n.324 \n.083 \n-.444 \n-.125 \n-.323 \n-.077 \n.036 \n.054 \n.032 \n.034 \n1 \n.436 \n.118 \n.310 \n.071 \n-.434 \n-.107 \n-.309 \n-.063 \n.040 \n.061 \n.034 \n.036 \n1.5 \n.422 \n.101 \n.294 \n.058 \n-.421 \n-.085 \n-.293 \n-.048 \n.043 \n.067 \n.035 \n.037 \n2 \n.410 \n.088 \n.280 \n.052 \n-.406 \n-.062 \n-.275 \n-.033 \n.047 \n.073 \n.036 \n.039 \nTable 4:    Comparison of MSE, bias, std from different estimators on contaminated EIV \ndata with different \u03bb and 5% outliers in the Y direction \n \n\u03bb \nRMSE( \u0302) \nbias( \u0302) \nstd( \u0302) \nOR \nLSS \nGMR \nRGM \nOR \nLSS \nGMR \nRGM \nOR \nLSS \nGMR \nRG\nM \n0 \n.704 \n.075 \n.420 \n.047 \n.695 \n.053 \n.415 \n.032 \n.115 \n.056 \n.067 \n.034 \n.5 \n.740 \n.106 \n.436 \n.062 \n.732 \n.088 \n.431 \n.051 \n.115 \n.067 \n.067 \n.039 \n1 \n.778 \n.133 \n.452 \n.075 \n.769 \n.116 \n.447 \n.064 \n.126 \n.072 \n.068 \n.039 \n1.5 \n.823 \n.165 \n.467 \n.087 \n.814 \n.151 \n.464 \n.079 \n.136 \n.082 \n.073 \n.042 \n2 \n.864 \n.201 \n.483 \n.101 \n.854 \n.187 \n.479 \n.093 \n.146 \n.089 \n.075 \n.043 \n"}