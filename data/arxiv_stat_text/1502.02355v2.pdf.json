{"text": "arXiv:1502.02355v2  [math.ST]  18 Dec 2015\nErrors-in-variables models with dependent measurements \u2217\u2020\nMark Rudelson\u22c6and Shuheng Zhou\u2020\n\u22c6Department of Mathematics,\n\u2020Department of Statistics,\nUniversity of Michigan, Ann Arbor, MI 48109\nDepartment of Statistics, Tech Report 538, July 31, 2015\nAugust 13, 2018\nAbstract\nSuppose that we observe y \u2208Rf and X \u2208Rf\u00d7m in the following errors-in-variables model:\ny\n=\nX0\u03b2\u2217+ \u01eb\nX\n=\nX0 + W\nwhere X0 is a f \u00d7 m design matrix with independent subgaussian row vectors, \u01eb \u2208Rf is a noise\nvector and W is a mean zero f \u00d7 m random noise matrix with independent subgaussian column vectors,\nindependent of X0 and \u01eb. This model is signi\ufb01cantly different from those analyzed in the literature in\nthe sense that we allow the measurement error for each covariate to be a dependent vector across its\nf observations. Such error structures appear in the science literature when modeling the trial-to-trial\n\ufb02uctuations in response strength shared across a set of neurons.\nUnder sparsity and restrictive eigenvalue type of conditions, we show that one is able to recover a\nsparse vector \u03b2\u2217\u2208Rm from the model given a single observation matrix X and the response vector\ny. We establish consistency in estimating \u03b2\u2217and obtain the rates of convergence in the \u2113q norm, where\nq = 1, 2 for the Lasso-type estimator, and for q \u2208[1, 2] for a Dantzig-type conic programming estimator.\nWe show error bounds which approach that of the regular Lasso and the Dantzig selector in case the errors\nin W are tending to 0.\n1\nIntroduction\nThe matrix variate normal model has a long history in psychology and social sciences, and is becoming\nincreasingly popular in biology and genomics, neuroscience, econometric theory, image and signal pro-\ncessing, wireless communication, and machine learning in recent years, see for example Dawid (1981);\nGupta and Varga (1992); Dutilleul (1999); Werner et al. (2008); Bonilla et al. (2008); Yu et al. (2009); Efron\n\u2217Mark Rudelson is partially supported by NSF grant DMS 1161372 and USAF Grant FA9550-14-1-0009. Shuheng Zhou was\nsupported in part by NSF under Grant DMS-1316731 and Elizabeth Caroline Crosby Funding from the Advance Program at the\nUniversity of Michigan. This manuscript was submitted for peer review in August 1, 2015; minor typos are being corrected in this\nversion.\n\u2020Keywords. Errors-in-variable models, measurement error data, subgaussian concentration, matrix variate distributions.\n1\n(2009); Allen and Tibshirani (2010); Kalaitzis et al. (2013), and the references therein. We call the random\nmatrix X which contains f rows and m columns a single data matrix, or one instance from the matrix variate\nnormal distribution. We say that an f \u00d7 m random matrix X follows a matrix normal distribution with a\nseparable covariance matrix \u03a3X = A \u2297B, which we write Xf\u00d7m \u223cNf,m(M, Am\u00d7m \u2297Bf\u00d7f). This is\nequivalent to say vec { X } follows a multivariate normal distribution with mean vec { M } and covariance\n\u03a3X = A \u2297B. Here, vec { X } is formed by stacking the columns of X into a vector in Rmf. Intuitively,\nA describes the covariance between columns of X while B describes the covariance between rows of X.\nSee Dawid (1981); Gupta and Varga (1992) for more characterization and examples.\nIn this paper, we introduce the related Kronecker Sum models to encode the covariance structure of a matrix\nvariate distribution. The proposed models and methods incorporate ideas from recent advances in graph-\nical models, high-dimensional regression model with observation errors, and matrix decomposition. Let\nAm\u00d7m, Bf\u00d7f be symmetric positive de\ufb01nite covariance matrices. Denote the Kronecker sum of A = (aij)\nand B = (bij) by\n\u03a3\n=\nA \u2295B := A \u2297If + Im \u2297B\n=\n\uf8ee\n\uf8ef\uf8ef\uf8f0\na11If + B\na12If\n. . .\na1mIf\na21If\na22If + B\n. . .\na2mIf\n. . .\nam1If\nam2If\n. . .\nammIf + B\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n(mf)\u00d7(mf)\nwhere If is an f \u00d7 f identity matrix. This covariance model arises naturally from the context of errors-in-\nvariables regression model de\ufb01ned as follows. Suppose that we observe y \u2208Rf and X \u2208Rf\u00d7m in the\nfollowing model:\ny\n=\nX0\u03b2\u2217+ \u01eb\n(1a)\nX\n=\nX0 + W\n(1b)\nwhere X0 is a f \u00d7 m design matrix with independent row vectors, \u01eb \u2208Rf is a noise vector and W is a mean\nzero f \u00d7 m random noise matrix, independent of X0 and \u01eb, with independent column vectors \u03c91, . . . , \u03c9m.\nIn particular, we are interested in the additive model of X = X0 + W such that\nvec { X } \u223cN(0, \u03a3)\nwhere\n\u03a3 = A \u2295B := A \u2297If + Im \u2297B\n(2)\nwhere we use one covariance component A \u2297If to describe the covariance of matrix X0 \u2208Rf\u00d7m, which\nis considered as the signal matrix, and the other component Im \u2297B to describe that of the noise matrix\nW \u2208Rf\u00d7m, where E\u03c9j \u2297\u03c9j = B for all j, where \u03c9j denotes the jth column vector of W. Our focus is on\nderiving the statistical properties of two estimators for estimating \u03b2\u2217in (1a) and (1b) despite the presence\nof the additive error W in the observation matrix X. We will show that our theory and analysis works with\na model much more general than that in (2), which we will de\ufb01ne in Section 1.1.\nBefore we go on to de\ufb01ne our estimators, we now use an example to motiviate (2) and its subgaussian\ngeneralization in De\ufb01nition 1.2. Suppose that there are f patients in a particular study, for which we use\nX0 to model the \u201dsystolic blood pressure\u201d and W to model the seasonal effects. In this case, X models the\nfact that among the f patients we measure, each patient has its own row vector of observed set of blood\npressures across time, and each column vector in W models the seasonal variation on top of the true signal\nat a particular day/time. Thus we consider X as measurement of X0 with W being the observation error.\nThat is, we model the seasonal effects on blood pressures across a set of patients in a particular study with\n2\na vector of dependent entries. Thus W is a matrix which consists of repeated independent sampling of\nspatially dependent vectors, if we regard the individuals as having spatial coordinates, for example, through\ntheir geographic locations. We will come back to discuss this example in Section 1.3.\n1.1\nThe model and the method\nWe \ufb01rst need to de\ufb01ne an independent isotropic vector with subgaussian marginals as in De\ufb01nition 1.1.\nDe\ufb01nition 1.1. Let Y be a random vector in Rp\n1. Y is called isotropic if for every y \u2208Rp, E\n\u0010\n| \u27e8Y, y \u27e9|2\u0011\n= \u2225y\u22252\n2.\n2. Y is \u03c82 with a constant \u03b1 if for every y \u2208Rp,\n\u2225\u27e8Y, y \u27e9\u2225\u03c82 := inf{t : E\n\u0000exp( \u27e8Y, y \u27e92/t2)\n\u0001\n\u22642} \u2264\u03b1 \u2225y\u22252 .\n(3)\nThe \u03c82 condition on a scalar random variable V is equivalent to the subgaussian tail decay of V , which\nmeans P (|V | > t) \u22642 exp(\u2212t2/c2), for all t > 0.\nThroughout this paper, we use \u03c82 vector, a vector with subgaussian marginals and subgaussian vector inter-\nchangeably.\nDe\ufb01nition 1.2. Let Z be an f \u00d7 m random matrix with independent entries Zij satisfying EZij = 0,\n1 = EZ2\nij \u2264\u2225Zij\u2225\u03c82 \u2264K. Let Z1, Z2 be independent copies of Z. Let X = X0 + W such that\n1. X0 = Z1A1/2 is the design matrix with independent subgaussian row vectors, and\n2. W = B1/2Z2 is a random noise matrix with independent subgaussian column vectors.\nAssumption (A1) allows the covariance model in (2) and its subgaussian variant in De\ufb01nition 1.2 to be\nidenti\ufb01able.\n(A1) We assume tr(A) = m is a known parameter, where tr(A) denotes the trace of matrix A.\nIn the kronecker sum model, we could assume we know tr(B), in order not to assume knowing tr(A).\nAssuming one or the other is known is unavoidable as the covariance model is not identi\ufb01able otherwise.\nMoreover, by knowing tr(A), we can construct an estimator for tr(B):\nbtr(B)\n=\n1\nm\n\u0000\u2225X\u22252\nF \u2212ftr(A)\n\u0001\n+\nand de\ufb01ne\nb\u03c4B := 1\nf btr(B) \u22650\n(4)\nwhere (a)+ = a \u22280.\nWe \ufb01rst introduce the Lasso-type estimator, adapted from those as considered\nin Loh and Wainwright (2012).\nSuppose that btr(B) is an estimator for tr(B)/f; for example, as constructed in (4). Let\nb\u0393\n=\n1\nf XT X \u22121\nf\nbtr(B)Im\nand b\u03b3 = 1\nf XT y.\n(5)\nFor a chosen penalization parameter \u03bb \u22650, and parameters b0 and d, we consider the following regularized\nestimation with the \u21131-norm penalty,\nb\u03b2\n=\narg min\n\u03b2:\u2225\u03b2\u22251\u2264b0\n\u221a\nd\n1\n2\u03b2T b\u0393\u03b2 \u2212\u27e8b\u03b3, \u03b2 \u27e9+ \u03bb\u2225\u03b2\u22251,\n(6)\n3\nwhich is a variation of the Lasso Tibshirani (1996) or the Basis Pursuit Chen et al. (1998) estimator. Al-\nthough in our analysis, we set b0 \u2265\u2225\u03b2\u2217\u22252 and d = |supp(\u03b2\u2217)| for simplicity. In practice, both b0 and d are\nunderstood to be parameters chosen to provide an upper bound on the \u21132 norm and the sparsity of the true\n\u03b2\u2217.\nRecently, Belloni et al. (2014) discussed the following conic programming compensated matrix uncertainly\n(MU) selector , which is a variant of the Dantzig selector Cand`es and Tao (2007); Rosenbaum and Tsybakov\n(2010, 2013). Adapted to our setting, it is de\ufb01ned as follows. Let \u03bb, \u00b5, \u03c4 > 0,\nb\u03b2\n=\narg min\n\b\n\u2225\u03b2\u22251 + \u03bbt : (\u03b2, t) \u2208\u03a5\n\t\nwhere\n(7)\n\u03a5\n=\nn\n(\u03b2, t) : \u03b2 \u2208Rm,\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\n\r\r\r\n\u221e\u2264\u00b5t + \u03c4, \u2225\u03b2\u22252 \u2264t\no\nwhere b\u03b3 and b\u0393 are as de\ufb01ned in (5) with \u00b5 \u223c\nq\nlog m\nf\n, \u03c4 \u223c\nq\nlog m\nf\n. We refer to this estimator as the Conic\nprogramming estimator from now on.\n1.2\nOur contributions\nWe provide a uni\ufb01ed analysis of the rates of convergence for both the Lasso-type estimator (6) as well as\nthe Conic Programming estimator (7), which is a Dantzig selector-type, although under slightly different\nconditions. We will show the rates of convergence in the \u2113q norm for q = 1, 2 for estimating a sparse vector\n\u03b2\u2217\u2208Rm in the model (1a) and (1b) using the Lasso-type estimator (6) in Theorems 2 and 4, and the Conic\nProgramming estimator (7) in Theorems 3 and 5 for 1 \u2264q \u22642. For the Conic Programming estimator, we\nalso show bounds on the predictive errors. The bounds we derive in both Theorems 2 and 3 focus on cases\nwhere the errors in W are not too small in their magnitudes in the sense that \u03c4B := tr(B)/f is bounded\nfrom below. For the extreme case when \u03c4B approaches 0, one hopes to recover bounds close to those for\nthe regular Lasso or the Dantzig selector as the effect of the noise in matrix W on the procedure becomes\nnegligible. We show in Theorems 4 and 5 that this is indeed the case. These results are new to the best of\nour knowledge.\nIn Theorems 2 to 5, we consider the regression model in (1a) and (1b) with subgaussian random design,\nwhere X0 = Z1A1/2 is a subgaussian random matrix with independent row vectors, and W = B1/2Z2 is a\nf\u00d7m random noise matrix with independent column vectors where Z1, Z2 are independent subgaussian ran-\ndom matrices with independent entries (cf. De\ufb01nition 1.2). This model is signi\ufb01cantly different from those\nanalyzed in the literature. For example, unlike the present work, the authors in Loh and Wainwright (2012)\napply Theorem 8 which states a general result on statistical convergence properties of the estimator (6) to\ncases where W is composed of independent subgaussian row vectors, when the row vectors of X0 are either\nindependent or follow a Gaussian vector auto-regressive model. See also Rosenbaum and Tsybakov (2010,\n2013); Chen and Caramanis (2013); Belloni et al. (2014) for the corresponding results on the compensated\nMU selectors, variant on the Orthogonal Matching Pursuit algorithm and the Conic Programming estimator\n(7).\nThe second key difference between our framework and the existing work is that we assume that only one\nobservation matrix X with the single measurement error matrix W is available. Assuming (A1) allows us to\nestimate EW TW as required in the estimation procedure (5) directly, given the knowledge that W is com-\nposed of independent column vectors. In contrast, existing work needs to assume that the covariance matrix\n\u03a3W := 1\nf EW TW of the independent row vectors of W or its functionals are either known a priori, or can\n4\nbe estimated from an dataset independent of X, or from replicated X measuring the same X0; see for exam-\nple Rosenbaum and Tsybakov (2010, 2013); Belloni et al. (2014); Loh and Wainwright (2012); Carroll et al.\n(2006). Such repeated measurements are not always available or are costly to obtain in practice Carroll et al.\n(2006).\nA noticeable exception is the work of Chen and Caramanis (2013), which deals with the scenario when\nthe noise covariance is not assumed to be known. We now elaborate on their result, which is a variant\nof the orthogonal matching pursuit (OMP) algorithm Tropp (2004); Tropp and Gilbert (2007). Their sup-\nport recovery result, that is, recovering the support set of \u03b2\u2217, applies only to the case when both signal\nmatrix and the measurement error matrix have isotropic subgaussian row vectors; that is, they assume\nindependence among both rows and columns in X (X0 and W); moreover, their algorithm requires the\nknowledge of the sparsity parameter d, which is the number of non-zero entries in \u03b2\u2217, as well as a \u03b2min con-\ndition: minj\u2208supp \u03b2\u2217\n\f\f\f\u03b2\u2217\nj\n\f\f\f = \u2126\n\u0010q\nlog m\nf\n(\u2225\u03b2\u2217\u22252 + 1)\n\u0011\n. They recover essentially the same \u21132-error bounds as\nin Loh and Wainwright (2012) and the current work when the covariance \u03a3W is known.\nIn summary, oblivion in \u03a3W and a general dependency condition in the data matrix X are not simultaneously\nallowed in existing work. In contrast, while we assume that X0 is composed of independent subgaussian\nrow vectors, we allow rows of W to be dependent, which brings dependency to the row vectors of the\nobservation matrix X. In the current paper, we focus on the proof-of-the-concept on using the kronecker\nsum covariance and additive model to model two way dependency in data matrix X, and derive bounds\nin statistical convergence for (6) and (7). In some sense, we are considering a parsimonious model for\n\ufb01tting observation data with two-way dependencies; that is, we use the signal matrix to encode column-\nwise dependency among covariates in X, and error matrix W to explain its row-wise dependency. When\nreplicates of X or W are available, we are able to study more sophisticated models and inference problems\nto be described in Section 1.3.\n1.3\nDiscussion\nThe key modeling question is: would each row vector in W for a particular patient across all time points be\na correlated normal or subgaussian vector as well? It is our conjecture that combining the newly developed\ntechniques, namely, the concentration of measure inequalities we have derived in the current framework\nwith techniques from existing work, we can handle the case when W follows a matrix normal distribution\nwith a separable covariance matrix \u03a3W = C \u2297B, where C is an m \u00d7 m positive semi-de\ufb01nite covariance\nmatrix. Moreover, for this type of \u201dseasonal effects\u201d as the measurement errors, the time varying covariance\nmodel would make more sense to model W, which we elaborate in the second example.\nAs a second example, in neuroscience applications, population coding refers to the information contained in\nthe combined activity of multiple neurons Kass et al. (2005). The relationship between population encod-\ning and correlations is complicated and is an area of active investigation, see for example Ruff and Cohen\n(2014); Cohen and Kohn (2011) It becomes more often that repeated measurements (trials) simultaneously\nrecorded across a set of neurons and over an ensemble of stimuli are available. In this context, one can\nimagine using a random matrix X0 \u223cNf,m(\u00b5, A \u2297B) which follows a matrix-variate normal distribution,\nor its subgaussian correspondent, to model the ensemble of mean response variables, e.g., the membrane\npotential, corresponding to the cross-trial average over a set of experiments. Here we use A to model the\ntask correlations and B to model the baseline correlation structure among all pairs of neurons at the sig-\nnal level. It has been observed that the onset of stimulus and task events not only change the cross-trial\n5\nmean response in \u00b5, but also alter the structure and correlation of the noise for a set of neurons, which\ncorrespond to the trial-to-trial \ufb02uctuations of the neuron responses. We use W to model such task-speci\ufb01c\ntrial-to-trial \ufb02uctuations of a set of neurons recorded over the time-course of a variety of tasks. Models\nas in (1a) and (1b) are useful in predicting the response of set of neurons based on the current and past\nmean responses of all neurons. Moreover, we could incorporate non-i.i.d. non-Gaussian W = [w1, . . . , wm]\nwhere wt = B1/2(t)z(t), where z(1), . . . , z(m) are independent isotropic subgaussian random vectors and\nB(t) \u227b0 for all t, to model the time-varying correlated noise as observed in the trial-to-trial \ufb02uctuations.\nIt is possible to combine the techniques developed in the present paper with those in Zhou et al. (2010);\nZhou (2014) to develop estimators for A, B and the time varying B(t) which is itself an interesting topic,\nhowever, beyond the scope of the current work.\nWe leave the investigation of this more general modeling framework and relevant statistical questions to\nfuture work. We refer to Carroll et al. (2006) for an excellent survey of the classical as well as modern\ndevelopments in measurement error models. In future work, we will also extend the estimation methods\nto the settings where the covariates are measured with multiplicative errors which are shown to be re-\nducible to the additive error problem as studied in the present work; see Rosenbaum and Tsybakov (2013);\nLoh and Wainwright (2012). Moreover, we are interested in applying the analysis and concentration of mea-\nsure results developed in the current paper and in our ongoing work to the more general contexts and settings\nwhere measurement error models are introduced and investigated; see for example Dempster et al. (1977);\nCarroll et al. (1985); Stefanski (1985); Hwang (1986); Fuller (1987); Stefanski (1990); Carroll and Wand\n(1991); Carroll et al. (1993); Cook and Stefanski (1994); Stefanski and Cook (1995); Iturria et al. (1999);\nLiang et al. (1999); Strimmer (2003); Xu and You (2007); Hall and Ma (2007); Liang and Li (2009); Ma and Li\n(2010); Allen and Tibshirani (2010); St\u00a8adler et al. (2014); S\u00f8resen et al. (2014b,a) and the references therein.\n2\nAssumptions and preliminary results\nWe will now de\ufb01ne some parameters related to the restricted and sparse eigenvalue conditions that are\nneeded to state our main results. We also state a preliminary result in Lemma 1 regarding the relationships\nbetween the two conditions in De\ufb01nitions 2.1 and 2.2.\nDe\ufb01nition 2.1. (Restricted eigenvalue condition RE(s0, k0, A)). Let 1 \u2264s0 \u2264p, and let k0 be a positive\nnumber. We say that a p \u00d7 q matrix A satis\ufb01es RE(s0, k0, A) condition with parameter K(s0, k0, A) if for\nany \u03c5 \u0338= 0,\n1\nK(s0, k0, A) :=\nmin\nJ\u2286{1,...,p},\n|J|\u2264s0\nmin\n\u2225\u03c5Jc\u22251\u2264k0\u2225\u03c5J\u22251\n\u2225A\u03c5\u22252\n\u2225\u03c5J\u22252\n> 0.\n(8)\nIt is clear that when s0 and k0 become smaller, this condition is easier to satisfy. We also consider the\nfollowing variation of the baseline RE condition.\nDe\ufb01nition 2.2. (Lower-RE condition) Loh and Wainwright (2012) The matrix \u0393 satis\ufb01es a Lower-RE con-\ndition with curvature \u03b1 > 0 and tolerance \u03c4 > 0 if\n\u03b8T \u0393\u03b8 \u2265\u03b1 \u2225\u03b8\u22252\n2 \u2212\u03c4 \u2225\u03b8\u22252\n1 \u2200\u03b8 \u2208Rm.\nAs \u03b1 becomes smaller, or as \u03c4 becomes larger, the Lower-RE condition is easier to be satis\ufb01ed.\n6\nLemma 1. Suppose that the Lower-RE condition holds for \u0393 := AT A with \u03b1, \u03c4 > 0 such that \u03c4(1 +\nk0)2s0 \u2264\u03b1/2. Then the RE(s0, k0, A) condition holds for A with\n1\nK(s0, k0, A) \u2265\nr\u03b1\n2 > 0.\nAssume that RE((k0 + 1)2, k0, A) holds. Then the Lower-RE condition holds for \u0393 = AT A with\n\u03b1 =\n1\n(k0 + 1)K2(s0, k0, A) > 0\nwhere s0 = (k0 + 1)2, and \u03c4 > 0 which satis\ufb01es\n\u03bbmin(\u0393) \u2265\u03b1 \u2212\u03c4s0/4.\n(9)\nThe condition above holds for any \u03c4 \u2265\n4\n(k0+1)3K2(s0,k0,A) \u22124\u03bbmin(\u0393)\n(k0+1)2 .\nThe \ufb01rst part of Lemma 1 means that, if k0 is \ufb01xed, then smaller values of \u03c4 guarantee RE(s0, k0, A)\nholds with larger s0, that is, a stronger RE condition. The second part of the Lemma implies that a weak\nRE condition implies that the Lower-RE (LRE) holds with a large \u03c4. On the other hand, if one assumes\nRE((k0 + 1)2, k0, A) holds with a large value of k0 (in other words, a strong RE condition), this would\nimply LRE with a small \u03c4. In short, the two conditions are similar but require tweaking the parameters.\nWeaker RE condition implies LRE condition holds with a larger \u03c4, and Lower-RE condition with a smaller\n\u03c4, that is, stronger LRE implies stronger RE. We prove Lemma 1 in Section 8.\nDe\ufb01nition 2.3. (Upper-RE condition) Loh and Wainwright (2012) The matrix \u0393 satis\ufb01es an upper-RE con-\ndition with curvature \u00af\u03b1 > 0 and tolerance \u03c4 > 0 if\n\u03b8T \u0393\u03b8 \u2264\u00af\u03b1 \u2225\u03b8\u22252\n2 + \u03c4 \u2225\u03b8\u22252\n1 \u2200\u03b8 \u2208Rm.\nDe\ufb01nition 2.4. De\ufb01ne the largest and smallest d-sparse eigenvalue of a p \u00d7 q matrix A to be\n\u03c1max(d, A)\n:=\nmax\nt\u0338=0;d\u2212sparse \u2225At\u22252\n2/ \u2225t\u22252\n2 , where\nd < p,\n(10)\nand\n\u03c1min(d, A)\n:=\nmin\nt\u0338=0;d\u2212sparse \u2225At\u22252\n2/ \u2225t\u22252\n2 .\n(11)\nThe rest of the paper is organized as follows. In Section 3, we present two main results Theorems 2 and 3.\nWe state results which improve upon Theorems 2 and Theorem 3 in Section 4, when the measurement\nerrors in W are small in their magnitudes in the sense of tr(B) being small. In Section 5, we outline the\nproof of the main theorems. In particular, In Section 5, we outline the proof for Theorems 2, 3, 4,and 5 in\nSection 5, 5.1, 5.3 and 5.4 respectively. In Section 6, we show a deterministic result as well as its application\nto the random matrix b\u0393 \u2212A for b\u0393 as in (5) with regards to the upper and Lower RE conditions. In section 7,\nwe show the concentration properties of the gram matrices XXT and XT X after we correct them with the\ncorresponding population error terms de\ufb01ned by tr(A)If and tr(B)Im respectively. These results might be\nof independent interests. The technical details of the proof are collected at the end of the paper. We prove\nTheorem 2 in Section 9. We prove Theorem 3 in Section 10. We prove Theorem 4 and 5 in Section 11\nand Section 12 respectively. The paper concludes with a discussion of the results in Section 13. Additional\nproofs and theoretical results are collected in the Appendix.\n7\nNotation. Let e1, . . . , ep be the canonical basis of Rp. For a set J \u2282{1, . . . , p}, denote EJ = span{ej :\nj \u2208J}. For a matrix A, we use \u2225A\u22252 to denote its operator norm. For a set V \u2282Rp, we let conv V denote\nthe convex hull of V . For a \ufb01nite set Y , the cardinality is denoted by |Y |. Let Bp\n1, Bp\n2 and Sp\u22121 be the\nunit \u21131 ball, the unit Euclidean ball and the unit sphere respectively. For a matrix A = (aij)1\u2264i,j\u2264m, let\n\u2225A\u2225max = maxi,j |aij| denote the entry-wise max norm. Let \u2225A\u22251 = maxj\nPm\ni=1 |aij| denote the matrix\n\u21131 norm. The Frobenius norm is given by \u2225A\u22252\nF = P\ni\nP\nj a2\nij. Let |A| denote the determinant and tr(A) be\nthe trace of A. Let \u03bbmax(A) and \u03bbmin(A) be the largest and smallest eigenvalues, and \u03ba(A) be the condition\nnumber for matrix A. The operator or \u21132 norm \u2225A\u22252\n2 is given by \u03bbmax(AAT ).\nFor a matrix A, denote by r(A) the effective rank tr(A)/ \u2225A\u22252. Let \u2225A\u22252\nF /\u2225A\u22252\n2 denote the stable rank for\nmatrix A. We write diag(A) for a diagonal matrix with the same diagonal as A. For a symmetric matrix A,\nlet \u03a5(A) = (\u03c5ij) where \u03c5ij = I(aij \u0338= 0), where I(\u00b7) is the indicator function. Let I be the identity matrix.\nWe let C be a constant which may change from line to line. For two numbers a, b, a \u2227b := min(a, b) and\na \u2228b := max(a, b). We write a \u224db if ca \u2264b \u2264Ca for some positive absolute constants c, C which\nare independent of n, f, m or sparsity parameters. Let (a)+ := a \u22280. We write a = O(b) if a \u2264Cb for\nsome positive absolute constants C which are independent of n, f, m or sparsity parameters. These absolute\nconstants C, C1, c, c1, . . . may change line by line.\n3\nMain results\nIn this section, we will state our main results in Theorems 2 and 3 where we consider the regression model\nin (1a) and (1b) with random matrices X0, W \u2208Rf\u00d7m as de\ufb01ned in De\ufb01nition 1.2.\nFor the Lasso-type estimator, we are interested in the case where the smallest eigenvalue of the column-wise\ncovariance matrix A does not approach 0 too quickly and the effective rank of the row-wise covariance\nmatrix B is bounded from below (cf. (14)). For the Conic Programming estimator, we impose a restricted\neigenvalue condition as formulated in Bickel et al. (2009); Rudelson and Zhou (2013) on A and assume that\nthe sparsity of \u03b2\u2217is bounded by o(\np\nf/ log m). These conditions will be relaxed in Section 4 where we\nallow \u03c4B to approach 0.\nBefore stating our main result for the Lasso-type estimator in Theorem 2, we need to introduce some more\nnotation and assumptions. Let amax = maxi aii and bmax = maxi bii be the maximum diagonal entries of\nA and B respectively. In general, under (A1), one can think of \u03bbmin(A) \u22641 and for s \u22651,\n1 \u2264amax \u2264\u03c1max(s, A) \u2264\u03bbmax(A),\nwhere \u03bbmax(A) denotes the maximum eigenvalue of A.\n(A2) The minimal eigenvalue \u03bbmin(A) of the covariance matrix A is bounded: 1 \u2265\u03bbmin(A) > 0.\n(A3) Moreover, we assume that the condition number \u03ba(A) is upper bounded by O\n\u0010q\nf\nlog m\n\u0011\nand \u03c4B =\nO(\u03bbmax(A)).\nThroughout the rest of the paper, s0 \u22651 is understood to be the largest integer chosen such that the following\ninequality still holds:\n\u221as0\u031f(s0) \u2264\u03bbmin(A)\n32C\ns\nf\nlog m where \u031f(s0) := \u03c1max(s0, A) + \u03c4B\n(12)\n8\nwhere we denote by \u03c4B = tr(B)/f and C is to be de\ufb01ned. Denote by\nMA = 64C\u031f(s0)\n\u03bbmin(A)\n\u226564C.\n(13)\nThroughout this paper, for the Lasso-type estimator, we will use the expression\n\u03c4 := \u03b1\ns0\n, where \u03b1 = \u03bbmin(A)/2;\n(A2) thus ensures that the Lower-RE condition as in De\ufb01nition 2.2 is not vacuous. (A3) ensures that (12)\nholds for some s0 \u22651.\nTheorem 2. (Estimation for the Lasso-type estimator) Set 1 \u2264f \u2264m. Suppose m is suf\ufb01ciently\nlarge. Suppose (A1), (A2) and (A3) hold. Consider the regression model in (1a) and (1b) with independent\nrandom matrices X0, W as in De\ufb01nition 1.2, and an error vector \u01eb \u2208Rf independent of X0, W, with\nindependent entries \u01ebj satisfying E\u01ebj = 0 and \u2225\u01ebj\u2225\u03c82 \u2264M\u01eb. Let C0, c\u2032 > 0 be some absolute constants.\nLet D2 := 2(\u2225A\u22252 + \u2225B\u22252). Suppose that \u2225B\u22252\nF / \u2225B\u22252\n2 \u2265log m. Suppose that c\u2032K4 \u22641 and\nr(B) := tr(B)\n\u2225B\u22252\n\u2265\n16c\u2032K4\nf\nlog m log Vm log m\nf\n(14)\nwhere V is a constant which depends on \u03bbmin(A), \u03c1max(s0, A) and tr(B)/f.\nLet b0, \u03c6 be numbers which satisfy\nM2\n\u01eb\nK2b2\n0\n\u2264\u03c6 \u22641.\n(15)\nAssume that the sparsity of \u03b2\u2217satis\ufb01es for some 0 < \u03c6 \u22641\nd := |supp(\u03b2\u2217)| \u2264c\u2032\u03c6K4\n128M2\nA\nf\nlog m < f/2.\n(16)\nLet b\u03b2 be an optimal solution to the Lasso-type estimator as in (6) with\n\u03bb \u22654\u03c8\ns\nlog m\nf\nwhere\n\u03c8 := C0D2K (K \u2225\u03b2\u2217\u22252 + M\u01eb)\n(17)\nThen for any d-sparse vectors \u03b2\u2217\u2208Rm, such that \u03c6b2\n0 \u2264\u2225\u03b2\u2217\u22252\n2 \u2264b2\n0, we have with probability at least\n1 \u221216/m3,\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2 \u226420\n\u03b1 \u03bb\n\u221a\nd and\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n1 \u226480\n\u03b1 \u03bbd.\nWe give an outline of the proof of Theorem 2 in Section 5.1. We prove Theorem 2 in Section 9.\nDiscussions. Denote the Signal-to-noise ratio by\nS/N := K2 \u2225\u03b2\u2217\u22252\n2/M2\n\u01eb where S := K2 \u2225\u03b2\u2217\u22252\n2 and\nN := M2\n\u01eb .\nThe two conditions on b0, \u03c6 imply that N \u2264\u03c6S. Notice that this could be restrictive if \u03c6 is small.\n9\nWe will show in Section 5.1 that condition (15) is not needed in order for the error bounds in terms of the\n\u2113p, p = 1, 2 norm of b\u03b2 \u2212\u03b2\u2217, as shown in the Theorem 2 statement to hold. It was indeed introduced so\nas to simplify the expression for the condition on d as shown in (16). There we provide a slightly more\ngeneral condition on d in (41), where (15) is not required. In summary, we prove that Theorem 2 holds with\nN = M2\n\u01eb and S = \u03c6K2b2\n0 in arbitrary orders, so long as condition (14) holds and\nd = O\n\u0012 1\nM2\nA\nf\nlog m\n\u0013\n.\nFor both cases, we require that \u03bb \u224d(\u2225A\u22252 + \u2225B\u22252)K\n\u221a\nS + N\nq\nlog m\nf\nas expressed in (17). That is, when\neither the noise level M\u01eb or the signal strength K \u2225\u03b2\u2217\u2225increases, we need to increase \u03bb correspondingly;\nmoreover, when N dominates the signal K2 \u2225\u03b2\u2217\u22252\n2, we have for d \u224d\n1\nM2\nA\nf\nlog m,\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2 / \u2225\u03b2\u2217\u22252 \u226420\n\u03b1 D2K2\nr\nN\nS\n1\nMA\n\u224dD2K2\nr\nN\nS\n1\n\u031f(s0)\nwhich eventually becomes a vacuous bound when N \u226bS. We will present an improved bound in The-\norem 4. We further elaborate on the relationships among the noise, the measurement error and the signal\nstrength in Section 4.2.\nTheorem 3. Suppose (A1) holds. Set 0 < \u03b4 < 1. Suppose that f < m \u226aexp(f) and 1 \u2264d0 < f.\nLet \u03bb > 0 be the same parameter as in (7). Assume that RE(2d0, 3(1 + \u03bb), A1/2) holds. Suppose that\n\u2225B\u22252\nF / \u2225B\u22252\n2 \u2265log m. Suppose that the sparsity of \u03b2\u2217is bounded by\nd0 := |supp(\u03b2\u2217)| \u2264c0\np\nf/ log m\n(18)\nfor some constant c0 > 0; Suppose k0 := 1 + \u03bb\nf\n\u2265\n2000dK4\n\u03b42\nlog\n\u001260em\nd\u03b4\n\u0013\nwhere\n(19)\nd\n=\n2d0 + 2d0amax\n16K2(2d0, 3k0, A1/2)(3k0)2(3k0 + 1)\n\u03b42\n.\n(20)\nConsider the regression model in (1a) and (1b) with X0, W as in De\ufb01nition 1.2 and an error vector \u01eb \u2208Rf,\nindependent of X0, W, with independent entries \u01ebj satisfying E\u01ebj = 0 and \u2225\u01ebj\u2225\u03c82 \u2264M\u01eb. Let b\u03b2 be an\noptimal solution to the Conic Programming estimator as in (7) with input (b\u03b3, b\u0393) as de\ufb01ned in (5), where\ntr(B) is as de\ufb01ned in (4). Choose for D2 = 2(\u2225A\u22252 + \u2225B\u22252) and D0 = \u221a\u03c4B + \u221aamax,\n\u00b5 \u224dD2K2\ns\nlog m\nf\nand\n\u03c4 \u224dD0KM\u01eb\ns\nlog m\nf\n.\nThen with probability at least 1 \u2212c\u2032\nm2 \u22122 exp(\u2212\u03b42f/2000K4), for 2 \u2265q \u22651,\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\nq \u2264CD2K2d1/q\n0\ns\nlog m\nf\n\u0012\n\u2225\u03b2\u2217\u22252 + M\u01eb\nK\n\u0013\n.\n(21)\n10\nUnder the same assumptions, the predictive risk admits the following bounds with the same probability as\nabove,\n1\nf\n\r\r\rX(b\u03b2 \u2212\u03b2\u2217)\n\r\r\r\n2\n2 \u2264C\u2032D2\n2K4d0\nlog m\nf\n\u0012\n\u2225\u03b2\u2225\u2217\n2 + M\u01eb\nK\n\u00132\nwhere c\u2032, C0, C, C\u2032 > 0 are some absolute constants.\nWe give an outline of the proof of Theorem 3 in Section 5 while leaving the detailed proof in Section 10.\nDiscussions. Similar results have been derived in Loh and Wainwright (2012); Belloni et al. (2014), how-\never, under different assumptions on the distribution of the noise matrix W. When W is a random matrix\nwith i.i.d. subgaussian noise, our results will essentially recover the results in Loh and Wainwright (2012)\nand Belloni et al. (2014). The choice of \u03bb for the Lasso estimator and parameters \u00b5, \u03c4 for the DS-type\nestimator satisfy\n\u03bb \u224d\u00b5 \u2225\u03b2\u2217\u22252 + \u03c4\nThis relationship is made clear through Theorem 8 regarding the Lasso-type estimator, which follows from\nTheorem 1 Loh and Wainwright (2012), Lemmas 6, 11, 14, and 16, which are the key results in proving\nTheorems 2, 3, 4, and 5. Finally, we note that following Theorem 2 as in Belloni et al. (2014), one can show\nthat without the relatively restrictive sparsity condition (18), a bound similar to that in (21) holds, however\nwith \u2225\u03b2\u2217\u22252 being replaced by \u2225\u03b2\u2217\u22251, so long as the sample size satis\ufb01es the requirement as in (27).\n4\nImproved bounds when the measurement errors are small\nThroughout our analysis of Theorems 2 and 3, we focused on the case when the errors in W are suf\ufb01ciently\nlarge in the sense that \u03c4B = tr(B)/f > 0 is bounded from below; for example, this is explicitly indicated\nby the lower bound on the effective rank r(B) = tr(B)/ \u2225B\u22252, when \u2225B\u22252 is bounded away from 0. More\nprecisely, by the condition on the effective rank as in (14), we have\n\u03c4B = tr(B)\nf\n\u2265\n16c\u2032K4 \u2225B\u22252\nlog m log Vm log m\nf\nwhere V = 3eM3\nA/2.\nThe bounds we derive in this section focus on cases where the measurement errors in W are small in their\nmagnitudes in the sense of \u03c4B being small. For the extreme case when \u03c4B approaches 0, one hopes to recover\na bound close to the regular Lasso or the Dantzig selector as the effect of the noise on the procedure should\nbecome negligible. We show in Theorems 4 and 5 that this is indeed the case. First, we de\ufb01ne some contants\nwhich we use throughout the rest of the paper. Denote by\nD0 = \u221a\u03c4B + a1/2\nmax,\nD\u2032\n0 = \u2225B\u22251/2\n2\n+ a1/2\nmax, and \u03c4 +\nB := (\u03c4 +/2\nB\n)2\n(22)\nwhere \u03c4 +/2\nB\n:= \u221a\u03c4B + Doracle\n\u221am\nand Doracle = 2(\u2225A\u22251/2\n2\n+ \u2225B\u22251/2\n2\n).\n(23)\nWe \ufb01rst state a more re\ufb01ned result for the Lasso-type estimator.\n11\nTheorem 4. Suppose all conditions in Theorem 2 hold, except that we drop (15) and replace (17) with\n\u03bb \u22652\u03c8\ns\nlog m\nf\nwhere\n\u03c8 := 2C0D\u2032\n0K\n\u0010\n\u03c4 +/2\nB\nK \u2225\u03b2\u2217\u22252 + M\u01eb\n\u0011\n.\n(24)\nSuppose that for 0 < \u03c6 \u22641 and CA :=\n1\n128M2\nA\nd := |supp(\u03b2\u2217)| \u2264CA\nf\nlog m\n\b\nc\u2032C\u03c6 \u22272\n\t\nwhere\n(25)\nC\u03c6 := \u2225B\u22252 + amax\nD2\nD\u03c6\nfor\nD\u03c6 = M2\n\u01eb K2\nb2\n0\n+ \u03c4 +\nB K4\u03c6,\nD = \u03c1max(s0, A) + \u03c4B, and c\u2032, \u03c6, b0, M\u01eb and K as de\ufb01ned in Theorem 2.\nThen for any d-sparse vectors \u03b2\u2217\u2208Rm, such that \u03c6b2\n0 \u2264\u2225\u03b2\u2217\u22252\n2 \u2264b2\n0, we have with probability at least\n1 \u221216/m3,\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2 \u226420\n\u03b1 \u03bb\n\u221a\nd and\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n1 \u226480\n\u03b1 \u03bbd.\nWe give an outline for the proof of Theorem 4 in Section 5.3, and show the actual proof in Section 11.\nRemark 4.1. Let us rede\ufb01ne the Signal-to-noise ratio by\nS/M\n:=\nK2 \u2225\u03b2\u2217\u22252\n2\n\u03c4 +\nB K2 \u2225\u03b2\u2217\u22252\n2 + M2\u01eb\nwhere\nS\n:=\nK2 \u2225\u03b2\u2217\u22252\n2 and\nM := M2\n\u01eb + \u03c4 +\nB K2 \u2225\u03b2\u2217\u22252\n2\nWe now only require that \u03bb \u224d(a1/2\nmax +\u2225B\u22251/2\n2\n)K\n\u221a\nM\nq\nlog m\nf\n. That is, when either the noise level M\u01eb or the\nmeasurement error strength in terms of \u03c4 +/2\nB\nK \u2225\u03b2\u2217\u22252 increases, we need to increase the penalty parameter\n\u03bb correspondingly; moreover, when d \u224d\n1\nM2\nA\nf\nlog m\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2\n\u2225\u03b2\u2217\u22252\n\u226420\n\u03b1 D\u2032\n0K2\nr\nM\nS\n1\nMA\n\u224dD\u2032\n0K2\nr\nM\nS\n1\n\u031f(s0),\nwhich eventually becomes a vacuous bound when M \u226bS.\n4.1\nA Corollary for Theorem 3\nWe next state in Theorem 5 an improved bound for the Conic programming estimator (7), which improves\nupon Theorem 3 when \u03c4B is small.\nTheorem 5. Suppose all conditions in Theorem 3 hold, except that we replace the condition on d as in (18)\nwith the following. Suppose that the sample size f and the size of the support of \u03b2\u2217satisfy the following\n12\nrequirements: for C6 \u2265Doracle and rm,m = 2C0\nq\nlog m\nfm ,\nd0\n=\nO\n \n\u03c4 \u2212\nB\ns\nf\nlog m\n!\nwhere \u03c4 \u2212\nB \u2264\n1\n\u03c4 1/2\nB\n+ 2C6Kr1/2\nm,m\n(26)\nand\nf\n\u2265\n2000dK4\n\u03b42\nlog\n\u001260em\nd\u03b4\n\u0013\nwhere\n(27)\nd\n=\n2d0 + 2d0amax\n16K2(2d0, 3k0, A1/2)(3k0)2(3k0 + 1)\n\u03b42\n.\n(28)\nLet b\u03b2 be an optimal solution to the Conic Programming estimator as in (7) with input (b\u03b3, b\u0393) as de\ufb01ned\nin (5), where btr(B) is as de\ufb01ned in (4). Suppose\n\u03c4\n\u224d\nD0M\u01ebrm,f\nwhere\nrm,f = C0K\ns\nlog m\nf\nand\n(29)\n\u00b5\n\u224d\nD\u2032\n0e\u03c4 1/2\nB Krm,f\nwhere e\u03c4 1/2\nB\n:= b\u03c4 1/2\nB\n+ C6Kr1/2\nmm.\n(30)\nThen with probability at least 1 \u2212c\u2032\u2032\nm2 \u22122 exp(\u2212\u03b42f/2000K4), for 2 \u2265q \u22651, and \u03c4 \u2020/2\nB\n= (\u03c4 1/2\nB\n+\n3\n2C6Kr1/2\nm,m)\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\nq \u2264C\u2032D\u2032\n0K2d1/q\n0\ns\nlog m\nf\n\u0012\n\u03c4 \u2020/2\nB\n\u2225\u03b2\u2217\u22252 + M\u01eb\nK\n\u0013\n;\n(31)\nUnder the same assumptions, the predictive risk admits the following bounds\n1\nf\n\r\r\rX(b\u03b2 \u2212\u03b2\u2217)\n\r\r\r\n2\n2 \u2264C\u2032\u2032(\u2225B\u22252 + amax)K2d0\nlog m\nf\n\u0010\n\u03c4 \u2021\nBK2 \u2225\u03b2\u2217\u22252\n2 + M2\n\u01eb\n\u0011\nwith the same probability as above, where c\u2032\u2032, C\u2032, C\u2032\u2032 > 0 are some absolute constants, and \u03c4 \u2021\nB \u224d2\u03c4B +\n3C2\n6K2rm,m.\n4.2\nDiscussions\nIn particular, when \u03c4B \u21920, Theorem 5 allows us to recover a rate close that of the Dantzig selector with\nan exact recovery if \u03c4B = 0 is known a priori; see Section 13. Moreover the constraint (18) on the sparsity\nparameter d0 appearing in Theorem 3 can now be relaxed as in (26). Roughly speaking, one can think of d0\nbeing bounded as follows for the Conic programming estimator (7):\nd0\n=\nO\n \n\u03c4 \u2212\nB\ns\nf\nlog m\n^\nf\nlog(m/d0)\n!\nwhere \u03c4 \u2212\nB \u224d\n1\n\u03c4 1/2\nB\n(32)\nThat is, when \u03c4B decreases, we allow larger values of d0; however, when \u03c4B \u21920, the sparsity level\nof d = O (f/ log(m/d)) starts to dominate, which enables the Conic Programming estimator to achieve\nresults similar to the Dantzig Selector when the design matrix X0 is a subgaussian random matrix satis-\nfying the Restricted Eigenvalue conditions; See for example Cand`es and Tao (2007); Bickel et al. (2009);\nRudelson and Zhou (2013).\n13\nThe condition on d (and D\u03c6) for the Lasso estimator as de\ufb01ned in (25) suggests that as \u03c4B \u21920, and thus\n\u03c4 +\nB \u21920 the requirement on the sparsity parameter d becomes slightly more stringent when K2M2\n\u01eb /b2\n0 \u224d1\nand much more restrictive when K2M2\n\u01eb /b2\n0 = o(1); however, suppose we require\nM2\n\u01eb = \u2126(\u03c4 +\nB K2 \u2225\u03b2\u2217\u22252\n2),\nthat is, the stochastic error \u01eb in the response variable y as in (1a) does not converge to 0 as quickly as the\nmeasurement error W in (1b) does, then the sparsity constraint becomes essentially unchanged as \u03c4 +\nB \u21920.\nIn this case, essentially, we require that for some c\u2032\u2032 := c\u2032C\u03c6\nd \u2264C\u2032\nA\nf\nlog m\n\u001ac\u2032\u2032K2M2\n\u01eb\nb2\n0\n\u22271\n\u001b\nwhere D\u03c6 \u224dK2M2\n\u01eb\nb2\n0\nand C\u2032\nA :=\n1\n64M2\nA\n,\ngiven that\n\u03c4 +\nB K4\u03c6 \u2264\u03c4 +\nB K4 \u2225\u03b2\u2217\u22252\n2\nb2\n0\n\u226aK2M2\n\u01eb\nb2\n0\n.\nThese tradeoffs are somehow different from the behavior of the Conic programming estimator (cf (32)).\n5\nProof of theorems\nWe \ufb01rst consider the following large deviation bound on\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r as stated in Lemma 6. This entity\nappears in the constraint set in the conic programming estimator (7), and is directly related to the choice of\n\u03bb for the lasso-type estimator in view of Theorem 8. Events B0 and B10 are de\ufb01ned in Section B.2 in the\nAppendix.\nLemma 6. Suppose (A1) holds. Let X = X0 +W, where X0, W are as de\ufb01ned in Theorem 2. Suppose that\n\u2225B\u22252\nF / \u2225B\u22252\n2 \u2265log m\nwhere m \u226516.\nLet b\u0393 and b\u03b3 be as in (5). On event B0, we have for D2 = 2(\u2225A\u22252 + \u2225B\u22252) and some absolute constant C0\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\u2264\u03c8\ns\nlog m\nf\nwhere \u03c8 = C0D2K (K \u2225\u03b2\u2217\u22252 + M\u01eb)\nis as de\ufb01ned in Theorem 2. Then P (B0) \u22651 \u221216/m3.\nLemma 7. Let m \u22652. Let X be de\ufb01ned as in De\ufb01nition 1.2 and b\u03c4B be as de\ufb01ned in (4). Denote by\n\u03c4B = tr(B)/f and \u03c4A = tr(A)/m. Suppose that f \u2228(r(A)r(B)) > log m. Denote by B6 the event such\nthat\n|b\u03c4B \u2212\u03c4B|\n\u2264\n2C0K2\ns\nlog m\nmf\n\u0012\u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221af\n\u0013\n=: D1K2rm,m\nwhere D1\n:=\n\u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221af\nand\nrm,m = 2C0\ns\nlog m\nmf .\nThen P (B6) \u22651 \u2212\n3\nm3 . If we replace \u221alog m with log m in the de\ufb01nition of event B6, then we can drop the\ncondition on f or r(A)r(B) = tr(A)\n\u2225A\u22252\ntr(B)\n\u2225B\u22252 to achieve the same bound on event B6.\n14\nWe prove Lemma 7 in Section B.3 in the Appendix. We prove Lemma 6 in Section C.1. We mention in\npassing that Lemma 6 is essential in proving Theorem 3 as well.\nWe state variations on this inequality in Lemma 14 and the remark which immediately follows.\nTheorem 8. Consider the regression model in (1a) and (1b). Let d \u2264f/2. Let b\u03b3, b\u0393 be as constructed in\n(5). Suppose that the matrix b\u0393 satis\ufb01es the Lower-RE condition with curvature \u03b1 > 0 and tolerance \u03c4 > 0,\n\u221a\nd\u03c4 \u2264min\n\u001a\n\u03b1\n32\n\u221a\nd\n, \u03bb\n4b0\n\u001b\n(33)\nwhere d, b0 and \u03bb are as de\ufb01ned in (6). Then for any d-sparse vectors \u03b2\u2217\u2208Rm, such that \u2225\u03b2\u2217\u22252 \u2264b0 and\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\u22641\n2\u03bb, the following bounds hold:\n(34)\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n2 \u226420\n\u03b1 \u03bb\n\u221a\nd,\nand\n\r\r\rb\u03b2 \u2212\u03b2\u2217\r\r\r\n1 \u226480\n\u03b1 \u03bbd\n(35)\nwhere b\u03b2 is an optimal solution to the Lasso-type estimator as in (6).\nWe defer the proof of Theorem 8 to Section D, for clarity of presentation. In section 5.1, we provide\ntwo Lemmas 9 and 10 in checking the RE conditions as well condition (33). One can then combine with\nTheorem 8, Lemmas 6, 9 and 10 to prove Theorem 2. In more details, Lemma 9 checks the Lower and the\nUpper RE conditions on the modi\ufb01ed gram matrix:\nb\u0393A := XT X \u2212btr(B)Im\n(36)\nwhile Lemma 10 checks condition (33) as stated in Theorem 8 for curvature \u03b1 and tolerance \u03c4 as derived in\nLemma 9. Finally Lemma 6 ensures that (34) holds with high probability for \u03bb chosen as in (17). We defer\nstating these lemmas in Section 5.1. The full proof of Theorem 2 appears in Section 9.\nFor Theorem 3, our \ufb01rst goal is to show that the following holds with high probability\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n=\n\r\r\r 1\nf XT (y \u2212X\u03b2\u2217) + 1\nf btr(B)\u03b2\u2217\r\r\r\n\u221e\u2264\u00b5 \u2225\u03b2\u2217\u22252 + \u03c4,\nwhere \u00b5, \u03c4 are as chosen in (43). This forms the basis for proving the \u2113q convergence, where q \u2208[1, 2],\nfor the Conic Programming estimator (7). This follows immediately from Lemma 6. More explicitly, we\nwill state it in Lemma 11. Before we proceed, we \ufb01rst need to introduce some notation and de\ufb01nitions. Let\nX0 = Z1A1/2 be de\ufb01ned as in De\ufb01nition 1.2. Let k0 = 1 + \u03bb. First we need to de\ufb01ne the \u2113q-sensitivity\nparameter for \u03a8 := 1\nf XT\n0 X0 following Belloni et al. (2014):\n\u03baq(d0, k0)\n=\nmin\nJ:|J|\u2264d0\nmin\n\u2206\u2208ConeJ(k0)\n\u2225\u03a8\u2206\u2225\u221e\n\u2225\u2206\u2225q\nwhere\n(37)\nConeJ(k0)\n=\n{x \u2208Rm | s.t. \u2225xJc\u22251 \u2264k0 \u2225xJ\u22251} .\n(38)\nSee also Gautier and Tsybakov (2011). Let (b\u03b2, bt) be the optimal solution to (7) and denote by v = b\u03b2 \u2212\u03b2\u2217.\nWe will state the following auxiliary lemmas, the \ufb01rst of which is deterministic in nature. The two lemmas\nre\ufb02ect the two geometrical constraints on the optimal solution to (7). The optimal solution b\u03b2 satis\ufb01es:\n1. v obeys the following cone constraint: \u2225vSc\u22251 \u2264k0 \u2225vS\u22251 and bt \u22641\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252.\n15\n2. \u2225\u03a8v\u2225\u221eis upper bounded by a quantity at the order of O (\u00b5(\u2225\u03b2\u2217\u22252 + \u2225v\u22251) + \u03c4)\nNow combining Lemma 6 of Belloni et al. (2014) and an earlier result of the two authors (cf.\nTheo-\nrem 25 Rudelson and Zhou (2013)), we can show that the RE(2d0, 3(1+\u03bb), A1/2) condition and the sample\nrequirement as in (27) are enough to ensure that the \u2113q-sensitivity parameter satis\ufb01es the following lower\nbound for all 1 \u2264q \u22642: for some contant c,\n\u03baq(d0, k0)\n\u2265\ncd\u22121/q\n0\nwhich ensures that for\nv = b\u03b2 \u2212\u03b2\u2217,\n\u2225\u03a8v\u2225\u221e\n\u2265\n\u03baq(d0, k0)\u2225v\u2225q \u2265cd\u22121/q\n0\n\u2225v\u2225q\nwhere\n\u03a8 = 1\nf XT\n0 X0.\n(39)\nCombining (39) with Lemmas 11, 12 and 13 gives us both the lower and upper bounds on \u2225\u03a8v\u2225\u221e, with\nthe lower bound being \u03baq(d0, k0) \u2225v\u2225q and the upper bound as speci\ufb01ed in Lemma 13. Following some\nalgebraic manipulation, this yields the bound on the \u2225v\u2225q for all 1 \u2264q \u22642. We state Lemmas 11 to 13 in\nSection 5.2 while leaving the proof for Theorem 3 in Section 10.\n5.1\nAdditional technical results for Theorem 2\nThe main focus of the current section is to apply Theorem 8 to show Theorem 2, which applies to the general\nsubgaussian model as considered in the present work. We \ufb01rst state Lemma 9, which follows immediately\nfrom Corollary 19. First, we replace (A3) with (A3\u2019) which reveals some additional information regarding\nthe constant hidden inside the O(\u00b7) notation.\n(A3\u2019) Suppose (A3) holds; moreover, for D2 = 2(\u2225A\u22252 + \u2225B\u22252), mf \u22651024C2\n0D2\n2K4 log m/\u03bb2\nmin(A) or\nequivalently,\n\u03bbmin(A)\n\u2225A\u22252 + \u2225B\u22252\n> CK\ns\nlog m\nmf\nfor some large enough contant CK.\nLemma 9. (Lower and Upper-RE conditions) Suppose (A1), (A2) and (A3\u2019) hold. Denote by V :=\n3eM3\nA/2, where MA is as de\ufb01ned in (13). Let s0 be as de\ufb01ned in (12). Suppose that for some c\u2032 > 0,\ntr(B)\n\u2225B\u22252\n\u2265\nc\u2032K4 s0\n\u03b52 log\n\u00123em\ns0\u03b5\n\u0013\nwhere \u03b5 =\n1\n2MA\n.\n(40)\nLet A0 be the event that the modi\ufb01ed gram matrix b\u0393A as de\ufb01ned in (36) satis\ufb01es the Lower as well as Upper\nRE conditions with\ncurvature\n\u03b1 = 1\n2\u03bbmin(A), smoothness \u00af\u03b1 = 3\u03bbmax(A)/2,\nand tolerance\n512C2\u031f(s0)2\n\u03bbmin(A)\nlog m\nf\n\u2264\u03c4 := \u03b1\ns0\n\u22641024C2\u031f2(s0 + 1)\n\u03bbmin(A)\nlog m\nf\nfor \u03b1, \u00af\u03b1 and \u03c4 as de\ufb01ned in De\ufb01nitions 2.2 and 2.3, and C, s0, \u031f(s0) in (12).\nThen P (A0) \u22651 \u2212\n4 exp\n\u0010\n\u2212\nc3f\nM2\nA log m log\n\u0010\nVm log m\nf\n\u0011\u0011\n\u22122 exp\n\u0010\n\u22124c2f\nM2\nAK4\n\u0011\n\u22126/m3.\n16\nLemma 10. Suppose all conditions in Lemma 9 hold. Suppose that s0 \u22653 and\nd := |supp(\u03b2\u2217)| \u2264CA\nf\nlog m\n\b\nc\u2032D\u03c6 \u22272\n\t\nwhere CA :=\n1\n128M2\nA\n,\n(41)\nD\u03c6 =\n\u0012K2M2\n\u01eb\nb2\n0\n+ K4\u03c6\n\u0013\n\u2265K4\u03c6 \u2265\u03c6\nwhere c\u2032, \u03c6, b0, M\u01eb and K are as de\ufb01ned in Theorem 2, where we assume that \u2225\u03b2\u2217\u22252\n2 \u2265\u03c6b2\n0 for some 0 <\n\u03c6 \u22641. Then the following condition holds\nd \u2264s0\n32\n^ \u0010s0\n\u03b1\n\u00112 log m\nf\n\u0012 \u03c8\nb0\n\u00132\n(42)\nwhere \u03c8 is as de\ufb01ned in (17) and \u03b1 = \u03bbmin(A)/2.\nWe prove Lemmas 9 and 10 in Sections D.1 and D.2 respectively.\nRemark 5.1. Clearly for d, b0, \u03c6 as bounded in Theorem 2, we have by assumption (15) the following upper\nand lower bound on D\u03c6:\n2K4\u03c6 \u2265D\u03c6 :=\n\u0012M2\n\u01eb K2\nb2\n0\n+ K4\u03c6\n\u0013\n\u2265K4\u03c6.\nIn this regime, the conditions on d as in (41) can be conveniently expressed as in (16).\n5.2\nTechnical lemmas for Theorem 3\nWe state the technical lemmas needed for proving Theorem 3. The proof for Lemma 12 follows directly\nfrom that in Belloni et al. (2014) in view of Lemma 11.\nLemma 11. Suppose all conditions in Lemma 6 hold. Then on event B0 as de\ufb01ned therein, the pair (\u03b2, t) =\n(\u03b2\u2217, \u2225\u03b2\u2217\u22252) belongs to the feasible set of the minimization problem (7) with rm,f := C0K\nq\nlog m\nf\n,\n\u00b5 \u224d2D2Krm,f\nand\n\u03c4 \u224dD0M\u01ebrm,f\n(43)\nwhere D0 = (\u221a\u03c4B + \u221aamax) and D2 = 2(\u2225A\u22252 + \u2225B\u22252) as in Theorem 3.\nLemma 12. Let \u00b5, \u03c4 > 0 be set. Suppose that the pair (\u03b2, t) = (\u03b2\u2217, \u2225\u03b2\u2217\u22252) belongs to the feasible set of\nthe minimization problem (7), for which (b\u03b2, bt) is an optimal solution. Denote by v = b\u03b2 \u2212\u03b2\u2217. Then\n\u2225vSc\u22251\n\u2264\n(1 + \u03bb) \u2225vS\u22251 and bt \u22641\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252 .\nLemma 13. On event B0 \u2229B10,\n\u2225\u03a8v\u2225\u221e\u2264\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52 \u2225v\u22251 + \u03c4\nwhere \u00b51 = 2\u00b5, \u00b52 = \u00b5( 1\n\u03bb + 1) and \u03c4 \u2032 = 2\u03c4 for \u00b5, \u03c4 as de\ufb01ned in (43).\nWe prove Lemmas 11, 12 and 13 in Section E.1.\n17\n5.3\nImproved bounds for the Lasso-type estimator\nWe give an outline illustrating where the improvement for the lasso error bounds as stated in Theorem 4 come\nfrom. We emphasize the impact of this improvement over sparsity parameter d0. The proof for Theorem 4\nfollows exactly the same line of arguments as in Theorem 2 except that we now use the improved bound\non the error term\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221egiven in Lemma 14 instead of that in Lemma 6 which is used in proving\nTheorems 2 and 3. See Section 11 for details, as well as the proof for Theorem 4 and the following two\nlemmas.\nLemma 14. Suppose all conditions in Lemma 6 hold. Let D0, D\u2032\n0, Doracle, and \u03c4 +/2\nB\n:= \u03c4 1/2\nB\n+ Doracle\n\u221am\nbe\nas de\ufb01ned in (22) and (23). On event B0,\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\n\u03c8\ns\nlog m\nf\n(44)\nwhere \u03c8 := C0K\n\u0010\nD\u2032\n0\u03c4 +/2\nB\nK \u2225\u03b2\u2217\u22252 + D0M\u01eb\n\u0011\n. Then P (B0) \u22651 \u221216/m3.\nMoreover, we replace Lemma 10 with Lemma 15, the proof of which follows from Lemma 10 with d now\nbeing bounded as in (25) and \u03c8 being rede\ufb01ned as immediately above in (44).\nLemma 15. Suppose all conditions in Lemma 9 hold. Suppose that (25) holds. Then (42) holds with \u03c8 as\nde\ufb01ned in Theorem 4 and \u03b1 = \u03bbmin(A)/2.\n5.4\nImproved bounds for the DS-type estimator\nAn \u201coracle\u201d rate for the Conic programming estimator (7) is de\ufb01ned as follows. Recall the following no-\ntation: rm,f = C0K\nq\nlog m\nf\n. The trick is that we assume that we know the noise level in W by knowing\n\u03c4B := tr(B)/f, then we can set\n\u00b5 \u224dD\u2032\n0(\u03c4 1/2\nB\n+ Doracle/\u221am)Krm,f\nwhile retaining \u03c4 \u224dD0Mrm,f\nin view of the improved error bounds over\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221eas given in Lemma 14. Without knowing this\nparameter, we could rely on the estimate from b\u03c4B as in (4), which is what we do next. For a chosen\nparameter C6, we use b\u03c4 1/2\nB\n+ C6Kr1/2\nm,m to replace \u03c4 +/2\nB\n:= \u03c4 1/2\nB\n+ Doracle/\u221am and set\n\u00b5\n\u224d\nC0D\u2032\n0(b\u03c4 1/2\nB\n+ C6Kr1/2\nm,m)K2\ns\nlog m\nf\nwhere\nC6 \u2265Doracle,\nrm,m\n=\nC0\ns\nlog m\nmf\n> C0\n\u221alog m\nm\nand Doracle := 2(\u2225A\u22251/2\n2\n+ \u2225B\u22251/2\n2\n).\nNotice that we know neither D\u2032\n0 nor Doracle, where recall D\u2032\n0 =\np\n\u2225B\u22252 + a1/2\nmax. However, assuming that\nwe normalize the column norms of design matrix X to be roughly at the same scale, we have\nD\u2032\n0 \u224d1\nwhile\nDoracle/\u221am = o(1) in case \u2225A\u22252 , \u2225B\u22252 \u2264M\nfor some large enough constant M. This is crucial in deriving and putting the faster rates of convergence in\nestimating b\u03b2 and in predictive error \u2225Xv\u22252\n2 when \u03c4B = o(1) in perspective, in view of Lemmas 16 and 18.\nLemma 16 follows directly from Lemma 14.\n18\nLemma 16. Suppose all conditions in Lemma 14 hold. Let D0 = (\u221a\u03c4B + \u221aamax) \u224d1 under (A1). Then\non event B0, the pair (\u03b2, t) = (\u03b2\u2217, \u2225\u03b2\u2217\u22252) belongs to the feasible set \u03a5 of the minimization problem (7) with\n\u00b5 \u2265D\u2032\n0\u03c4 +/2\nB\nKrm,f\nand\n\u03c4 \u2265D0M\u01ebrm,f.\n(45)\nwhere \u03c4 +/2\nB\n:= \u03c4 1/2\nB\n+ Doracle\n\u221am\nis as de\ufb01ned in (23).\nLemma 17. On event B6 and (A1), the choice of e\u03c4 1/2\nB\nas in (30) satis\ufb01es for m \u226516 and C0 \u22651,\n\u03c4 +/2\nB\n\u2264\ne\u03c4 1/2\nB\n\u2264\u03c4 1/2\nB\n+ 3\n2C6Kr1/2\nmm =: \u03c4 \u2020/2\nB\n(46)\ne\u03c4B\n\u2264\n2\u03c4B + 3C2\n6K2rmm \u224d\u03c4 \u2021\nB and moreover\ne\u03c4 1/2\nB \u03c4 \u2212\nB \u22641\n(47)\nWe next state an updated result in Lemma 18.\nLemma 18. On event B0 \u2229B10, the solution b\u03b2 to (7) with \u00b5, \u03c4 as in (30) and (29), satis\ufb01es for v := b\u03b2 \u2212\u03b2\u2217\n\r\r\r 1\nf XT\n0 X0v\n\r\r\r\n\u221e\u2264\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52 \u2225v\u22251 + \u03c4 \u2032\nwhere \u00b51 = 2\u00b5, \u00b52 = 2\u00b5(1 + 1\n2\u03bb) and \u03c4 \u2032 = 2\u03c4.\n6\nLower and Upper RE conditions\nThe goal of this section is to show that for \u2206de\ufb01ned in (51), the presumption in Lemmas 32 and 34 as\nrestated in (48) holds with high probability (cf Theorem 20). We \ufb01rst state a deterministic result showing\nthat the Lower and Upper RE conditions hold for b\u0393A under condition (48) in Corollary 19. This allows\nus to prove Lemma 9 in Sections D.1. See Sections G and H, where we show that Corollary 19 follows\nimmediately from the geometric analysis result as stated in Lemma 34.\nCorollary 19. Let 1/8 > \u03b4 > 0. Let 1 \u2264\u03b6 < m/2. Let Am\u00d7m be a symmetric positive semide\ufb01nite\ncovariance matrice. Let b\u0393A be an m \u00d7 m symmetric matrix and \u2206= b\u0393A \u2212A. Let E = \u222a|J|\u2264\u03b6EJ, where\nEJ = span{ej : j \u2208J}. Suppose that \u2200u, v \u2208E \u2229Sm\u22121\n\f\fuT \u2206v\n\f\f \u2264\u03b4 \u22641\n8\u03bbmin(A).\n(48)\nThen the Lower and Upper RE conditions holds: for all \u03c5 \u2208Rm,\n\u03c5T b\u0393A\u03c5\n\u2265\n1\n2\u03bbmin(A) \u2225\u03c5\u22252\n2 \u2212\u03bbmin(A)\n2\u03b6\n\u2225\u03c5\u22252\n1\n(49)\n\u03c5T b\u0393A\u03c5\n\u2264\n3\n2\u03bbmax(A) \u2225\u03c5\u22252\n2 + \u03bbmin(A)\n2\u03b6\n\u2225\u03c5\u22252\n1 .\n(50)\nTheorem 20. Let Am\u00d7m, Bf\u00d7f be symmetric positive de\ufb01nite covariance matrices. Let E = \u222a|J|\u2264\u03b6EJ for\n1 \u2264\u03b6 < m/2. Let Z, X be f \u00d7 m random matrices de\ufb01ned as in Theorem 2. Let b\u03c4B be de\ufb01ned as in (4).\nLet\n\u2206:= b\u0393A \u2212A := 1\nf XT X \u2212b\u03c4BIm \u2212A.\n(51)\n19\nSuppose that for some absolute constant c\u2032 > 0 and 0 < \u03b5 \u22641\nC\ntr(B)\n\u2225B\u22252\n\u2265\n\u0012\nc\u2032K4 \u03b6\n\u03b52 log\n\u00123em\n\u03b6\u03b5\n\u0013\u0013 _\nlog m\n(52)\nwhere C = C0/\n\u221a\nc\u2032 for C0 as chosen to satisfy (86).\nThen with probability at least 1 \u22124 exp\n\u0010\n\u2212c2\u03b52\ntr(B)\nK4\u2225B\u22252\n\u0011\n\u22122 exp\n\u0010\n\u2212c2\u03b52 f\nK4\n\u0011\n\u22126/m3, where c2 \u22652, we\nhave for all u, v \u2208E \u2229Sm\u22121 and \u031f(\u03b6) = \u03c4B + \u03c1max(\u03b6, A), and D1 \u2264\u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221af ,\n\f\fuT \u2206v\n\f\f \u22648C\u031f(\u03b6)\u03b5 + 4C0D1K2\ns\nlog m\nmf .\nWe prove Theorem 20 in Section I. As a corollary of Theorem 20, we will state Corollary 23 in Section 7.\n7\nConcentration bounds for error-corrected gram matrices\nIn this section, we show an upper bound on the operator norm convergence as well as an isometry property\nfor estimating B using the corrected gram matrix eB := 1\nm(XXT \u2212tr(A)If). Theorem 21 and Corollary 22\nstate that for the matrix B \u227b0 with the smaller dimension, eB tends to stay positive de\ufb01nite after this error\ncorrection step with an overwhelming probability, where we rely on f being dominated by the effective rank\nof the positive de\ufb01nite matrix A. When we subtract a diagonal matrix \u03c4BIm from the gram matrix 1\nf XT X\nto form an estimator, we clearly introduce a large number of negative eigenvalues when f \u226am. This in\ngeneral is a bad idea. However, the sparse eigenvalues for eA can stay pretty close to those of A as we will\nshow in Corollary 23.\nTheorem 21. Let \u03b5 > 0. Let X be de\ufb01ned as in De\ufb01nition 1.2. Suppose that for some c\u2032 > 0 and\n0 < \u03b5 < 1/2,\ntr(A)\n\u2225A\u22252\n\u2265\nc\u2032fK4log(3/\u03b5)\n\u03b52\n.\n(53)\nThen with probability at least 1 \u22122 exp\n\u0000\u2212c\u03b52 m\nK4\n\u0001\n\u22124 exp\n\u0010\n\u2212c5\u03b52\ntr(A)\nK4\u2225A\u22252\n\u0011\n,\n\r\r\r\r\n1\nmXXT \u2212tr(A)If\nm\n\u2212B\n\r\r\r\r\n2\n\u2264\nC2\u03b5 (\u03c4A + \u2225B\u22252)\nwhere C2, c5 are absolute constants depending on c\u2032, C, where C > 4 max( 1\ncc\u2032 ,\n1\n\u221a\n4cc\u2032 ) is a large enough\nconstant.\nCorollary 22. Suppose all conditions in Theorem 21 hold. Suppose\ntr(A)\n\u2225A\u22252\n\u2265\nc\u2032fK4C2\n3\n\u03b42 log\n\u00123C3\n\u03b4\n\u0013\n.\n(54)\nwhere C3 = C2\n\u0010\n\u03c4A\n\u03bbmin(B) \u22281\n\u0011\nfor C2 as in Theorem 21. Then with the probability as stated in Theorem 21,\n(1 + 2\u03b4)B \u227bXXT\nm\n\u2212tr(A)If\nm\n\u227b(1 \u22122\u03b4)B \u227b0\nwhere for the last inequality to hold, we assume that \u03bbmin(B) > 0.\n20\nNext we show a large deviation bound on the sparse eigenvalues of the error corrected eA: eA := 1\nf XT X \u2212\n\u03c4BIm.\nCorollary 23. Let X be de\ufb01ned as in De\ufb01nition 1.2. Let eA := 1\nf XT X \u2212\u03c4BIm. Suppose\ntr(B)\n\u2225B\u22252\n\u2265\nc\u2032kK4 log(3em/k\u03b5)\n\u03b52\n.\n(55)\nThen with probability at least 1 \u22122 exp(\u2212c4\u03b52 f\nK4) \u22124 exp(\u2212c4\u03b52\ntr(B)\nK4\u2225B\u22252 ),\n\u03c1max(k, e\nA) \u2264\u03c1max(k, A)(1 + 10\u03b5) + C4\u03b5\u03c4B\nwhere C4 is an absolute constant. Moreover, suppose for C5 = C4\n\u0010\n\u03c4B\n\u03c1min(k,A) \u22281\n\u0011\ntr(B)\n\u2225B\u22252\n\u2265\nc\u2032kK4 C2\n5\n\u03b42 log(12C5em\nk\u03b4\n).\n(56)\nThen with the probability as stated immediately above, we have\n\u03c1min(k, eA) \u2265\u03c1min(k, A)(1 \u22122\u03b4).\nWe prove Theorem 21 in Section J. We also prove the concentration of measure bounds on error-corrected\ngram matrices in Corollaries 22 and 23 in Sections J.1 and J.2 respectively.\n8\nProof of Lemma 1\nWe de\ufb01ne Cone(d0, k0), where 0 < d0 < m and k0 is a positive number, as the set of vectors in Rm which\nsatisfy the following cone constraint:\nCone(d0, k0) = {x \u2208Rm | \u2203I \u2208{1, . . . , p}, |I| = d0 s.t. \u2225xIc\u22251 \u2264k0 \u2225xI\u22251} .\nFor each vector x \u2208Rp, let T0 denote the locations of the s0 largest coef\ufb01cients of x in absolute values. The\nfollowing elementary estimate Rudelson and Zhou (2013) will be used in conjunction with the RE condition.\nLemma 24. For each vector x \u2208Cone(s0, k0), let T0 denotes the locations of the s0 largest coef\ufb01cients of\nx in absolute values. Then\n\u2225xT0\u22252 \u2265\n\u2225x\u22252\n\u221a1 + k0\n.\n(57)\nProof of Lemma 1.\nPart I: Suppose that the Lower-RE condition holds for \u0393 := AT A. Let x \u2208\nCone(s0, k0). Then\n\u2225x\u22251 \u2264(1 + k0) \u2225xT0\u22251 \u2264(1 + k0)\u221as0 \u2225xT0\u22252 .\n21\nThus for x \u2208Cone(s0, k0) \u2229Sp\u22121 and \u03c4(1 + k0)2s0 \u2264\u03b1/2, we have\n\u2225Ax\u22252 = (xT AT Ax)1/2\n\u2265\n\u0010\n\u03b1 \u2225x\u22252\n2 \u2212\u03c4 \u2225x\u22252\n1\n\u00111/2\n\u2265\n\u0010\n\u03b1 \u2225x\u22252\n2 \u2212\u03c4(1 + k0)2s0 \u2225xT0\u22252\n2\n\u00111/2\n\u2265\n\u0000\u03b1 \u2212\u03c4(1 + k0)2s0\n\u00011/2 \u2265\nr\u03b1\n2 .\nThus the RE(s0, k0, A) condition holds with\n1\nK(s0, k0, A)\n:=\nmin\nx\u2208Cone(s0,k0)\n\u2225Ax\u22252\n\u2225xT0\u22252\n\u2265\nr\u03b1\n2\nwhere we use the fact that for any J \u2208{1, . . . , p} such that |J| \u2264s0, \u2225xJ\u22252 \u2264\u2225xT0\u22252. We now show the\nother direction.\nPart II. Assume that RE(4R2, 2R \u22121, A) holds for some integer R > 1. Assume that for some R > 1\n\u2225x\u22251 \u2264R \u2225x\u22252 .\nLet (x\u2217\ni )p\ni=1 be non-increasing arrangement of (|xi|)p\ni=1. Then\n\u2225x\u22251\n\u2264\nR\n\uf8eb\n\uf8ed\ns\nX\nj=1\n(x\u2217\nj)2 +\n\u221e\nX\nj=s+1\n\u0012\u2225x\u22251\nj\n\u00132\n\uf8f6\n\uf8f8\n1/2\n\u2264\nR\n\u0012\n\u2225x\u2217\nJ\u22252\n2 + \u2225x\u22252\n1\n1\ns\n\u00131/2\n\u2264R\n\u0012\n\u2225x\u2217\nJ\u22252 + \u2225x\u22251\n1\n\u221as\n\u0013\nwhere J := {1, . . . , s}. Choose s = 4R2. Then\n\u2225x\u22251 \u2264R \u2225x\u2217\nJ\u22252 + 1\n2 \u2225x\u22251 .\nThus we have\n\u2225x\u22251\n\u2264\n2R \u2225x\u2217\nJ\u22252 \u22642R \u2225x\u2217\nJ\u22251\nand hence\n(58)\n\u2225x\u2217\nJc\u22251\n\u2264\n(2R \u22121) \u2225x\u2217\nJ\u22251 .\n(59)\nThen x \u2208Cone(4R2, 2R \u22121). Then for all x \u2208Sp\u22121 such that \u2225x\u22251 \u2264R \u2225x\u22252, we have for k0 = 2R \u22121\nand s0 := 4R2,\nxT \u0393x \u2265\n\u2225xT0\u22252\n2\nK2(s0, k0, A) \u2265\n\u2225x\u22252\n2\n\u221as0K2(s0, k0, A) =: \u03b1 \u2225x\u22252\n2\nwhere we use the fact that (1 + k0) \u2225xT0\u22252\n2 \u2265\u2225x\u22252\n2 by Lemma 24 with xT0 as de\ufb01ned therein. Otherwise,\nsuppose that \u2225x\u22251 \u2265R \u2225x\u22252. Then for a given \u03c4 > 0,\n\u03b1 \u2225x\u22252\n2 \u2212\u03c4 \u2225x\u22252\n1 \u2264(\n1\n\u221as0K2(s0, k0, A) \u2212\u03c4R2) \u2225x\u22252\n2 .\n(60)\n22\nThus we have by the choice of \u03c4 as in (29) and (60)\nxT \u0393x \u2265\u03bbmin(\u0393) \u2225x\u22252\n2\n\u2265\n(\n1\n\u221as0K2(s0, k0, A) \u2212\u03c4R2) \u2225x\u22252\n2\n\u2265\n\u03b1 \u2225x\u22252\n2 \u2212\u03c4 \u2225x\u22252\n1 .\nThe Lemma thus holds.\n\u25a1\n9\nProof of Theorem 2\nFirst we note that it is suf\ufb01cient to have (14) in order for (40) to hold.\n(14) guarantees that for V =\n3eM3\nA/2\nr(B) := tr(B)\n\u2225B\u22252\n\u2265\n16c\u2032K4\nf\nlog m log Vm log m\nf\n\u2265\n16c\u2032K4\nf\nlog m log\n\u00123emM3\nA log m\n2f\n\u0013\n=\nc\u2032K4 1\n\u03b52\n4\nM2\nA\nf\nlog m log\n\uf8eb\n\uf8ed\n6emMA\n4\nM2\nA (f/ log m)\n\uf8f6\n\uf8f8\n\u2265\nc\u2032K4 1\n\u03b52 s0 log\n\u00126emMA\ns0\n\u0013\n= c\u2032K4 s0\n\u03b52 log\n\u00123em\ns0\u03b5\n\u0013\n(61)\nwhere \u03b5 =\n1\n2MA \u2264\n1\n128C , and the last inequality holds given that k log(cm/k) on the RHS of (61) is a\nmonotonically increasing function of k, and\ns0\n\u2264\n4f\nM2\nA log m and MA = 64C(\u03c1max(s0, A) + \u03c4B)\n\u03bbmin(A)\n\u226564C.\nNext we check that the choice of d as in (16) ensures that (41) holds. Indeed, for c\u2032K4 \u22641, we have\nd\n\u2264\nCA(c\u2032K4 \u22271) \u03c6f\nlog m \u2264CA\n\u0000c\u2032D\u03c6 \u22271\n\u0001\nf\nlog m.\nBy Lemma 9, we have on event A0, the modi\ufb01ed gram matrix b\u0393A := 1\nf (XT X \u2212btr(B)Im) satis\ufb01es the\nLower RE conditions with\ncurvature \u03b1 = 1\n2\u03bbmin(A) and tolerance\n\u03c4 = \u03bbmin(A)\n2s0\n= \u03b1\ns0\n.\n(62)\nTheorem 2 follows from Theorem 8, so long as we can show that condition (33) holds for \u03bb \u22654\u03c8\nq\nlog m\nf\nwhere the parameter \u03c8 is as de\ufb01ned (17), and \u03b1 and \u03c4 =\n\u03b1\ns0 are as de\ufb01ned immediately above. Combin-\ning (62) and (33), we need to show (42) holds. This is precisely the content of Lemma 10. This is the end\nof the proof for Theorem 2\n\u25a1\n23\n10\nProof of Theorem 3\nFor the set ConeJ(k0) as in (57),\n\u03baRE(d0, k0)\n:=\nmin\nJ:|J|\u2264d0\nmin\n\u2206\u2208ConeJ(k0)\n\f\f\u2206T \u03a8\u2206\n\f\f\n\u2225\u2206J\u22252\n2\n=\n\u0012\n1\nK(d0, k0, (1/\u221af)Z1A1/2)\n\u00132\n.\nRecall the following Theorem 25 from Rudelson and Zhou (2013).\nTheorem 25. Rudelson and Zhou (2013) Set 0 < \u03b4 < 1, k0 > 0, and 0 < d0 < p. Let A1/2 be an m \u00d7 m\nmatrix satisfying RE(d0, 3k0, A1/2) condition as in De\ufb01nition 2.1. Let d be as de\ufb01ned in (63)\nd\n=\nd0 + d0 max\nj\n\r\r\rA1/2ej\n\r\r\r\n2\n2\n16K2(d0, 3k0, A1/2)(3k0)2(3k0 + 1)\n\u03b42\n.\n(63)\nLet \u03a8 be an n \u00d7 m matrix whose rows are independent isotropic \u03c82 random vectors in Rm with constant \u03b1.\nSuppose the sample size satis\ufb01es\nn \u22652000d\u03b14\n\u03b42\nlog\n\u001260em\nd\u03b4\n\u0013\n.\n(64)\nThen with probability at least 1 \u22122 exp(\u2212\u03b42n/2000\u03b14), RE(d0, k0, (1/\u221an)\u03a8A1/2) condition holds for\nmatrix (1/\u221an)\u03a8A with\n0 < K(d0, k0, (1/\u221an)\u03a8A1/2) \u2264K(d0, k0, A1/2)\n1 \u2212\u03b4\n.\n(65)\nProof of Theorem 3.\nSuppose RE(2d0, 3k0, A1/2) holds. Then for d as de\ufb01ned in (28) and f =\n\u2126(dK4 log(m/d)), we have with probability at least 1 \u22122 exp(\u03b42f/2000K4), the RE(2d0, k0,\n1\n\u221af Z1A1/2)\ncondition holds with\n\u03baRE(2d0, k0)\n=\n\u0012\n1\nK(2d0, k0, (1/\u221af)Z1A1/2)\n\u00132\n\u2265\n\u0012\n1\n2K(2d0, k0, A1/2)\n\u00132\nby Theorem 25.\nThe rest of the proof follows from Belloni et al. (2014) Theorem 1 and thus we only provide a sketch. In\nmore details, in view of the lemmas shown in Section 5, we need\n\u03baq(d0, k0) \u2265cd\u22121/q\n0\nto hold for some constant c for \u03a8 := 1\nf XT\n0 X0. It is shown in Appendix C in Belloni et al. (2014) that under\nthe RE(2d0, k0,\n1\n\u221af Z1A1/2) condition, for any d0 \u2264m/2 and 1 \u2264q \u22642, we have\n\u03ba1(d0, k0)\n\u2265\ncd\u22121\n0 \u03baRE(d0, k0),\n\u03baq(d0, k0)\n\u2265\nc(q)d\u22121/q\n0\n\u03baRE(2d0, k0)\n(66)\nwhere c(q) > 0 depends on k0 and q. The theorem is thus proved following exactly the same line of\narguments as in the proof of Theorem 1 in Belloni et al. (2014) in view of the \u2113q sensitivity condition\n24\nderived immediately above, in view of Lemmas 11, 12 and 13. Indeed, we have for v := b\u03b2 \u2212\u03b2\u2217, we have\nby de\ufb01nition of \u2113q sensitivity as in (37)\nc(q)d\u22121/q\n0\n\u03baRE(2d0, k0) \u2225v\u2225q\n\u2264\n\u03baq(d0, k0) \u2225v\u2225q \u2264\n\r\r\r 1\nf XT\n0 X0v\n\r\r\r\n\u221e\n\u2264\n\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52 \u2225v\u22251 + \u03c4\n\u2264\n\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52(2 + \u03bb) \u2225vS\u22251 + \u03c4\n\u2264\n\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52(2 + \u03bb)d1\u22121/q\n0\n\u2225vS\u2225q + \u03c4\n\u2264\n\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52(2 + \u03bb)d1\u22121/q\n0\n\u2225v\u2225q + \u03c4.\n(67)\nThus we have for d0 = c0\np\nf/ log m where c0 is suf\ufb01ciently small,\nd\u22121/q\n0\n(c(q)\u03baRE(2d0, k0) \u2212\u00b52(2 + \u03bb)d0) \u2225v\u2225q \u2264\u00b51 \u2225\u03b2\u2217\u22252 + \u03c4\nhence\n\u2225v\u2225q \u2264C(4D2rm,fK \u2225\u03b2\u2217\u22252 + 2D0M\u01ebrm,f)d1/q\n0\n\u22644CD2rm,f(K \u2225\u03b2\u2217\u22252 + M\u01eb)d1/q\n0\nfor some constant C = 1/ (c(q)\u03baRE(2d0, k0) \u2212\u00b52(2 + \u03bb)d0) \u22651/ (2c(q)\u03baRE(2d0, k0)) given that\n\u00b52(2 + \u03bb)d0 = 2D2Krm,f( 1\n\u03bb + 1)(2 + \u03bb)c0\np\nf/ log m = 2c0C0D2K2(2 + \u03bb)( 1\n\u03bb + 1)\nis suf\ufb01ciently small and thus (21) holds. The prediction error bound follows exactly the same line of ar-\nguments as in Belloni et al. (2014) which we omit here. See proof of Theorem 5 in Section F for details.\n\u25a1\n11\nProof of Theorem 4\nThe proof is identical to the proof of Theorem 2 up till (62), except that we replace the condition on d as\nin the theorem statement by (25): that is,\nd := |supp(\u03b2\u2217)| \u2264CA\nf\nlog m\n\b\nc\u2032C\u03c6 \u22272\n\t\nwhere CA :=\n1\n128M2\nA\n,\nC\u03c6 := \u2225B\u22252 + amax\nD2\n\u0012K2M2\n\u01eb\nb2\n0\n+ \u03c4 +\nB K4\u03c6\n\u0013\n\u2265\u2225B\u22252 + amax\nD2\n\u03c4 +\nB\nwhere c\u2032, \u03c6, b0, M\u01eb and K are as de\ufb01ned in Theorem 2, where we assume that b2\n0 \u2265\u2225\u03b2\u2217\u22252\n2 \u2265\u03c6b2\n0 for some 0 <\n\u03c6 \u22641.\nTheorem 4 follows from Theorem 8, so long as we can show that condition (33) holds for\n\u03bb \u22652\u03c8\nq\nlog m\nf\nwhere the parameter \u03c8 is as de\ufb01ned (44), and \u03b1 and \u03c4 = \u03b1\ns0 are as de\ufb01ned in (62). Combin-\ning (62) and (33), we need to show (42) holds. This is precisely the content of Lemma 15. This is the end\nof the proof for Theorem 4.\n\u25a1\n25\n12\nProof of Theorem 5\nThroughout this proof, we assume that B0 \u2229B10 holds. The rest of the proof follows that of Theorem 3,\nexcept for the last part. Let \u00b51, \u00b52, \u03c4 be as de\ufb01ned in Lemma 13. We have for \u00b52 := 2\u00b5(1 +\n1\n2\u03bb) where\n\u00b5 = D\u2032\n0Krm,fe\u03c4 1/2\nB , and d0 = c0\u03c4 \u2212\nB\np\nf/ log m,\n\u00b52(2 + \u03bb)d0\n=\n2C0D\u2032\n0K2e\u03c4 1/2\nB ( 1\n2\u03bb + 1)(2 + \u03bb)c0\u03c4 \u2212\nB\n(68)\n\u2264\n2c0C0D\u2032\n0K2(2 + \u03bb)( 1\n2\u03bb + 1) \u22641\n2c(q)\u03baRE(2d0, k0)\nwhich holds when c0 is suf\ufb01ciently small, where by (47) \u03c4 \u2212\nB e\u03c4 1/2\nB\n\u22641. Hence\n\u00b52d0 \u2264c(q)\u03baRE(2d0, k0)\n2(2 + \u03bb)\nThus for c0 suf\ufb01ciently small, \u00b51 = 2\u00b5, by (66), (68), (67) and (46),\nd\u22121/q\n0\n1\n2(c(q)\u03baRE(2d0, k0)) \u2225v\u2225q\n=\nd\u22121/q\n0\n(c(q)\u03baRE(2d0, k0) \u2212\u00b52(2 + \u03bb)d0) \u2225v\u2225q\n\u2264\n(\u03baq(d0, k0) \u2212\u00b52(2 + \u03bb)d1\u22121/q\n0\n) \u2225v\u2225q \u2264\u00b51 \u2225\u03b2\u2217\u22252 + \u03c4\n\u2264\n2D\u2032\n0rm,fK2((\u03c4 1/2\nB\n+ (3/2)C6Kr1/2\nm,m) \u2225\u03b2\u2217\u22252 + M\u01eb/K)\n(69)\nand thus (31) holds, following the proof in Theorem 3. The prediction error bound follows exactly the same\nline of arguments as in Belloni et al. (2014), which we now include for the sake completeness. Follow-\ning (31), we have by (69),\n\u2225v\u22251\n\u2264\nC11d0(\u00b51 \u2225\u03b2\u2217\u22252 + \u03c4) where C11 = 2/ (c(q)\u03baRE(2d0, k0))\nand hence \u00b52 \u2225v\u22251\n\u2264\nC11\u00b52d0(\u00b51 \u2225\u03b2\u2217\u22252 + \u03c4)\n\u2264\nC11\n1\n2(2 + \u03bb) (c(q)\u03baRE(2d0, k0)) (\u00b51 \u2225\u03b2\u2217\u22252 + \u03c4)\n=\n1\n2 + \u03bb(\u00b51 \u2225\u03b2\u2217\u22252 + \u03c4)\nThus we have by (69), the bounds immediately above, and (47)\n1\nf\n\r\r\rX(b\u03b2 \u2212\u03b2\u2217)\n\r\r\r\n2\n2\n\u2264\n\u2225v\u22251\n\r\r\r 1\nf XT\n0 X0v\n\r\r\r\n\u221e\n\u2264\nC11d0(\u00b51 \u2225\u03b2\u2217\u22252 + \u03c4) (\u00b51 \u2225\u03b2\u2217\u22252 + \u00b52 \u2225v\u22251 + 2\u03c4)\n\u2264\nC11d0(\u00b51 \u2225\u03b2\u2217\u22252 + \u03c4)(1 +\n1\n2 + \u03bb) (\u00b51 \u2225\u03b2\u2217\u22252 + 2\u03c4)\n=\nC\u2032(D\u2032\n0)2K4d0\nlog m\nf\n\u0012\ne\u03c4 1/2\nB\n\u2225\u03b2\u2217\u22252 + M\u01eb\nK\n\u00132\n\u2264\nC\u2032\u2032(\u2225B\u22252 + amax)K2d0\nlog m\nf\n\u0010\n(2\u03c4B + 3C2\n6K2rm,m)K2 \u2225\u03b2\u2217\u22252\n2 + M2\n\u01eb\n\u0011\nwhere (D\u2032\n0)2 \u22642 \u2225B\u22252 + 2amax. The theorem is thus proved.\n\u25a1\n26\n13\nConclusion\nIn view of the main Theorems 2 and 3, at this point, we do not really think one estimator is preferable to\nthe other. While the rates we obtain for both estimators are at the same order for q = 1, 2, the conditions\nunder which these rates are obtained are somewhat different. Lasso estimator allows large values of sparsity,\nwhile Conic-programming estimator conceptually is more adaptive by not \ufb01xing an upper bound on \u2225\u03b2\u2217\u22252\na priori, the cost of which seems to be a more stringent requirement on the sparsity level. The lasso-type\nprocedure can recover a sparse model using O(log m) number of measurements per nonzero component\ndespite the measurement error in X and the stochastic noise \u01eb while the Dantzig selector-type allows only\nd \u224d\np\nf/ log m to achieve the error rate at the same order as the Lasso-type estimator.\nHowever, we show in Theorem 5 in Section 5.4 that this restriction on the sparsity can be relaxed for\nthe Conic programming estimator (7), when we make a different choice for the parameter \u00b5 based on a\nmore re\ufb01ned analysis. Eventually, as \u03c4B \u21920, this relaxation on d as in (32) enables the Conic Program-\nming estimator to achieve bounds which are essentially identical to the Dantzig Selector when the design\nmatrix X0 is a subgaussian random matrix satisfying the Restricted Eigenvalue conditions; See for exam-\nple Cand`es and Tao (2007); Bickel et al. (2009); Rudelson and Zhou (2013). For the Lasso estimator, when\nwe require that the stochastic error \u01eb in the response variable y as in (1a) does not converge to 0 as quickly\nas the measurement error W in (1b) does, then the sparsity constraint becomes essentially unchanged as\n\u03c4B \u21920. These tradeoffs are somehow different from the behavior of the Conic programming estimator\nversus the Lasso estimator; however, we believe the differences are minor.\nWe now state a slightly sharper bound than those in Lemma 14 which provides a signi\ufb01cant improvement\non the error bounds in case \u03c4B = o(1) while \u2225A\u22252 \u22651 for the Lasso-type estimator in (6) as well as the\nConic programming estimator (7). Recall D\u2032\n0 :=\np\n\u2225B\u22252 + a1/2\nmax. By (74),\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\nD\u2032\n0K\u03c4 1/2\nB\n\u2225\u03b2\u2217\u22252 rm,f + 2D1K\n\u221am \u2225\u03b2\u2217\u2225\u221erm,f + D0M\u01ebrm,f\nWhen \u03c4B \u21920, we have for D0 = \u221a\u03c4B + a1/2\nmax \u2192a1/2\nmax\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n=\nO\n\u0012\nD1K 1\n\u221am \u2225\u03b2\u2217\u2225\u221e+ D0KM\u01eb\n\u0013\nK\ns\nlog m\nf\nwhere D1 = \u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221af\n\u2192\u2225A\u22251/2\n2\nunder (A1), given that \u2225B\u2225F /\u221af \u2264\u03c4 1/2\nB\n\u2225B\u22251/2\n2\n\u21920, and the \ufb01rst\nterm inside the bracket comes from the estimation error in btr(B)/f, which can be made go away if we were\nto assume that tr(B) is also known. In this case, the error term involving \u2225\u03b2\u2217\u22252 in (17) vanishes, and we\nonly need to set\n\u03bb \u22652\u03c8\ns\nlog m\nf\nwhere\n\u03c8 \u224dD0KM\u01eb + \u2225A\u22251/2\n2\nK2m\u22121/2 \u2225\u03b2\u2217\u2225\u221e.\n(70)\nMoreover, suppose that tr(B) is given, then one can drop the second term in \u03c8 as in (70) and hence recover\nthe lasso bound when the design matrix X is assumed to be free of measurement errors.\nFinally, we note that the bounds corresponding to the Upper RE condition as stated in Corollary 19,\nTheorem 20 and Lemma 9 are not needed for Theorem 2. They are useful to ensure algorithmic con-\nvergence and to bound the optimization error for the gradient descent-type of algorithms as considered\n27\nin Loh and Wainwright (2012), when one is interested in approximately solving the non-convex optimiza-\ntion function (6). Our numerical results validate such algorithmic and statistical convergence properties.\nAcknowledgements\nThe authors are grateful for the helpful discussions with Prof. Rob Kass.\nA\nOutline\nIn Sections B and B.2, we present variations of the Hanson-Wright inequality as recently derived in Rudelson and Vershynin\n(2013) (cf. Lemma 27), concentration of measure bounds and stochastic error bounds in Lemma 29.\nIn Sections C and E, we prove the technical lemmas for Theorems 2 and 3 respectively. In Section F, we\nprove the Lemmas needed for Proof of Theorem 5. In order to prove Corollary 19, we need to \ufb01rst state\nsome geometric analysis results Section G. We prove Corollary 19 in Section H and Theorem 20 in Section I.\nResults presented in Section 7 are proved in Section J. In particular, we prove Theorem 21 in Section J. We\nalso prove the concentration of measure bounds on error-corrected gram matrices in Corollaries 22 and 23\nin Sections J.1 and J.2 respectively. The results appearing in Section J are proved in Section K.\nB\nSome auxiliary results\nWe \ufb01rst need to state the following form of the Hanson-Wright inequality as recently derived in Rudel-\nson and Vershynin Rudelson and Vershynin (2013), and an auxiliary result in Lemma 27 which may be of\nindependent interests.\nTheorem 26. Let X = (X1, . . . , Xm) \u2208Rm be a random vector with independent components Xi which\nsatisfy E (Xi) = 0 and \u2225Xi\u2225\u03c82 \u2264K. Let A be an m \u00d7 m matrix. Then, for every t > 0,\nP\n\u0000\f\fXT AX \u2212E\n\u0000XT AX\n\u0001\f\f > t\n\u0001\n\u22642 exp\n\"\n\u2212c min\n \nt2\nK4 \u2225A\u22252\nF\n,\nt\nK2 \u2225A\u22252\n!#\n.\nWe note that following the proof of Theorem 26, it is clear that the following holds: Let X = (X1, . . . , Xm) \u2208\nRm be a random vector as de\ufb01ned in Theorem 26. Let Y, Y \u2032 be independent copies of X. Let A be an m\u00d7m\nmatrix. Then, for every t > 0,\nP\n\u0000\f\fY T AY \u2032\f\f > t\n\u0001\n\u22642 exp\n\"\n\u2212c min\n \nt2\nK4 \u2225A\u22252\nF\n,\nt\nK2 \u2225A\u22252\n!#\n.\n(71)\nWe next need to state Lemma 27, which we prove in Section B.1.\nLemma 27. Let u, w \u2208Sf\u22121. Let A \u227b0 be a m \u00d7 m symmetric positive de\ufb01nite matrix. Let Z be an\nf \u00d7 m random matrix with independent entries Zij satisfying EZij = 0 and \u2225Zij\u2225\u03c82 \u2264K. Let Z1, Z2 be\n28\nindependent copies of Z. Then for every t > 0,\nP\n\u0010\f\f\fuT Z1A1/2ZT\n2 w\n\f\f\f > t\n\u0011\n\u2264\n2 exp\n \n\u2212c min\n \nt2\nK4tr(A),\nt\nK2 \u2225A\u22251/2\n2\n!!\n,\nP\n\u0000\f\fuT ZAZTw \u2212EuT ZAZTw\n\f\f > t\n\u0001\n\u2264\n2 exp\n \n\u2212c min\n \nt2\nK4 \u2225A\u22252\nF\n,\nt\nK2 \u2225A\u22252\n!!\nwhere c is the same constant as de\ufb01ned in Theorem 26.\nB.1\nProof of Lemma 27\nLemma 28 is a well-known fact.\nLemma 28. Let Auw := (u\u2297w)\u2297A where u, w \u2208Sp\u22121 where p \u22652. Then \u2225Auw\u22252 \u2264\u2225A\u22252 and \u2225Auw\u2225F \u2264\n\u2225A\u2225F .\nProof of Lemma 27.\nLet z1, . . . , zf, z\u2032\n1, . . . , z\u2032\nf \u2208Rm be the row vectors Z1, Z2 respectively. Notice\nthat we can write the quadratic form as follows:\nuT Z1A1/2ZT\n2 w\n=\nX\ni,j=1,m\nuiwjziA1/2z\u2032\nj\n=\nvec\n\b\nZT\n1\n\tT \u0000(u \u2297w) \u2297A1/2\u0001\nvec\n\b\nZT\n2\n\t\n=:\nvec\n\b\nZT\n1\n\tT A1/2\nuw vec\n\b\nZT\n2\n\t\n,\nuT ZAZTw\n=\nvec\n\b\nZT \tT \u0000(u \u2297w) \u2297A\n\u0001\nvec\n\b\nZT \t\n=:\nvec\n\b\nZT \tT Auwvec\n\b\nZT \t\nwhere clearly by independence of Z1, Z2,\nEvec\n\b\nZT\n1\n\tT \u0000(u \u2297w) \u2297A1/2\u0001\nvec\n\b\nZT\n2\n\t\n=\n0,\nand\nEvec { Z }T \u0000(u \u2297u) \u2297A\n\u0001\nvec { Z }\n=\ntr\n\u0000(u \u2297u) \u2297A\n\u0001\n= tr(A).\nThus we invoke (71) and Lemma 28 to show the concentration bounds on event {\n\f\fuT Z1A1/2ZT\n2 w\n\f\f > t}:\nP\n\u0010\f\f\fuT Z1A1/2ZT\n2 w\n\f\f\f > t\n\u0011\n\u2264\n2 exp\n\uf8eb\n\uf8ec\n\uf8ed\u2212min\n\uf8eb\n\uf8ec\n\uf8ed\nt2\nK4\n\r\r\rA1/2\nuw\n\r\r\r\n2\nF\n,\nt\nK2\n\r\r\rA1/2\nuw\n\r\r\r\n2\n\uf8f6\n\uf8f7\n\uf8f8\n\uf8f6\n\uf8f7\n\uf8f8\n\u2264\n2 exp\n \n\u2212min\n \nt2\nK4tr(A),\nt\nK2 \r\rA1/2\r\r\n2\n!!\n.\nSimilarly, we have by Theorem 26 and Lemma 28,\nP\n\u0000\f\fuT ZAZTw \u2212EuTZAZT w\n\f\f > t\n\u0001\n\u2264\n2 exp\n \n\u2212c min\n \nt2\nK4 \u2225Auw\u22252\nF\n,\nt\nK2 \u2225Auw\u22252\n!!\n\u2264\n2 exp\n \n\u2212c min\n \nt2\nK4 \u2225A\u22252\nF\n,\nt\nK2 \u2225A\u22252\n!!\n.\n29\nThe Lemma thus holds.\n\u25a1\nB.2\nStochastic error terms\nThe following large deviation bounds in Lemmas 29 and 7 are the key results in proving Lemmas 6 and 13.\nLet C0 satisfy (86) for c as de\ufb01ned in Theorem 26. Throughout this section, we denote by:\nrm,f = C0K\ns\nlog m\nf\nand rm,m = 2C0\ns\nlog m\nmf .\nWe also de\ufb01ne some events B4, B5, B6, B10; Denote by B0 := B4 \u2229B5 \u2229B6, which we use throughout this\npaper.\nLemma 29. Assume that the stable rank of B, \u2225B\u22252\nF / \u2225B\u22252\n2 \u2265log m. Let Z, X0 and W as de\ufb01ned in\nTheorem 2. Let Z0, Z1 and Z2 be independent copies of Z. Let \u01ebT \u223cY M\u01eb/K where Y := eT\n1 ZT\n0 . Let\n\u03c4B = tr(B)\nf\n. Denote by B4 the event such that\n1\nf\n\r\r\rA\n1\n2 ZT\n1 \u01eb\n\r\r\r\n\u221e\n\u2264\nrm,fM\u01eba1/2\nmax\nand\n1\nf\n\r\r\rZT\n2 B\n1\n2 \u01eb\n\r\r\r\n\u221e\n\u2264\nrm,fM\u01eb\n\u221a\u03c4B.\nThen P (B4) \u22651 \u22124/m3. Moreover, denote by B5 the event such that\n1\nf\n\r\r(ZT BZ \u2212tr(B)Im)\u03b2\u2217\r\r\n\u221e\n\u2264\nrm,fK \u2225\u03b2\u2217\u22252\n\u2225B\u2225F\n\u221af\nand\n1\nf\n\r\rXT\n0 W\u03b2\u2217\r\r\n\u221e\n\u2264\nrm,fK \u2225\u03b2\u2217\u22252\n\u221a\u03c4Ba1/2\nmax.\nThen P (B5) \u22651 \u22124/m3.\nFinally, denote by B10 the event such that\n1\nf\n\r\r(ZT BZ \u2212tr(B)Im)\n\r\r\nmax\n\u2264\nrm,fK \u2225B\u2225F\n\u221af\nand\n1\nf\n\r\rXT\n0 W\n\r\r\nmax\n\u2264\nrm,fK \u2225\u03b2\u2217\u22252\n\u221a\u03c4Ba1/2\nmax.\nThen P (B10) \u22651 \u22124/m2.\nWe prove Lemmas 29 in Section B.3.\nB.3\nStochastic error bounds\ns Following Lemma 27, we have for all t > 0, B \u227b0 being an f \u00d7 f symmetric positive de\ufb01nite matrix,\nand v, w \u2208Rm\nP\n\u0010\f\f\fvT ZT\n1 B1/2Z2w\n\f\f\f > t\n\u0011\n\u2264\n2 exp\n\"\n\u2212c min\n \nt2\nK4tr(B),\nt\nK2 \u2225B\u22251/2\n2\n!#\n(72)\nP\n\u0000\f\fvT ZTBZw \u2212EvT ZT BZw\n\f\f > t\n\u0001\n\u2264\n2 exp\n \n\u2212c min\n \nt2\nK4 \u2225B\u22252\nF\n,\nt\nK2 \u2225B\u22252\n!!\n.\n30\nB.4\nProof for Lemma 29\nLet e1, . . . , em \u2208Rm be the canonical basis spanning Rm. Let x1, . . . , xm, x\u2032\n1, . . . , x\u2032\nm \u2208Rf be the\ncolumn vectors Z1, Z2 respectively. Let Y \u223ceT\n1 ZT\n0 . Let wi =\nA1/2ei\n\u2225A1/2ei\u22252\nfor all i. Clearly the condition on\nthe stable rank of B guarantees that\nf \u2265r(B) = tr(B)\n\u2225B\u22252\n= tr(B) \u2225B\u22252\n\u2225B\u22252\n2\n\u2265\u2225B\u22252\nF / \u2225B\u22252\n2 \u2265log m.\nBy (71), we obtain for t\u2032 = C0M\u01ebK\np\ntr(B) log m and t = C0K2\u221alog mtr(B)1/2:\nP\n\u0010\n\u2203j,\n\f\f\f\u01ebT B1/2Z2ej\n\f\f\f > t\u2032\u0011\n=\nP\n\u0012\n\u2203j, M\u01eb\nK\n\f\f\feT\n1 ZT\n0 B1/2Z2ej\n\f\f\f > C0M\u01ebK\np\nlog mtr(B)\n1\n2\n\u0013\n\u2264\nexp(log m)P\n\u0010\f\f\fY T B1/2x\u2032\nj\n\f\f\f > C0K2p\nlog mtr(B)\n1\n2\n\u0011\n\u22642/m3\nwhere the last inequality holds by the union bound, given that tr(B)\n\u2225B\u22252 \u2265log m, and for all j\nP\n\u0010\f\f\fY T B1/2x\u2032\nj\n\f\f\f > t\n\u0011\n\u2264\n2 exp\n \n\u2212c min\n \nt2\nK4tr(B),\nt\nK2 \u2225B\u22251/2\n2\n!!\n,\n\u2264\n2 exp\n \n\u2212c min\n \nC2\n0 log m, C0 log1/2 m\np\ntr(B)\n\u2225B\u22251/2\n2\n!!\n\u2264\n2 exp\n\u0000\u2212c min(C2\n0, C0) log m\n\u0001\n\u22642 exp (\u22124 log m) .\nLet v, w \u2208Sm\u22121. Thus we have by Lemma 27, for t0 = C0M\u01ebK\u221af log m and \u03c4 = C0K2\u221af log m,\nwj =\nA1/2ej\n\u2225A1/2ej\u22252\nand f \u2265log m,\nP\n\u0000\u2203j,\n\f\f\u01ebT Z1wj\n\f\f > t0\n\u0001\n\u2264P\n\u0012\n\u2203j, M\u01eb\nK\n\f\fY T Z1wj\n\f\f > C0M\u01ebK\np\nf log m\n\u0013\n\u2264\nmP\n\u0010\f\fY T Z1wj\n\f\f > C0K2p\nf log m\n\u0011\n=\nexp(log m)P\n\u0000\f\feT\n1 ZT\n0 Z1wj\n\f\f > \u03c4\n\u0001\n\u22642 exp\n\u0012\n\u2212c min\n\u0012 \u03c4 2\nK4f , \u03c4\nK2\n\u0013\u0013\n,\n\u2264\n2 exp\n\u0012\n\u2212c min\n\u0012(C0K2\u221af log m)2\nK4f\n, C0K2\u221af log m\nK2\n\u0013\n+ log m\n\u0013\n\u2264\n2m exp\n\u0010\n\u2212c min\n\u0010\nC2\n0 log m, C0 log1/2 m\np\nf\n\u0011\u0011\n\u2264\n2m exp\n\u0000\u2212c min(C2\n0, C0) log m\n\u0001\n\u22642 exp (\u22123 log m) .\nTherefore we have with probability at least 1 \u22124/m3,\n\r\r\rZT\n2 B\n1\n2\u01eb\n\r\r\r\n\u221e\n:=\nmax\nj=1,...,m \u27e8\u01ebT B1/2Z2, ej \u27e9\u2264t\u2032 = C0M\u01ebK\np\ntr(B) log m\n\r\r\rA\n1\n2 ZT\n1 \u01eb\n\r\r\r\n\u221e\n:=\nmax\nj=1,...,m \u27e8A1/2ej, ZT\n1 \u01eb \u27e9\u2264\nmax\nj=1,...,m\n\r\r\rA1/2ej\n\r\r\r\n2\nmax\nj=1,...,m \u27e8wj, ZT\n1 \u01eb \u27e9\n\u2264\na1/2\nmaxt0 = a1/2\nmaxC0M\u01ebK\np\nf log m.\n31\nThe \u201cmoreover\u201d part follows exactly the same arguments as above. Denote by \u00af\u03b2\u2217:= \u03b2\u2217/ \u2225\u03b2\u2217\u22252 \u2208E\u2229Sm\u22121\nand wi := A1/2ei/\n\r\rA1/2ei\n\r\r\n2. By (72)\nP\n\u0010\n\u2203i, \u27e8wi, ZT\n1 B1/2Z2 \u00af\u03b2\u2217\u27e9\u2265C0K2p\nlog mtr(B)1/2\u0011\n\u2264\nm\nX\ni=1\nP\n\u0010\n\u27e8wi, ZT\n1 B1/2Z2 \u00af\u03b2\u2217\u27e9\u2265C0K2p\nlog mtr(B)\n\u0011\n\u2264\n2 exp\n\u0000\u2212c min\n\u0000C2\n0 log m, C0 log m\n\u0001\n+ log m\n\u0001\n\u22642/m3.\nNow for t = C0K2\u221alog m \u2225B\u2225F , and \u2225B\u2225F / \u2225B\u22252 \u2265\u221alog m,\nP\n\u0010\n\u2203ei : \u27e8ei, (ZT BZ \u2212tr(B)Im)\u00af\u03b2\u2217\u27e9\u2265C0K2p\nlog m \u2225B\u2225F\n\u0011\n\u2264\n2m exp\n\"\n\u2212c min\n \nt2\nK4 \u2225B\u22252\nF\n,\nt\nK2 \u2225B\u22252\n!#\n\u22642/m3.\nBy the two inequalities immediately above, we have with probability at least 1 \u22124/m3,\n\r\rXT\n0 W\u03b2\u2217\r\r\n\u221e=\n\r\r\rA1/2ZT\n1 B1/2Z2\u03b2\u2217\r\r\r\n\u221e\n\u2264\n\u2225\u03b2\u2217\u22252 max\nei\n\r\r\rA1/2ei\n\r\r\r\n2\n\u0012\nsup\nwi\n\u27e8wi, ZT\n1 B1/2Z2 \u00af\u03b2\u2217\u27e9\n\u0013\n\u2264\nC0K2 \u2225\u03b2\u2217\u22252\np\nlog ma1/2\nmax\np\ntr(B)\nand\n\r\r(ZT BZ \u2212tr(B)Im)\u03b2\u2217\r\r\n\u221e=\n\r\r(ZT BZ \u2212tr(B)Im)\u00af\u03b2\u2217\r\r\n\u221e\u2225\u03b2\u2217\u22252\n=\n\u2225\u03b2\u2217\u22252\n\u0012\nsup\nei\n\u27e8ei, (ZT BZ \u2212tr(B)Im)\u00af\u03b2\u2217\u27e9\n\u0013\n\u2264\nC0K2 \u2225\u03b2\u2217\u22252\np\nlog m \u2225B\u2225F .\nThe last two bounds follow exactly the same arguments as above, except that we replace \u03b2\u2217with ej, j =\n1, . . . , m and apply the union bounds to m2 events instead of m, and thus P (B10) \u22651 \u22124/m2,\n\u25a1\nC\nProofs for the Lasso-type estimator\nC.1\nProof of Lemma 6\nClearly the condition on the stable rank of B guarantees that\nf \u2265r(B) = tr(B)\n\u2225B\u22252\n= tr(B) \u2225B\u22252\n\u2225B\u22252\n2\n\u2265\u2225B\u22252\nF / \u2225B\u22252\n2 \u2265log m.\nThus the conditions in Lemmas 29 and 7 hold. First notice that\nb\u03b3\n=\n1\nf\n\u0000XT\n0 X0\u03b2\u2217+ W T X0\u03b2\u2217+ XT\n0 \u01eb + W T \u01eb\n\u0001\n( 1\nf XT X \u2212\nbtr(B)\nf\nIm)\u03b2\u2217\n=\n1\nf (XT\n0 X0 + W T X0 + XT\n0 W + W TW \u2212\nbtr(B)\nf\nIm)\u03b2\u2217\n32\nThus\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\n\r\r\rb\u03b3 \u22121\nf\n\u0000XT X \u2212btr(B)Im\n\u0001\n\u03b2\u2217\r\r\r\n\u221e\n=\n1\nf\n\r\rXT\n0 \u01eb + W T\u01eb \u2212\n\u0000W TW + XT\n0 W \u2212btr(B)Im\n\u0001\n\u03b2\u2217\r\r\n\u221e\n\u2264\n1\nf\n\r\rXT\n0 \u01eb + W T\u01eb\n\r\r\n\u221e+ 1\nf\n\r\r(W T W \u2212btr(B)Im)\u03b2\u2217\r\r\n\u221e+\n\r\r\r\r\n1\nf XT\n0 W\u03b2\u2217\n\r\r\r\r\n\u221e\n\u2264\n1\nf\n\r\rXT\n0 \u01eb + W T\u01eb\n\r\r\n\u221e+ 1\nf (\n\r\r(ZT BZ \u2212tr(B)Im)\u03b2\u2217\r\r\n\u221e) + 1\nf\n\r\rXT\n0 W\u03b2\u2217\r\r\n\u221e\n+ 1\nf\n\f\fbtr(B) \u2212tr(B)\n\f\f \u2225\u03b2\u2217\u2225\u221e=: U1 + U2 + U3 + U4\nBy Lemma 29 we have on B4 for D0 := \u221a\u03c4B + a1/2\nmax,\nU1 = 1\nf\n\r\rXT\n0 \u01eb + W T \u01eb\n\r\r\n\u221e= 1\nf\n\r\r\rA\n1\n2ZT\n1 \u01eb + ZT\n2 B\n1\n2 \u01eb\n\r\r\r\n\u221e\u2264rm,fM\u01ebD0\nand on event B5 for D\u2032\n0 :=\np\n\u2225B\u22252 + a1/2\nmax,\nU2 + U3 = 1\nf\n\r\r(ZT BZ \u2212tr(B)Im)\u03b2\u2217\r\r\n\u221e+ 1\nf\n\r\rXT\n0 W\u03b2\u2217\r\r\n\u221e\n\u2264\nrm,fK \u2225\u03b2\u2217\u22252\n\u0012\u2225B\u2225F\n\u221af\n+ \u221a\u03c4Ba1/2\nmax\n\u0013\n\u2264Krm,f \u2225\u03b2\u2217\u22252 \u03c4 1/2\nB D\u2032\n0\nwhere recall \u2225B\u2225F \u2264\np\ntr(B) \u2225B\u22251/2\n2\n. Denote by B0 := B4 \u2229B5 \u2229B6. We have on B0 and under (A1), by\nLemmas 29 and 7 and D1 de\ufb01ned therein,\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\nU1 + U2 + U3 + U4\n\u2264\nrm,fM\u01ebD0 + D\u2032\n0\u03c4 1/2\nB Krm,f \u2225\u03b2\u2217\u22252 + 1\nf\n\f\fbtr(B) \u2212tr(B)\n\f\f \u2225\u03b2\u2217\u2225\u221e\n\u2264\nD0M\u01ebrm,f + D\u2032\n0K\u03c4 1/2\nB\n\u2225\u03b2\u2217\u22252 rm,f + D1K2 \u2225\u03b2\u2217\u2225\u221erm,m\n(73)\n\u2264\nD0M\u01ebrm,f + D\u2032\n0K\u03c4 1/2\nB\n\u2225\u03b2\u2217\u22252 rm,f + 2D1K 1\n\u221am \u2225\u03b2\u2217\u2225\u221erm,f\n(74)\n\u2264\nrm,f\n\u0012\u00123\n4D2 + D2\n1\n\u221am\n\u0013\nK \u2225\u03b2\u2217\u22252 + D0M\u01eb\n\u0013\nwhere 2D1 \u22642 \u2225A\u22252 + 2 \u2225B\u22252 = D2, for (D\u2032\n0)2 \u22642 \u2225B\u22252 + 2amax\nD0 \u2264D\u2032\n0\n\u2264\nq\n2(\u2225B\u22252 + amax) \u22642(amax + \u2225B\u22252) = D2,\nand\nD\u2032\n0\u03c4 1/2\nB\n\u2264\n(\u2225B\u22251/2\n2\n+ a1/2\nmax)\u03c4 1/2\nB\n\u2264\u03c4B + 1\n2(\u2225B\u22252 + amax) \u22643\n4D2\ngiven that under (A1) : \u03c4A = 1, \u2225A\u22252 \u2265amax \u2265a1/2\nmax \u22651. Hence the lemma holds for m \u226516 and\n\u03c8 = C0D2K (K \u2225\u03b2\u2217\u22252 + M\u01eb). Finally, we have by the union bound, P (B0) \u22651 \u221216/m3.\n\u25a1\n33\nC.2\nProof of Lemma 7\nFirst we write\nXXT \u2212tr(A)If\n=\n\u0000Z1A1/2 + B1/2Z2)\n\u0000Z1A1/2 + B1/2Z2\n\u0001T \u2212tr(A)If\n=\n\u0000Z1A1/2 + B1/2Z2)\n\u0000ZT\n2 B1/2 + A1/2ZT\n1\n\u0001\n\u2212tr(A)If\n=\nZ1A1/2ZT\n2 B1/2 + B1/2Z2ZT\n2 B1/2\n+B1/2Z2A1/2ZT\n1 + Z1AZT\n1 \u2212tr(A)If.\nThus we have for \u02c7tr(B) := 1\nm\n\u0000\u2225X\u22252\nF \u2212ftr(A)\n\u0001\n1\nf (\u02c7tr(B) \u2212tr(B)) :=\n1\nmf\n\u0000\u2225X\u22252\nF \u2212ftr(A) \u2212mtr(B)\n\u0001\n=\n1\nmf (tr(XXT ) \u2212ftr(A) \u2212mtr(B))\n=\n2\nmf tr(Z1A1/2ZT\n2 B1/2) +\n \ntr(B1/2Z2ZT\n2 B1/2)\nmf\n\u2212tr(B)\nf\n!\n+tr(Z1AZT\n1 )\nmf\n\u2212tr(A)\nm\nBy constructing a new matrix Af = If \u2297A which is block diagonal with f identical submatrices A along\nits diagonal, we prove the following large deviation bound: for t1 = C0K2 \u2225A\u2225F\n\u221af log m and f > log m,\nP\n\u0000\f\ftr(Z1AZT\n1 ) \u2212ftr(A)\n\f\f \u2265t1\n\u0001\n= P\n\u0010\f\f\fvec { Z1 }T (I \u2297A)vec { Z1 } \u2212ftr(A)\n\f\f\f \u2265t1\n\u0011\n\u2264\nexp\n \n\u2212c min\n \nt2\n1\nK4 \u2225Af\u22252\nF\n,\nt1\nK2 \u2225Af\u22252\n!!\n\u2264\n2 exp\n \n\u2212c min\n \n(C0K2\u221af log m \u2225A\u2225F)2\nK4f \u2225A\u22252\nF\n, C0K2\u221af log m \u2225A\u2225F\nK2 \u2225A\u22252\n!!\n\u2264\n2 exp (\u22124 log m)\nwhere the \ufb01rst inequality holds by Theorem 26 and the second inequality holds given that \u2225Af\u22252\nF = f \u2225A\u2225F\nand \u2225Af\u22252\n2 = \u2225A\u22252. Similarly, by constructing a new matrix Bm = Im \u2297B which is block diagonal\nwith m identical submatrices B along its diagonal, we prove the following large deviation bound: for\nt2 = C0K2 \u2225B\u2225F\n\u221am log m and m \u22652,\nP\n\u0000\f\ftr(ZT\n2 BZ2) \u2212mtr(B)\n\f\f \u2265t2\n\u0001\n= P\n\u0010\f\f\fvec { Z2 }T (Im \u2297B)vec { Z2 } \u2212mtr(B)\n\f\f\f \u2265t2\n\u0011\n\u2264\nexp\n \n\u2212c min\n \nt2\n2\nK4m \u2225B\u22252\nF\n,\nt2\nK2 \u2225B\u22252\n!!\n\u2264\n2 exp\n \n\u2212c min\n \n(C0K2\u221am log m \u2225B\u2225F)2\nK4m \u2225B\u22252\nF\n, C0K2\u221am log m \u2225B\u2225F\nK2 \u2225B\u22252\n!!\n\u2264\n2 exp (\u22124 log m) .\n34\nFinally, we have by (71) for t0 = C0K2p\ntr(A)tr(B) log m,\nP\n\u0010\f\f\fvec { Z1 }T B1/2 \u2297A1/2vec { Z2 }\n\f\f\f > t0\n\u0011\n\u2264\n2 exp\n \n\u2212c min\n \nt2\n0\nK4 \r\rB1/2 \u2297A1/2\r\r2\nF\n,\nt0\nK2 \r\rB1/2 \u2297A1/2\r\r\n2\n!!\n=\n2 exp\n \n\u2212c min\n \n(C0\np\ntr(A)tr(B) log m)2\ntr(A)tr(B)\n, C0\np\ntr(A)tr(B) log m\n\u2225B\u22251/2\n2\n\u2225A\u22251/2\n2\n!!\n\u2264\n2 exp(\u22124 log m)\nwhere we used and the fact that r(A)r(B) \u2265log m,\n\r\rB1/2 \u2297A1/2\r\r\n2 = \u2225B\u22251/2\n2\n\u2225A\u22251/2\n2\nand\n\r\r\rB1/2 \u2297A1/2\r\r\r\n2\nF\n=\ntr((B1/2 \u2297A1/2)(B1/2 \u2297A1/2)) = tr(B \u2297A) = tr(A)tr(B).\nThus we have with probability 1 \u22126/m4,\n1\nf\n\f\f\u02c7tr(B) \u2212tr(B)\n\f\f =\n1\nmf\n\f\ftr(XXT ) \u2212ftr(A) \u2212mtr(B)\n\f\f\n\u2264\n2\nmf\n\f\f\fvec { Z1 }T (B1/2 \u2297A1/2)vec { Z2 }\n\f\f\f\n+\n\f\f\f\f\ntr(ZT\n2 BZ2)\nmf\n\u2212tr(B)\nf\n\f\f\f\f +\n\f\f\f\f\ntr(Z1AZT\n1 )\nmf\n\u2212tr(A)\nm\n\f\f\f\f\n\u2264\n1\nmf (2t0 + t1 + t2) =\n\u221alog m\n\u221amf C0K2\n\u0012\u2225A\u2225F\n\u221am + 2\u221a\u03c4A\u03c4B + \u2225B\u2225F\n\u221af\n\u0013\n\u2264\n2C0\n\u221alog m\n\u221amf K2D1 =: D1K2rm,m\nwhere recall rm,m = 2C0\n\u221alog m\n\u221amf , D1 = \u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221af , and\n2\u221a\u03c4A\u03c4B \u2264\u03c4A + \u03c4B \u2264\u2225A\u2225F\n\u221am + \u2225B\u2225F\n\u221af .\nTo see this, recall\nm\u03c4A\n=\nm\nX\ni=1\n\u03bbi(A) \u2264\u221am(\nm\nX\ni=1\n\u03bb2\ni (A))1/2 = \u221am \u2225A\u2225F\n(75)\nf\u03c4B\n=\nf\nX\ni=1\n\u03bbi(B) \u2264\np\nf(\nf\nX\ni=1\n\u03bb2\ni (B))1/2 =\np\nf \u2225B\u2225F\nwhere \u03bbi(A), i = 1, . . . , m and \u03bbi(B), i = 1, . . . , f denote the eigenvalues of positive semide\ufb01nite covari-\nance matrices A and B respectively.\nDenote by B6 the following event\nn\n1\nf\n\f\f\u02c7tr(B) \u2212tr(B)\n\f\f \u2264D1K2rm,m\no\n35\nClearly btr(B) := (\u02c7tr(B))+ by de\ufb01nition (4). As a consequence, on B6, btr(B) = \u02c7tr(B) > 0 when \u03c4B >\nD1K2rm,m; hence\n1\nf\n\f\fbtr(B) \u2212tr(B)\n\f\f = 1\nf\n\f\f\u02c7tr(B) \u2212tr(B)\n\f\f \u2264D1K2rm,m.\nOtherwise, it is possible that \u02c7tr(B) < 0. However, suppose we set\nb\u03c4B := 1\nf btr(B) := 1\nf (\u02c7tr(B) \u22280),\nthen we can also guarantee that\n|b\u03c4B \u2212\u03c4B| = |\u03c4B| \u2264D1K2rm,m\nin case\n\u03c4B \u2264D1K2rm,m.\nThe lemma is thus proved.\n\u25a1\nD\nProof of Theorem 8\nDenote by \u03b2 = \u03b2\u2217. Let S := supp \u03b2, d = |S| and\n\u03c5 = b\u03b2 \u2212\u03b2.\nwhere b\u03b2 is as de\ufb01ned in (6). We \ufb01rst show Lemma 30, followed by the proof of Theorem 8.\nLemma 30. Bickel et al. (2009); Loh and Wainwright (2012) Suppose that (34) holds. Suppose that there\nexists a parameter \u03c8 such that\n\u221a\nd\u03c4 \u2264\u03c8\nb0\ns\nlog m\nf\n,\nand\n\u03bb \u22654\u03c8\ns\nlog m\nf\nwhere b0, \u03bb are as de\ufb01ned in (6). Then \u2225\u03c5Sc\u22251 \u22643 \u2225\u03c5S\u22251 .\nProof. By the optimality of b\u03b2, we have\n\u03bbn \u2225\u03b2\u22251 \u2212\u03bbn\n\r\r\rb\u03b2\n\r\r\r\n1\n\u2265\n1\n2\nb\u03b2b\u0393b\u03b2 \u22121\n2\u03b2b\u0393\u03b2 \u2212\u27e8b\u03b3, v \u27e9\n=\n1\n2\u03c5b\u0393\u03c5 + \u27e8\u03c5, b\u0393\u03b2 \u27e9\u2212\u27e8\u03c5, b\u03b3 \u27e9\n=\n1\n2\u03c5b\u0393\u03c5 \u2212\u27e8\u03c5, b\u03b3 \u2212b\u0393\u03b2 \u27e9\nHence, we have for \u03bb \u22654\u03c8\nq\nlog m\nf\n,\n1\n2\u03c5b\u0393\u03c5\n\u2264\n\u27e8\u03c5, b\u03b3 \u2212b\u0393\u03b2 \u27e9+ \u03bbn\n\u0010\n\u2225\u03b2\u22251 \u2212\n\r\r\rb\u03b2\n\r\r\r\n1\n\u0011\n(76)\n\u2264\n\u03bbn\n\u0010\n\u2225\u03b2\u22251 \u2212\n\r\r\rb\u03b2\n\r\r\r\n1\n\u0011\n+\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\n\r\r\r\n\u221e\u2225\u03c5\u22251\n36\nHence\n\u03c5b\u0393\u03c5\n\u2264\n\u03bbn\n\u0010\n2 \u2225\u03b2\u22251 \u22122\n\r\r\rb\u03b2\n\r\r\r\n1\n\u0011\n+ 2\u03c8\ns\nlog m\nf\n\u2225\u03c5\u22251\n(77)\n\u2264\n\u03bbn\n\u0012\n2 \u2225\u03b2\u22251 \u22122\n\r\r\rb\u03b2\n\r\r\r\n1 + 1\n2 \u2225\u03c5\u22251\n\u0013\n\u2264\n\u03bbn\n1\n2 (5 \u2225\u03c5S\u22251 \u22123 \u2225\u03c5Sc\u22251) .\n(78)\nwhere by the triangle inequality, and \u03b2Sc = 0, we have\n2 \u2225\u03b2\u22251 \u22122\n\r\r\rb\u03b2\n\r\r\r\n1 + 1\n2 \u2225\u03c5\u22251\n=\n2 \u2225\u03b2S\u22251 \u22122\n\r\r\rb\u03b2S\n\r\r\r\n1 \u22122 \u2225\u03c5Sc\u22251 + 1\n2 \u2225\u03c5S\u22251 + 1\n2 \u2225\u03c5Sc\u22251\n\u2264\n2 \u2225\u03c5S\u22251 \u22122 \u2225\u03c5Sc\u22251 + 1\n2 \u2225\u03c5S\u22251 + 1\n2 \u2225\u03c5Sc\u22251\n\u2264\n1\n2 (5 \u2225\u03c5S\u22251 \u22123 \u2225\u03c5Sc\u22251) .\n(79)\nWe now give a lower bound on the LHS of (76), applying the lower-RE condition as in De\ufb01nition 2.2,\n\u03c5T b\u0393\u03c5\n\u2265\n\u03b1 \u2225\u03c5\u22252\n2 \u2212\u03c4 \u2225\u03c5\u22252\n1 \u2265\u2212\u03c4 \u2225\u03c5\u22252\n1\nthus \u2212\u03c5T b\u0393\u03c5\n\u2264\n\u2225\u03c5\u22252\n1 \u03c4 \u2264\u2225\u03c5\u22251 2b0\n\u221a\nd\u03c4\n\u2264\n\u2225\u03c5\u22251 2b0\n\u03c8\nb0\ns\nlog m\nf\n= \u2225\u03c5\u22251 2\u03c8\ns\nlog m\nf\n\u2264\n1\n2\u03bb(\u2225\u03c5S\u22251 + \u2225\u03c5Sc\u22251)\n(80)\nwhere we use the assumption that\n\u221a\nd\u03c4 \u2264\u03c8\nb0\ns\nlog m\nf\n,\nand \u2225\u03c5\u22251 \u2264\n\r\r\rb\u03b2\n\r\r\r\n1 + \u2225\u03b2\u22251 \u22642b0\n\u221a\nd\nwhich holds by the triangle inequality and the fact that both b\u03b2 and \u03b2 have \u21131 norm being bounded by b0\n\u221a\nd.\nHence by (78) and (80)\n0\n\u2264\n\u2212\u03c5b\u0393\u03c5 + 5\n2\u03bb \u2225\u03c5S\u22251 \u22123\n2\u03bb \u2225\u03c5Sc\u22251\n(81)\n\u2264\n1\n2\u03bb \u2225\u03c5S\u22251 + 1\n2\u03bb \u2225\u03c5Sc\u22251 + 5\n2\u03bb \u2225\u03c5S\u22251 \u22123\n2\u03bb \u2225\u03c5Sc\u22251\n\u2264\n3\u03bb \u2225\u03c5S\u22251 \u2212\u03bb \u2225\u03c5Sc\u22251\n(82)\nThus we have\n\u2225\u03c5Sc\u22251 \u22643 \u2225\u03c5S\u22251\nThus Lemma 30 holds.\n\u25a1\n37\nProof of Theorem 8.\nFollowing the conclusion of Lemma 30, we have\n\u2225\u03c5\u22251 \u22644 \u2225\u03c5S\u22251 \u22644\n\u221a\nd \u2225\u03c5\u22252 .\n(83)\nMoreover, we have by the lower-RE condition as in De\ufb01nition 2.2\n\u03c5T b\u0393\u03c5\n\u2265\n\u03b1 \u2225\u03c5\u22252\n2 \u2212\u03c4 \u2225\u03c5\u22252\n1 \u2265(\u03b1 \u221216d\u03c4) \u2225\u03c5\u22252\n2 \u22651\n2\u03b1 \u2225\u03c5\u22252\n2\n(84)\nwhere the last inequality follows from the assumption that 16d\u03c4 \u2264\u03b1/2.\nCombining the bounds in (84), (83) and (77), we have\n1\n2\u03b1 \u2225\u03c5\u22252\n2\n\u2264\n\u03c5T b\u0393\u03c5 \u2264\u03bbn\n\u0010\n2 \u2225\u03b2\u22251 \u22122\n\r\r\rb\u03b2\n\r\r\r\n1\n\u0011\n+ 2\u03c8\ns\nlog m\nf\n\u2225\u03c5\u22251\n\u2264\n5\n2\u03bb \u2225\u03c5S\u22251 \u226410\u03bb\n\u221a\nd \u2225\u03c5\u22252\nAnd thus we have \u2225\u03c5\u22252 \u226420\u03bb\n\u221a\nd. The theorem is thus proved.\n\u25a1\nD.1\nProof of Lemma 9\nIn view of Remark D.1, Condition (40) implies that (52) in Theorem 20 holds for \u03b6 = s0 and \u03b5 =\n1\n2MA .\nNow, by Theorem 20, we have \u2200u, v \u2208E \u2229Sm\u22121, under (A1) and (A3), condition (48) holds under event\nA0, and so long as mf \u22651024C2\n0D2\n2K4 log m/\u03bbmin(A)2,\n\f\fuT \u2206v\n\f\f\n\u2264\n8C\u031f(s0)\u03b5 + 2C0D2K2\ns\nlog m\nmf\n=: \u03b4 with \u03b4 \u22641\n8\u03bbmin(A) \u22641\n8\nwhich holds for all\n\u03b5 \u22641\n2\n\u03bbmin(A)\n64C\u031f(s0) :=\n1\n2MA\n\u2264\n1\n128C\nwith P (A0) \u22651\u22124 exp\n\u0010\n\u2212c2\u03b52\ntr(B)\nK4\u2225B\u22252\n\u0011\n\u22122 exp\n\u0010\n\u2212c2\u03b52 f\nK4\n\u0011\n\u22126/m3. Hence, by Corollary 19, \u2200\u03b8 \u2208Rm,\n\u03b8T b\u0393A\u03b8 \u2265\u03b1 \u2225\u03b8\u22252\n2 \u2212\u03c4 \u2225\u03b8\u22252\n1\nand\n\u03b8T b\u0393A\u03b8 \u2264\u00af\u03b1 \u2225\u03b8\u22252\n2 + \u03c4 \u2225\u03b8\u22252\n1\nwhere \u03b1 = 1\n2\u03bbmin(A) and \u00af\u03b1 = 3\n2\u03bbmax(A) and\n512C2\u031f(s0)2\n\u03bbmin(A)\nlog m\nf\n\u2264\u03c4 = \u03b1\ns0\n\u2264\n2\u03b1\ns0 + 1\n\u2264\n1024C2\u031f2(s0 + 1)\n\u03bbmin(A)\nlog m\nf\n.\nwhere we plugged in s0 as de\ufb01ned in (12). The lemma is thus proved in view of Remark D.1.\n\u25a1\n38\nRemark D.1. Clearly the condition on tr(B)/ \u2225B\u22252 as stated in Lemma 9 ensures that we have for \u03b5 =\n1\n2MA\nand s0 \u224d\n4f\nM2\nA log m\n\u03b52\ntr(B)\nK4 \u2225B\u22252\n\u2265\n\u03b52\nK4 c\u2032K4 s0\n\u03b52 log\n\u00123em\ns0\u03b5\n\u0013\n\u2265\n1\n4M2\nAK4 4c\u2032K4M2\nAs0 log\n\u00126emMA\ns0\n\u0013\n\u2265\nc\u2032s0 log\n\u00126emMA\ns0\n\u0013\nand hence\nexp\n\u0012\n\u2212c2\u03b52\ntr(B)\nK4 \u2225B\u22252\n\u0013\n\u2264\nexp\n\u0012\n\u2212c\u2032c2s0 log\n\u00126emMA\ns0\n\u0013\u0013\n\u224d\nexp\n\u0012\n\u2212c3\n4f\nM2\nA log m log\n\u00123eM3\nAm log m\n2f\n\u0013\u0013\nD.2\nProof of Lemma 10\nLet\nM+\n=\n64C\u031f(s0 + 1)\n\u03bbmin(A)\nwhere\n\u031f(s0 + 1) = \u03c1max(s0 + 1, A) + \u03c4B =: D\nBy de\ufb01nition of s0, we have\n\u221a\ns0 + 1\u031f(s0 + 1)\n\u2265\n\u03bbmin(A)\n32C\ns\nf\nlog m\nand hence\ns0 + 1\n\u2265\n\u03bb2\nmin(A)\n1024C2\u031f2(s0 + 1)\nf\nlog m =\n\u0010\n\u03b1\n16CD\n\u00112\nf\nlog m \u2265\n1\nM2\nA\nf\nlog m\nThe \ufb01rst inequality in (33) holds given that M+ \u22642MA and hence\nd \u2264\n1\n64M2\nA\nf\nlog m\n\u2264\n1\n16M2\n+\nf\nlog m \u2264s0 + 1\n64\n\u2264s0\n32\nMoreover, for D = \u03c1max(s0 + 1, A) + \u03c4B \u2264D2 and C = C0/\n\u221a\nc\u2032, we have\nd\n\u2264\nCAc\u2032D\u03c6\nf\nlog m \u2264\n1\n128M2\nA\n\u0012C0D2\nCD\n\u00132\nD\u03c6\nf\nlog m\n\u2264\n1\n2\n\u0012\n1\n16CD\n\u00132\n4C2\n0D2\n2D\u03c6\nf\nM2\nA log m\n\u2264\n1\n2\n(s0 + 1)2\n\u03b12\nlog m\nf\n\u0012 \u03c8\nb0\n\u00132\n\u2264(s0)2\n\u03b12\nlog m\nf\n\u0012 \u03c8\nb0\n\u00132\n39\nwhere assuming that s0 \u22653, we have\n2s2\n0\n\u03b12\n\u2265\n\u0012s0 + 1\n\u03b1\n\u00132\n\u2265\n\u03b12\n(16CD)4\n\u0012\nf\nlog m\n\u00132\n\u0012 \u03c8\nb0\n\u00132\n=\n4C2\n0D2\n2\nK2\nb2\n0\n(M\u01eb + K \u2225\u03b2\u2217\u22252)2\n(85)\n\u2265\n4C2\n0D2\n2D\u03c6 = 4C2\n0D2\n2\n\u0012K2M2\n\u01eb\nb2\n0\n+ K4\u03c6\n\u0013\n.\nWe have shown that (42) indeed holds, and the lemma is thus proved.\n\u25a1\nRemark D.2. Throughout this paper, we assume that C0 is a large enough constant such that for c as de\ufb01ned\nin Theorem 26,\nc min{C2\n0, C0} \u22654.\n(86)\nBy de\ufb01nition of s0, we have for \u031f2(s0) \u22651,\ns0\u031f2(s0)\n\u2264\nc\u2032\u03bb2\nmin(A)\n1024C2\n0\nf\nlog m\nand hence\ns0\n\u2264\nc\u2032\u03bb2\nmin(A)\n1024C2\n0\nf\nlog m \u2264\u03bb2\nmin(A)\n1024C2\n0\nf\nlog m =: \u02c7s0.\nRemark D.3. The proof shows that one can take C = C0/\n\u221a\nc\u2032, and take\nV = 3eM3\nA/2 = 3e643C3\u031f3(s0)\n2\u03bb3\nmin(A)\n\u22643e643C3\n0\u031f3(\u02c7s0)\n2(c\u2032)3/2\u03bb3\nmin(A).\nHence a suf\ufb01cient condition on r(B) is:\nr(B) \u226516c\u2032K4\nf\nlog m\n\u0012\n3 log 64C0\u031f(\u02c7s0)\n\u221a\nc\u2032\u03bbmin(A)\n+ log 3em log m\n2f\n\u0013\n.\n(87)\nIt remains to prove Lemmas 14 and 15.\nProof of Lemma 14.\nSuppose that event B0 holds. By (74) and that fact that 2D1 := 2(\u2225A\u2225F\n\u221am +\n\u2225B\u2225F\n\u221af ) \u22642(\u2225A\u22251/2\n2\n+ \u2225B\u22251/2\n2\n)(\u221a\u03c4A + \u221a\u03c4B) \u2264DoracleD\u2032\n0, where recall D\u2032\n0 = \u2225B\u22251/2\n2\n+ a1/2\nmax,\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\nD\u2032\n0K\u03c4 1/2\nB\n\u2225\u03b2\u2217\u22252 rm,f + 2D1K 1\n\u221am \u2225\u03b2\u2217\u2225\u221erm,f + D0M\u01ebrm,f\n\u2264\nD\u2032\n0K \u2225\u03b2\u2217\u22252 rm,f\n\u0012\n\u03c4 1/2\nB\n+ Doracle\n\u221am\n\u0013\n+ D0M\u01ebrm,f\n\u2264\nD\u2032\n0\n\u0012\n\u03c4 1/2\nB\n+ Doracle\n\u221am\n\u0013\nK \u2225\u03b2\u2217\u22252 rm,f + D0M\u01ebrm,f\nThe lemma is thus proved.\n\u25a1\n40\nProof of Lemma 15.\nRecall that we require\nd\n\u2264\nCA\n\b\nc\u2032C\u03c6 \u22272\n\t\nf\nlog m where\nC\u03c6 = \u2225B\u22252 + amax\nD2\nD\u03c6\nwhere CA =\n1\n128M2\nA\nand\nb2\n0 \u2265\u2225\u03b2\u2217\u22252\n2 \u2265\u03c6b2\n0.\nThe proof for d \u2264s0/32 follows exactly that of Lemma 10. In order to show the second inequality, we\nfollow the same line of arguments where we need to replace one inequality. By de\ufb01nition of D\u2032\n0, we have\n\u2225B\u22252 + amax \u2264(D\u2032\n0)2 \u22642(\u2225B\u22252 + amax). Now suppose that for C\u03c6 = \u2225B\u22252+amax\nD2\nD\u03c6\nd\n:=\nCAc\u2032C\u03c6\nf\nlog m \u2264CA\nf\nlog m\n\u0012C0D\u2032\n0\nCD\n\u00132\nD\u03c6\nwhere 1 \u2264D = \u03c1max(s0 + 1, A) + \u03c4B \u2264D2 and C = C0/\n\u221a\nc\u2032.\nd\n\u2264\nCAc\u2032C\u03c6\nf\nlog m \u2264\n1\n128M2\nA\n\u0012C0D\u2032\n0\nCD\n\u00132\nD\u03c6\nf\nlog m\n\u2264\n1\n2\n\u0012\n1\n16CD\n\u00132\n4C2\n0(D\u2032\n0)2D\u03c6\nf\nM2\nA log m\n\u2264\n1\n2\n(s0 + 1)2\n\u03b12\nlog m\nf\n\u0012 \u03c8\nb0\n\u00132\n\u2264(s0)2\n\u03b12\nlog m\nf\n\u0012 \u03c8\nb0\n\u00132\nwhere assuming that s0 \u22653, we have the following inequality by de\ufb01nition of s0 and \u03b1 = \u03bbmin(A)/2\n2s2\n0\n\u03b12\n\u2265\n\u0012s0 + 1\n\u03b1\n\u00132\n\u2265\n\u03b12\n(16CD)4\n\u0012\nf\nlog m\n\u00132\nwhich is identical in the proof of Lemma 10, while we replace (85) with\n4C2\n0(D\u2032\n0)2D\u03c6\n=\n4C2\n0(D\u2032\n0)2(K2M2\n\u01eb\nb2\n0\n+ \u03c4 +\nB K4\u03c6)\n\u2264\n4C2\n0(D\u2032\n0)2 K2\nb2\n0\n\u0010\nM\u01eb + \u03c4 +/2\nB\nK \u2225\u03b2\u2217\u22252\n\u00112\n\u2264\n\u0012 \u03c8\nb0\n\u00132\nwhere D\u03c6 := K2M2\n\u01eb\nb2\n0\n+ \u03c4 +\nB K4\u03c6 and \u03c8 = 2C0\n\u0010\nD\u2032\n0K2(\u03c4 1/2\nB\n+ Doracle\n\u221am ) \u2225\u03b2\u2217\u22252 + D0M\u01ebK\n\u0011\nas in (44).\n\u25a1\nE\nProofs for the Conic Programming estimator\nE.1\nProof of Lemmas 11 and 13\nWe next provide proofs for Lemmas 11 and 13 in this section.\n41\nProof of Lemma 11.\nSuppose event B0 holds. Then by the proof of Lemma 6,\n\r\r\r 1\nf XT (y \u2212X\u03b2\u2217) + 1\nf btr(B)\u03b2\u2217\r\r\r\n\u221e\n=\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\n2C0D2K2 \u2225\u03b2\u2217\u22252\ns\nlog m\nf\n+ C0D0KM\u01eb\ns\nlog m\nf\n=:\n\u00b5 \u2225\u03b2\u2217\u22252 + \u03c4\nThe lemma follows immediately for the chosen \u00b5, \u03c4 as in (43) given that (\u03b2\u2217, \u2225\u03b2\u2217\u22252) \u2208\u03a5.\n\u25a1\nProof of Lemma 12.\nBy optimality of (b\u03b2, bt), we have\n\r\r\rb\u03b2\n\r\r\r\n1 + \u03bb\n\r\r\rb\u03b2\n\r\r\r\n2 \u2264\n\r\r\rb\u03b2\n\r\r\r\n1 + \u03bbbt \u2264\u2225\u03b2\u2217\u22251 + \u03bb \u2225\u03b2\u2217\u22252\nThus we have for S := supp(\u03b2\u2217),\n\r\r\rb\u03b2\n\r\r\r\n1 =\n\r\r\rb\u03b2Sc\n\r\r\r\n1 +\n\r\r\rb\u03b2S\n\r\r\r\n1\n\u2264\n\u2225\u03b2\u2217\u22251 + \u03bb(\u2225\u03b2\u2217\u22252 \u2212\n\r\r\rb\u03b2\n\r\r\r\n2)\nNow by the triangle inequality,\n\r\r\rb\u03b2Sc\n\r\r\r\n1 = \u2225vSc\u22251\n\u2264\n\u2225\u03b2\u2217\nS\u22251 \u2212\n\r\r\rb\u03b2S\n\r\r\r\n1 + \u03bb(\u2225\u03b2\u2217\u22252 \u2212\n\r\r\rb\u03b2\n\r\r\r\n2)\n\u2264\n\u2225vS\u22251 + \u03bb(\u2225\u03b2\u2217\u22252 \u2212\n\r\r\rb\u03b2\n\r\r\r\n2)\n\u2264\n\u2225vS\u22251 + \u03bb(\u2225\u03b2\u2217\u22252 \u2212\n\r\r\rb\u03b2S\n\r\r\r\n2)\n=\n\u2225vS\u22251 + \u03bb \u2225vS\u22252 \u2264(1 + \u03bb) \u2225vS\u22251 .\nThe lemma thus holds given\nbt\n\u2264\n1\n\u03bb(\u2225\u03b2\u2217\u22251 \u2212\n\r\r\rb\u03b2\n\r\r\r\n1) + \u2225\u03b2\u2217\u22252 \u22641\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252\n\u25a1\nProof of Lemma 13.\nRecall the following shorthand notation:\nD0\n=\n(\u221a\u03c4B + \u221aamax)\nand D2 = 2(\u2225A\u22252 + \u2225B\u22252)\nFirst we rewrite an upper bound for v = b\u03b2 \u2212\u03b2\u2217, D = tr(B) and bD = btr(B)\n\r\rXT\n0 X0v\n\r\r\n\u221e\n=\n\r\r\r(X \u2212W)T X0(b\u03b2 \u2212\u03b2\u2217)\n\r\r\r\n\u221e\u2264\n\r\r\rXT X0(b\u03b2 \u2212\u03b2\u2217)\n\r\r\r\n\u221e+\n\r\rW T X0v\n\r\r\n\u221e\n\u2264\n\r\r\rXT (X b\u03b2 \u2212y) \u2212bDb\u03b2\n\r\r\r\n\u221e+\n\r\rXT \u01eb\n\r\r\n\u221e+\n\r\r\r(XT W \u2212D)b\u03b2\n\r\r\r\n\u221e\n+\n\r\r\r( bD \u2212D)b\u03b2\n\r\r\r\n\u221e+\n\r\rW TX0v\n\r\r\n\u221e\n42\nwhere\n\r\r\rXT X0(b\u03b2 \u2212\u03b2\u2217)\n\r\r\r\n\u221e\n\u2264\n\r\r\rXT (X0 b\u03b2 \u2212y + \u01eb)\n\r\r\r\n\u221e\n=\n\r\r\rXT ((X \u2212W)b\u03b2 \u2212y)\n\r\r\r\n\u221e+\n\r\rXT \u01eb\n\r\r\n\u221e\n\u2264\n\r\r\rXT (X b\u03b2 \u2212y) \u2212bDb\u03b2\n\r\r\r\n\u221e+\n\r\rXT \u01eb\n\r\r\n\u221e\n+\n\r\r\r(XT W \u2212D)b\u03b2\n\r\r\r\n\u221e+\n\r\r\r( bD \u2212D)b\u03b2\n\r\r\r\n\u221e.\nOn event B0, we have by Lemma 12 and the fact that b\u03b2 \u2208\u03a5\nI :=\n\r\r\rb\u03b3 \u2212b\u0393b\u03b2\n\r\r\r\n\u221e\n=\n\r\r\r 1\nf XT (y \u2212X b\u03b2) + 1\nf bD b\u03b2\n\r\r\r\n\u221e\u2264\u00b5bt + \u03c4\n\u2264\n\u00b5( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + \u03c4\n=\n2D2Krm,f( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + D0rm,fM\u01eb\nand on event B4,\nII\n:=\n1\nf\n\r\rXT \u01eb\n\r\r\n\u221e\u22641\nf (\n\r\rXT\n0 \u01eb\n\r\r\n\u221e+\n\r\rW T\u01eb\n\r\r\n\u221e)\n\u2264\nrm,fM\u01eb(a1/2\nmax + \u221a\u03c4B) = D0rm,fM\u01eb\nThus on event B0, we have\nI + II \u22642D2Krm,f( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + 2D0rm,fM\u01eb = \u00b5(( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + 2\u03c4.\nNow on event B6, we have for 2D1 \u2264D2\nIV :=\n\r\r\r( bD \u2212D)b\u03b2\n\r\r\r\n\u221e\n\u2264\n\f\f\f bD \u2212D\n\f\f\f\n\r\r\rb\u03b2\n\r\r\r\n\u221e\u22642D1K 1\n\u221amrm,f(\u2225\u03b2\u2217\u2225\u221e+ \u2225v\u2225\u221e)\n\u2264\nD2K 1\n\u221amrm,f(\u2225\u03b2\u2217\u22252 + \u2225v\u22251)\nOn event B5 \u2229B10, we have\nIII := 1\nf\n\r\r\r(XT W \u2212D)b\u03b2\n\r\r\r\n\u221e\n\u2264\n1\nf\n\r\r(XT W \u2212D)\u03b2\u2217\r\r\n\u221e+ 1\nf\n\r\r(XT W \u2212D)v\n\r\r\n\u221e\n\u2264\n1\nf\n\r\rXT\n0 W\u03b2\u2217\r\r\n\u221e+\n\r\r(W T W \u2212D)\u03b2\u2217\r\r\n\u221e\n+\n1\nf\n\u0010\r\r(ZT BZ \u2212tr(B)Im)\n\r\r\nmax + 1\nf\n\r\rXT\n0 W\n\r\r\nmax\n\u0011\n\u2225v\u22251\n\u2264\nrm,fK\n\u0012\u2225B\u2225F\n\u221af\n+ \u221a\u03c4Ba1/2\nmax\n\u0013\n(\u2225v\u22251 + \u2225\u03b2\u2217\u22252)\nand V = 1\nf\n\r\rW T X0v\n\r\r\n\u221e\n\u2264\n1\nf\n\r\rW TX0\n\r\r\nmax \u2225v\u22251 \u2264rm,fK\u221a\u03c4Ba1/2\nmax \u2225v\u22251 .\nThus we have on B0 \u2229B10, for D0 \u2264D2 and \u03c4A = 1\nIII + IV + V\n\u2264\nrm,fK\n\u0012\n\u2225B\u22252 + \u03c4B + amax +\n2\n\u221am(\u2225A\u22252 + \u2225B\u22252)\n\u0013\n(\u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n\u2264\nrm,fK (4 \u2225B\u22252 + 3 \u2225A\u22252) (\u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n\u2264\n2D2Krm,f(\u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n\u2264\n\u00b5(\u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n43\nThus we have\n\r\r\r 1\nf XT\n0 X0v\n\r\r\r\n\u221e\n\u2264\nI + II + III + IV + V\n\u2264\n\u00b5( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + 2D0M\u01ebrm,f + \u00b5(\u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n\u2264\n2\u00b5 \u2225\u03b2\u2217\u22252 + \u00b5( 1\n\u03bb + 1) \u2225v\u22251 + 2\u03c4.\nThe lemma thus holds.\n\u25a1\nF\nProof for Theorem 5\nWe prove Lemmas 16 to 18 in this section.\nProof of Lemma 16.\nSuppose event B0 holds. Then by the proof of Lemma 14, we have for D\u2032\n0 =\n\u2225B\u22251/2\n2\n+ a1/2\nmax and \u03c4 +/2\nB\n= \u221a\u03c4B + Doracle\n\u221am , where Doracle = 2(\u2225B\u22251/2\n2\n+ \u2225A\u22251/2\n2\n),\n\r\r\rb\u03b3 \u2212b\u0393\u03b2\u2217\r\r\r\n\u221e\n\u2264\nD\u2032\n0\u03c4 +/2\nB\nKrm,f \u2225\u03b2\u2217\u22252 + D0M\u01ebrm,f.\nThe lemma follows immediately for \u00b5, \u03c4 as chosen in (45).\n\u25a1\nProof of Lemma 17.\nWe \ufb01rst show (46) and (47). Recall rm,m := 2C0\nq\nlog m\nmf\n\u22652C0\nlog1/2 m\nm\n. By\nLemma 7, we have on event B6,\n|b\u03c4B \u2212\u03c4B|\n\u2264\nD1K2rm,m.\nMoreover, we have under (A1) 1 = \u03c4A \u2264D1 := \u2225A\u2225F\nm1/2 + \u2225B\u2225F\nf1/2 in view of (75). And\nD1 \u2264\u2225A\u22252 + \u2225B\u22252 \u2264(Doracle\n2\n)2\nand hence\np\nD1 \u2264Doracle\n2\n= \u2225B\u22251/2\n2\n+ \u2225A\u22251/2\n2\n.\nBy de\ufb01nition and construction, we have \u03c4B, b\u03c4B \u22650,\n\f\f\fb\u03c4 1/2\nB\n\u2212\u03c4 1/2\nB\n\f\f\f\n\u2264\nb\u03c4 1/2\nB\n+ \u03c4 1/2\nB ;\nand hence\n\f\f\fb\u03c4 1/2\nB\n\u2212\u03c4 1/2\nB\n\f\f\f\n2\n\u2264\n\f\f\f(b\u03c4 1/2\nB\n+ \u03c4 1/2\nB )(b\u03c4 1/2\nB\n\u2212\u03c4 1/2\nB )\n\f\f\f = |b\u03c4B \u2212\u03c4B|\nThus, on event B6, we have\n\f\f\fb\u03c4 1/2\nB\n\u2212\u03c4 1/2\nB\n\f\f\f\n\u2264\np\n|b\u03c4B \u2212\u03c4B| \u2264\np\nD1Kr1/2\nm,m \u2264Doracle\n2\nKr1/2\nm,m\nThus we have for C6 \u2265Doracle \u22652\u221aD1 and Doracle = 2(\u2225A\u22251/2\n2\n+ \u2225B\u22251/2\n2\n),\nb\u03c4 1/2\nB\n\u2212Doracle\n2\nKr1/2\nmm \u2264\u03c4 1/2\nB\n\u2264b\u03c4 1/2\nB\n+ Doracle\n2\nKr1/2\nmm\n(88)\n44\nThus we have for \u03c4 +/2\nB\nas de\ufb01ned in (23), (88) and the fact that\nr1/2\nm,m :=\np\n2C0\n(log m)1/4\n\u221am\n\u22652/\u221am for m \u226516 and C0 \u22651,\nthe following inequalities hold: for K \u22651,\n\u03c4 +/2\nB\n:=\n\u03c4 1/2\nB\n+ Doraclem\u22121/2\n(89)\n\u2264\nb\u03c4 1/2\nB\n+ Doracle\n2\nKr1/2\nmm + Doracle\n2\nr1/2\nm,m\n\u2264\nb\u03c4 1/2\nB\n+ DoracleKr1/2\nmm \u2264e\u03c4 1/2\nB\nwhere the last inequality holds by the choice of e\u03c4 1/2\nB\n\u2265b\u03c4 1/2\nB\n+ DoracleKr1/2\nmm as in (30). Moreover, we have\non event B6, by (88)\ne\u03c4 1/2\nB\n:=\nb\u03c4 1/2\nB\n+ C6Kr1/2\nmm \u2264\u03c4 1/2\nB\n+ Doracle\n2\nKr1/2\nmm + C6Kr1/2\nmm\n\u2264\n\u03c4 1/2\nB\n+ 3\n2C6Kr1/2\nmm\ne\u03c4B\n:=\n(b\u03c4 1/2\nB\n+ C6Kr1/2\nmm)2 \u22642b\u03c4B + 2C2\n6K2rmm\n\u2264\n2\u03c4B + 2D1K2rm,m + 2C2\n6K2rmm\n\u2264\n2\u03c4B + D2\noracle\n2\nK2rm,m + 2C2\n6K2rmm \u22642\u03c4B + 3C2\n6K2rmm\nand thus (46) and (47) hold given that 2D1 \u2264D2\noracle/2 \u2264C2\n6/2. Finally, we have\ne\u03c4 1/2\nB \u03c4 \u2212\nB \u2264(\u03c4 1/2\nB\n+ 3\n2C6Kr1/2\nmm)\u03c4 \u2212\nB \u2264\u03c4 1/2\nB\n+ 3\n2C6Kr1/2\nmm\n\u03c4 1/2\nB\n+ 2C6Kr1/2\nm,m\n\u22641\nfor \u03c4 \u2212\nB as de\ufb01ned in (26).\n\u25a1\nRemark F.1. The set \u03a5 in our setting is equivalent to the following: for \u00b5, \u03c4 as de\ufb01ned in (30) and \u03b2 \u2208Rm,\n\u03a5 =\nn\n(\u03b2, t) :\n\r\r\r 1\nf XT (y \u2212X\u03b2) + 1\nf btr(B)\u03b2\n\r\r\r\n\u221e\u2264\u00b5t + \u03c4, \u2225\u03b2\u22252 \u2264t\no\n.\n(90)\nProof of Lemma 18.\nFor the rest of the proof, we will follow the notation in the proof for Lemma 13.\nNotice that the bounds as stated in Lemma 12 remain true with \u03c4, \u00b5 chosen as in (45), so long as (\u03b2\u2217, \u2225\u03b2\u2217\u22252) \u2208\n\u03a5. This indeed holds by Lemma 16: for \u03c4 (29) and \u00b5 (30) as chosen in Theorem 5, we have by (89),\n\u00b5 \u224dD\u2032\n0e\u03c4 1/2\nB Krm,f \u2265D\u2032\n0Krm,f\u03c4 +/2\nB\nwhere \u03c4 +/2\nB\n= (\u221a\u03c4B + Doracle\n\u221am ), which ensures that (\u03b2\u2217, \u2225\u03b2\u2217\u22252) \u2208\u03a5 by Lemma 16.\nOn event B0, we have by Lemma 12 and the fact that b\u03b2 \u2208\u03a5 as in (90)\nI + II\n:=\n\r\r\rb\u03b3 \u2212b\u0393b\u03b2\n\r\r\r\n\u221e+ 1\nf\n\r\rXT \u01eb\n\r\r\n\u221e\n\u2264\n\r\r\r 1\nf XT (y \u2212X b\u03b2) + 1\nf bDb\u03b2\n\r\r\r\n\u221e+ \u03c4 \u2264\u00b5bt + 2\u03c4\n\u2264\n\u00b5( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + 2\u03c4\n45\nfor \u00b5, \u03c4 as chosen in (30) and (29) respectively. Now on event B6, we have\nIV :=\n\r\r\r( bD \u2212D)b\u03b2\n\r\r\r\n\u221e\n\u2264\n\f\f\f bD \u2212D\n\f\f\f\n\r\r\rb\u03b2\n\r\r\r\n\u221e\u22642D1K 1\n\u221amrm,f(\u2225\u03b2\u2217\u2225\u221e+ \u2225v\u2225\u221e)\n\u2264\nD\u2032\n0\nDoracle\n\u221am Krm,f(\u2225\u03b2\u2217\u22252 + \u2225v\u22251)\nwhere 2D1 \u2264DoracleD\u2032\n0 for 1 \u2264D\u2032\n0 := \u2225B\u22251/2\n2\n+ a1/2\nmax and Doracle = 2\n\u0010\n\u2225B\u22251/2\n2\n+ \u2225A\u22251/2\n2\n\u0011\n, where\namax \u2265\u03c4A = 1 under (A1). Hence\nIII + IV + V \u2264rm,fK\u221a\u03c4B\n\u0010\n\u2225B\u22251/2\n2\n+ a1/2\nmax\n\u0011\n(\u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n+2D1K 1\n\u221amrm,f(\u2225\u03b2\u2217\u22252 + \u2225v\u22251) + rm,fK\u221a\u03c4Ba1/2\nmax \u2225v\u22251\n\u2264\nD\u2032\n0Krm,f(\u2225v\u22251 + \u2225\u03b2\u2217\u22252)(\u221a\u03c4B + Doracle\n\u221am ) + rm,fK\u221a\u03c4Ba1/2\nmax \u2225v\u22251\n\u2264\nD\u2032\n0Krm,f\u03c4 +/2\nB\n(\u2225v\u22251 + \u2225\u03b2\u2217\u22252) + D\u2032\n0Krm,f\n\u221a\u03c4B \u2225v\u22251\n\u2264\nC0D\u2032\n0K2\ns\nlog m\nf\n(\u03c4 1/2\nB\n+ Doracle\n\u221am )(2 \u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n\u2264\n\u00b5(2 \u2225v\u22251 + \u2225\u03b2\u2217\u22252)\nfor \u00b5 as de\ufb01ned in (30) in view of (89). Thus we have\nI + II + III + IV + V\n\u2264\n\u00b5( 1\n\u03bb \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + 2\u03c4 + \u00b5(2 \u2225v\u22251 + \u2225\u03b2\u2217\u22252)\n=\n2\u00b5((1 + 1\n2\u03bb) \u2225v\u22251 + \u2225\u03b2\u2217\u22252) + 2\u03c4\nand the improved bounds as stated in the Lemma thus holds.\n\u25a1\nG\nSome geometric analysis results\nLet us de\ufb01ne the following set of vectors in Rm:\nCone(s0) := {\u03c5 : \u2225\u03c5\u22251 \u2264\u221as0 \u2225\u03c5\u22252}\nFor each vector x \u2208Rm, let T0 denote the locations of the s0 largest coef\ufb01cients of x in absolute values.\nAny vector x \u2208Sm\u22121 satis\ufb01es:\n\r\rxT c\n0\n\r\r\n\u221e\u2264\u2225xT0\u22251 /s0\n\u2264\n\u2225xT0\u22252\n\u221as0\n(91)\nWe need to state the following result from Mendelson et al. (2008). Let Sm\u22121 be the unit sphere in Rm, for\n1 \u2264s \u2264m,\nUs := {x \u2208Rm : | supp(x)| \u2264s}\n(92)\nThe sets Us is an union of the s-sparse vectors. The following three lemmas are well-known and mostly\nstandard; See Mendelson et al. (2008) and Loh and Wainwright (2012).\n46\nLemma 31. For every 1 \u2264s0 \u2264m and every I \u2282{1, . . . , m} with |I| \u2264s0,\np\n|I|Bm\n1 \u2229Sm\u22121 \u22822 conv(Us0 \u2229Sm\u22121) =: 2 conv\n\uf8eb\n\uf8ed[\n|J|\u2264s0\nEJ \u2229Sm\u22121\n\uf8f6\n\uf8f8\nand moreover, for \u03c1 \u2208(0, 1].\np\n|I|Bm\n1 \u2229\u03c1Bm\n2 \u2282(1 + \u03c1) conv(Us0 \u2229Bm\n2 ) =: (1 + \u03c1) conv\n\uf8eb\n\uf8ed[\n|J|\u2264s0\nEJ \u2229Sm\u22121\n\uf8f6\n\uf8f8\nProof. Fix x \u2208Rm. Let xT0 denote the subvector of x con\ufb01ned to the locations of its s0 largest coef\ufb01cients\nin absolute values; moreover, we use it to represent its 0-extended version x\u2032 \u2208Rp such that x\u2032\nT c = 0 and\nx\u2032\nT0 = xT0. Throughout this proof, T0 is understood to be the locations of the s0 largest coef\ufb01cients in\nabsolute values in x.\nMoreover, let (x\u2217\ni )m\ni=1 be non-increasing rearrangement of (|xi|)m\ni=1. Denote by\nL\n=\n\u221as0Bm\n1 \u2229\u03c1Bm\n2\nR\n=\n2 conv\n\uf8eb\n\uf8ed[\n|J|\u2264s\nEJ \u2229Bm\n2\n\uf8f6\n\uf8f8= 2 conv\n\u0000E \u2229Bm\n2\n\u0001\nAny vector x \u2208Rm satis\ufb01es:\n\r\rxT c\n0\n\r\r\n\u221e\u2264\u2225xT0\u22251 /s0\n\u2264\n\u2225xT0\u22252\n\u221as0\n(93)\nIt follows that for any \u03c1 > 0, s0 \u22651 and for all z \u2208L, we have the ith largest coordinate in absolute value\nin z is at most \u221as0/i,\nsup\nz\u2208L\n\u27e8x, z \u27e9\n\u2264\nmax\n\u2225z\u22252\u2264\u03c1 \u27e8xT0, z \u27e9+\nmax\n\u2225z\u22251\u2264\u221as0\n\u27e8xT c\n0 , z \u27e9\n\u2264\n\u03c1 \u2225xT0\u22252 +\n\r\rxT c\n0\n\r\r\n\u221e\n\u221as0\n\u2264\n\u2225xT0\u22252 (\u03c1 + 1)\nwhere clearly max\u2225z\u22252\u2264\u03c1 \u27e8xT0, z \u27e9= \u03c1 Ps0\ni=1(x\u22172\ni )1/2. And denote by SJ := Sm\u22121 \u2229EJ,\nsup\nz\u2208R\n\u27e8x, z \u27e9\n=\n(1 + \u03c1) max\nJ:|J|\u2264s0\nmax\nz\u2208SJ \u27e8x, z \u27e9\n=\n(1 + \u03c1) \u2225xT0\u22252\ngiven that for a convex function \u27e8x, z \u27e9, the maximum happens at an extreme point, and in this case, it\nhappens for z such that z is supported on T0, such that zT0 =\nxT0\n\u2225xT0\u22252\n, and zT c\n0 = 0.\n\u25a1\nLemma 32. Let 1/5 > \u03b4 > 0. Let E = \u222a|J|\u2264s0EJ for 0 < s0 < m/2 and k0 > 0. Let \u2206be a m \u00d7 m\nmatrix such that\n\f\fuT \u2206v\n\f\f \u2264\u03b4\n\u2200u, v \u2208E \u2229Sm\u22121\n(94)\nThen for all v \u2208\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001\n, we have\n\f\f\u03c5T \u2206\u03c5\n\f\f\n\u2264\n4\u03b4.\n(95)\n47\nProof. First notice that\nmax\n\u03c5\u2208\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001\n\f\f\u03c5T \u2206\u03c5\n\f\f\n\u2264\nmax\nw,u\u2208\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001\n\f\fwT \u2206u\n\f\f\n(96)\nNow that we have decoupled u and w on the RHS of (96), we \ufb01rst \ufb01x u. Then for any \ufb01xed u \u2208Sm\u22121 and\nmatrix \u2206\u2208Rm\u00d7m, f(w) =\n\f\fwT \u2206u\n\f\f is a convex function of w, and hence for w \u2208\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001\n\u2282\n2 conv\n\u0010S\n|J|\u2264s0 EJ \u2229Sm\u22121\u0011\n,\nmax\nw\u2208\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001\n\f\fwT \u2206u\n\f\f\n\u2264\n2\nmax\nw\u2208conv(E\u2229Sm\u22121)\n\f\fwT \u2206u\n\f\f\n=\n2\nmax\nw\u2208E\u2229Sm\u22121\n\f\fwT \u2206u\n\f\f\nwhere the maximum occurs at an extreme point of the set conv(E \u2229Sm\u22121), because of the convexity of the\nfunction f(w),\nClearly the RHS of (96) is bounded by\nmax\nu,w\u2208\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001\n\f\fwT \u2206u\n\f\f\n=\nmax\nu\u2208\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001\nmax\nw\u2208\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001\n\f\fwT \u2206u\n\f\f\n\u2264\n2\nmax\nu\u2208\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001\nmax\nw\u2208\n\u0000E\u2229Sm\u22121\u0001\n\f\fwT \u2206u\n\f\f\n=\n2\nmax\nu\u2208\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001 g(u)\nwhere the function g of u \u2208\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001\nis de\ufb01ned as\ng(u) =\nmax\nw\u2208\n\u0000E\u2229Sm\u22121\u0001\n\f\fwT \u2206u\n\f\f\nwhich is convex since it is the maximum of a function fw(u) :=\n\f\fwT \u2206u\n\f\f which is convex in u for\neach w \u2208(E \u2229Sm\u22121). Thus we have for u \u2208(\u221as0Bm\n1 \u2229Bm\n2 ) \u22822 conv\n\u0010S\n|J|\u2264s0 EJ \u2229Sm\u22121\u0011\n=:\n2 conv\n\u0000E \u2229Sm\u22121\u0001\nmax\nu\u2208\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001 g(u)\n\u2264\n2\nmax\nu\u2208conv (E\u2229Sm\u22121) g(u)\n=\n2\nmax\nu\u2208E\u2229Sm\u22121 g(u)\n(97)\n=\n2\nmax\nu\u2208E\u2229Sm\u22121\nmax\nw\u2208E\u2229Sm\u22121\n\f\fwT \u2206u\n\f\f \u22644\u03b4\n(98)\nwhere (97) holds given that the maximum occurs at an extreme point of the set conv(E \u2229Bm\n2 ), because of\nthe convexity of the function g(u).\n\u25a1\nCorollary 33. Suppose all conditions in Lemma 32 hold. Then \u2200\u03c5 \u2208Cone(s0),\n\f\f\u03c5T \u2206\u03c5\n\f\f\n\u2264\n4\u03b4 \u2225\u03c5\u22252\n2 .\n(99)\n48\nProof. It is suf\ufb01cient to show that \u2200\u03c5 \u2208Cone(s0) \u2229Sm\u22121,\n\f\f\u03c5T \u2206\u03c5\n\f\f\n\u2264\n4\u03b4.\nDenote by Cone := Cone(s0). Clearly this set of vectors satisfy:\nCone \u2229Sm\u22121 \u2282\n\u0000\u221as0Bm\n1 \u2229Bm\n2\n\u0001\nThus (99) follows from (95).\n\u25a1\nRemark G.1. Suppose we relax the de\ufb01nition of Cone(s0) to be:\nCone(s0) := {\u03c5 : \u2225\u03c5\u22251 \u22642\u221as0 \u2225\u03c5\u22252}\nClearly, Cone(s0, 1) \u2282Cone(s0). given that \u2200u \u2208Cone(s0, 1), we have\n\u2225u\u22251 \u22642 \u2225uT0\u22251 \u22642\u221as0 \u2225uT0\u22252 \u22642\u221as0 \u2225u\u22252\nLemma 34. Suppose all conditions in Lemma 32 hold. Then for all \u03c5 \u2208Rm,\n\f\f\u03c5T \u2206\u03c5\n\f\f \u22644\u03b4(\u2225\u03c5\u22252\n2 + 1\ns0\n\u2225\u03c5\u22252\n1)\n(100)\nProof. The lemma follows given that \u2200\u03c5 \u2208Rm, one of the following must hold:\nif \u03c5 \u2208Cone(s0)\n\f\f\u03c5T \u2206\u03c5\n\f\f\n\u2264\n4\u03b4 \u2225\u03c5\u22252\n2\n(101)\notherwise\n\f\f\u03c5T \u2206\u03c5\n\f\f\n\u2264\n4\u03b4\ns0\n\u2225\u03c5\u22252\n1 ,\n(102)\nleading to the same conclusion in (100). We have shown (101) in Lemma 32. Let Cone(s0)c be the com-\nplement set of Cone(s0)c in Rm. That is, we focus now on the set of vectors such that\nCone(s0)c := {\u03c5 : \u2225\u03c5\u22251 \u2265\u221as0 \u2225\u03c5\u22252}\nand show that for u = \u221as0\nv\n\u2225v\u22251,\n\f\fvT \u2206v\n\f\f\n\u2225v\u22252\n1\n:=\n1\ns0\n\f\fuT \u2206u\n\f\f \u22641\ns0\n\u03b4\nwhere the last inequality holds by Lemma 32 given that\nu \u2208(\u221as0Bm\n1 \u2229Bm\n2 ) \u22822 conv\n\uf8eb\n\uf8ed[\n|J|\u2264s0\nEJ \u2229Bm\n2\n\uf8f6\n\uf8f8\nand thus\n\f\fvT \u2206v\n\f\f\n\u2225v\u22252\n1\n\u2264\n1\ns0\nsup\nu\u2208\u221as0Bm\n1 \u2229Bm\n2\n\f\fuT \u2206u\n\f\f \u22641\ns0\n4\u03b4\n\u25a1\n49\nH\nProof of Corollary 19\nFirst we show that for all \u03c5 \u2208Rm, (103) holds. It is suf\ufb01cient to check that the condition (94) in\nLemma 32 holds. Then, (103) follows from Lemma 34: for \u03c5 \u2208Rm,\n\f\f\u03c5T \u2206\u03c5\n\f\f \u22644\u03b4(\u2225\u03c5\u22252\n2 + 1\n\u03b6 \u2225\u03c5\u22252\n1) \u22641\n2\u03bbmin(A)(\u2225\u03c5\u22252\n2 + 1\n\u03b6 \u2225\u03c5\u22252\n1).\n(103)\nThe Lower and Upper RE conditions thus immediately follow. The Corollary is thus proved.\n\u25a1\nI\nProof of Theorem 20\nWe \ufb01rst state the following preliminary results in Lemmas 35 and 36; their proofs appear in Section K.\nThroughout this section, the choice of C = C0/\n\u221a\nc\u2032 satis\ufb01es the conditions on C in Lemmas 35 and 36,\nwhere recall min{C0, C2\n0} \u22654/c for c as de\ufb01ned in Theorem 26. For a set J \u2282{1, . . . , m}, denote\nFJ = A1/2EJ where recall EJ = span{ej : j \u2208J}.\nLemma 35. Suppose all conditions in Theorem 20 hold. Let\nE =\n[\n|J|=k\nEJ \u2229Sm\u22121.\nSuppose that for some c\u2032 > 0 and \u03b5 \u22641\nC , where C = C0/\n\u221a\nc\u2032,\nr(B) := tr(B)\n\u2225B\u22252\n\u2265\nc\u2032kK4 log(3em/k\u03b5)\n\u03b52\n.\n(104)\nThen for all vectors u, v \u2208E \u2229Sm\u22121, on event B1, where P (B1) \u22651 \u22122 exp\n\u0010\n\u2212c2\u03b52\ntr(B)\nK4\u2225B\u22252\n\u0011\nfor c2 \u22652,\n\f\fuT ZTBZv \u2212EuT ZTBZv\n\f\f\n\u2264\n4C\u03b5tr(B).\nLemma 36. Suppose that \u03b5 \u22641/C, where C is as de\ufb01ned in Lemma 35. Suppose that (104) holds. Let\nE =\n[\n|J|=k\nEJ\nand\nF =\n[\n|J|=k\nFJ.\n(105)\nThen on event B2, where P (B2) \u22651 \u22122 exp\n\u0010\n\u2212c2\u03b52\ntr(B)\nK4\u2225B\u22252\n\u0011\nfor c2 \u22652, we have for all vectors u \u2208\nE \u2229Sm\u22121 and w \u2208F \u2229Sm\u22121,\n\f\f\fwT ZT\n1 B1/2Z2u\n\f\f\f\n\u2264\nC\u03b5tr(B)\n(1 \u2212\u03b5)2 \u2225B\u22251/2\n2\n\u22644C\u03b5tr(B)/\u2225B\u22251/2\n2\nwhere Z1, Z2 are independent copies of Z, as de\ufb01ned in Theorem 20.\nIn fact, the same conclusion holds for all y, w \u2208F \u2229Sm\u22121; and in particular, for B = I, we have the\nfollowing.\n50\nCorollary 37. Suppose all conditions in Lemma 35 hold. Suppose that F = A1/2E for E as de\ufb01ned in\nLemma 35. Let\nf\n\u2265\nc\u2032kK4 log(3em/k\u03b5)\n\u03b52\n.\n(106)\nThen on event B3, where P (B3) \u22651 \u22122 exp\n\u0000\u2212c2\u03b52f 1\nK4\n\u0001\n, we have for all vectors w, y \u2208F \u2229Sm\u22121 and\n\u03b5 \u22641/C for C is as de\ufb01ned in Lemma 35,\n\f\f\fyT( 1\nf ZT Z \u2212I)w\n\f\f\f\n\u2264\n4C\u03b5.\n(107)\nWe prove Lemmas 35 and 36 and Corollary 37 in Section K. We are now ready to prove Theorem 20.\nProof of Theorem 20.\nRecall the following for X0 = Z1A1/2,\n\u2206:= b\u0393A \u2212A := 1\nf XT X \u22121\nf btr(B)Im \u2212A\n=\n( 1\nf XT\n0 X0 \u2212A) + 1\nf\n\u0000W TX0 + XT\n0 W\n\u0001\n+ 1\nf\n\u0000W TW \u2212btr(B)Im\n\u0001\n.\nNotice that\f\f\fuT (b\u0393A \u2212A)\u03c5\n\f\f\f =\n\f\fuT (XT X \u2212btr(B)Im \u2212A)\u03c5\n\f\f\n\u2264\n\f\f\fuT ( 1\nf XT\n0 X0 \u2212A)\u03c5\n\f\f\f +\n\f\f\fuT 1\nf (W TX0 + XT\n0 W)\u03c5\n\f\f\f +\n\f\f\fuT ( 1\nf W T W \u2212\nbtr(B)\nf\nIm)\u03c5\n\f\f\f\n\u2264\n\f\f\fuT A1/2 1\nf ZT\n1 Z1A1/2\u03c5 \u2212uT A\u03c5\n\f\f\f +\n\f\f\fuT 1\nf (W T X0 + XT\n0 W)\u03c5\n\f\f\f\n+\n\f\f\fuT ( 1\nf ZT\n2 BZ2 \u2212\u03c4BIm)\u03c5\n\f\f\f + 1\nf\n\f\fbtr(B) \u2212tr(B)\n\f\f \f\fuT \u03c5\n\f\f =: I + II + III + IV.\nFor u \u2208E \u2229Sm\u22121, de\ufb01ne h(u) :=\nA1/2u\n\u2225A1/2u\u22252\n. The conditions in (104) and (106) hold for k. We \ufb01rst bound\nthe middle term as follows. Fix u, \u03c5 \u2208E \u2229Sm\u22121 Then on event B2, for \u03a5 = ZT\n1 B1/2Z2,\n\f\fuT (W T X0 + XT\n0 W)\u03c5\n\f\f\n=\n\f\f\fuT ZT\n2 B1/2Z1A1/2\u03c5 + uT A1/2ZT\n1 B1/2Z2\u03c5\n\f\f\f\n\u2264\n\f\fuT \u03a5T h(v)\n\f\f\n\r\r\rA1/2v\n\r\r\r\n2 +\n\f\fh(u)T \u03a5\u03c5\n\f\f\n\r\r\rA1/2u\n\r\r\r\n2\n\u2264\n2\nmax\nw\u2208F \u2229Sm\u22121,\u03c5\u2208E\u2229Sm\u22121\n\f\fwT \u03a5\u03c5\n\f\f \u03c11/2\nmax(k, A)\n\u2264\n8C\u03b5tr(B)\n\u0012\u03c1max(k, A)\n\u2225B\u22252\n\u00131/2\n.\nWe now use Lemma 35 to bound both I and III. We have for C as de\ufb01ned in Lemma 35, on event B1 \u2229B3,\n\f\fuT (ZT\n2 BZ2 \u2212tr(B)Im)\u03c5\n\f\f\n\u22644C\u03b5tr(B).\nMoreover, by Corollary 37, we have on event B3, for all u, v \u2208E \u2229Sm\u22121,\n\f\f\fuT ( 1\nf XT\n0 X0 \u2212A)\u03c5\n\f\f\f\n=\n\f\f\fuT A1/2ZT ZA1/2\u03c5 \u2212uT A\u03c5\n\f\f\f\n=\n\f\f\fh(u)T ( 1\nf ZT Z \u2212I)h(\u03c5)\n\f\f\f\n\r\r\rA1/2u\n\r\r\r\n2\n\r\r\rA1/2\u03c5\n\r\r\r\n2\n\u2264\n1\nf maxw,y\u2208F \u2229Sm\u22121\n\f\fwT (ZTZ \u2212I)y\n\f\f \u03c1max(k, A)\n\u2264\n4C\u03b5\u03c1max(k, A).\n51\nThus we have on event B1 \u2229B2 \u2229B3 and for \u03c4B := tr(B)/f\nI + II + III\n\u2264\n4C\u03b5\n \n\u03c1max(k, A) + 2\u03c4B\n\u0012\u03c1max(k, A)\n\u2225B\u22252\n\u00131/2\n+ \u03c4B\n!\n\u2264\n8C\u03b5 (\u03c4B + \u03c1max(k, A)) .\nOn event B6, we have for D1 as de\ufb01ned in Lemma 7,\nIV \u2264|b\u03c4B \u2212\u03c4B| \u22642C0D1K2\ns\nlog m\nfm .\nThe theorem thus holds by the union bound.\n\u25a1\nJ\nProof for Theorem 21\nWe \ufb01rst state the following bounds in (108) before we prove Theorem 21. On event A2, where P (A2) \u2265\n1 \u22122 exp\n\u0010\n\u2212c3\u03b52\ntr(A)\nK4\u2225A\u22252\n\u0011\n\u2200u, w \u2208Sf\u22121\n\f\f\fuT Z1A1/2ZT\n2 w\n\f\f\f \u22644C\u03b5tr(A)\n\u2225A\u22251/2\n2\n.\n(108)\nTo see this, \ufb01rst note that by Lemma 27, we have for t = C\u03b5tr(A)/ \u2225A\u22251/2\n2\nand \u03b5 \u22641/2,\nP\n\u0010\f\f\fuT Z1A1/2ZT\n2 w\n\f\f\f > t\n\u0011\n\u2264\n2 exp\n\u0012\n\u2212c min\n\u0012C2\u03b52tr(A)\nK4 \u2225A\u22252\n, C\u03b5tr(A)\nK2 \u2225A\u22252\n\u0013\u0013\n\u2264\n2 exp\n\u0012\n\u2212c min\n\u0000C2, 2C\n\u0001 \u03b52tr(A)\nK4 \u2225A\u22252\n\u0013\nwhere recall\nC\u2032 = cc\u2032 min\n\u00002C, C2\u0001\n> 4.\nBefore we proceed, we state the following well-known result on volumetric estimate; see e.g. Milman and Schechtman\n(1986).\nLemma 38. Given m \u22651 and \u03b5 > 0. There exists an \u03b5-net \u03a0 \u2282Bm\n2 of Bm\n2 with respect to the Euclidean\nmetric such that Bm\n2\n\u2282(1 \u2212\u03b5)\u22121 conv \u03a0 and |\u03a0| \u2264(1 + 2/\u03b5)m. Similarly, there exists an \u03b5-net of the\nsphere Sm\u22121, \u03a0\u2032 \u2282Sm\u22121 such that |\u03a0\u2032| \u2264(1 + 2/\u03b5)m.\nChoose an \u03b5-net \u03a0 \u2282Sf\u22121 such that |\u03a0| \u2264(1 + 2/\u03b5)f = exp(f log(3/\u03b5)). The existence of such \u03a0 is\nguaranteed by Lemma 38. By the union bound and Lemma 27, we have for some C \u22652 and c\u2032 \u22651 large\nenough such that\nP\n \n\u2203u, w \u2208\u03a0s.t.\n\f\f\fuT Z1A1/2ZT\n2 w\n\f\f\f \u2265C\u03b5 tr(A)\n\u2225A\u22251/2\n2\n!\n\u2264\n2 exp\n\u0012\n\u2212c3\n\u03b52tr(A)\nK4 \u2225A\u22252\n\u0013\n.\nHence, (108) follows from a standard approximation argument.\n52\nLemma 39. Let \u03b5 > 0. Let Z as de\ufb01ned in De\ufb01nition 1.2. Assume that\ntr(A)\n\u2225A\u2225\u2265c\u2032f log(3/\u03b5)\n\u03b52\n.\nThen\nP\n\u0010\n\u2203x \u2208Sf\u22121 \f\f\f\n\r\r\rA1/2ZT x\n\r\r\r\n2 \u2212(tr(A))1/2\f\f\f > \u03b5(tr(A))1/2\u0011\n\u2264exp\n\u0012\n\u2212c\u03b52 tr(A)\nK4 \u2225A\u2225\n\u0013\n.\nProof. Let x \u2208Sf\u22121. Then Y = ZTx \u2208Rm is a random vector with independent coordinates satisfying\nEYj = 0 and \u2225Yj\u2225\u03c82 \u2264CK for all j \u22081 . . . m. The last estimate follows from Hoeffding inequality. By\nTheorem 2.1 Rudelson and Vershynin (2013),\nP\n\u0010\f\f\f\n\r\r\rA1/2Y\n\r\r\r\n2 \u2212(tr(A))1/2\f\f\f > \u03b5(tr(A))1/2\u0011\n\u2264exp\n\u0012\n\u2212c\u03b52 tr(A)\nK4 \u2225A\u2225\n\u0013\n.\nChoose an \u03b5-net \u03a0 \u2282Sf\u22121 such that |\u03a0| \u2264(3/\u03b5)f . By the union bound and the assumption of the Lemma,\nP\n\u0010\n\u2203x \u2208\u03a0\n\f\f\f\n\r\r\rA1/2ZTx\n\r\r\r\n2 \u2212(tr(A))1/2\f\f\f > \u03b5(tr(A))1/2\u0011\n\u2264\n|\u03a0| \u00b7 exp\n\u0012\n\u2212c\u03b52 tr(A)\nK4 \u2225A\u2225\n\u0013\n\u2264\nexp\n\u0012\n\u2212c\u2032\u03b52 tr(A)\nK4 \u2225A\u2225\n\u0013\n.\nA standard approximation argument shows that if\n\f\f\r\rA1/2ZTx\n\r\r\n2 \u2212(tr(A))1/2\f\f \u2264\u03b5(tr(A))1/2 for all x \u2208\u03a0,\nthen\n\f\f\r\rA1/2ZT x\n\r\r\n2 \u2212(tr(A))1/2\f\f \u22643\u03b5(tr(A))1/2 for all x \u2208Sf\u22121. This \ufb01nishes the proof of the Lemma.\n\u25a1\nProof of Theorem 21.\nFirst we write\nXXT \u2212tr(A)If =\n\u0000Z1A1/2 + B1/2Z2)\n\u0000Z1A1/2 + B1/2Z2\n\u0001T \u2212tr(A)If\n=\n\u0000Z1A1/2 + B1/2Z2)\n\u0000ZT\n2 B1/2 + A1/2ZT\n1\n\u0001\n\u2212tr(A)If\n=\nZ1A1/2ZT\n2 B1/2 + B1/2Z2ZT\n2 B1/2 + B1/2Z2A1/2ZT\n1 + Z1AZT\n1 \u2212tr(A)If.\nHence,\n\f\f\f\f\nuT (XXT )u\nm\n\u2212uT tr(A)Iu\nm\n\u2212uT Bu\n\f\f\f\f \u2264\n\f\f\f\f\n1\nmuT Z1AZT\n1 u \u2212tr(A)\nm\nuT u\n\f\f\f\f\n+\n\f\f\f\f\n1\nmuT B1/2Z2ZT\n2 B1/2u \u2212uT Bu\n\f\f\f\f + 2\nm\n\f\f\fuT Z1A1/2ZT\n2 B1/2u\n\f\f\f .\nwhere by (108), we have on event A2, for \u03c4A := tr(A)\nm\nand w :=\nB1/2u\n\u2225B1/2u\u22252\n,\n2\nm\n\f\f\fuT Z1A1/2ZT\n2 B1/2u\n\f\f\f = 2\nm\n\f\f\fuT Z1A1/2ZT\n2 w\n\f\f\f\n\r\r\rB1/2u\n\r\r\r\n2\n\u2264\n8C\u03b5tr(A)\n\r\rB1/2u\n\r\r\n2\n\u2225A\u22251/2\n2\nm\n=: 8C\u03b5\u03c4A\n\r\r\rB1/2u\n\r\r\r\n2 /\u2225A\u22251/2\n2\n.\n53\nMoreover, by the union bound and Lemma 39, we have on event A1, where P (A1) \u22651 \u2212exp(c\u03b52 m\nK4) \u2212\nexp(c\u03b52\ntr(A)\nK4\u2225A\u22252),\n(1 \u2212\u03b5)\n\r\r\rB1/2u\n\r\r\r\n2\n\u2264\n1\n\u221am\n\r\r\rZ2B1/2u\n\r\r\r\n2 \u2264(1 + \u03b5)\n\r\r\rB1/2u\n\r\r\r\n2\n(1 \u2212\u03b5)tr(A)1/2\n\u221am\n\u2264\n1\n\u221am\n\r\r\rA1/2ZT\n1 u\n\r\r\r\n2 \u2264(1 + \u03b5)tr(A)1/2\n\u221am\n.\nHence on event A1, we have\n1\nm\n\f\f\f\f\n\r\r\rA1/2ZT\n1 u\n\r\r\r\n2\n2 \u2212tr(A)\n\f\f\f\f\n\u2264\nmax((1 + \u03b5)2 \u22121, 1 \u2212(1 \u2212\u03b5)2)tr(A)\nm\n,\n\f\f\f\f\n1\nm\n\r\r\rZT\n2 B1/2u\n\r\r\r\n2\n2 \u2212uT Bu\n\f\f\f\f\n\u2264\nmax((1 + \u03b5)2 \u22121, 1 \u2212(1 \u2212\u03b5)2)\n\r\r\rB1/2u\n\r\r\r\n2\n2 .\nThus we have for all u \u2208Sf\u22121, on event A1 \u2229A2, for C2 := 4C + 3\n\f\f\f\f\n1\nmuT (XXT )u \u2212uT tr(A)If\nm\nu \u2212uT Bu\n\f\f\f\f \u2264\n\u2264\n\f\f\f\f\n\r\r\rZT\n2 B1/2u\n\r\r\r\n2\n2 /m \u2212uT Bu\n\f\f\f\f + 1\nm\n\f\f\f\f\n\r\r\rA1/2ZT\n1 u\n\r\r\r\n2\n2 \u2212tr(A)\n\f\f\f\f + 8C\u03b5\u03c4A\n\r\r\rB1/2u\n\r\r\r\n2 /\u2225A\u22251/2\n2\n\u2264\n3\u03b5\n\r\r\rB1/2u\n\r\r\r\n2\n2 + 3\u03b5\u03c4A + 8C\u03b5\u03c4A\n\r\r\rB1/2u\n\r\r\r\n2 /\u2225A\u22251/2\n2\n\u2264C2\u03b5\n\r\r\rB1/2u\n\r\r\r\n2\n2 + C2\u03b5\u03c4A\nwhere 2\u03c4 1/2\nA\n\r\rB1/2u\n\r\r\n2 \u2264\u03c4A +\n\r\rB1/2u\n\r\r2\n2. The theorem thus holds.\n\u25a1\nJ.1\nProof of Corollary 22\nLower bound: For all u \u2208Sf\u22121 and\n1\nmuT (XXT )u \u2212uT tr(A)If\nm\nu\n\u2265\nuT Bu(1 \u22123\u03b5) \u22123\u03b5\u03c4A \u22128C\n\r\r\rB1/2u\n\r\r\r\n2 \u03b5\u03c4A/ \u2225A\u22251/2\n2\n\u2265\nuT Bu(1 \u22123\u03b5 \u22124C\u03b5) \u22123\u03b5\u03c4A \u22124C\u03b5\u03c4A\n\u2265\nuT Bu(1 \u2212C2\u03b5) \u2212C2\u03b5\u03c4A \u2265uT Bu(1 \u22122\u03b4)\nwhere we bound the term using the fact that 1 \u2264\u03c4A \u2264\u03bbmax(B) and\nC2\u03c4A\u03b5\n\u2264\n\u03b4\u03bbmin(B) and \u03b5 \u2264\u03b4\u03bbmin(B)/(C2\u03c4A)\nC2\u03b5\n\u2264\n\u03b4 and C3\u03b5 \u2264\u03b4 min\n\u0012\u03bbmin(B)\n\u03c4A\n, 1\n\u0013\n.\nBy a similar argument, we can prove the upper bound on the isometry property as stated in the corollary.\n\u25a1\n54\nJ.2\nProof of Corollary 23\nRecall the following\neA := XT X \u2212tr(B)Im =\n\u0000Z1A1/2 + B1/2Z2)T \u0000Z1A1/2 + B1/2Z2\n\u0001\n\u2212tr(B)Im\n=\n\u0000ZT\n2 B1/2 + A1/2ZT\n1\n\u0001\u0000Z1A1/2 + B1/2Z2) \u2212tr(B)Im\n=\n\u0000ZT\n2 B1/2Z1A1/2 + A1/2ZT\n1 B1/2Z2\n\u0001\n+ A1/2ZT\n1 Z1A1/2 +\n\u0000ZT\n2 BZ2 \u2212tr(B)Im\n\u0001\n.\nHence, for all vectors u \u2208Sm\u22121 \u2229E\nuT (XT X)u\nf\n\u2212uT tr(B)Iu\nf\n\u2212uT Au \u22641\nf\n\f\fuT Z2BZT\n2 u \u2212tr(B)uT u\n\f\f\n+\n\f\f\f\f\n1\nf uT A1/2ZT\n1 Z1A1/2u \u2212uT Au\n\f\f\f\f + 2\nf\n\f\f\fuT A1/2ZT\n1 B1/2Z2u\n\f\f\f .\nBy Lemma 35, we have on event B1,\n\u2200u \u2208E \u2229Sm\u22121 \f\fuT ZT BZu \u2212tr(B)\n\f\f\n\u2264\n4C\u03b5tr(B);\nBy Lemma 36, we have on event B2,\n\u2200u \u2208E \u2229Sm\u22121 \f\f\fuT A1/2ZT\n1 B1/2Z2u\n\f\f\f\n\u2264\n4C\u03b5tr(B)\n\r\r\rA1/2u\n\r\r\r\n2 / \u2225B\u22251/2\n2\n.\nFor all u \u2208Sm\u22121 \u2229E,\n8C\u03b5\u03c4B\n\r\r\rA1/2u\n\r\r\r\n2 / \u2225B\u22251/2\n2\n\u22642(2C\u03b51/2\n\u03c4B\n\u2225B\u22251/2\n2\n)(2\u03b51/2 \r\r\rA1/2u\n\r\r\r\n2)\n\u2264\n4C2\u03b5 \u03c4 2\nB\n\u2225B\u22252\n+ 4\u03b5\n\r\r\rA1/2u\n\r\r\r\n2\n2 \u22644C2\u03b5\u03c4B + 4\u03b5\n\r\r\rA1/2u\n\r\r\r\n2\n2 .\nAnd \ufb01nally, we have also shown that for all u \u2208E on event B9,\n(1 \u2212\u03b5)\n\r\r\rA1/2u\n\r\r\r\n2 \u2264\n1\n\u221af\n\r\r\rZ1A1/2u\n\r\r\r\n2 \u2264(1 + \u03b5)\n\r\r\rA1/2u\n\r\r\r\n2 .\nThus we have for all u \u2208Sm\u22121 \u2229E, on event B1 \u2229B2 \u2229B9,\n\f\f\f\f\nuT (XT X)u\nf\n\u2212uT tr(B)Iu\nf\n\u2212uT Au\n\f\f\f\f \u22641\nf\n\f\fuT ZT\n2 BZ2u \u2212tr(B)uT u\n\f\f\n+\n\f\f\f\f\n1\nf\n\r\r\rZ1A1/2u\n\r\r\r\n2\n2 \u2212\n\r\r\rA1/2u\n\r\r\r\n2\n2\n\f\f\f\f + 2\nf\n\f\f\fuT A1/2ZT\n1 B1/2Z2u\n\f\f\f\n\u2264\n4C\u03b5\u03c4B + 6\u03b5\n\r\r\rA1/2u\n\r\r\r\n2\n2 + 8C\u03b5\u03c4B\n\r\r\rA1/2u\n\r\r\r\n2 / \u2225B\u22251/2\n2\n\u2264\n4C\u03b5\u03c4B + 6\u03b5\n\r\r\rA1/2u\n\r\r\r\n2\n2 + 4C2\u03b5\u03c4B + 4\u03b5\n\r\r\rA1/2u\n\r\r\r\n2\n2\n\u2264\n10\u03b5\n\r\r\rA1/2u\n\r\r\r\n2\n2 + 4(C2 + C)\u03b5\u03c4B.\n(109)\n55\nUpper bound: Thus we have by (109) for the maximum sparse eigenvalue of eA at order k:\n\u03c1max(k, eA)\n:=\nmax\nu\u2208E\u2229Sm\u22121\n\f\f\fuT eAu\n\f\f\f \u2264\nmax\nu\u2208E\u2229Sm\u22121\n\f\f\fuT eAu \u2212uT Au\n\f\f\f + \u03c1max(k, A)\n\u2264\n\u03c1max(k, A)(1 + 10\u03b5) + C4\u03b5\u03c4B\nwhere C4 = 4(C + C2). The upper bound on \u03c1max(k, eA \u2212A) in the theorem statement thus holds.\nLower bound: Suppose C4 = 4(C + C2) \u222810\n\u03b5 \u2264\u03b4\nC4\nmin\n\u0012\u03c1min(k, A)\n\u03c4B\n, 1\n\u0013\n= \u03b4\nC5\nand C4\u03b5 \u2264\u03b4 min\n\u0012\u03c1min(k, A)\n\u03c4B\n, 1\n\u0013\n.\nWe have by (109) for all u \u2208Sm\u22121 \u2229E, on event B1 \u2229B2 \u2229B9,\n1\nf uT (XT X)u \u2212uT tr(B)Im\nf\nu\n\u2265\nuT Au \u2212\n\u0010\n6\u03b5uT Au + 4C\u03b5\u03c4B + 8C\u03b5\u03c4B\n\r\r\rA1/2u\n\r\r\r\n2 / \u2225B\u22251/2\n2\n\u0011\n\u2265\nuT Au \u22126\u03b5uT Au \u22124C\u03b5\u03c4B \u22128C\u03b5\u03c4 1/2\nB\n\r\r\rA1/2u\n\r\r\r\n2\n\u2265\nuT Au \u221210\u03b5uT Au \u22124(C + C2)\u03b5\u03c4B \u2265uT Au(1 \u221210\u03b5 \u2212\u03b4)\n\u2265\nuT Au(1 \u22122\u03b4)\nwhere 4(C + C2)\u03b5\u03c4B \u2264\u03b4\u03c1min(k, A) and 10\u03b5 \u2264\u03b4.\n\u25a1\nK\nProofs of Lemmas 35 and 36 and Corollary 37\nThroughout the following proofs, we denote by r(B) = tr(B)\n\u2225B\u22252 . Let \u03b5 \u22641\nC where C is large enough so that\ncc\u2032C2 \u22654, and hence the choice of C = C0/\n\u221a\nc\u2032 satis\ufb01es our need.\nProof of Lemma 35.\nFirst we prove concentration bounds for all pairs of u, v \u2208\u03a0\u2032, where \u03a0\u2032 \u2282Sm\u22121\nis an \u03b5-net of E. Let t = CK2\u03b5tr(B). We have by Lemma 27, and the union bound,\nP\n\u0000\u2203u, v \u2208\u03a0\u2032,\n\f\fuT ZTBZv \u2212EuT ZTBZv\n\f\f > t\n\u0001\n\u2264\n2\n\f\f\u03a0\u2032\f\f2 exp\n\"\n\u2212c min\n \nt2\nK4 \u2225B\u22252\nF\n,\nt\nK2 \u2225B\u22252\n!#\n\u2264\n2\n\f\f\u03a0\u2032\f\f2 exp\n\u0014\n\u2212c min\n\u0012\nC2, CK2\n\u03b5\n\u0013 \u03b52r(B)\nK4\n\u0015\n\u22642 exp\n\u0000\u2212c2\u03b52r(B)/K4\u0001\nwhere we use the fact that \u2225B\u22252\nF \u2264\u2225B\u22252 tr(B), and\n\f\f\u03a0\u2032\f\f \u2264\n\u0012m\nk\n\u0013\n(3/\u03b5)k \u2264exp(k log(3em/k\u03b5))\n56\nwhile\nc min\n\u0012\nC2, CK2\n\u03b5\n\u0013\n\u03b52 r(B)\nK4 = cC2\u03b52\ntr(B)\n\u2225B\u22252 K4 \u2265cC2\n0k log\n\u00003em\nk\u03b5\n\u0001\n\u22654k log\n\u00003em\nk\u03b5\n\u0001\nDenote by B2 the event such that for \u039b :=\n1\ntr(B)(ZT BZ \u2212I),\nsup\nu,v\u2208\u03a0\u2032\n\f\fvT \u039bu\n\f\f\n\u2264\nC\u03b5 =: r\u2032\nf,k\nholds. A standard approximation argument shows that under B2 and for \u03b5 \u22641/2,\nsup\nx,y\u2208Sm\u22121\u2229E\n\f\fyT \u039bx\n\f\f \u2264\nr\u2032\nk,f\n(1 \u2212\u03b5)2 \u22644C\u03b5.\n(110)\nThe lemma is thus proved.\n\u25a1\nProof of Lemma 36.\nBy Lemma 27, we have for t = C\u03b5tr(B)/ \u2225B\u22251/2\n2\nfor C = C0/\n\u221a\nc\u2032\nP\n\u0010\f\f\fwT ZT\n1 B1/2Z2u\n\f\f\f > t\n\u0011\n\u2264\nexp\n\uf8eb\n\uf8ed\u2212c min\n\uf8eb\n\uf8edC2 tr(B)2\n\u2225B\u22252 \u03b52\nK4tr(B) , C\u03b5tr(B)\nK2 \u2225B\u22252\n\uf8f6\n\uf8f8\n\uf8f6\n\uf8f8\n\u2264\n2 exp\n\u0012\n\u2212c min\n\u0012C2\u03b52rB\nK4\n, C\u03b5rB\nK2\n\u0013\u0013\n\u2264\n2 exp\n\u0012\n\u2212c min\n\u0012\nC2, CK2\n\u03b5\n\u0013\n\u03b52rB/K4\n\u0013\nChoose an \u03b5-net \u03a0\u2032 \u2282Sm\u22121 such that\n\u03a0\u2032 =\n[\n|J|=k\n\u03a0\u2032\nJ\nwhere\n\u03a0\u2032\nJ \u2282EJ \u2229Sm\u22121\n(111)\nis an \u03b5-net for EJ \u2229Sm\u22121 and\n\f\f\u03a0\u2032\f\f \u2264\n\u0012m\nk\n\u0013\n(3/\u03b5)k \u2264exp(k log(3em/k\u03b5)).\nSimilarly, choose \u03b5-net \u03a0 of F \u2229Sm\u22121 of size at most exp(k log(3em/k\u03b5)). By the union bound and\nLemma 27, and for K2 \u22651,\nP\n\u0010\n\u2203w \u2208\u03a0, u \u2208\u03a0\u2032 s.t.\n\f\f\fwT ZT\n1 B1/2Z2u\n\f\f\f \u2265C\u03b5tr(B)/\u2225B\u22251/2\n2\n\u0011\n\u2264\n\f\f\u03a0\u2032\f\f |\u03a0| 2 exp\n\u0000\u2212c min\n\u0000CK2/\u03b5, C2\u0001\n\u03b52rB/K4\u0001\n\u2264\nexp (2k log(3em/k\u03b5)) 2 exp\n\u0000\u2212cC2\u03b52rB/K4\u0001\n\u2264\n2 exp\n\u0000\u2212c2\u03b52rB/K4\u0001\nwhere C is large enough such that cc\u2032C2 := C\u2032 > 4 and for \u03b5 \u22641\nC ,\nc min\n\u0000CK2/\u03b5, C2\u0001\n\u03b52\ntr(B)\n\u2225B\u22252 K4 \u2265C\u2032k log(3em/k\u03b5) \u22654k log(3em/k\u03b5).\n57\nDenote by \u03a5 := ZT\n1 B1/2Z2. A standard approximation argument shows that if\nsup\nw\u2208\u03a0,u\u2208\u03a0\u2032\n\f\fwT \u03a5u\n\f\f \u2264C\u03b5 tr(B)\n\u2225B\u22251/2\n2\n=: rk,f\nan event which we denote by B2, then for all u \u2208E and w \u2208F,\n\f\f\fwT ZT\n1 B1/2Z2u\n\f\f\f \u2264\nrk,f\n(1 \u2212\u03b5)2 .\n(112)\nThe lemma thus holds for c2 \u2265C\u2032/2 \u22652.\n\u25a1\nProof of Corollary 37.\nClearly (107) implies that (104) holds for B = I. Clearly (106) holds following\nthe analysis of Lemma 35 by setting B = I, while replacing event B1 with B3, which denotes an event such\nthat\nsup\nu,v\u2208\u03a0\n1\nf\n\f\fvT (ZT Z \u2212I)u\n\f\f\n\u2264\nC\u03b5\nThe rest of the proof follows by replacing E with F everywhere. The corollary thus holds.\n\u25a1\nReferences\nALLEN, G. and TIBSHIRANI, R. (2010). Transposable regularized covariance models with an application\nto missing data imputation. Annals of Applied Statistics 4 764\u2013790.\nBELLONI, A., ROSENBAUM, M. and TSYBAKOV, A. (2014). Linear and conic programming estimators in\nhigh-dimensional errors-in-variables models. ArXiv:1408.0241.\nBICKEL, P., RITOV, Y. and TSYBAKOV, A. (2009). Simultaneous analysis of Lasso and Dantzig selector.\nThe Annals of Statistics 37 1705\u20131732.\nBONILLA, E., CHAI, K. and WILLIAMS, C. (2008). Multi-task gaussian process prediction. In In Advances\nin Neural Information Processing Systems 20 (NIPS 2010).\nCAND`ES, E. and TAO, T. (2007). The Dantzig selector: statistical estimation when p is much larger than n.\nAnnals of Statistics 35 2313\u20132351.\nCARROLL, R., RUPPERT, D., STEFANSKI, L. and CRAINICEANU, C. M. (2006). Measurement Error in\nNonlinear Models (Second Edition). Chapman & Hall.\nCARROLL, R. and WAND, M. (1991). Semiparametric estimation in logistic measurement error models. J.\nR. Statist. Soc. B 53 573\u2013585.\nCARROLL, R. J., GAIL, M. H. and LUBIN, J. H. (1993). Case-control studies with errors in predictors.\nJournal of American Statistical Association 88 177 \u2013 191.\nCARROLL, R. J., GALLO, P. P. and GLESER, L. J. (1985). Comparison of least squares and errors-in-\nvariables regression with special reference to randomized analysis of covariance. Journal of American\nStatistical Association 80 929 \u2013 932.\n58\nCHEN, S., DONOHO, D. and SAUNDERS, M. (1998).\nAtomic decomposition by basis pursuit.\nSIAM\nJournal on Scienti\ufb01c and Statistical Computing 20 33\u201361.\nCHEN, Y. and CARAMANIS, C. (2013). Noisy and missing data regression: Distribution-oblivious support\nrecovery. In Proceedings of The 30th International Conference on Machine Learning ICML-13.\nCOHEN, M. and KOHN, A. (2011). Measuring and interpreting neuronal correlations. Nature Neuroscience\n14 809\u2013811.\nCOOK, J. R. and STEFANSKI, L. A. (1994). Simulation-extrapolation estimation in parametric measure-\nment error models. Journal of the American Statistical Association 89 1314\u20131328.\nDAWID, A. P. (1981). Some matrix-variate distribution theory: Notational considerations and a bayesian\napplication. Biometrika 68 265\u2013274.\nDEMPSTER, A., LAIRD, N. and RUBIN, D. (1977). Maximum likelihood from incomplete data via the em\nalgorithm. Journal of the Royal Statistical Society, Series B 39 1\u201338.\nDUTILLEUL, P. (1999). The mle algorithm for the matrix normal distribution. Journal of Statistical Com-\nputation and Simulation 64 105\u2013123.\nEFRON, B. (2009). Are a set of microarrays independ of each other? Ann. App. Statist. 3 922\u2013942.\nFULLER, W. A. (1987). Measurement error models. John Wiley and Sons.\nGAUTIER, E. and TSYBAKOV, A. (2011). High-dimensional instrumental variables regression and con\ufb01-\ndence sets. ArXiv:1105.2454.\nGUPTA, A. and VARGA, T. (1992). Characterization of matrix variate normal distributions. Journal of\nMultivariate Analysis 41 80\u201388.\nHALL, P. and MA, Y. (2007). Semiparametric estimators of functional measurement error models with\nunknown error. Journal of the Royal Statistical Society B 69 429\u2013446.\nHWANG, J. T. (1986). Multiplicative errors-in-variables models with applications to recent data released by\nthe u.s. department of energy. Journal of American Statistical Association 81 680\u2013688.\nITURRIA, S. J., CARROLL, R. J. and FIRTH, D. (1999). Polynomial regression and estimating functions\nin the presence of multiplicative measurement error. Journal of the Royal Statistical Society, Series B,\nMethodological 61 547\u2013561.\nKALAITZIS, A., LAFFERTY, J., LAWRENCE, N. and ZHOU, S. (2013). The bigraphical lasso. In Proceed-\nings of The 30th International Conference on Machine Learning ICML-13.\nKASS, R., VENTURA, V. and BROWN, E. (2005). Statistical issues in the analysis of neuronal data. J\nNeurophysiol 94 8\u201325.\nLIANG, H., H \u00a8ARDLE, W. and CARROLL, R. J. (1999). Estimation in a semiparametric partially linear\nerrors-in-variables model. Ann. Statist. 27 1519\u20131535.\nLIANG, H. and LI, R. (2009). Variable selection for partially linear models with measurement errors.\nJournal of the American Statistical Association 104 234\u2013248.\n59\nLOH, P. and WAINWRIGHT, M. (2012). High-dimensional regression with noisy and missing data: Provable\nguarantees with nonconvexity. The Annals of Statistics 40 1637\u20131664.\nMA, Y. and LI, R. (2010). Variable selection in measurement error models. Bernoulli 16 274\u2013300.\nMENDELSON, S., PAJOR, A. and TOMCZAK-JAEGERMANN, N. (2008). Uniform uncertainty principle for\nbernoulli and subgaussian ensembles. Constructive Approximation 28 277\u2013289.\nMILMAN, V. D. and SCHECHTMAN, G. (1986). Asymptotic Theory of Finite Dimensional Normed Spaces.\nLecture Notes in Mathematics 1200. Springer.\nROSENBAUM, M. and TSYBAKOV, A. (2010). Sparse recovery under matrix uncertainty. The Annals of\nStatistics 38 2620\u20132651.\nROSENBAUM, M. and TSYBAKOV, A. (2013). Improved matrix uncertainty selector. IMS Collections 9\n276\u2013290.\nRUDELSON, M. and VERSHYNIN, R. (2013). Hanson-Wright inequality and sub-gaussian concentration.\nElectronic Communications in Probability 18 1\u20139.\nRUDELSON, M. and ZHOU, S. (2013). Reconstruction from anisotropic random measurements. IEEE\nTransactions on Information Theory 59 3434\u20133447.\nRUFF, D. and COHEN, M. (2014). Attention can either increase or decrease spike count correlations in\nvisual cortex. Nature Neuroscience 17 1591\u20137.\nS\u00d8RESEN, \u00d8., FRIGENSSI, A. and THORESEN, M. (2014a). Covariate selection in high-dimensional gen-\neralized linear models with measurement error. ArXiv:1407.1070.\nS\u00d8RESEN, \u00d8., FRIGENSSI, A. and THORESEN, M. (2014b). Measurement error in Lasso: Impact and\nlikelihood bias correction. Statistical Sinica Preprint .\nST \u00a8ADLER, N., STEKHOVEN, D. J. and B \u00a8UHLMANN, P. (2014). Pattern alternating maximization algorithm\nfor missing data in high-dimensional problems. Journal of Machine Learning Research 15 1903\u20131928.\nSTEFANSKI, L. A. (1985). The effects of measurement error on parameter estimation. Biometrika 72\n583\u2013592.\nSTEFANSKI, L. A. (1990). Rates of convergence of some estimators in a class of deconvolution problems.\nStatistics and Probability Letters 9 229\u2013235.\nSTEFANSKI, L. A. and COOK, J. R. (1995). Simulation-extrapolation: The measurement error jackknife.\nJournal of the American Statistical Association 90 1247\u20131256.\nSTRIMMER, K. (2003). Modeling gene expression measurement error: a quasi-likelihood approach. BMC\nBioinformatics 4.\nTIBSHIRANI, R. (1996). Regression shrinkage and selection via the Lasso. J. Roy. Statist. Soc. Ser. B 58\n267\u2013288.\nTROPP, J. and GILBERT, A. (2007). Signal recovery from random measurements via orthogonal matching\npur- suit. IEEE Trans. Inform. Theory 53 4655\u20134666.\n60\nTROPP, J. A. (2004). Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Inform.\nTheory 50 2231\u20132241.\nWERNER, K., JANSSON, M. and STOICA, P. (2008). On estimation of covariance matrices with kronecker\nproduct structure. IEEE Transactions on Signal Processing 56 478 \u2013 491.\nXU, Q. and YOU, J. (2007). Covariate selection for linear errors-in-variables regression models. Commu-\nnications in Statistics \u2013 Theory and Methods 36.\nYU, K., LAFFERTY, J., ZHU, S. and GONG, Y. (2009). Large-scale collaborative prediction using a non-\nparametric random effects model. Proceedings of the 26th International Conference on Machine Learning\n.\nZHOU, S. (2014). Gemini: Graph estimation with matrix variate normal instances. Annals of Statistics 42\n532\u2013562.\nZHOU, S., LAFFERTY, J. and WASSERMAN, L. (2010). Time varying undirected graphs. Machine Learning\n80 295\u2013319.\n61\n"}