{"text": "Dropout Training is Distributionally Robust Optimal\nDropout Training is Distributionally Robust Optimal\nJos\u00b4e Blanchet\njose.blanchet@stanford.edu\nDepartment of Management Science and Engineering\nStanford University\nStanford, CA 94305, USA\nYang Kang\nyangkang@stat.columbia.edu\nDepartment of Statistics\nColumbia University\nNew York, NY 10027, USA\nJos\u00b4e Luis Montiel Olea\nmontiel.olea@gmail.com\nDepartment of Economics\nColumbia University\nNew York, NY 10027, USA\nViet Anh Nguyen\nviet-anh.nguyen@stanford.edu\nDepartment of Management Science and Engineering\nStanford University\nStanford, CA 94305, USA\nXuhui Zhang\nxuhui.zhang@stanford.edu\nDepartment of Management Science and Engineering\nStanford University\nStanford, CA 94305, USA\nEditor:\nAbstract\nThis paper shows that dropout training in Generalized Linear Models is the minimax so-\nlution of a two-player, zero-sum game where an adversarial nature corrupts a statistician\u2019s\ncovariates using a multiplicative nonparametric errors-in-variables model. In this game,\nnature\u2019s least favorable distribution is dropout noise, where nature independently deletes\nentries of the covariate vector with some \ufb01xed probability \u03b4.\nThis result implies that\ndropout training indeed provides out-of-sample expected loss guarantees for distributions\nthat arise from multiplicative perturbations of in-sample data. In addition to the decision-\ntheoretic analysis, the paper makes two more contributions.\nFirst, there is a concrete\nrecommendation on how to select the tuning parameter \u03b4 to guarantee that, as the sample\nsize grows large, the in-sample loss after dropout training exceeds the true population loss\nwith some pre-speci\ufb01ed probability.\nSecond, the paper provides a novel, parallelizable,\nUnbiased Multi-Level Monte Carlo algorithm to speed-up the implementation of dropout\ntraining. Our algorithm has a much smaller computational cost compared to the naive\nimplementation of dropout, provided the number of data points is much smaller than the\ndimension of the covariate vector.\nKeywords:\nGeneralized Linear Models, Distributionally Robust Optimization, Machine\nLearning, Minimax Theorem, Multi-Level Monte Carlo.\n1\narXiv:2009.06111v2  [stat.ML]  14 Apr 2021\nBlanchet et al.\n1. Introduction\nDropout training is an increasingly popular estimation method in machine learning.1 The\ngeneral idea consists in ignoring some dimensions of the covariate vector at random while\nestimating the parameters of a statistical model. A common motivation for dropout train-\ning is that the random feature selection implicitly performs model averaging, potentially\nimproving out-of-sample prediction error and thus mitigating over\ufb01tting. See Hinton et al.\n(2012) for a discussion about this point in the context of neural networks. See also Draper\n(1994) and Raftery et al. (1997) for classical results on the optimality of model averaging\nfor prediction purposes.\nOur main goal is to contribute to the growing literature explaining the success of dropout\ntraining in mitigating over\ufb01tting; e.g., Wager et al. (2013), Helmbold and Long (2015),\nWei et al. (2020). Our main result (Theorem 2) shows that dropping out input features\nwhen training Generalized Linear Models can be viewed as the minimax solution to an\nadversarial game known in the stochastic optimization literature (Shapiro et al. (2014)) as\na Distributionally Robust Optimization (DRO) problem.\nBroadly speaking, a DRO problem is a two-player, zero-sum game between a decision\nmaker (a statistician) and an adversary (nature).\nThe statistician wishes to choose an\naction to minimize a given expected loss (e.g., squared loss in a typical linear regression\nsetting or, more generally, the negative of the log-likelihood function), while nature intends\nthis loss to be maximal. We consider a framework in which nature is allowed to harm the\nstatistician by corrupting the available data using a multiplicative nonparametric errors-in-\nvariables model; as in the classical work of Hwang (1986). The statistician is aware of the\ndata corruption and knows the distribution used by nature, but does not have access to\nthe realizations of the corruption noise. Under mild assumptions, nature\u2019s least favorable\ndistribution in this game is shown to be dropout noise, where nature independently deletes\nentries of the covariate vector with some \ufb01xed probability \u03b4. The Minimax Theorem (Mor-\ngenstern and von Neumann, 1953) is shown to also hold for this game: the minimax value\ncoincides with the maximin solution, and these coincide with the payo\ufb00s in the game\u2019s Nash\nequilibrium. One direct consequence is that the statistician\u2019s selected procedure in the face\nof multiplicative nonparametric noise maintains optimal performance even if the adversary\nis allowed to corrupt after the statistician uses the training data.\nOur main result (Theorem 2) shows that, by construction, dropout training indeed pro-\nvides out-of-sample performance guarantees for distributions that arise from multiplicative\nperturbations of in-sample data. More precisely, given any \ufb01xed sample size, the out-of-\nsample expected loss is no larger than that obtained by dropout training in-sample, provided\n1. Section 7.12 of Goodfellow et al. (2016) provides a textbook treatment on dropout training. Bishop\n(1995) and Srivastava et al. (2014) are seminal references on this topic.\n2\nDropout Training is Distributionally Robust Optimal\nwe consider out-of-sample distributions obtained as multiplicative perturbations of the in-\nsample distribution. Therefore, our result formally quali\ufb01es the ability of dropout training\nto enhance out-of-sample performance, which is one of the reasons often invoked to use the\ndropout method. Moreover, our results show that for any parameter value the loss used in\ndropout training is larger than the negative log-likelihood of Generalized Linear Models.\nWe make two additional contributions. First, we suggest a novel procedure to select\nthe dropout probability \u03b4. To this end, we study how often the in-sample loss of dropout\ntraining exceeds the true (and unknown) population expected loss. When \u03b4 = 0, the Central\nLimit Theorem implies this event occurs with approximately .5 probability. When \u03b4 is \ufb01xed,\nTheorem 2 implies this event occurs with probability 1. We show that picking \u03b4 to be of\nthe form c/\u221an (where n denotes the number of training examples) makes the in-sample\nloss of dropout training exceed the population loss with probability that depends on c.\nConsequently, by choosing a target probability, say 95%, it is possible to provide a concrete\nrecommendation for the selection of c, and therefore, of \u03b4.\nSecond, we suggest a new stochastic optimization implementation of dropout training.\nA well-known drawback of dropout is its computational cost.\nAs we will explain, a d-\ndimensional covariate vector requires 2d evaluations of the loss in order to integrate out\nthe dropout noise for a particular data point.\nThe computational cost is alleviated by\nimplementing dropout training by using either Stochastic Gradient Descent (Robbins and\nMonro (1951)) or naive Monte-Carlo approximations to the expected loss, both of which\nrequire draws from the joint distribution of the data and dropout noise. Unfortunately,\nboth of these approximations introduce bias to the solution of dropout training. Also, none\nof these procedures can exploit the increasing availability of parallel computing in order to\nalleviate their computational burden. We borrow ideas from the Multi-level Monte Carlo\nliterature\u2014in particular from the work of Blanchet et al. (2019a)\u2014to suggest an unbiased (in\na sense we will make precise) dropout training routine that is easily parallelizable and that\nhas a much smaller computational cost compared to naive dropout training methods when\nthe number of features is large (Theorem 4). Our algorithm thus complements the recent\nliterature suggesting approaches to speed-up dropout training by either using a parallelized\nimplementation of Stochastic Gradient Descent (Zinkevich et al., 2010) or a fast dropout\ntraining based on Gaussian approximations (Wang and Manning, 2013).\nThe rest of the paper is organized as follows. Section 2 explains dropout training in the\ncontext of Generalized Linear Models. Section 3 presents a general description of the DRO\nframework used in this paper. Section 4 specializes the DRO problem by using the negative\nlog-likelihood of Generalized Linear Models to de\ufb01ne a loss function for the statistician, and\nby allowing nature to harm the statistician via a multiplicative errors-in-variables model\nfor the covariates. This section also presents our main theorem. Section 5 presents our\n3\nBlanchet et al.\napproach to select the dropout probability, \u03b4. Section 6 discusses di\ufb00erent computational\nmethods available for implementing dropout training (full integration, Stochastic Gradient\nDescent, Naive Monte Carlo integration) and presents our suggested Unbiased Multi-level\nMonte Carlo algorithm. Section 7 presents some simulations comparing our recommended\nselection of \u03b4 to cross-validation, as well as our preferred implementation of dropout training\nto Stochastic Gradient Descent. Finally, Section 8 discusses extensions of our results to a\nparticular class of feed-forward neural networks with a single hidden layer. We show that\ndropout training of the hidden units in the hidden layer is distributionally robust optimal.\nAll the proofs are collected in the Appendix.\n2. Dropout Training in Generalized Linear Models\nThis section describes dropout training in the context of Generalized Linear Models. As\nsome other recent papers in the literature, we view Generalized Linear Models as a conve-\nnient, transparent, and relevant framework to better understand the theoretical and algo-\nrithmic properties of dropout training.\n2.1 Generalized Linear Models (GLMs)\nA Generalized Linear Model\u2014with parameters \u03b2 and \u03c6\u2014is de\ufb01ned by a conditional density\nfor the response variable Y \u2208Y \u2286R given X \u2208Rd\nf(Y |X, \u03b2, \u03c6) \u2261h(Y, \u03c6) exp\n\u0010\u0010\nY \u03b2\u22a4X \u2212\u03a8(\u03b2\u22a4X)\n\u0011\n/a(\u03c6)\n\u0011\n,\n(1)\nsee McCullagh and Nelder (1989, Equation 2.4). In our notation h(\u00b7, \u03c6) is a real-valued\nfunction (integrable with respect to the true data distribution), parameterized by \u03c6 de\ufb01ned\non the domain Y; a(\u00b7) is a positive function of \u03c6; and \u03a8(\u00b7) is the log-partition function,\nwhich we assume to be de\ufb01ned on all the real line. It is well-known that in GLMs with\na scalar response variable the log-partition function is in\ufb01nitely di\ufb00erentiable and strictly\nconvex on its domain; see Proposition 3.1 in Wainwright and Jordan (2008).\nNormal, Logistic, and Poisson Regression have conditional densities of the form (1). For\nthe sake of exposition, we provide details below for linear and logistic regression.\nExample 1 (Linear regression with unknown variance) Consider the linear model Y =\n\u03b2\u22a4X + \u03b5, in which \u03b5 \u223cN(0, \u03c32) with unknown variance \u03c32 \u2208R++ and \u03b5\u22a5X. The condi-\ntional distribution of Y given X satis\ufb01es (1) with \u03c6 = \u03c32, a(\u03c6) = \u03c6, \u03a8(\u03b2\u22a4X) = (\u03b2\u22a4X)2/2\nand h(Y, \u03c6) = (2\u03c0\u03c6)\u22121\n2 exp(\u2212Y 2/(2\u03c6)).\n4\nDropout Training is Distributionally Robust Optimal\nExample 2 (Logistic regression) Consider Y |X \u223cBernoulli(1/(1 + exp(\u2212\u03b2\u22a4X)) with\nY = {0, 1}.\nThe conditional probability mass function of Y given X satis\ufb01es (1) with\na(\u03c6) = 1, \u03a8(\u03b2\u22a4X) = log(1 + exp(\u03b2\u22a4X)) and h(Y, \u03c6) = 1.\nGeneralized Linear Models are typically estimated via Maximum Likelihood using (1).\nGiven n i.i.d. data realizations or training examples (xi, yi), the Maximum Likelihood esti-\nmator (b\u03b2ML, b\u03c6ML) is de\ufb01ned as any solution of the problem\nmin\n\u03b2,\u03c6\nn\nX\ni=1\n\u2212ln f(yi|xi, \u03b2, \u03c6).\n(2)\nMaximum Likelihood estimators in GLMs are known to be consistent and asymptotically\nnormal under mild regularity conditions on the joint distribution of (Xi, Yi) (Fahrmeir and\nKaufmann, 1985). We denote this distribution as P \u22c6.\n2.2 Dropout Training\nAn alternative to standard Maximum Likelihood estimation in GLMs is dropout training.\nThe general idea consists in ignoring some randomly chosen dimensions of xi while training\na statistical model.\nFor a given covariate vector xi\u2014and an user-selected constant, \u03b4 \u2208[0, 1)\u2014de\ufb01ne the\nd-dimensional random vector\n\u03bei = (\u03bei,1, . . . , \u03bei,d)\u22a4\u2208{0, 1/(1 \u2212\u03b4)}d,\nwhere each of the d entries of \u03bei is an independent draw from a scaled Bernoulli distribution\nwith parameter 1 \u2212\u03b4. This is, for j = 1, . . . , d:\n\u03bei,j =\n\uf8f1\n\uf8f2\n\uf8f3\n0\nwith probability \u03b4,\n(1 \u2212\u03b4)\u22121\nwith probability (1 \u2212\u03b4).\n(3)\nNote that when \u03b4 = 0, the distribution of \u03bei,j collapses to \u03bei,j = 1 with probability 1. Let\n\u2299denote the binary operator de\ufb01ning element-wise multiplication between two vectors of\nthe same dimension. Consider the covariate vector\nxi \u2299\u03bei \u2261(xi,1\u03bei,1, . . . , xi,d\u03bei,d)\u22a4.\n(4)\nSome entries of the new covariate vector are 0 (those for which \u03bei,j = 0) and the rest are\nequal to xi,j/(1 \u2212\u03b4).\n5\nBlanchet et al.\nIn a slight abuse of notation, let E\u03b4 denote the distribution of the random vector \u03bei,\nwhose distribution is parameterized by \u03b4. The estimators of (\u03b2, \u03c6) obtained by dropout\ntraining correspond to any parameters (b\u03b2(\u03b4), b\u03c6(\u03b4)) that solve the problem\nmin\n\u03b2,\u03c6\n1\nn\nn\nX\ni=1\nE\u03b4 [\u2212ln f(yi|xi \u2299\u03bei, \u03b2, \u03c6)] .\n(5)\nOne possibility to solve (5) is to use Stochastic Gradient Descent (Robbins and Monro\n(1951)). This is tantamount to i) taking a draw of (xi, yi) according to its empirical distri-\nbution, ii) independently taking a draw of \u03bei using the distribution in (3) and iii) computing\nthe stochastic gradient descent update using\n\u2207ln f(yi|xi \u2299\u03bei, \u03b2, \u03c6).\nWe provide further details about the Stochastic Gradient Descent implementation of dropout\ntraining in Section 6.\n2.3 Question of Interest\nAdding noise to the Maximum Likelihood objective in (2) seems, at \ufb01rst glance, arbitrary.\nOur \ufb01rst obvious observation is that dropout training estimators will generally not share\nthe same probability limit as the Maximum Likelihood estimators whenever \u03b4 \u0338= 0. This\ncan be formalized under the following assumptions:\nAssumption 1 The log-partition function \u03a8(\u00b7) has a bounded second derivative.\nAssumption 2 The second moment matrix EP \u22c6[XX\u22a4] is \ufb01nite, positive de\ufb01nite.\nProposition 1 (Consistency) Suppose that Assumptions 1 and 2 hold.\nThen for any\nsequence \u03b4n \u2192\u03b4 \u2208[0, 1) as n \u2192\u221e, b\u03b2(\u03b4n) converges in probability to\n\u03b2\u22c6(\u03b4) \u2261arg min\n\u03b2\nEP \u22c6[E\u03b4 [\u2212ln f(Y |X \u2299\u03be, \u03b2, \u03c6)]] ,\n(6)\nwhere the minimizer in (6) is unique and does not depend on \u03c6.\nProof See Appendix A.1.\nThe proof of this result consists of expressing dropout training as an extremum estimator\nand verifying standard conditions for consistency in Newey and McFadden (1994). The main\nmessage of the proposition above is that the parameter \u03b2\u22c6(0)\u2014which gives the probability\n6\nDropout Training is Distributionally Robust Optimal\nlimit of the Maximum Likelihood estimator\u2014generally di\ufb00ers from \u03b2\u22c6(\u03b4) when \u03b4 \u0338= 0,\nwhich is the probability limit of the dropout estimator.2 To further illustrate this point,\nit is helpful to workout the details of the probability limit of the dropout estimator in the\nlinear regression model. Algebra shows that in this model\n\u03b2\u22c6(\u03b4) =\n\u0010\nEP \u22c6[XX\u22a4] + (\u03b4/1 \u2212\u03b4)diag(EP \u22c6[XX\u22a4])\n\u0011\u22121\nEP \u22c6[Y X],\nwhich can be interpreted as a population version of the Ridge estimator. This estimator\ndi\ufb00ers from the best linear predictor of y using x as long as EP \u22c6[Y X] \u0338= 0 and \u03b4 \u0338= 0. This\nestimator di\ufb00ers from Ridge regression in that EP \u22c6[XX\u22a4] replaces the identity matrix (and\nthis simple adjustment makes the estimator scale equivariant).\nDespite the lack of consistency, there is some literature that has provided empirical\nevidence that using intentionally corrupted features for training has the potential to improve\nthe performance of machine learning algorithms; see Maaten et al. (2013). Even if one is\nwilling to accept that corrupting features is desirable for estimation, the choice of dropout\nnoise in (3) remains quite arbitrary.\nThe main contribution of this paper is to provide a novel decision-theoretic interpreta-\ntion of dropout training (in the population and in the sample). We will argue there is a\nnatural two-player, zero-sum game between a decision maker (statistician) and an adversary\n(nature) in which dropout training emerges naturally as a minimax solution. In this game,\ndropout noise turns out to be nature\u2019s least favorable distribution, and dropout training\nbecomes the statistician\u2019s optimal action. The framework we use is known in the stochastic\noptimization literature as Distributionally Robust Optimization and we describe it very\ngenerally in the next section. We will therein also revisit the interpretation of \u03b2\u2217(\u03b4) in the\nlinear regression model.\n3. Problem Setup\nConsider a general problem where there is a multivariate predictor X \u2208Rd and a scalar\noutcome variable Y \u2208R. A Distributionally Robust Optimization (DRO) problem is a\nsimultaneous two-player zero sum game between a decision maker (statistician) and an\n2. Relatedly, Farrell et al. (2020)\u2014who study deep neural networks and their use in semiparametric\ninference\u2014report that their numerical exploration of dropout increased bias and interval length compared\nto nonregularized models.\n7\nBlanchet et al.\nadversary (nature).3\nIn this section we describe the action space for each player, their\nstrategies, and the payo\ufb00function.\nActions and Payoff: The statistician\u2019s action space consists of vectors \u03b8 \u2208\u0398. The\nranking of the statistician\u2019s actions is contingent on the realization of (X, Y ), and this is\ncaptured by a real-valued loss function \u2113(X, Y, \u03b8). We assume that the statistician is called\nto choose an action before observing the realization of (X, Y ). If the statistician knew the\ndistribution of (X, Y )\u2014which we denote by Q\u2014the statistician\u2019s preferred choice of \u03b8 would\nbe the solution to\ninf\n\u03b8\u2208\u0398 EQ [\u2113(X, Y, \u03b8)] .\n(7)\nInstead of assuming that the distribution Q is exogenously determined, we think of\nthe distribution Q as being chosen by nature. Thus, nature\u2019s action space consists of a\nset of probability distributions denoted as U. We refer to this set as the distributionally\nuncertainty set. If nature knew the action selected by the statistician, nature\u2019s preferred\naction would be\nsup\nQ\u2208U\nEQ[\u2113(X, Y, \u03b8)].\n(8)\nStrategies and Solution: The choice of \u03b8 and Q are assumed to happen simulta-\nneously. A statistician\u2019s strategy for this game consists of a choice of \u03b8. Likewise, nature\u2019s\nstrategy for this game consists of a choice of Q.\nA Nash equilibrium for this game is a pair (\u03b8\u22c6, Q\u22c6) such that: a) given Q\u22c6, the parameter\n\u03b8\u22c6solves (7) and b) given \u03b8\u22c6, the distribution Q\u22c6solves (8).\nThe minimax solution for this game is a pair (\u03b8\u2217, Q\u2217) that solves\ninf\n\u03b8\u2208\u0398\nsup\nQ\u2208U\nEQ[\u2113(X, Y, \u03b8)],\n(9a)\nwhile the maximin solution is based on the program\nsup\nQ\u2208U\ninf\n\u03b8\u2208\u0398\nEQ[\u2113(X, Y, \u03b8)].\n(9b)\nIf Q\u22c6solves (9b), we say that Q\u22c6is nature\u2019s least favorable distribution. The mathematical\nprogram in (9a) is typically referred to as a DRO problem.\n3. A seminal reference is the robust inventory control problem of Scarf (1958). Recent references describing\nthe use of distributionally robust stochastic programs (as those considered in this paper) are Delage and\nYe (2010) and Shapiro (2017). Christensen and Connault (2019) used distributionally robust optimization\nto characterize the sensitivity of counterfactual analysis with respect to distributional assumptions in a\nclass of structural econometric models.\n8\nDropout Training is Distributionally Robust Optimal\n4. Dropout Training is Distributionally Robust Optimal\nThe previous section provided a general description of a Distributionally Robust Optimiza-\ntion problem. We specialize the general framework of Section 3 by imposing two restric-\ntions. First, we use the negative log-likelihood of Generalized Linear Models (McCullagh\nand Nelder, 1989) as a loss function for the statistician. Second, we de\ufb01ne nature\u2019s uncer-\ntainty set (i.e., the possible data distributions that nature can take) using the multiplicative\nerrors-in-variables model of Hwang (1986).\n4.1 Statistician\u2019s Payo\ufb00\nWe de\ufb01ne the loss function for the statistician to be the negative of the logarithm of the\nlikelihood in (1), that is,\n\u2113(X, Y, \u03b8) = \u2212ln h(Y, \u03c6) + (\u03a8(\u03b2\u22a4X) \u2212Y (\u03b2\u22a4X))/a(\u03c6),\n(10)\nwhere \u03b8 \u2261(\u03b2\u22a4, \u03c6\u22a4)\u22a4\u2208\u0398. Equation (10) de\ufb01nes the statistician\u2019s objective and its set of\nactions.\n4.2 Nature\u2019s Distributionally Uncertainty Set\nWe now de\ufb01ne the possible distributions that nature can choose. We start out by letting Q0\ndenote some benchmark or reference distribution over (X, Y ). This distribution need not\ncorrespond to that induced by a Generalized Linear Model. In other words, our framework\nallows for the statistician\u2019s model to be misspeci\ufb01ed.\nNext, we de\ufb01ne nature\u2019s action space by considering perturbations of Q0. Although\nthere are di\ufb00erent ways of doing this\u2014for example, by using either f-divergences (such as\nthe Kullback-Leibler as in Nguyen et al. (2020)) or the optimal transport distance (such as\nthe Wasserstein distance as in Blanchet et al. (2019b)) to de\ufb01ne a neighborhood\u2014we herein\nuse a nonparametric multiplicative errors-in-variables model as in Hwang (1986).\nThe idea is to allow nature to independently introduce measurement error to the covari-\nates using multiplicative noise. Multiplicative errors have found di\ufb00erent applications in\nempirical work across di\ufb00erent disciplines, from economics to epidemiology. For example,\nAlan et al. (2009) use it to account for measurement error in consumption data when esti-\nmating the elasticity of intertemporal substitution via Euler equations. Pierce et al. (1992)\nand Lyles and Kupper (1997) use it to relate health outcomes to the exposure of a chemical\ntoxicant that is observed with error. Moreover, due to privacy considerations, statistical\nagencies such as the U.S. Census Bureau sometimes mask data using multiplicative noise;\nsee the discussion in Kim and Winkler (2003) and Nayak et al. (2011). Examples of datasets\nthat contain variables masked with multiplicative noise include the Commodity Flow Survey\n9\nBlanchet et al.\nData (2017), the Survey of Business Owners (2012)\u2014both from the U.S. Census Bureau\u2014\nand the U.S. Energy Information Administration Residential Energy Consumption survey.\nLet \u03be \u2261(\u03be1, . . . , \u03bed)\u22a4be de\ufb01ned as a d-dimensional vector of random variables that are\nindependent of (X, Y ). We perturb the distribution Q0 by considering the transformation\n(X, Y ) 7\u2192(X1\u03be1, . . . , Xd\u03bed, Y )\u22a4.\nAs a result, each covariate Xj is distorted in a multiplicative fashion by \u03bej.\nWe often\nabbreviate (X1\u03be1, . . . , Xd\u03bed)\u22a4by X \u2299\u03be, where \u2299is the element-wise multiplication.\nWe restrict the distribution of \u03be in the following way. First, for a parameter \u03b4 \u2208[0, 1),\nwe de\ufb01ne Qj(\u03b4) to be the set of distributions for \u03bej that are supported on the interval\n[0, 1/(1 \u2212\u03b4)] and that have mean equal to 1. More speci\ufb01cally,\nQj(\u03b4) \u2261\n\b\nQj : Qj is a probability distribution on R, Qj([0, (1 \u2212\u03b4)\u22121]) = 1, EQj[\u03bej] = 1\n\t\n.\n(11)\nThis set of distributions prescribed using support and \ufb01rst-order moment information is\npopular in the DRO literature thanks to its simplicity and tractability (Wiesemann et al.,\n2014). From the perspective of an errors-in-variables model, these distributions are also\nattractive because they preserve the expected value of the covariates, assuming that Xj\nand \u03bej are drawn independently.\nConsider now the joint random vector (X, Y, \u03be) \u2208Rd \u00d7 R \u00d7 Rd. For a constant \u03b4 \u2208[0, 1)\nconsider the joint distributions over (X, Y, \u03be) de\ufb01ned by\nU(Q0, \u03b4) = {Q0 \u2297Q1 \u2297. . . \u2297Qd : Qj \u2208Qj(\u03b4) \u2200j = 1, . . . , d} ,\n(12)\nwhere \u2297is used to denote the product measure (meaning that the joint distribution is the\nproduct of the independent marginals Qj, j = 0, . . . , d). Thus, in the game we consider\nU(Q0, \u03b4) is nature\u2019s action space or nature\u2019s distributionally uncertainty set.\nWe will make only one assumption about the reference distribution Q0 :\nAssumption 3 The distribution Q0 satis\ufb01es EQ[\u2113(X \u2299\u03be, Y, \u03b8)] < \u221efor any Q \u2208U(Q0, \u03b4),\nany \u03b8 \u2208\u0398, and any scalar \u03b4 \u2208[0, 1).\nThis assumption implies a minimal regularity condition to guarantee that the expected\nloss is well-de\ufb01ned for both the statistician and nature. Assumption 3 is trivially satis\ufb01ed\nwhen Q0 is the empirical distribution of the data, which is one of the main cases of interest\nin the paper.\n10\nDropout Training is Distributionally Robust Optimal\n4.3 Dropout Training is DRO\nWe now present the main result of this section.\nTheorem 2 Consider the two-player zero sum game where the statistician has the loss\nfunction in (10) and nature has the action space in (12) for some reference distribution\nQ0 and a scalar \u03b4 \u2208[0, 1). If Assumption 3 is satis\ufb01ed, then the minimax solution of the\ntwo-player zero sum game de\ufb01ned by (10) and (12)\ninf\n\u03b8\u2208\u0398\nsup\nQ\u2208U(Q0,\u03b4)\nEQ [\u2113(X \u2299\u03be, Y, \u03b8)]\n(13)\nis equivalent to\ninf\n\u03b8\u2208\u0398 EQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8)],\n(14)\nwhere Q\u22c6= Q0 \u2297Q\u22c6\n1 \u2297. . . \u2297Q\u22c6\nd, and Q\u22c6\nj = (1 \u2212\u03b4)\u22121 \u00d7 Bernoulli(1 \u2212\u03b4) is a scaled Bernoulli\ndistribution for any j = 1, . . . , d, i.e., under Q\u22c6\nj\n\u03bej =\n\uf8f1\n\uf8f2\n\uf8f3\n0\nwith probability \u03b4,\n(1 \u2212\u03b4)\u22121\nwith probability (1 \u2212\u03b4).\n(15)\nIn addition, let \u03b8\u22c6\u2208\u0398 be a solution to (14). Then (\u03b8\u22c6, Q\u22c6) constitutes a Nash equilibrium\nof the two-player zero sum game de\ufb01ned by (10) and (12) and Q\u22c6is nature\u2019s least favorable\ndistribution.\nProof See Appendix A.2.\nThe \ufb01rst part of theorem characterizes the statistician\u2019s best response to an adversarial\nnature that is allowed to corrupt the covariates using a multiplicative errors-in-variables\nmodel. From the statistician\u2019s perspective, nature\u2019s worst-case perturbation of Q0 is given\nby Q\u22c6in (15). Under this worst-case distribution, nature independently corrupts each of the\nentries of X = (X1, . . . , Xd)\u22a4, by either dropping the j-th component (if \u03bej = 0) or replacing\nit by Xj/(1 \u2212\u03b4). Dropout training\u2014which here refers to estimating the parameter \u03b8 after\nadding dropout noise to X\u2014thus becomes the statistician\u2019s preferred way of estimating the\nparameter \u03b8 when facing an adversarial nature. This gives a decision-theoretic foundation\nfor the use of dropout training.\nNote that in order to recover the objective function introduced in (5) (the sample av-\nerage of the contaminated log-likelihood) it su\ufb03ces to set the reference measure\u2014Q0\u2014as\nthe empirical distribution bPn of {(xi, yi)}n\ni=1, which satis\ufb01es Assumption 3. Likewise, The-\norem 2 allows to interpret the probability limit \u03b2\u22c6(\u03b4) of the dropout estimator derived in\n11\nBlanchet et al.\nProposition 1 as a solution to a DRO problem. In particular\n\u03b2\u22c6(\u03b4) = arg min\n\u03b2\nsup\nQ\u2208U(P \u2217,\u03b4)\nEQ [\u2212ln f(Y |X \u2299\u03be, \u03b2, \u03c6)] ,\nprovided the data-generating distribution P \u2217satis\ufb01es Assumption 3. In the speci\ufb01c case\nof the linear regression model, this means that we can interpret \u03b2\u22c6(\u03b4) as giving us the\npopulation\u2019s best linear predictor for y in terms of x, provided nature is allowed to perturb\nthe distribution of covariates using a multiplicative error-in-variables model.\nMore generally, because dropout noise is nature\u2019s best response for every \u03b2, the dropout\nloss is an upper bound for the true loss:\nEQ\u22c6[\u2212ln f(Y |X \u2299\u03be, \u03b2\u22c6(\u03b4), \u03c6)] \u2265EP \u2217[\u2212ln f(Y |X \u2299\u03be, \u03b2\u22c6(0), \u03c6)] .\nWe provide now some intuition about how dropout noise becomes nature\u2019s worst-case distri-\nbution. Algebra shows that, in light of Assumption 3, the expected loss under an arbitrary\ndistribution Q is \ufb01nite and can be written as\nEQ[\u2113(X \u2299\u03be, Y, \u03b8)] = \u2212EQ0 [ln h(Y, \u03c6)]\n+ EQ0\nh\nEQ1\u2297...\u2297Qd[(\u03a8((\u03b2 \u2299X)\u22a4\u03be) \u2212Y ((\u03b2 \u2299X)\u22a4\u03be))/a(\u03c6)]\ni\n,\nwhere the \ufb01rst expectation is taken with respect to the reference distribution, and the\nsecond one with respect to \u03be. For \ufb01xed values of (X, Y, \u03b8) we can de\ufb01ne\nA(X,Y,\u03b8)((\u03b2 \u2299X)\u22a4\u03be) \u2261(\u03a8((\u03b2 \u2299X)\u22a4\u03be) \u2212Y ((\u03b2 \u2299X)\u22a4\u03be))/a(\u03c6).\nBecause \u03a8(\u00b7) is a convex function de\ufb01ned on all of the real line, the function A(X,Y,\u03b8)(\u00b7)\ninherits these properties. We show in the appendix that for these type of functions\nsup\nn\nEQ1\u2297...\u2297Qd[A(X,Y,\u03b8)((\u03b2 \u2299X)\u22a4\u03be)] : Qj \u2208Qj(\u03b4)\no\n= EQ\u22c6\n1\u2297...\u2297Q\u22c6\nd[A(X,Y,\u03b8)((\u03b2 \u2299X)\u22a4\u03be)],\n(16)\nfor any \u03b8, and this establishes the equivalence between (13) and (14). The proof of the\nequality above exploits convexity. In fact, to derive our result we \ufb01rst characterize the worst-\ncase distribution for the expectation of a real-valued convex function (Lemma 5) and then\nwe generalize this result to functions that depend on \u03be only through linear combinations,\nas A(X,Y,\u03b8)(\u00b7) (Proposition 6).\nHow about the Nash Equilibrium of the two-player zero sum game de\ufb01ned by (10) and\n(12)? The equality in (16) clearly shows that Q\u22c6is nature\u2019s best response for any \u03b8 \u2208\u0398.\nIf there is a vector \u03b8\u22c6that solves the dropout training problem in (14), then this vector is\n12\nDropout Training is Distributionally Robust Optimal\nthe statistician best\u2019s response to nature\u2019s choice of Q\u22c6. Consequently, (\u03b8\u22c6, Q\u22c6) is a Nash\nequilibrium.\nFinally, we discuss the extent to which Q\u22c6can be referred to as nature\u2019s least favorable\ndistribution, which has been de\ufb01ned as nature\u2019s solution to the maximin problem. It is well\nknown that the maximin value of a game is always smaller than its minimax value:4\nsup\nQ\u2208U(Q0,\u03b4)\ninf\n\u03b8\u2208\u0398\nEQ[\u2113(X \u2299\u03be, Y, \u03b8)] \u2264inf\n\u03b8\u2208\u0398\nsup\nQ\u2208U(Q0,\u03b4)\nEQ[\u2113(X \u2299\u03be, Y, \u03b8)].\nWe have shown that the right-hand side of the display above equals (14). Therefore,\nif there is a \u03b8\u22c6\u2208\u0398 that solves such program, then Q\u22c6achieves the upper bound to the\nmaximin value of the game. This makes dropout noise nature\u2019s least favorable distribution.\nNow that we have established that dropout training gives the minimax solution of the\nDRO game, we discuss the implications of this result regarding the out-of-sample perfor-\nmance of dropout training. Suppose Q0 is the empirical measure bPn supported on n training\nsamples {(xi, yi)}n\ni=1. The in-sample loss of dropout training is\n1\nn\nn\nX\ni=1\nEQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8\u22c6)|X = xi, Y = yi].\n(17)\nA typical concern with estimation procedures is whether their performance in a speci\ufb01c\nsample translates to good performance out of sample. In our context, the out-of-sample\nperformance of dropout training can be thought of as the expected loss that would arise\nfor some other data distribution \u02dcQ0 over (X, Y ) at the parameter estimated via dropout\ntraining:\nE\u02dcQ0[\u2113(X, Y, \u03b8\u22c6)].\nThe minimaxity of dropout training shows that for any distribution \u02dcQ0 over (X, Y ) that\ncan be obtained from bPn by perturbing covariates with mean-one independent multiplicative\nerror \u03bej \u2208[0, (1 \u2212\u03b4)\u22121] we have\nE\u02dcQ0[\u2113(X, Y, \u03b8\u22c6)] \u22641\nn\nn\nX\ni=1\nEQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8\u22c6)|X = xi, Y = yi].\n4. This follows from the fact that for any Q \u2208U(Q0, \u03b4) :\ninf\n\u03b8\u2208\u0398 EQ[\u2113(X \u2299\u03be, Y, \u03b8)] \u2264EQ[\u2113(X \u2299\u03be, Y, \u03b8)] \u2264\nsup\nQ\u2208U(Q0,\u03b4)\nEQ[\u2113(X \u2299\u03be, Y, \u03b8)].\nSee also the discussion of the minimax theorem in Ferguson (1967) p. 81.\n13\nBlanchet et al.\nThis means that the out-of-sample loss will be upper-bounded by the in-sample loss. Thus,\nour results give a concrete result about the class of distributions for which dropout training\nestimation \u201cgeneralizes\u201d well.\nFinally, we note that Theorem 2 was stated for a scalar \u03b4 that is homogeneous across\nthe multiplicative noise \u03bej. To model non-identical dropout noise, we can substitute the\nsets in (11) and (12) by Qj(\u03b4j) for a collection of parameters (\u03b41, . . . , \u03b4d) \u2208[0, 1)d. In this\ncase, the results of Theorem 2 hold with Q\u22c6\nj = (1\u2212\u03b4j)\u22121 \u00d7Bernoulli(1\u2212\u03b4j) for j = 1, . . . , d.\n5. Statistical Guidance on Choosing \u03b4\nTheorem 2 in the previous section showed that dropout training is distributionally robust\noptimal and that nature\u2019s least favorable distribution is dropout noise with probability\n\u03b4 \u2208[0, 1). This section suggests a strategy to pick this parameter. Broadly speaking, our\napproach relies on a simple idea: we study how often the in-sample loss obtained from\ndropout training exceeds the population loss. If over\ufb01tting is successfully mitigated, this\nprobability ought to be large. We show that by appropriately tuning the parameter \u03b4 of\nthe dropout noise, it is possible to control the probability of such event as the sample size\ngrows large.\n5.1 Additional Notation\nThroughout this section, we use b\u03c6 to denote an arbitrary \u221an-consistent, asymptotically\nnormal estimator for the scale parameter \u03c6. Such estimator can be obtained, for example,\nby using b\u03c6ML in (2). We use \u03c6\u22c6to denote the true, unknown scale parameter. Just as\nbefore, we let b\u03b2(\u03b4) denote the dropout estimator of the true \u03b2\u22c6under dropout probability\n\u03b4.\nThe in-sample loss of dropout training\u2014given a dropout probability of \u03b4\u2014evaluated at\nparameters \u03b2 and \u03c6 is given by\nLn(\u03b2, \u03c6, \u03b4) \u22611\nn\nn\nX\ni=1\nE\u03b4 [\u2212ln f(yi|xi \u2299\u03bei, \u03b2, \u03c6)] .\n(18)\nThe goal of this section is to understand how likely is that the in-sample loss in (18)\u2014when\nevaluated at the dropout estimator b\u03b2(\u03b4) and some estimator b\u03c6\u2014exceeds the true population\nloss. Thus, if we de\ufb01ne the population loss as\nL(\u03b2\u22c6, \u03c6\u22c6) \u2261EP \u22c6[\u2212ln f(Y |X, \u03b2\u22c6, \u03c6\u22c6)] ,\n(19)\n14\nDropout Training is Distributionally Robust Optimal\nwe are interested in understanding how often\nLn(b\u03b2(\u03b4n), b\u03c6, \u03b4n) \u2265L(\u03b2\u22c6, \u03c6\u22c6),\n(20)\nwhere the sequence \u03b4n is allowed to change with the sample size.\n5.2 Additional Assumptions\nIt is well-known that under mild regularity conditions on the true data generating process\u2014\nand without the need of dropout training\u2014the usual in-sample loss evaluated at the Maxi-\nmum Likelihood estimators, which we can denote as Ln(b\u03b2ML, b\u03c6ML, 0), provides a consistent\nestimator for the population loss. This remains true if the Maximum Likelihood estima-\ntor for \u03c6 is replaced by another \u221an-consistent estimator b\u03c6. One su\ufb03cient condition that\nguarantees such behavior is the following high-level assumption:\nAssumption 4 The following central limit theorem result holds for some \u03c32\n\u221an\n\u0010\nLn(\u03b2\u22c6, b\u03c6, 0) \u2212L(\u03b2\u22c6, \u03c6\u22c6)\n\u0011 d\u2192N(0, \u03c32).\nIn particular, the identity\n\u221an\n\u0010\nLn(b\u03b2ML, b\u03c6, 0) \u2212L(\u03b2\u22c6, \u03c6\u22c6)\n\u0011\n=\u221an\n\u0010\nLn(b\u03b2ML, b\u03c6, 0) \u2212Ln(\u03b2\u22c6, b\u03c6, 0)\n\u0011\n+ \u221an\n\u0010\nLn(\u03b2\u22c6, b\u03c6, 0) \u2212L(\u03b2\u22c6, \u03c6\u22c6)\n\u0011\n,\nand Assumptions 1, 2, 4 imply\n\u221an\n\u0010\nLn(b\u03b2ML, b\u03c6, 0) \u2212L(\u03b2\u22c6, \u03c6\u22c6)\n\u0011 d\u2192N(0, \u03c32).\n(21)\nWe view the result in (21) concerning the in-sample loss of Maximum Likelihood estimators,\nwhich is quite standard, as unsatisfactory. Our main complaint is that the probability of\nthe event\nLn(b\u03b2ML, b\u03c6, 0) \u2264L(\u03b2\u22c6, \u03c6\u22c6)\napproaches 1/2 as the sample size grows large. Our interpretation is that the in-sample\nloss at the Maximum Likelihood estimator is deceivingly small, as the true population loss\nwill be above it 50% of the time if the sample size is large enough. We argue that this\nprobability can be made smaller by appropriately tuning dropout noise.\n15\nBlanchet et al.\n5.3 In-sample loss of dropout training\nTheorem 2 showed that dropout noise is nature\u2019s choice to in\ufb02ict the highest loss for the\nstatistician at any parameter values. Therefore, invoking Theorem 2 using the empirical\ndistribution as the reference distribution, the random variable\n\u00b5n(\u03b2, \u03c6, \u03b4) \u2261Ln(\u03b2, \u03c6, \u03b4) \u2212Ln(\u03b2, \u03c6, 0)\n(22)\nis nonnegative for any \u03b4 \u2208[0, 1).\nSince Proposition 1 has shown that b\u03b2(\u03b4n)\np\u2192\u03b2\u22c6for\nany sequence \u03b4n \u21920, intuition suggests that the in-sample loss of dropout training,\nLn(b\u03b2(\u03b4n), b\u03c6, \u03b4n), may provide a consistent estimate of the population loss that does not\nunderestimate this limit frequently.\nOur result is the following proposition:\nProposition 3 Suppose that Assumptions 1, 2, 4 hold. Then for any sequence \u03b4n = c/\u221an,\n\u221an\n\u0010\nLn(b\u03b2(\u03b4n), b\u03c6, \u03b4n) \u2212L(\u03b2\u22c6, \u03c6\u22c6)\n\u0011 d\u2192N(\u00b5\u221e(\u03b2\u22c6, \u03c6\u22c6, c), \u03c32),\nwhere \u00b5\u221e(\u03b2\u22c6, \u03c6\u22c6, c) \u22650 is the probability limit of\n\u221an\u00b5n\n\u0010\n\u03b2\u22c6, b\u03c6, \u03b4n\n\u0011\n,\nand \u00b5n(\u00b7) is de\ufb01ned as in (22).\nProof See Appendix A.3.\nThe main message of Proposition 3 is that the probability of the event (20) can be\napproximated, as the sample size goes large, by the probability\u2014under a normal random\nvariable with positive mean\u2014of the positive half of the real line. Some elementary algebra\ncan be used to illustrate the main argument behind the proof. Note that\n\u221an\n\u0010\nLn(b\u03b2(\u03b4n), b\u03c6, \u03b4n) \u2212L(\u03b2\u22c6, \u03c6\u22c6)\n\u0011\n= \u221an\n\u0010\nLn(b\u03b2(\u03b4n), b\u03c6, \u03b4n) \u2212Ln(\u03b2\u22c6, b\u03c6, \u03b4n)\n\u0011\n+ \u221an\u00b5n(\u03b2\u22c6, b\u03c6, \u03b4n)\n+ \u221an\n\u0010\nLn(\u03b2\u22c6, b\u03c6, 0) \u2212L(\u03b2\u22c6, b\u03c6)\n\u0011\n.\nWe start by showing that the \ufb01rst term converges in probability to zero. To do this we\nshow that\n\u221an(b\u03b2(c/\u221an) \u2212\u03b2\u22c6)\n16\nDropout Training is Distributionally Robust Optimal\nis asymptotically normal and that the derivative of Ln(\u00b7) with respect to \u03b2 (evaluated at\n\u03b2\u22c6) converges in probability to zero.\nThe key step in the proof shows that the second term has a \ufb01nite probability limit. In\nfact, we can characterize this limit explicitly and show that\n\u221an\u00b5n(\u03b2\u22c6, b\u03c6, \u03b4n)\np\u2192c \u00b7 \u00b5,\nwhere\n\u00b5 \u2261\n\uf8eb\n\uf8ed\n\uf8eb\n\uf8edX\n\u03be\u2208A\nEP \u22c6[\u03a8((X \u2299\u03be)\u22a4\u03b2\u22c6)]\n\uf8f6\n\uf8f8\u2212dEP \u22c6[\u03a8(X\u22a4\u03b2\u22c6)] + EP \u22c6[Y X\u22a4]\u03b2\u22c6\n\uf8f6\n\uf8f8\n.\na(\u03c6\u22c6),\nand A is the collection of all vectors in {0, 1}d for which there is only one zero. In Section\n7 we provide an expression for this term in the linear regression model.\nIt is important to mention that the DRO interpretation of dropout training can be\nleveraged to select the dropout parameter \u03b4. For example, a possible approach consists\nin choosing \u03b4 so that true data generating process belongs to nature\u2019s choice set with\nsome prespeci\ufb01ed probability. This approach, which is often advocated in the literature\nin machine learning and robustness (Hansen and Sargent, 2008), often leads to a very\npessimistic selection of \u03b4 simply because this criterion is not informed at all by the loss\nfunction de\ufb01ning the decision problem. Further, in our problem, it is not possible to apply\nthis approach given that the set of multiplicative perturbations of the empirical distribution\nwill, in general, not cover the true data generating process.\nAnother approach involves using generalization bounds leading to \ufb01nite sample guaran-\ntees; see, for instance a summary of this discussion in Section 6.2 of Rahimian and Mehrotra\n(2019). This method, while appealing, often requires either distributions with compact sup-\nport or strong control on the tails of the underlying distributions. Also, often, the bounds\ndepend on constants that may be too pessimistic or di\ufb03cult to compute.\nFinally, there is a recent method introduced in Blanchet et al. (2019b) for the case\nin which nature\u2019s choice set is de\ufb01ned in terms of the Wasserstein\u2019s distance around the\nempirical distribution.\nThe idea therein is that\u2014for a \ufb01xed \u03b4\u2014every distribution that\nbelongs to nature\u2019s choice set corresponds to an optimal parameter choice for the statistician.\nThus, one can collect each and every of the statistician\u2019s optimal choices associated to each\ndistribution in nature\u2019s uncertainty set, and treat the resulting region as a con\ufb01dence set\nfor the true parameter. This con\ufb01dence set grows bigger (in the sense of nested con\ufb01dence\nregions) as \u03b4 increases. The goal is then to minimize \u03b4 subject to a desired level of coverage\nin the underlying parameter to estimate. This leads to a data-driven choice of \u03b4 that is\nexplicitly linked to the statistician\u2019s decision problem. However, this approach is not feasible\n17\nBlanchet et al.\nin our problem because, once again, regardless of the value of \u03b4, the parameter choices for\neach of the multiplicative perturbations of the empirical distribution will, in general, fail to\ncover the true parameter.\nHence, we advocate the strategy of choosing the parameter \u03b4 to control how often the\nin-sample loss obtained from dropout training exceeds the population loss. The proof of\nProposition 3 shows that \u00b5\u221e(\u03b2\u22c6, \u03c6\u22c6, c) is of the form c \u00b7 \u00b5, where \u00b5 depends on (\u03b2\u22c6, \u03c6\u22c6).\nConsequently, as long as \u00b5 > 0, it is straightforward to pick c to guarantee a pre-speci\ufb01ed\n\u201ccoverage\u201d of the population loss: for any \u03b1 \u2208(0, 1), if we pick c to be\nz1\u2212\u03b1 \u00b7 \u03c3/\u00b5,\nwhere z1\u2212\u03b1 is the 1-\u03b1 quantile of a standard normal, then the probability of the event (20)\nasymptotically approaches 1 \u2212\u03b1. Thus, over\ufb01tting can be successfully mitigated.\n6. An Algorithm for Dropout Training\nThe goal of this section is to suggest an algorithm for solving the dropout training problem\ninf\n\u03b8\u2208\u0398 EQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8)],\nwhere Q\u22c6= bPn\u2297Q\u22c6\n1\u2297. . .\u2297Q\u22c6\nd and Q\u22c6\nj, j = 1, . . . , d is the dropout noise distribution de\ufb01ned\nin (15). Notice that we here consider the speci\ufb01c case in which Q0 is set to the empirical\nmeasure bPn supported on n training samples {(xi, yi)}n\ni=1. We will use \u03b8\u22c6\nn to denote the\nsolution of the dropout training problem above. It will sometimes be convenient to rewrite\nthis dropout training problem as\nmin\n\u03b8\n1\nn\nn\nX\ni=1\nEQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8) | X = xi, Y = yi],\n(23)\nwhich coincides with expression (5). Conditioning on the values of (xi, yi) makes it clear that\nthe expectation is computed over the d-dimensional vector \u03be. We now brie\ufb02y describe three\ncommon approaches to implement dropout training and we discuss some of its limitations.\n6.1 Naive Dropout Training\nBecause Q\u22c6\nj places mass on only two points, namely 0 and (1 \u2212\u03b4)\u22121, the support of the\njoint distribution Q\u22c6\n1 \u2297. . . \u2297Q\u22c6\nd has cardinality 2d. Thus, a naive approach to solve the\ndropout training problem (23) is to expand the objective function as a sum with n \u00b7 2d\nterms, then to apply a tailored gradient descent algorithm to the resulting optimization\nproblem. Unfortunately, this approach is computationally demanding because the number\n18\nDropout Training is Distributionally Robust Optimal\nof individual terms in the objective function grows exponentially with the dimension d of\nthe features.\n6.2 Dropout Training via Stochastic Gradient Descent\nAnother method to solve the dropout training problem in (14) is by stochastic gradient\ndescent (henceforth, SGD). This gives us the commonly used dropout training algorithm.\nFor the sake of comparison, we provide concrete details about this algorithm below.\nGiven a current estimate b\u03b8, we compute an unbiased estimate of the gradient to the\nobjective function of (14), and move in the direction of the negative gradient with a suitable\nstep size. Since Q\u22c6is discrete, the expectation under Q\u22c6can be written as a \ufb01nite sum and\nby di\ufb00erentiating under the expectation, we have\n\u2207\u03b8EQ\u22c6[\u2113(X \u2299\u03be, Y, b\u03b8)] = EQ\u22c6\nh\n\u2207\u03b8\u2113(X \u2299\u03be, Y, b\u03b8)\ni\n.\n(24)\nThe standard SGD algorithm uses a naive Monte Carlo estimator as an estimate of the\ngradient (24), that is, at iterate k \u2208N with incumbent solution b\u03b8k,\n\u2207\u03b8EQ\u22c6[\u2113(X \u2299\u03be, Y, b\u03b8k)] \u2248\u2207\u03b8\u2113(xk \u2299\u03bek, yk, b\u03b8k),\nwhere (xk, yk, \u03bek) is an independent draw from Q\u22c6.\nOne drawback of using SGD to solve (14) is that it is not easily parallelizable, and thus\nits implementation can be quite slow. Moreover, under strong convexity assumption of the\nloss function \u2113, SGD only exhibits linear convergence rate (Nemirovski et al., 2009, Section\n2.1). By contrast, the gradient descent (GD) enjoys exponential convergence rate (Boyd\nand Vandenberghe, 2004, Section 9.3.1).\n6.3 Naive Monte Carlo Approximation for Dropout Training\nConsider solving the dropout training problem in (23) using a naive Monte Carlo approxi-\nmation. Instead of using 2d terms to compute\nEQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8) | X = xi, Y = yi],\nwe approximate this expectation by taking a large number of K i.i.d. draws {\u03bek\ni }K\nk=1,\n\u03bek\ni \u2208Rd, according to the distribution Q\u2217\n1 \u2297. . . \u2297Q\u2217\nd. When d is large this approximation\nis computationally cheaper than the naive dropout training procedure described above,\nprovided that K \u226a2d.\n19\nBlanchet et al.\nThus, the naive Monte Carlo approximation of the dropout training problem is\nmin\n\u03b8\u2208\u0398\n1\nn\nn\nX\ni=1\n\"\n1\nK\nK\nX\nk=1\n\u2113(xi \u2299\u03bek\ni , yi, \u03b8)\n#\n,\n(25)\nwhere the random vectors \u03bek\ni are sampled independently\u2014over both k and i\u2014using the\ndistribution Q\u22c6\n1 \u2297. . . \u2297Q\u22c6\nd.\nRelative to the solution of the dropout training problem\u2014which we denoted as \u03b8\u22c6\nn\u2014the\nminimizer of (25) is consistent and asymptotically normal as K \u2192\u221e. This follows by\nstandard arguments; for example, those in Shapiro et al. (2014, Section 5.1). There are,\nhowever, two problems that arise when using (25) as a surrogate for the dropout training\nproblem. First, the solution to (25) is a biased estimator for \u03b8\u22c6\nn. This means that if we\naverage the solution of (25) over the K \u00b7 n di\ufb00erent values of \u03bek\ni , the average solution need\nnot equal \u03b8\u22c6\nn. Second, implementing (25) requires a choice of K and, to the best of our\nknowledge, there is no o\ufb00-the-shelf procedure for picking this number.\n6.4 Unbiased Multi-level Monte Carlo Approximation for Dropout Training\nTo address these two issues, we apply the recent techniques suggested in Blanchet et al.\n(2019a) that we refer to as Unbiased Multi-level Monte Carlo Approximations. Multi-level\nMonte Carlo methods (Giles, 2008, 2015) refer to a set of techniques for approximating\nthe expectation of random variables. The adjective \u201cmulti-level\u201d emphasizes the fact that\nrandom samples of di\ufb00erent levels of accuracy are used in the approximation. Before pre-\nsenting the detailed algorithm, we provide a heuristic description. To this end, let b\u03b8\u22c6\nn(K)\ndenote the level K solution of the problem in (25); that is, the solution based on K draws.\nDe\ufb01ne the random variable\n\u2206K \u2261b\u03b8\u22c6\nn(K) \u2212b\u03b8\u22c6\nn(K \u22121).\nand, for simplicity, assume b\u03b8\u22c6\nn(0) is de\ufb01ned to equal a vector of zeros.\nUnder suitable\nregularity conditions, there holds\n\u221e\nX\nK=1\nE[\u2206K] = lim\nK\u2192\u221eE[b\u03b8\u22c6\nn(K)] = \u03b8\u22c6\nn.\nConsider now picking K\u2217at random from some discrete distribution supported on the\nnatural numbers. Let p(\u00b7) denote the probability mass function of such distribution and\nconsider a Monte Carlo approximation scheme in which\u2014after drawing K\u2217\u2014we sample\n20\nDropout Training is Distributionally Robust Optimal\nK\u2217\u00b7 n di\ufb00erent random vectors \u03bek\ni \u2208Rd according to Q\u22c6\n1 \u2297. . . \u2297Q\u22c6\nd. The estimator\nZ(K\u2217) \u2261\u2206K\u2217\np(K\u2217)\nhas two sources of randomness. Firstly, the random choice of K\u2217and, secondly, the random\ndraws \u03bek\ni . Averaging over both yields\nE[Z(K\u2217)] =\n\u221e\nX\nK=1\nE[Z(K\u2217)|K\u2217= K] \u00b7 p(K) =\n\u221e\nX\nK=1\n(E[\u2206K]/p(K)) \u00b7 p(K) = \u03b8\u22c6\nn.\nThus, by taking into account the randomness in the selection of K, we have managed to\nprovide a rule for deciding the number of draws (speci\ufb01cally, our recommendation is to\npick K\u2217at random) and at the same time we have removed the bias of naive Monte Carlo\napproximations.\nOne possible concern with our suggested implementation is that the expected computa-\ntional cost of Z(K\u2217) could be in\ufb01nitely large. Fortunately, this issue can be easily resolved\nby an appropriate choice of the distribution p(\u00b7). To see this, de\ufb01ne the computational cost\nsimply as the number of random draws that are required to obtain Z(K\u2217). In the construc-\ntion we have described above, we need K\u2217\u00b7 n draws for the construction of the estimator.\nThus, the average cost is\nE[K\u2217\u00b7 n] = n\n\u221e\nX\nK=1\nK \u00b7 p(K)\nwhich, under mild integrability conditions on p(\u00b7), will be \ufb01nite.5\nWe now present the algorithm that will be used to solve the dropout training problem.\nTo ensure that the estimator Z(K\u2217) has a \ufb01nite variance, instead of de\ufb01ning \u2206K as the\ndi\ufb00erence between the level K and K \u22121 solutions to problem (25) in the above heuristic\narguments, we use solutions to problem (25) with a sample of size 2K+1 and with its odd\nand even sub-samples of size 2K.\nAlgorithm for the Unbiased Multilevel Monte Carlo: We present a paral-\nlelized version of it using L processors, but the suggested algorithm works even when L = 1.\nParallel computing reduces the variance of the estimator, and our suggestion is to use as\nmany processors as available in one run.\nFix an integer m0 \u2208N such that 2m0+1 \u226a2d.\nFor each processor l = 1, . . . , L we\nconsider the following steps.\n5. For example, if p(\u00b7) is selected as a geometric distribution with parameter r, the expected computational\ncost will be n(1 \u2212r)/r.\n21\nBlanchet et al.\ni) Take a random (integer) draw, m\u2217\nl , from a geometric distribution with parameter\nr > 1/2.6\nii) Given m\u2217\nl , take 2K\u2217\nl +1 i.i.d. draws from the d-dimensional vector \u03bei \u223cQ\u22c6\n1 \u2297. . . \u2297Q\u22c6\nd,\nwhere\nK\u2217\nl \u2261m0 + m\u2217\nl .\nRepeat this step independently for each i = 1, . . . , n.\niii) Solve problem (25) using the \ufb01rst 2m0 i.i.d. draws of \u03bei for each i. Let \u03b8l,m0 denote a\nminimizer.\niv) Denote by b\u03b8\u22c6\nn(2K\u2217\nl +1), b\u03b8O\nn (2K\u2217\nl ), and b\u03b8E\nn (2K\u2217\nl ) any solution to the following optimization\nproblems (all of which are based on sample average approximations as (25)):\nb\u03b8\u22c6\nn(2K\u2217\nl +1) \u2208arg min\n\u03b8\u2208\u0398\n1\nn\nn\nX\ni=1\n\uf8eb\n\uf8ed\n1\n2K\u2217\nl +1\n2K\u2217\nl +1\nX\nk=1\n\u2113(xi \u2299\u03bek\ni , yi, \u03b8)\n\uf8f6\n\uf8f8,\nb\u03b8O\nn (2K\u2217\nl ) \u2208arg min\n\u03b8\u2208\u0398\n1\nn\nn\nX\ni=1\n\uf8eb\n\uf8ed1\n2K\u2217\nl\n2K\u2217\nl\nX\nk=1\n\u2113(xi \u2299\u03be2k\u22121\ni\n, yi, \u03b8)\n\uf8f6\n\uf8f8,\nb\u03b8E\nn (2K\u2217\nl ) \u2208arg min\n\u03b8\u2208\u0398\n1\nn\nn\nX\ni=1\n\uf8eb\n\uf8ed1\n2K\u2217\nl\n2K\u2217\nl\nX\nk=1\n\u2113(xi \u2299\u03be2k\ni , yi, \u03b8)\n\uf8f6\n\uf8f8.\nIntuitively, b\u03b8O\nn and b\u03b8E\nn denote the solutions to problem (25) but using a sample of size\n2Kl with only odd and even indices, respectively.\nv) De\ufb01ne\n\u00af\u2206K\u2217\nl \u2261b\u03b8\u22c6\nn(2K\u2217\nl +1) \u22121\n2(b\u03b8O\nn (2K\u2217\nl ) + b\u03b8E\nn (2K\u2217\nl ))\nand let\nZ(K\u2217\nl ) =\n\u00af\u2206K\u2217\nl\nr(1 \u2212r)K\u2217\nl \u2212m0 + \u03b8l,m0.\n6. To see why we require that r > 1/2, notice if the computational cost of evaluating Z(K\u2217) (as in\nthe heuristic description above) increases exponentially in K and takes the form C \u00b7 2K, the expected\ncomputational cost will be\n\u221e\nX\nK=1\nCr(2(1 \u2212r))K = Cr(1/2(1 \u2212r)),\nprovided 2(1 \u2212r) < 1, or equivalently, r > 1/2. As we show in the proof of Theorem 4, constraining the\nvariance requires then imposing r < 3/4. Ultimately, optimizing the product of computational cost and\nvariance leads to the optimal selection r = 1 \u22122\u22123/2.\n22\nDropout Training is Distributionally Robust Optimal\nOur recommended estimator is\n1\nL\nL\nX\nl=1\nZ(K\u2217\nl ).\nWe now show that the suggested algorithm gives an estimator with desirable properties.\nWe do so under the following regularity assumptions.\nAssumption 5 Suppose that the parameter space \u0398 is compact. Suppose in addition that\nthe optimal solution \u03b8\u22c6\nn to the dropout training problem in (23) is (globally) unique.\nAssumption 6 Let b\u03b8\u22c6\nn(K) denote the solution of the problem in (25) based on K draws.\nSuppose that as K \u2192\u221e,\nE[\u2225K\n1\n2 (b\u03b8\u22c6\nn(K) \u2212\u03b8\u22c6\nn)\u22254\n2) = O(1),\nwhere the expectation is taken over the i.i.d dropout noise distribution used to generate \u03bek\ni .\nAssumption 7 Assume that for each (X, Y, \u03be), \u2113(X \u2299\u03be, Y, \u00b7) is thrice continuously di\ufb00er-\nentiable over \u0398 and that\n\u2207\u03b8\u03b8EQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8\u22c6\nn)]\nis non-singular.\nTheorem 4 Under Assumption 5, E[Z(K\u2217\nl )] = \u03b8\u22c6\nn. The number of random draws required\nto compute Z(K\u2217\nl ) is n\u00b72K\u2217\nl +1 and thus the expected computational complexity for producing\nZ(K\u2217\nl ) equals\nn(2m0+1)r\n2r \u22121\n< n(2m0+1) \u226an2d.\nSuppose, in addition, that b\u03b8\u22c6\nn(K) is almost surely in the interior of \u0398 for K large enough.\nIf Assumptions 6 and 7 hold, and r < 3/4. Then Var(Z(K\u2217\nl )) < \u221e.\nProof See Appendix A.4.\nOur suggested algorithm has \ufb01nite expected computational complexity that does not\ngrow exponentially with the dimension d, thus every time we need to obtain b\u03b8\u22c6\nn(2K\u22c6\nl +1),\nwe can do so by applying a gradient descent algorithm. Combined with parallelization,\nthe Unbiased Multi-level Monte Carlo approach produces an unbiased estimator with a\nvariance that can be made arbitrarily small if L is large enough, provided that the regularity\nassumptions that give Var(Z(K\u2217\nl )) < \u221eare satis\ufb01ed.\n23\nBlanchet et al.\n7. Numerical Experiment\nWe conduct numerical experiments in this section to compare our preferred implementation\nof dropout training to Stochastic Gradient Descent, as well as our recommended selection\nof \u03b4 to cross-validation. The bene\ufb01ts of our suggested Unbiased Multi-Level Monte Carlo\nalgorithm are analyzed using a high-dimensional regression, whereas our selection of \u03b4 is\nanalyzed using a low-dimensional regression model.\n7.1 Advantage of the Unbiased Multi-level Monte Carlo Estimator\nWe present a simple numerical experiment to illustrate the advantage of using the Unbiased\nMulti-level Monte Carlo estimator suggested in Section 6.4. We consider the linear regres-\nsion problem with known variance and we focus on solving the dropout training problem\nwith our recommended \u03b4 chosen according to Proposition 3.\nOur simulation setting considers a linear regression model with covariate vector having\ndimension d = 100 and sample size n = 50. We pick a known regression coe\ufb03cient \u03b20 \u2208Rd\nbeing a vector with all entries equal to 1. With \ufb01xed coe\ufb03cients, we assume the covariate\nvector follows independent Gaussian, as well as for the regression noise. More speci\ufb01cally,\nwe can get our n = 50 observations (xi, yi) via\n\u2022 sampling xi \u223cN(0, Id), i = 1, . . . , n,\n\u2022 sampling yi \u2208R conditional on xi, where yi is given by the linear assumption and \u03b5i\nare i.i.d. random noise following N(0, 102), for i = 1, . . . , n.\nOur simulation setting considers \ufb01rst a high-dimension setting (relative low ratio between\nsample size per dimension n/d = 0.5) with high noise to signal ratio (variability on residual\nnoise is high compared to the variability on xi).\nIf we set Q0 to be the empirical distribution of {(xi, yi)n\ni=1}, the dropout training problem\nin the linear regression model is\nmin\n\u03b2\u2208Rd EQ\u22c6\n\u0014\u0010\n\u03b2\u22a4(X \u2299\u03be) \u2212Y\n\u00112\u0015\n.\nCorollary 7 in Appendix A.5 shows that in the linear regression model the dropout training\nproblem can be written as\nmin\n\u03b2\u2208Rd\n1\nn\n\u0014\n(Y \u2212X\u03b2)\u22a4(Y \u2212X\u03b2) +\n\u03b4\n1 \u2212\u03b4\u03b2\u22a4\u039b\u03b2\n\u0015\n,\nwhere Y = [y1, y2, . . . , yn]\u22a4, X = [x1, x2, . . . , xn]\u22a4and \u039b is the diagonal matrix with its\ndiagonal elements given by the diagonals of X\u22a4X. Moreover, there is a closed-form solution\n24\nDropout Training is Distributionally Robust Optimal\nfor the dropout training problem and it is given by the ridge regression formula:\n\u03b2\u22c6\nn =\n\u0012\nX\u22a4X +\n\u03b4\n1 \u2212\u03b4\u039b\n\u0013\u22121\nX\u22a4Y.\nWe choose the dropout probability \u03b4 following Proposition 3. More speci\ufb01cally, Propo-\nsition 3 suggests the choice \u03b4 = c/\u221an where c = z1\u2212\u03b1 \u00b7 \u03c3/\u00b5. For linear regression with\nknown variance, it is straightforward to compute\n\u00b5 =\n1\n2\u03c6\u22c6\nd\nX\nj=1\nEP \u22c6[X2\nj ](\u03b2\u22c6\nj )2,\nand\n\u03c32 = VarP \u22c6\n\u00141\n2 log(2\u03c0\u03c6\u22c6) + (Y \u2212(\u03b2\u22c6)\u22a4X)2\n2\u03c6\u22c6\n\u0015\n.\nChoosing \u03b1 = 0.1 and note that \u03b2\u22c6= \u03b20, \u03c6\u22c6= 102, we have \u03b4 \u22480.26.\nSince neither our suggested Multi-level Monte Carlo algorithm nor standard SGD (as\nde\ufb01ned in Section 6.2) uses closed-form formulae for their implementation, we analyze the\nextent to which these procedures can approximate the parameter \u03b2\u22c6\nn. We provide more\ndetails of the algorithms as follows. The two algorithms we compare are:\n\u2022 Standard SGD algorithm with a learning rate 0.0001, and initialization at the origin.\nNote that however we take batched SGD instead of single-sample SGD introduced in\nSection 6.2.\n\u2022 Multi-level Monte Carlo algorithm with the geometric rate r = 0.6 and the burn-in\nperiod m0 = 5. Note that in each parallel running, we use gradient descent (GD) with\n0.01 learning rate and initialization at origin for steps iii) and iv) in Section 6.4.\nWe run our simulation on a cluster with two Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz\nprocessors (with 10 cores each), and a total memory of 128 GB. We \ufb01x 60 seconds as a\n\u201cwall-clock time\u201d, so that we terminate the two algorithms after 60 seconds.7\nWe run 1000 independent experiments.\nFor each run, we calculate and report the\naverage parameter estimation divergence to \u03b2\u22c6\nn and 1-standard deviation error bar for the\ndivergence. We consider di\ufb00erence number of parallelizations (i.e., L in Section 6.4) from\n400 to 2400. We cap the run at 2400 due to the saturation of divergence after \u223c2000\nparallelizations.\n7. The parameters for the SGD algorithm are appropriately tuned to achieve good convergence within 60s\n(see Appendix A.6 for the tuning procedure). However, we do not claim that this choice of parameters\nis optimal.\n25\nBlanchet et al.\nFigure 1 shows the l2 divergence to the true \u03b2\u22c6\nn of the two algorithms for varying L,\nwhile Figure 2 and Figure 3 show l\u221eand l1 divergence, respectively. We observed that\nour unbiased estimator outperforms standard SGD algorithm once the number of parallel\niterations reaches above some moderate threshold (\u223c1000 here). We provide supporting\nevidence in Appendix A.6 to argue our choice of learning rate, initialization, and wall-clock\ntime, where our proposed algorithm is robust to any reasonable choices.\nFigure 1: l2 di\ufb00erence\nFigure 2: l\u221edi\ufb00erence\n7.2 Coverage of the True Loss of Dropout Training\nWe validate that our recommended selection of \u03b4 guarantees that the in-sample loss of\ndropout training is covering the true loss with arbitrary high probability as prescribed by\nProposition 3.\n26\nDropout Training is Distributionally Robust Optimal\nFigure 3: l1 di\ufb00erence\nWe use the same linear regression model with dimension d = 10 and training samples\nn \u2208{103, 104}. We choose di\ufb00erent quantiles of the normal as in Proposition 3. We also\ninclude 10-fold cross validation and ordinary least squares for comparison. See Table 1,\nwhere we estimate the frequency of coverage over 1000 independent runnings. The main\nmessage is that our suggested choice of \u03b4 guarantees that the in-sample loss of dropout\ntraining exceeds the true, unknown, population loss with probability 1\u2212\u03b1. Using standard\nOLS or choosing \u03b4 by cross-validation the in-sample loss is smaller than the population loss\nwith probability close to 1/2, which implies that these methods are unsatisfactory in terms\nof frequency of coverage.\n\u03b1 = 0.2\n\u03b1 = 0.1\n\u03b1 = 0.05\n10-fold CV\nplain OLS\nn = 103\n0.77 \u00b1 0.01\n0.88 \u00b1 0.01\n0.94 \u00b1 0.01\n0.52 \u00b1 0.02\n0.40 \u00b1 0.01\nn = 104\n0.79 \u00b1 0.01\n0.90 \u00b1 0.01\n0.94 \u00b1 0.01\n0.49 \u00b1 0.02\n0.47 \u00b1 0.02\nTable 1: Frequency of in-sample loss covering the true population loss. Our recommended\nselection of \u03b4 = c/\u221an with c = z1\u2212\u03b1\u03c3/\u00b5 has a theoretical 1\u2212\u03b1 coverage probability.\n8. Extensions\nIn this section we discuss the extent to which the decision-theoretic support for dropout\ntraining carries over to Neural Networks. The main idea is that we use a GLM model where\nthe natural parameter is no longer a linear function of the covariates, but instead a neural\nnetwork.\n27\nBlanchet et al.\n8.1 One-hidden-layer Feed-Forward Neural Networks\nSuppose the scalar response variable Y is generated by the conditional density\nf(Y |X, \u03b8, \u03c6) \u2261h(Y, \u03c6) exp ((Y \u2126\u03b8(X)) \u2212\u03a8(\u2126\u03b8(X))) /a(\u03c6)) ,\n(26)\nwhere \u2126\u03b8(X) is a neural network with parameters \u03b8 and X \u2208Rd. This is a simple extension\nof the regression model that has been used recently to study deep neural networks; see\nSchmidt-Hieber (2020) in which the conditional density is Gaussian.\nIn this section, we will assume that \u2126\u03b8(X) is a neural network with a single hidden\nlayer, a di\ufb00erentiable activation (squashing) function, and linear ouput function. A function\nh : R \u2192[0, 1] is a squashing function if it is non-decreasing and if\nlim\nr\u2192\u221eh(r) = 1,\nlim\nr\u2192\u2212\u221eh(r) = 0.\nSee De\ufb01nition 2.3 in Hornik et al. (1989).\nAlthough these types of networks\u2014which will be formally described below\u2014are restric-\ntive compared to the modern deep learning architectures, they can approximate any Borel\nmeasurable function from a \ufb01nite-dimensional space to another, provided the hidden units\nin the hidden layer are large; see Hornik et al. (1989).\nConsider a neural network with K units in the hidden layer, each using input weights\nwk \u2208Rd, k = 1, . . . , K. Denote the activation function in the hidden layer as h(\u00b7). Assume\nthe output function is linear with vector of weights \u03b2 \u2208RK. Thus, the network under\nconsideration is de\ufb01ned by the function:\n\u2126\u03b8(X) \u2261\u03b21h(w\u22a4\n1 X) + . . . + \u03b2Kh(w\u22a4\nKX) = \u03b2\u22a4H(X),\nwhere H(X) = (h(w\u22a4\n1 X), . . . , h(w\u22a4\nKX))\u22a4. The neural network is parameterized by \u03b8 \u2261\n(\u03b2\u22a4, w\u22a4\n1 , . . . , w\u22a4\nk )\u22a4. Under this model, the distribution of Y |X is a GLM model with co-\nvariates H(X).\n8.1.1 Statistician\u2019s Objective Function\nWe will endow the statistician with the loss function given by the negative of the conditional\nlog-likelihood for the model in (26).\n8.1.2 Nature\u2019s Uncertainty Set\nWe allow nature to introduce additional noise to the statistician\u2019s model. We do this in\ntwo steps. First, we allow nature to distort the distribution of X using a multiplicative\nnoise denoted as \u03be(1) \u2208Rd. This is exactly analogous to what we did in the GLM model,\n28\nDropout Training is Distributionally Robust Optimal\nwhere nature was allowed to pick a distribution for the covariates of the form (X \u2299\u03be(1)).\nUsing the jargon of neural networks, we allow nature to contaminate the input layer with\nindependent and multiplicative noise. Second, we also allow nature to contaminate each of\nthe hidden units with multiplicative noise \u03be(2) \u2208RK. That is, nature is also allowed to pick\na vector \u03be(2) = (\u03be(2)1, . . . , \u03be(2)K)\u22a4, independently of \u03be(1) \u2208Rd, to distort the each of the\nK units in the hidden layer as\nH(X) \u2299\u03be(2) \u2261(h(w\u22a4\n1 X)\u03be(2)1, . . . , h(w\u22a4\nKX)\u03be(2)K)\u22a4.\nOur choice of a one-hidden neural network was simply for expositional simplicity, but\nthe analysis would be the same with a feed-forward neural network with L hidden layers.\n8.1.3 Minimax Solution\nThe minimax solution of the DRO game is given by\ninf\n\u03b8 sup\nQ\nEQ [\u2212ln f(Y |H(X \u2299\u03be1) \u2299\u03be2, \u03b2, \u03c6)] ,\n(27)\nwhere Q now refers to the joint distribution of (X, Y, \u03be(1), \u03be(2)) and f(Y |X, \u03b2, \u03c6) is the GLM\ndensity de\ufb01ned in (1). We continue working with the assumption that \u03be \u2261(\u03be(1)\u22a4, \u03be(2)\u22a4)\u22a4\nhas independent marginals and that it is independent of (X, Y ).\nWe would like to solve for the worst-case distributions of the random vectors \u03be(1) and\n\u03be(2), assuming that both of these satisfy the restrictions analogous to (12). The solution\nfor the distribution of \u03be(2) can be obtained as a corollary to Theorem 2, as it su\ufb03ces to\nde\ufb01ne\n\u02dcX \u2261H(X \u2299\u03be(1)),\nand view (27) as the DRO problem in a linear regression model, in which the data is\n( \u02dcX, Y ) and \u03be(2) \u2208RK is simply the multiplicative noise that transforms the covariates into\n( \u02dcX \u2299\u03be(2)).\nThe worst-case choice of \u03be(1), the multiplicative error for the inputs, is more di\ufb03cult\nto characterize and we were not able to \ufb01nd general results for it. Below, we provide a\nheuristic argument suggesting that dropout noise might approximate the worst-case choice\nwhen the output layer is a Gaussian linear model. Let \u03be(1)j denote the j-th coordinate of\n\u03be(1). Suppose that the distribution of this random variable places most of its mass on the\ninterval [1 \u2212\u03f5, 1 + \u03f5].8 This allows us to \u2018linearize\u2019 the output of each of the hidden units\n8. This is compatible with dropout noise for which \u03b4 is very close to zero.\n29\nBlanchet et al.\naround the output corresponding to unperturbed inputs as\nh(w\u22a4\nk (X \u2299\u03be(1)))\n=\nh(w\u22a4\nk (X \u2299(\u03be(1) \u22121)) + w\u22a4\nk X)\n\u2248\nh(w\u22a4\nk X) +\n\u0010\n\u02d9h(w\u22a4\nk X) \u00b7 (wk \u2299X)\u22a4(\u03be(1) \u22121)\n\u0011\n.\nIn the notation above, 1 denotes the d-dimensional vector of ones. For the sake of exposition,\nignore the approximation error in the linearization above. If we \ufb01x (X, Y, \u03be(2)), then the\nworst-case choice for the distribution of \u03be(1), denoted by Q(1), maximizes\nEQ(1)\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8ed\nK\nX\nk=1\n\u03b2k \u00b7 \u03be(2)k \u00b7\n\uf8ee\n\uf8f0h(w\u22a4\nk X) +\n\uf8eb\n\uf8ed\u02d9h(w\u22a4\nk X) \u00b7\nd\nX\nj=1\nwk,j \u00b7 Xj \u00b7 (\u03be(1)j \u22121)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\uf8f6\n\uf8f8\n2\uf8f9\n\uf8fb\namong all distributions with independent marginals for which EQ(1)[\u03be(1)j] = 1 for all j =\n1, . . . , d. Algebra shows that such maximization problem is equivalent to maximizing\nEQ(1)\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8ed\nK\nX\nk=1\n\u03b2k \u00b7 \u03be(2)k \u00b7 \u02d9h(w\u22a4\nk X) \u00b7\n\uf8ee\n\uf8f0\nd\nX\nj=1\nwk,j \u00b7 Xj \u00b7 (\u03be(1)j \u22121)\n\uf8f9\n\uf8fb\n\uf8f6\n\uf8f8\n2\uf8f9\n\uf8fb,\n(28)\nwhich in turn can be written as\nEQ(1)\n\u0014\u0010\na\u22a4(\u03be(1) \u22121)\n\u00112\u0015\nfor an appropriate choice of a vector a \u2208Rd that depends only on (\u03b2, \u03be(2), h, \u02d9h, w, X).\nProposition 6 in Appendix A.2 shows that the solution to this problem is dropout noise.\n9. Concluding Remarks\nIn this paper we studied dropout training, an increasingly popular estimation method in\nmachine learning. Dropout training is a fundamental part of the modern machine learning\ntechniques for training very deep networks (Goodfellow et al., 2016).\nOur main result (Theorem 2) established a novel decision-theoretic foundation for the\nuse of dropout training. We showed that this method, when applied to Generalized Linear\nModels, can be viewed as the minimax solution to an adversarial two-player, zero-sum\ngame between a statistician and nature. The framework used in this paper is known in\nthe stochastic optimization literature (Shapiro et al., 2014) as a Distributionally Robust\nOptimization (DRO) problem.\nOur minimaxity result showed, by construction, that dropout training indeed provides\nout-of-sample performance guarantees for distributions that arise from multiplicative per-\n30\nDropout Training is Distributionally Robust Optimal\nturbations of the in-sample data. Our result thus justi\ufb01ed explicitly the ability of dropout\ntraining to enhance the out-of-sample performance, which is one of the reasons often invoked\nto promote the dropout method.\nIn addition to our theoretical result, we also suggested a new strategy to select the\ndropout probability and a new stochastic optimization implementation of dropout training.\nFor the latter, we borrowed ideas from the Multi-level Monte Carlo literature\u2014in particular\nfrom the work of (Blanchet et al., 2019a)\u2014to suggest an unbiased dropout training routine\nthat is easily parallelizable and that has a smaller computational cost compared to naive\ndropout training methods when the number of features is large (Theorem 4). Crucially,\nwe showed that under some regularity conditions our estimator has \ufb01nite variance (which\nmeans there are also theoretical, and not just practical, gains from parallelization).\nWe also discussed the extent to which our theoretical results extended to Neural Net-\nworks (in particular, to the universal approximators in (Hornik et al., 1989) consisting of a\nsingle-hidden layer and a squashing activation function). Our results showed that Theorem\n2 can be used to establish the optimality of dropout training to estimate the parameters of\nthe last hidden layer in general feed-forward neural networks, where the output layer takes\nthe form of a Generalized Linear Model. We hope that our analysis serves as a foundation\nto understand the bene\ufb01ts of dropout training in Neural Networks.\nAcknowledgments\nWe would like to thank Matias Cattaneo, Max Farrell, Michael Leung, Ulrich M\u00a8uller, Mark\nPeletier, Hashem Pesaran, Ashesh Rambachan, Roger Moon, Frank Schorfheide, Stefan\nWager, and participants at the Statistics Seminar series at Columbia University for helpful\ncomments and suggestions. Jos\u00b4e Blanchet acknowledges support from NSF grants 1915967,\n1820942, 1838576 and the Chinese Merchant Bank. Material in this paper is based upon\nwork supported by the Air Force O\ufb03ce of Scienti\ufb01c Research under award number FA9550-\n20-1-0397.\n31\nBlanchet et al.\nAppendix A.\nA.1 Proof of Proposition 1\nAlgebra shows that the dropout estimator of \u03b2 maximizes\nQn(\u03b2) \u22611\nn\nn\nX\ni=1\nyi(\u03b2\u22a4xi) \u2212E\u03b4n[\u03a8(\u03b2\u22a4(xi \u2299\u03be))].\nIn a slight abuse of notation, let \u03be\u03b4 denote a realization of dropout noise parameterized by \u03b4.\nThen it is possible to re-write the objective function as a weighted average of the functions\nQn,\u03be\u03b4n(\u03b2) \u22611\nn\nn\nX\ni=1\nyi(\u03b2\u22a4xi) \u2212\u03a8(\u03b2\u22a4(xi \u2299\u03be\u03b4n)).\nIt will be convenient then to de\ufb01ne the limiting objective function to be\nQ(\u03b2) \u2261EP \u22c6[Y (\u03b2\u22a4X)] \u2212EP \u22c6[E\u03b4[\u03a8(\u03b2\u22a4(X \u2299\u03be))]],\nwhich, by Assumptions 1 and 2, is \ufb01nite and strictly concave. The population objective\nfunction is then the average (over dropout noise) of\nQ\u03be\u03b4(\u03b2) \u2261EP \u22c6[Y (\u03b2\u22a4X)] \u2212EP \u22c6[\u03a8(\u03b2\u22a4(X \u2299\u03be\u03b4))].\nIt is straightforward to show that \u03b2\u2217(\u03b4) in (6) denote the unique maximizer of Q(\u03b2).\nProof The proof follows from standard arguments in the theory of extremum estimators.\nIn particular, it su\ufb03ces to verify the conditions of Theorem 2.7 in Newey and McFadden\n(1994).\nCondition i) in Newey and McFadden (1994) requires Q(\u03b2) to be uniquely maximized\nat \u03b2\u2217(\u03b4). This holds because Assumptions 1 and 2 imply that Q(\u03b2) is strictly concave.\nCondition ii) in Newey and McFadden (1994) requires \u03b2\u2217(\u03b4) to be an element in the\ninterior of a strictly convex set, which holds because in the GLM models under consideration\nthe parameter space is Rd. Furthermore, Qn(\u03b2) is trivially concave by Assumption 1.\nCondition iii) requires Qn(\u03b2) to converge in probability to Q(\u03b2) for every \u03b2. For this\npurpose, it su\ufb03ces to show that Qn,\u03be\u03b4n(\u03b2) converges in probability to Q\u03be\u03b4(\u03b2) for each \ufb01xed\n\u03b2, and for a sequence \u03be\u03b4n and \u03be\u03b4 that have zeros and non-zeros in exactly the same entries.\nAssumptions 1 and 2 imply EP \u2217[Y (\u03b2\u22a4X)] < \u221efor all \u03b2. Thus, using the Law of Large\n32\nDropout Training is Distributionally Robust Optimal\nNumbers for i.i.d sequences\n1\nn\nn\nX\ni=1\nYi(\u03b2\u22a4Xi)\np\u2192EP \u2217[Y (\u03b2\u22a4X)].\nFinally, Assumptions 1 and 2 imply that the triangular array\nZn,i = \u03a8(\u03b2\u22a4(Xi \u2299\u03be\u03b4n)),\n1 \u2264i \u2264n,\nsatis\ufb01es the conditions for the Law of Large Numbers for triangular arrays (Theorem 2.2.11\nin Durrett (2019)), and consequently\n1\nn\nn\nX\ni=1\n\u03a8(\u03b2\u22a4(Xi \u2299\u03be\u03b4n))\np\u2192EP \u2217\nh\n\u03a8(\u03b2\u22a4(X \u2299\u03be\u03b4))\ni\n.\nThis completes the proof.\nA.2 Proof of Theorem 2\nThe proof of Theorem 2 relies on the following two preparatory results.\nLemma 5 (Extremal expectation of a univariate convex function) For any \u2212\u221e<\na < b < +\u221e, let \u03b6 be a random variable in [a, b] with mean \u00b5 \u2208[a, b]. For any function\nf : [a, b] \u2192R convex and continuous, the distribution of \u03b6 that maximizes E[f(\u03b6)] among\nall distributions over [a, b] with a given mean \u00b5 \u2208[a, b] is a scaled and shifted Bernoulli\ndistribution, i.e.,\n\u03b6 =\n\uf8f1\n\uf8f2\n\uf8f3\na\nwith probability (b \u2212\u00b5)/(b \u2212a),\nb\nwith probability (\u00b5 \u2212a)/(b \u2212a).\n(29)\nProof Let Q\u2217denote the probability measure induced by the random variable in (29). By\nde\ufb01nition\nEQ\u2217[f(\u03b6)] = b \u2212\u00b5\nb \u2212af(a) + \u00b5 \u2212a\nb \u2212a f(b).\nSuppose \ufb01rst that \u00b5 = a.\nIn this case, Jensen\u2019s inequality implies that for any other\nprobability measure Q over [a, b] with mean \u00b5 = a,\nEQ[f(\u03b6)] \u2264f(EQ[\u03b6]) = f(a) = EQ\u2217[f(\u03b6)].\nAn analogous result holds if \u00b5 = b.\n33\nBlanchet et al.\nConsider then the case in which \u00b5 \u2208(a, b). For an arbitrary probability measure Q over\n[a, b] with mean \u00b5 \u2208(a, b), we have\nZ\n[a,b]\nf(\u03b6)dQ =\nZ\n[a,b]\nf\n\u0000ab \u2212\u03b6\nb \u2212a + b\u03b6 \u2212a\nb \u2212a\n\u0001\ndQ \u2264\nZ\n[a,b]\n\u0000b \u2212\u03b6\nb \u2212af(a) + \u03b6 \u2212a\nb \u2212a f(b)\n\u0001\ndQ,\nwhere the inequality follows from the convexity of f. By the linearity of the integral operator\nand the fact that\nR\n[a,b] \u03b6dQ = \u00b5, we \ufb01nd\nZ\n[a,b]\nf(\u03b6)dQ \u2264b \u2212\u00b5\nb \u2212af(a) + \u00b5 \u2212a\nb \u2212a f(b).\nBecause the probability measure Q was chosen arbitrarily, this implies that the distribution\nof \u03b6 in (29) maximizes the expectation of f(\u03b6).\nProposition 6 Fix a vector of tuning parameters \u03b4 \u2208(0, 1)d. Let Qj(\u03b4j) be de\ufb01ned as in\n(11). Suppose that A is a convex and continuous function on R. For any \u03b8 \u2208Rd, we have\nsup\nn\nEQ1\u2297...\u2297Qd[A(\u03b8\u22a4\u03be)] : Qj \u2208Qj(\u03b4j)\no\n= EQ\u22c6\n1\u2297...\u2297Q\u22c6\nd[A(\u03b8\u22a4\u03be)],\nwhere Q\u22c6\nj is a scaled Bernoulli distribution of the form Q\u22c6\nj = (1\u2212\u03b4j)\u22121 \u00d7Bernoulli((1\u2212\u03b4j))\nfor each j = 1, . . . , d.\nProof First note that Q\u22c6\nj \u2208Qj(\u03b4j) for each j, and thus Q\u22c6\n1 \u2297. . . \u2297Q\u22c6\nd is a feasible solution\nto the maximization problem.\nIt su\ufb03ces to show that for any set of feasible measures\nQj \u2208Qj(\u03b4j), j = 1, . . . , d, we have\nEQ1\u2297...\u2297Qd[A(\u03b8\u22a4\u03be)] \u2264EQ\u22c6\n1\u2297...\u2297Q\u22c6\nd[A(\u03b8\u22a4\u03be)].\nTowards this end, pick any k \u2208{1, . . . , d}. By Fubini\u2019s theorem, we can write\nEQ1\u2297...\u2297Qd[A(\u03b8\u22a4\u03be)] = EQ1\u2297...\u2297Qk\u22121\u2297Qk+1\u2297...\u2297QdEQk[A(\u03b8\u22a4\u03be)].\nFor any \ufb01xed value (\u03be1, . . . , \u03bek\u22121, \u03bek+1, . . . , \u03bed) the function \u03bek 7\u2192A(P\nj\u0338=k \u03b8j\u03bej + \u03b8k\u03bek) is\nconvex in the variable \u03bek over the interval [0, (1 \u2212\u03b4k)\u22121]. Thus by Lemma 5,\nEQk[A(\nX\nj\u0338=k\n\u03b8j\u03bej +\u03b8k\u03bek)] \u2264EQ\u22c6\nk[A(\nX\nj\u0338=k\n\u03b8j\u03bej +\u03b8k\u03bek)]\nfor any \ufb01xed (\u03be1, . . . , \u03bek\u22121, \u03bek+1, . . . , \u03bed).\n34\nDropout Training is Distributionally Robust Optimal\nThus by the monotonicity of the expectation operator,\nEQ1\u2297...\u2297Qd[A(\u03b8\u22a4\u03be)] \u2264EQ1\u2297...\u2297Qk\u22121\u2297Qk+1\u2297...\u2297QdEQ\u22c6\nk[A(\u03b8\u22a4\u03be)] = EQ1\u2297...\u2297Qk\u22121\u2297Q\u22c6\nk\u2297Qk+1\u2297...\u2297Qd[A(\u03b8\u22a4\u03be)].\nBy cycling through all possible values of k \u2208{1, . . . , d} we conclude that\nEQ1\u2297...\u2297Qd[A(\u03b8\u22a4\u03be)] \u2264EQ\u22c6\n1\u2297...\u2297Q\u22c6\nd[A(\u03b8\u22a4\u03be)].\nTherefore, the postulated claim holds.\nWe are now ready to prove Theorem 2.\nProof Note that for Q \u2208U(Q0, \u03b4), Assumption 3 implies EQ[\u2113(X \u2299\u03be, Y, \u03b8)] is \ufb01nite for any\n\u03b8 \u2208\u0398 and any scalar \u03b4 \u2208[0, 1). Therefore, from Fubini\u2019s theorem and the de\ufb01nition of loss\nfunction:\nEQ[\u2113(X \u2299\u03be, Y, \u03b8)] = EQ0 [EQ1\u2297...\u2297Qd[\u2113(X \u2299\u03be, Y, \u03b8)]]\n= EQ0\nh\nEQ1\u2297...\u2297Qd[\u2212ln h(Y, \u03c6) + (\u03a8(\u03b2\u22a4(X \u2299\u03be)) \u2212Y (\u03b2\u22a4(X \u2299\u03be)))/a(\u03c6)]\ni\n= \u2212EQ0 [ln h(Y, \u03c6)]\n+ EQ0\nh\nEQ1\u2297...\u2297Qd[(\u03a8(\u03b2\u22a4(X \u2299\u03be)) \u2212Y (\u03b2\u22a4(X \u2299\u03be)))/a(\u03c6)]\ni\n.\nAlgebra shows that for any \u03b2, X and \u03be:\n\u03b2\u22a4(X \u2299\u03be) = (\u03b2 \u2299X)\u22a4\u03be.\nThus, we can \ufb01x the values of (X, Y, \u03b8) and de\ufb01ne the function\nA(X,Y,\u03b8)((\u03b2 \u2299X)\u22a4\u03be) \u2261(\u03a8(\u03b2\u22a4(X \u2299\u03be)) \u2212Y \u03b2\u22a4(X \u2299\u03be))/a(\u03c6).\nNote that A(X,Y,\u03b8) satis\ufb01es the condition of Proposition 6. Therefore\nsup\nn\nEQ1\u2297...\u2297Qd[A(X,Y,\u03b8)((\u03b2 \u2299X)\u22a4\u03be)] : Qj \u2208Qj(\u03b4j)\no\n= EQ\u22c6\n1\u2297...\u2297Q\u22c6\nd[A(X,Y,\u03b8)((\u03b2 \u2299X)\u22a4\u03be)],\nfor any (X, Y, \u03b8), which completes the proof.\n35\nBlanchet et al.\nA.3 Proof of Proposition 3\nProof We write \u221an(Ln(b\u03b2(\u03b4n), b\u03c6, \u03b4n) \u2212L(\u03b2\u2217, \u03c6\u2217)) as the sum of the following three terms\n\u221an\n\u0010\nLn(b\u03b2(\u03b4n), b\u03c6, \u03b4n) \u2212Ln(\u03b2\u2217, b\u03c6, \u03b4n)\n\u0011\n,\n(30a)\n\u221an\n\u0010\nLn(\u03b2\u2217, b\u03c6, \u03b4n) \u2212Ln(\u03b2\u2217, b\u03c6, 0)\n\u0011\n,\n(30b)\n\u221an\n\u0010\nLn(\u03b2\u2217, b\u03c6, 0) \u2212L(\u03b2\u2217, \u03c6\u2217, 0)\n\u0011\n.\n(30c)\nThe last term converges in distribution to a normal random variable, so we only need to\nanalyze (30a) and (30b).\nBy Assumptions 1 and 2 the term in (30a) admits an exact second-order Taylor expan-\nsion around \u03b2\u2217for every \u03c6 and \u03b4. We argue that because of this, the term in question if\nop(1). First, using the same arguments as in Theorem 3.1 in Newey and McFadden (1994)\nwe can show that for any sequence \u03b4n = c/\u221an\n\u221an(b\u03b2(\u03b4n) \u2212\u03b2\u22c6) d\u2192\u03a3(\u03b2\u2217)\u22121Nd(\u2212c\u02dc\u00b5, a(\u03c6\u2217)\u03a3(\u03b2\u2217)),\nwhere\n\u03a3(\u03b2) \u2261EP \u2217[\u00a8\u03a8(X\u22a4\u03b2)XX\u22a4],\nand\n\u02dc\u00b5 \u2261\n\uf8eb\n\uf8edX\n\u03be\u2208A\nEP \u22c6[ \u02d9\u03a8((X \u2299\u03be)\u22a4\u03b2\u22c6)(X \u2299\u03be)]\n\uf8f6\n\uf8f8\u2212(d \u22121)EP \u22c6[Y X] + \u03a3(\u03b2\u2217)\u03b2\u2217.\nThe set A above is de\ufb01ned as {\u03be \u2208{0, 1}d : exactly one entry of \u03be is zero}. The argument\nis essentially the same as in every proof of asymptotic normality for extremum (or M-\nestimators), with the only di\ufb00erence being that, because of the dropout noise, the score\nterm is asymptotically normal with a nonzero mean. In fact,\n\u2207\u03b2Ln(\u03b2, b\u03c6, \u03b4n) \u2261\u2207\u03b2Ln(\u03b2, b\u03c6, 0)+\n1\na(b\u03c6)\n \n1\nn\nn\nX\ni=1\nE\u03b4n[(Xi \u2299\u03be) \u02d9\u03a8(\u03b2\u22a4(Xi \u2299\u03be))] \u2212Xi \u02d9\u03a8(X\u22a4\ni \u03b2)\n!\n,\nwhere\n\u2207\u03b2Ln(\u03b2, b\u03c6, 0) \u2261\u2212\n1\na(b\u03c6)\n1\nn\nn\nX\ni=1\nXi(Yi \u2212\u02d9\u03a8(X\u22a4\ni \u03b2)).\nRecognizing the term \u2207\u03b2Ln(\u03b2, b\u03c6, 0) as the negative of the score function in the GLM model\nand doing some algebra, it is possible to show that \u2207\u03b2Ln(\u03b2, b\u03c6, \u03b4n) is op(1).\n36\nDropout Training is Distributionally Robust Optimal\nFor the term in (30b), note \ufb01rst that it is nonnegative. Also: Ln(\u03b2\u2217, b\u03c6, \u03b4n)\u2212Ln(\u03b2\u2217, b\u03c6, 0)\nequals\n1\na(b\u03c6)\n \n1\n\u221an\nn\nX\ni=1\nE\u03b4n[\u03a8((Xi \u2299\u03be)\u22a4\u03b2\u2217)] \u2212\u03a8(X\u22a4\ni \u03b2\u2217)\n!\n.\nThe term in parenthesis has \ufb01nite mean equal to\n\u2206n \u2261EP \u2217E\u03b4n[\u03a8((X \u2299\u03be)\u22a4\u03b2\u2217)] \u2212EP \u2217[\u03a8(X\u22a4\u03b2\u2217)].\n(31)\nIt can be shown\u2014by verifying the conditions for the Law of Large Numbers for triangular\narrays (Theorem 2.2.11 in Durrett (2019))\u2014that\n\u221an(Ln(\u03b2\u2217, \u03b4n) \u2212Ln(\u03b2\u2217, 0) \u2212a(b\u03c6)\u22121\u2206n)\np\u21920.\nMoreover, Assumptions 1 and 2 imply\n\u221an\u2206n\np\u2192\u2206,\nwhere\n\u2206\u2261c\n\uf8eb\n\uf8edX\n\u03be\u2208A\nEP \u22c6[\u03a8((X \u2299\u03be)\u22a4\u03b2\u22c6)] \u2212dEP \u22c6[\u03a8(X\u22a4\u03b2\u22c6)] + EP \u22c6[ \u02d9\u03a8(X\u22a4\u03b2\u2217)X\u22a4\u03b2\u22c6]\n\uf8f6\n\uf8f8.\nThis gives the desired result.\nA.4 Proof of Theorem 4\nProof By de\ufb01nition\nZ(K\u2217\nl ) =\n\u00af\u2206K\u2217\nl\nr(1 \u2212r)m\u2217\nl + \u03b8l,m0,\nwhere K\u2217\nl is a discrete random variable with probability mass function:\np(K\u2217\nl ) = r(1 \u2212r)K\u2217\nl \u2212m0,\nand supported on the integers larger than m0.\n37\nBlanchet et al.\nWe \ufb01rst show that the estimator Z(K\u2217\nl ) is unbiased (as we average over both K\u2217\nl and\n\u03bek\ni ). Algebra shows that\nE[Z(K\u2217\nl )]\n=\n\u221e\nX\nK=m0\nE[Z(K\u2217\nl )|K\u2217\nl = K]p(K)\n=\n\u221e\nX\nK=m0\nE\n\" \u00af\u2206K\u2217\nl\np(K\u2217\nl ) + \u03b8l,m0\n\f\f\f\f\fK\u2217\nl = K\n#\np(K)\n=\n\u221e\nX\nK=m0\nE\n\" \u00af\u2206K\np(K) + \u03b8l,m0\n\f\f\f\f\fK\u2217\nl = K\n#\np(K)\n=\n\uf8eb\n\uf8ed\n\u221e\nX\nK=m0\nE\n\u0014\nb\u03b8\u22c6\nn(2K+1) \u22121\n2(b\u03b8O\nn (2K) + b\u03b8E\nn (2K))\n\u0015\uf8f6\n\uf8f8+ E[\u03b8l,m0]\n=\n\u22121\n2\n\u0010\nE[b\u03b8O\nn (2m0)] + E[b\u03b8E\nn (2m0)]\n\u0011\n+ E[\u03b8l,m0] + lim\nK\u2192\u221eE[b\u03b8\u22c6\nn(2K+1)].\nThe expectations in the last line are all \ufb01nite because \u0398 is compact. In addition, since the\ndraws are i.i.d. and \u03b8l,m0 is the solution to the problem (25) when 2m0 draws are used we\nhave\n\u22121\n2\n\u0010\nE[b\u03b8O\nn (2m\n0 )] + E[b\u03b8E\nn (2m\n0 )]\n\u0011\n+ E[\u03b8l,m0] = 0.\nMoreover, the sequence of random variables\n{b\u03b8\u22c6\nn(2K+1)}\nis uniformly integrable, because \u0398 is a compact subset of a \ufb01nite-dimensional Euclidean\nspace. Finally, we know that\nb\u03b8\u22c6\nn(2K+1)\np\u2192\u03b8\u22c6\nn\nas K \u2192\u221e. The uniform integrability of the sequence of estimators then implies\nlim\nK\u2192\u221eE[b\u03b8\u22c6\nn(2K+1)] = E\n\u0014\nlim\nK\u2192\u221e\nb\u03b8\u22c6\nn(2K+1)\n\u0015\n= \u03b8\u22c6\nn,\nsee Theorem 6.2 in DasGupta (2008). We conclude that\nE[Z(K\u2217\nl )] = lim\nK\u2192\u221eE[b\u03b8\u22c6\nn(2K+1)] = \u03b8\u22c6\nn.\nNow we show that the expected computational cost of Z(K\u2217\nl ) is \ufb01nite. In order to com-\npute Z(K) for a given K we need n\u00b72K+1 random draws. Thus, the expected computational\n38\nDropout Training is Distributionally Robust Optimal\ncost of Z(K\u2217\nl ) is\n\u221e\nX\nK=m0\nn2K+1r(1 \u2212r)K\u2212m0\n=\nn \u00b7 (2m0+1) \u00b7 r\n\u221e\nX\nK=m0\n2K\u2212m0(1 \u2212r)K\u2212m0\n=\nn \u00b7 (2m0+1) \u00b7 r\n\u221e\nX\nK=m0\n(2(1 \u2212r))K\u2212m0.\nThe term above converges to\nn \u00b7 (2m0+1) \u00b7 r\n1 \u22122(1 \u2212r) = n \u00b7 (2m0+1) \u00b7 r\n2r \u22121\nprovided that 2(1 \u2212r) < 1, which holds because we have chosen r > 1/2.\nFor the proof on \ufb01nite variance, we intend to show that\nE\nh\n\u00af\u2206\u22a4\nK \u00af\u2206K\ni\n= O(2\u22122K)\n(32)\nas K \u2192\u221e. Equation (32) guarantees that every processor generates an estimator Z(K\u2217\nl )\nwith \ufb01nite variance. Since K\u2217\nl is a discrete random variable with probability mass function\np(K\u2217\nl ) = r(1 \u2212r)K\u2217\u2212m0,\nand\nE[Z(K\u2217\nl )\u22a4Z(K\u2217\nl )]\n=\n\u221e\nX\nK=m0\nE\nh\nZ(K\u2217\nl )\u22a4Z(K\u2217\nl )|K\u2217\nl = K\ni\np(K)\n=\n\u221e\nX\nK=m0\nE\n\"\u0012 \u00af\u2206K\np(K) + \u03b8l,m0\n\u0013\u22a4\u0012 \u00af\u2206K\np(K) + \u03b8l,m0\n\u0013#\np(K)\n\u2264\n2\n\uf8eb\n\uf8ed\n\u221e\nX\nK=m0\nE\n\u0014 \u00af\u2206\u22a4\nK \u00af\u2206K\np(K)2\n\u0015\np(K) +\n\u221e\nX\nK=m0\nE\nh\n\u03b8\u22a4\nl,m0\u03b8l,m0\ni\np(K)\n\uf8f6\n\uf8f8\n\u2264\nC\n\uf8eb\n\uf8ed\n\u221e\nX\nK=m0\n2\u22122K\np(K) + sup\n\u03b8\u2208\u0398\n\u2225\u03b8\u22252\n2p(K)\n\uf8f6\n\uf8f8\n\u2264\nC\n\uf8eb\n\uf8ed\n\u221e\nX\nK=m0\n1\n22m022(K\u2212m0)p(K) + sup\n\u03b8\u2208\u0398\n\u2225\u03b8\u22252\n2p(K)\n\uf8f6\n\uf8f8\n\u2264\nC1\n\uf8eb\n\uf8ed\n\u221e\nX\nK=m0\n1\nr4m0\n1\n(4(1 \u2212r))K\u2212m0\n\uf8f6\n\uf8f8+ C2.\n39\nBlanchet et al.\nThe geometric sum in the last expression is \ufb01nite because we have assumed that r < 3\n4.\nTo show (32), we do a Taylor expansion of the \ufb01rst-order conditions of the problem (25)\naround \u03b8\u22c6\nn. The Karush-Kuhn-Tucker optimality condition for the level 2K solution b\u03b8\u22c6\nn(2K)\nof the problem in (25) implies\n0 =\nn\nX\ni=1\n\uf8ee\n\uf8f01\n2K\n2K\nX\nk=1\n\u2207\u03b8\u2113(xi \u2299\u03bek\ni , yi, b\u03b8\u22c6\nn(2K))\n\uf8f9\n\uf8fb.\nIt follows by the Taylor expansion and Assumption 4 that\n0\n=\nn\nX\ni=1\n\uf8ee\n\uf8f01\n2K\n2K\nX\nk=1\n\u2207\u03b8\u2113(xi \u2299\u03bek\ni , yi, \u03b8\u22c6\nn)\n\uf8f9\n\uf8fb+\nn\nX\ni=1\n\uf8ee\n\uf8f01\n2K\n2K\nX\nk=1\n\u2207\u03b8\u03b8\u2113(xi \u2299\u03bek\ni , yi, \u03b8\u22c6\nn)\n\uf8f9\n\uf8fb\n\u0010\nb\u03b8\u22c6\nn(2K) \u2212\u03b8\u22c6\nn\n\u0011\n+RK,\u03b8\n=\nn\nX\ni=1\n\uf8ee\n\uf8f01\n2K\n2K\nX\nk=1\n\u2207\u03b8\u2113(xi \u2299\u03bek\ni , yi, \u03b8\u22c6\nn)\n\uf8f9\n\uf8fb+\nn\nX\ni=1\n\u2207\u03b8\u03b8EQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8\u22c6\nn)|X = xi, Y = yi]]\n\u0010\nb\u03b8\u22c6\nn(2K) \u2212\u03b8\u22c6\nn\n\u0011\n+RK + RK,\u03b8,\n(33)\nwhere\nRK \u2261\n\uf8eb\n\uf8ed\nn\nX\ni=1\n\uf8eb\n\uf8ed1\n2K\n2K\nX\nk=1\n\u2207\u03b8\u03b8\u2113(xi \u2299\u03bek\ni , yi, \u03b8\u22c6\nn) \u2212\u2207\u03b8\u03b8EQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8\u22c6\nn)|X = xi, Y = yi]\n\uf8f6\n\uf8f8\n\uf8f6\n\uf8f8\n\u0010\nb\u03b8\u22c6\nn(2K) \u2212\u03b8\u22c6\nn\n\u0011\nand\n\u2225RK,\u03b8\u22252 \u2264\nn\nX\ni=1\nsup\n\u03b8\u2208\u0398,\u03be\n\u2225\u2207\u03b8\u03b8\u03b8\u2113(xi \u2299\u03be, yi, \u03b8)\u22252\n\r\r\rb\u03b8\u22c6\nn(2K) \u2212\u03b8\u22c6\nn\n\r\r\r\n2\n2 \u2264C3\n\r\r\rb\u03b8\u22c6\nn(2K) \u2212\u03b8\u22c6\nn\n\r\r\r\n2\n2\nby Assumption 4. Thus by Assumption 3, we have\nE[R\u22a4\nK,\u03b8RK,\u03b8] = O(2\u22122K)\nas K \u2192\u221e. Moreover, by the multivariate version of Theorem 2 in Bahr (1965) which\nfollows from the Cram\u00b4er-Wold theorem, we have that\nE\n\uf8ee\n\uf8f0\n\r\r\r\r\r\r\nn\nX\ni=1\n\uf8eb\n\uf8ed1\n2K\n2K\nX\nk=1\n\u2207\u03b8\u03b8\u2113(xi \u2299\u03bek\ni , yi, \u03b8\u22c6\nn) \u2212\u2207\u03b8\u03b8EQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8\u22c6\nn)|X = xi, Y = yi]\n\uf8f6\n\uf8f8\n\r\r\r\r\r\r\n4\n2\n\uf8f9\n\uf8fb\nis O(2\u22122K).\n40\nDropout Training is Distributionally Robust Optimal\nWe can express R\u22a4\nKRK as \u2225RK\u22252. The Cauchy-Schwarz inequality implies\nE[R\u22a4\nKRK]\n\u2264E\n\uf8ee\n\uf8f0\n\r\r\r\r\r\r\nn\nX\ni=1\n\uf8eb\n\uf8ed1\n2K\n2K\nX\nk=1\n\u2207\u03b8\u03b8\u2113(xi \u2299\u03bek\ni , yi, \u03b8\u22c6\nn) \u2212\u2207\u03b8\u03b8EQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8\u22c6\nn)|X = xi, Y = yi]\n\uf8f6\n\uf8f8\n\r\r\r\r\r\r\n2\n2\n\u00b7\n\r\r\rb\u03b8\u22c6\nn(2K) \u2212\u03b8\u22c6\nn\n\r\r\r\n2\n2\n\u0015\n.\nBy H\u00a8older\u2019s inequality we have\nE[R\u22a4\nKRK]\n\u2264E\n\uf8ee\n\uf8f0\n\r\r\r\r\r\r\nn\nX\ni=1\n\uf8eb\n\uf8ed1\n2K\n2K\nX\nk=1\n\u2207\u03b8\u03b8\u2113(xi \u2299\u03bek\ni , yi, \u03b8\u22c6\nn) \u2212\u2207\u03b8\u03b8EQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8\u22c6\nn)|X = xi, Y = yi]\n\uf8f6\n\uf8f8\n\r\r\r\r\r\r\n4\n2\n\uf8f9\n\uf8fb\n1\n2\n\u00d7\nE\n\u0014\r\r\rb\u03b8\u22c6\nn(2K) \u2212\u03b8\u22c6\nn\n\r\r\r\n4\n2\n\u0015 1\n2\n\u2264O(2\u22122K).\nFinally, consider the solutions b\u03b8\u22c6\nn(2K\u22c6\nl +1), b\u03b8O\nn (2K\u22c6\nl ), b\u03b8E\nn (2K\u22c6\nl ) conditional on K\u22c6\nl = K. De-\nnote the remainder terms in (33) corresponding to the level 2K+1 solution b\u03b8\u22c6\nn(2K+1) as\nR\u22c6\nK+1, R\u22c6\nK+1,\u03b8. Similarly, denote the remainder terms in (33) corresponding to the level 2K\nsolution b\u03b8O\nn (2K) (and, respectively, b\u03b8E\nn (2K)) as RO\nK, RO\nK,\u03b8 ( RE\nK, RE\nK,\u03b8). By the construction\nof b\u03b8O\nn (2K), b\u03b8E\nn (2K) using odd and even indices, we have, from (33)\n\u2212\nn\nX\ni=1\n\u2207\u03b8\u03b8EQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8\u22c6\nn)|X = xi, Y = yi]\n\u0012\nb\u03b8\u22c6\nn(2K+1) \u22121\n2(b\u03b8O\nn (2K) + b\u03b8E\nn (2K))\n\u0013\n=R\u22c6\nK+1 \u22121\n2(RO\nK + RE\nK) + R\u22c6\nK+1,\u03b8 \u22121\n2(RO\nK,\u03b8 + RE\nK,\u03b8).\nBy Assumption 4,\nn\nX\ni=1\n\u2207\u03b8\u03b8EQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8\u22c6\nn)|X = xi, Y = yi] = n \u00b7 \u2207\u03b8\u03b8EQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8\u22c6\nn)]\n41\nBlanchet et al.\nis invertible. Thus, we have shown that\n\u00af\u2206K \u2261b\u03b8\u22c6\nn(2K+1) \u22121\n2(b\u03b8O\nn (2K) + b\u03b8E\nn (2K))\n= (n \u00b7 \u2207\u03b8\u03b8EQ\u22c6[\u2113(X \u2299\u03be, Y, \u03b8\u22c6\nn)])\u22121\n\u0012\nR\u22c6\nK+1 \u22121\n2(RO\nK + RE\nK) + R\u22c6\nK+1,\u03b8 \u22121\n2(RO\nK,\u03b8 + RE\nK,\u03b8)\n\u0013\n.\nSince each of the terms on the right-hand side have been shown to be O(2\u22122K), we conclude\nthat E[ \u00af\u2206\u22a4\nK \u00af\u2206K] = O(2\u22122K).\nA.5 Dropout Training in Linear Regression\nCorollary 7 (Linear regression with \u03c6 = 1) For linear regression with \u2113(x, y, \u03b2) = (\u03b2\u22a4x\u2212\ny)2, we have\nmin\n\u03b2\u2208Rd\nmax\nQ\u2208U(bPn,\u03b4)\nEQ\nh\u0000\u03b2\u22a4(X \u2299\u03be) \u2212Y\n\u00012i\n= min\n\u03b2\u2208Rd EQ\u22c6\nh\u0000\u03b2\u22a4(X \u2299\u03be) \u2212Y\n\u00012i\n,\nwhere Q\u22c6= bPn \u2297Q\u22c6\n1 \u2297. . . \u2297Q\u22c6\nd and Q\u22c6\nj = (1 \u2212\u03b4)\u22121 \u00d7 Bernoulli(1 \u2212\u03b4) for each j = 1, . . . , d.\nMoreover,\nmin\n\u03b2\u2208Rd EQ\u22c6\nh\u0000\u03b2\u22a4(X \u2299\u03be) \u2212Y\n\u00012i\n= min\n\u03b2\u2208Rd\n1\nn\n\u0014\n(Y \u2212X\u03b2)\u22a4(Y \u2212X\u03b2) +\n\u03b4\n1 \u2212\u03b4\u03b2\u22a4\u039b\u03b2\n\u0015\n,\n(34)\nwhich implies that the dropout training estimator equals\nb\u03b2(\u03b4) =\n\u0012\nX\u22a4X +\n\u03b4\n1 \u2212\u03b4diag(X\u22a4X)\n\u0013\u22121\nX\u22a4Y.\nFinally, if EP \u22c6[XX\u22a4] is a diagonal matrix with strictly positive entries then\nb\u03b2(\u03b4)\np\u2192(1 \u2212\u03b4)EP \u2217[XX\u22a4]\u22121E[XY ].\nProof\nThe \ufb01rst part of the corollary follows directly from (13) and (14) in our main\ntheorem. The second part of the corollary follows from Proposition 1. According to this\nproposition, the limit of b\u03b2(\u03b4) is\n\u03b2\u2217(\u03b4) =\n\u0010\nEP \u2217[XX\u22a4] + (\u03b4/1 \u2212\u03b4)diag(EP \u2217[XX\u22a4])\n\u0011\u22121\nEP \u2217[Y X].\nThus, if EP \u2217[XX\u22a4] is a diagonal matrix, we obtained the desired limit.\n42\nDropout Training is Distributionally Robust Optimal\nA.6 Additional Numerical Results\nHere we try to provide some justi\ufb01cations for our choice of parameter.\nLearning Rate: We \ufb01rst \ufb01x an all zeros initialization scheme, and vary the learning\nrate. We summarize the average parameter divergence and 1-standard deviation error for\n20 repetitions of the SGD algorithm in Table 2. We can observe the learning rate 0.0001\nshows a clear advantage.\nInitialization: Next we \ufb01x the learning rate to be 0.0001, and consider di\ufb00erent initial-\nization schemes. We note that the mean value (resp., absolute value) of elements in \u03b2\u22c6is\n0.3947 (resp., 0.6977). Table 3 shows the average parameter divergence and the 1-standard\ndeviation from 20 repetitions of the SGD algorithm. We see that the initialization at origin\nis a fair choice.\nLearning rate\n0.001\n0.0001\n0.00001\n\u2225b\u03b2SGD \u2212\u03b2\u22c6\u2225\u221e\n0.0827 \u00b1 0.0133\n0.0301 \u00b1 0.0025\n0.6702 \u00b1 0.1082\nTable 2: Comparison for di\ufb00erent learning rates, with \ufb01xed zero initializations.\nInitializations\nall zeros\nall 0.2\u2019s\nall 1\u2019s\n\u2225b\u03b2SGD \u2212\u03b2\u22c6\u2225\u221e\n0.0301 \u00b1 0.0025\n0.0317 \u00b1 0.0047\n0.0614 \u00b1 0.0196\nInitializations\ni.i.d N(0, 1)\ni.i.d N(0, 10)\ni.i.d N(0, 102)\n\u2225b\u03b2SGD \u2212\u03b2\u22c6\u2225\u221e\n0.0376 \u00b1 0.0067\n0.1006 \u00b1 0.0469\n0.3208 \u00b1 0.1432\nTable 3: Comparison for di\ufb00erent initialization schemes with \ufb01xed learning rate 0.0001.\nWall-Clock Time: We then document the numerical results for 120s/180s wall-clock\ntime, see Figures 4 - 6 for the case of 120s and Figures 7 - 9 for the case of 180s. We see\nthat the proposed unbiased approach outperforms the standard SGD when the number of\nparallel iterations reaches above some threshold.\n43\nBlanchet et al.\nFigure 4: l2 di\ufb00erence for 120s wall-clock time\nFigure 5: l\u221edi\ufb00erence for 120s wall-clock time\nFigure 6: l1 di\ufb00erence for 120s wall-clock time\n44\nDropout Training is Distributionally Robust Optimal\nFigure 7: l2 di\ufb00erence for 180s wall-clock time\nFigure 8: l\u221edi\ufb00erence for 180s wall-clock time\nFigure 9: l1 di\ufb00erence for 180s wall-clock time\n45\nBlanchet et al.\nReferences\nSule Alan, Orazio Attanasio, and Martin Browning. Estimating Euler equations with noisy\ndata: two exact GMM estimators. Journal of Applied Econometrics, 24(2):309\u2013324, 2009.\nBengt Von Bahr. On the convergence of moments in the central limit theorem. Annals of\nMathematical Statistics, 36(3):808\u2013818, 06 1965.\nChris M Bishop.\nTraining with noise is equivalent to Tikhonov regularization.\nNeural\nComputation, 7(1):108\u2013116, 1995.\nJose Blanchet, Peter Glynn, and Yanan Pei. Unbiased multilevel Monte Carlo: Stochastic\noptimization, steady-state simulation, quantiles, and other applications. arXiv preprint\narXiv:1904.09929, 2019a.\nJose Blanchet, Yang Kang, and Karthyek Murthy. Robust Wasserstein pro\ufb01le inference and\napplications to machine learning. Journal of Applied Probability, 56:830\u2013857, 2019b.\nStephen Boyd and Lieven Vandenberghe.\nConvex Optimization.\nCambridge University\nPress, 2004.\nTimothy Christensen and Benjamin Connault. Counterfactual sensitivity and robustness.\narXiv preprint arXiv:1904.00989, 2019.\nA. DasGupta. Asymptotic Theory of Statistics and Probability. Springer Verlag, 2008.\nErick Delage and Yinyu Ye. Distributionally robust optimization under moment uncertainty\nwith application to data-driven problems. Operations Research, 58(3):595\u2013612, 2010.\nDavid Draper. Assessment and propagation of model uncertainty. Journal of the Royal\nStatistical Society, Series B, 56, 1994.\nRick Durrett. Probability: Theory and Examples. Cambridge University Press, 2019.\nLudwig Fahrmeir and Heinz Kaufmann.\nConsistency and asymptotic normality of the\nmaximum likelihood estimator in generalized linear models. The Annals of Statistics,\npages 342\u2013368, 1985.\nMax H Farrell, Tengyuan Liang, and Sanjog Misra. Deep neural networks for estimation\nand inference. Forthcoming at Econometrica, 2020.\nT.S. Ferguson. Mathematical Statistics: A Decision Theoretic Approach, volume 7. Aca-\ndemic Press New York, 1967.\n46\nDropout Training is Distributionally Robust Optimal\nMichael B Giles.\nMultilevel Monte Carlo path simulation.\nOperations Research, 56(3):\n607\u2013617, 2008.\nMichael B Giles. Multilevel Monte Carlo methods. Acta Numerica, 24:259, 2015.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\nLars Peter Hansen and Thomas J. Sargent. Robustness. Princeton University Press, 2008.\nDavid P Helmbold and Philip M Long. On the inductive bias of dropout. The Journal of\nMachine Learning Research, 16(1):3403\u20133454, 2015.\nGeo\ufb00rey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R\nSalakhutdinov. Improving neural networks by preventing co-adaptation of feature de-\ntectors. arXiv preprint arXiv:1207.0580, 2012.\nKurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks\nare universal approximators. Neural Networks, 2(5):359\u2013366, 1989.\nJiunn T Hwang. Multiplicative errors-in-variables models with applications to recent data\nreleased by the us department of energy. Journal of the American Statistical Association,\n81(395):680\u2013688, 1986.\nJ Kim and W Winkler. Multiplicative noise for masking continuous data. Statistics, 1:9,\n2003.\nRobert H Lyles and Lawrence L Kupper. A detailed evaluation of adjustment methods for\nmultiplicative measurement error in linear regression with applications in occupational\nepidemiology. Biometrics, pages 1008\u20131025, 1997.\nLaurens Maaten, Minmin Chen, Stephen Tyree, and Kilian Weinberger.\nLearning with\nmarginalized corrupted features. In International Conference on Machine Learning, pages\n410\u2013418, 2013.\nPeter McCullagh and J.A. Nelder. Generalized Linear Models. Chapman & Hall, 1989.\nOskar Morgenstern and John von Neumann. Theory of Games and Economic Behavior.\nPrinceton University Press, 1953.\nTapan K Nayak, Bimal Sinha, and Laura Zayatz. Statistical properties of multiplicative\nnoise masking for con\ufb01dentiality protection. Journal of O\ufb03cial Statistics, 27(3):527, 2011.\nA. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation\napproach to stochastic programming. SIAM Journal on Optimization, 19(4):1574\u20131609,\n2009.\n47\nBlanchet et al.\nWhitney K Newey and Daniel McFadden. Large sample estimation and hypothesis testing.\nHandbook of Econometrics, 4:2111\u20132245, 1994.\nViet Anh Nguyen, Xuhui Zhang, Jose Blanchet, and Angelos Georghiou. Distributionally\nrobust parametric maximum likelihood estimation. In Advances in Neural Information\nProcessing Systems 33, 2020.\nDonald A Pierce, Daniel O Stram, Michael Vaeth, and Daniel W Schafer. The errors-in-\nvariables problem: considerations provided by radiation dose-response analyses of the\na-bomb survivor data. Journal of the American Statistical Association, 87(418):351\u2013359,\n1992.\nAdrian E Raftery, David Madigan, and Jennifer A Hoeting.\nBayesian model averaging\nfor linear regression models. Journal of the American Statistical Association, 92(437):\n179\u2013191, 1997.\nHamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review.\narXiv preprint arXiv:1908.05659, 2019.\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of\nMathematical Statistics, pages 400\u2013407, 1951.\nH. Scarf. A min-max solution of an inventory problem. Studies in the Mathematical Theory\nof Inventory and Production, 10:201\u2013209, 1958.\nJohannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU\nactivation function. The Annals of Statistics, 48(4):1875 \u2013 1897, 2020.\nAlexander Shapiro.\nDistributionally robust stochastic programming.\nSIAM Journal on\nOptimization, 27(4):2258\u20132275, 2017.\nAlexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy\u00b4nski. Lectures on Stochastic\nProgramming: Modeling and Theory. SIAM, 2014.\nNitish Srivastava, Geo\ufb00rey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. Dropout: a simple way to prevent neural networks from over\ufb01tting. The Journal\nof Machine Learning Research, 15(1):1929\u20131958, 2014.\nStefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization.\nIn Advances in Neural Information Processing Systems 26, pages 351\u2013359. 2013.\nMartin J Wainwright and Michael Irwin Jordan. Graphical Models, Exponential Families,\nand Variational Inference. Now Publishers Inc, 2008.\n48\nDropout Training is Distributionally Robust Optimal\nSida Wang and Christopher Manning. Fast dropout training. In Proceedings of the 30th\nInternational Conference on Machine Learning, pages 118\u2013126, 2013.\nColin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization e\ufb00ects\nof dropout. In Proceedings of the International Conference of Machine Learning, 2020.\nWolfram Wiesemann, Daniel Kuhn, and Melvyn Sim. Distributionally robust convex opti-\nmization. Operations Research, 62(6):1358\u20131376, 2014.\nMartin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic\ngradient descent. In Advances in Neural Information Processing Systems, pages 2595\u2013\n2603, 2010.\n49\n"}