{"text": "arXiv:1605.04055v1  [stat.ME]  13 May 2016\nBayesian D-optimal designs for error-in-variables models\nto the memory of Kathryn Chaloner\nMaria Konstantinou and Holger Dette\nRuhr-Universit\u00a8at Bochum\nFakult\u00a8at f\u00a8ur Mathematik\n44780 Bochum, Germany\nAbstract\nBayesian optimality criteria provide a robust design strategy to parameter misspeci-\n\ufb01cation. We develop an approximate design theory for Bayesian D-optimality for non-\nlinear regression models with covariates subject to measurement errors. Both maximum\nlikelihood and least squares estimation are studied and explicit characterisations of the\nBayesian D-optimal saturated designs for the Michaelis-Menten, Emax and exponential\nregression models are provided. Several data examples are considered for the case of no\npreference for speci\ufb01c parameter values, where Bayesian D-optimal saturated designs are\ncalculated using the uniform prior and compared to several other designs, including the\ncorresponding locally D-optimal designs, which are often used in practice.\nKeywords: error-in-variables models, classical errors, Bayesian optimal designs, D-optimality\nAMS Subject Classi\ufb01cation: 62K05\n1\nIntroduction\nLocally optimal designs, as termed by Cherno\ufb00[1953], depend on the model parameters when\nthe model generating the data is nonlinear. In many cases these parameters are unknown at\nthe design stage and therefore, a best guess of the parameter values is required for the locally\noptimal designs to be used in practice. This approach however, can result in ine\ufb03cient designs\n1\nif the parameters are misspeci\ufb01ed. Hence there is the need to overcome this dependence and\nconstruct robust designs that estimate the model parameters with high precision and thus\nperform well even when there is imperfect knowledge of the true parameter values.\nIn many practical situations some information about the parameter values, such as a range of\nplausible values, can be provided by the experimenter. Based on such an uncertainty space a\nrobust design strategy is that of Bayesian optimal designs introduced by Pronzato and Walter\n[1985], Chaloner [1989] and Chaloner and Larntz [1989]. Bayesian optimality incorporates the\nparameter uncertainty in the formulation of the optimality criteria through a prior distribution\non the parameter space and the proposed criteria are based on classical optimality criteria (see,\nfor example, Chaloner [1993] and Chaloner and Verdinelli [1995] for more details). Therefore,\nmany of the well established results of classical design theory can be directly extended to the\nBayesian framework. The construction of Bayesian optimal designs for several regression models\nhas been studied by many authors such as Chaloner and Larntz [1992], Dette and Neugebauer\n[1997], Han and Chaloner [2003], Dette et al. [2007] and Burghaus and Dette [2014].\nIn this paper we investigate Bayesian optimal designs for a class of error-in-variables models,\nthat is, of regression models where one or more of the covariates involved cannot be observed\ndirectly. The relationship between the true (unobserved) and observed covariates is described\nby the error model and according to its structure a distinction is made between the classical\nand Berkson errors. For a detailed review see, for example, Fuller [1987] and Carroll et al.\n[1995]. Our focus is on classical errors which include the sampling and instrument recording\nerrors frequently arising in practice.\nDespite of their importance, the literature on optimal designs for error-in-variables models with\nclassical errors is rather scarce [see Keeler and Reilly [1992] and Dovi et al. [1993] for early\nreferences]. Recently, Konstantinou and Dette [2015] develop an approximate optimal design\ntheory for local optimality criteria in error-in-variables models with classical errors and provide\nanalytical results on locally D-optimal designs for some commonly used nonlinear models when\nthese are subject to the classical error structure. This paper extends their work and provides\nthe corresponding approximate design theory for Bayesian optimality. We thus obtain designs\nwhich are optimal for parameter estimation and robust over the speci\ufb01ed parameter space.\nIn Section 2 we introduce the approximate design problem in the context of error-in-variables\nmodels subject to classical errors and present the limiting properties of the maximum likelihood\nand least squares estimators. The approximate design theory for Bayesian optimality is then\nprovided in Section 3 along with the general equivalence theorem and a su\ufb03cient condition\nfor Bayesian D-optimality for maximum likelihood and least squares estimation, respectively.\nIn Section 4 we provide analytical characterisations of Bayesian D-optimal saturated designs\nfor the Michaelis-Menten, Emax and exponential regression models when these are subject to\nclassical errors. Finally, in Section 5 we consider the case of a uniform prior on the parameter\nspace. Via several data examples, we establish the superiority of the resulting Bayesian D-\n2\noptimal designs by comparing them to the corresponding locally D-optimal designs, explicitly\nde\ufb01ned in Konstantinou and Dette [2015], as well as to other designs frequently used in practice.\n2\nApproximate designs and parameter estimation\nWe assume that the observations are generated by a nonlinear model and consider a repeated\nobservations set-up under which a total of ri (i = 1, . . . , n) measurements are taken at each\nof the \ufb01xed experimental conditions x1, . . . , xn.\nWe further assume that one is unable to\nobserve the true covariate values xi\u2019s, i = 1, . . . , n, directly due to measurement errors such as\nsampling and instrumental error. Therefore, a classical error model specifying the conditional\ndistribution of the observed given the true (unobserved) covariates is considered. Throughout\nthis paper we assume classical additive errors, that is,\nYij = m(xi, \u03b8) + \u03b7ij,\ni = 1, . . . , n;\nj = 1, . . . , ri\nXij = xi + \u03b5ij,\n(2.1)\nwhere \u03b8 = (\u03b80, . . . , \u03b8p)T is the vector of unknown model parameters, xi = (xi1, . . . , xiq)T \u2208X\nis the vector of true covariates with X \u2282Rq denoting the design space and Xij denotes the\nobserved vector of the jth repeated measurement at the ith experimental condition. Further-\nmore, the vectors (\u03b7ij, \u03b5ij)T of response errors \u03b7ij and covariate errors \u03b5ij are assumed to be\nindependent and identically normally distributed with mean 0 and variance covariance ma-\ntrix \u03a3\u03b7\u03b5 being positive de\ufb01nite and the regression function m(x, \u03b8) is continuous and twice\ndi\ufb00erentiable with respect to both x and \u03b8.\nWe consider approximate designs in the sense of Kiefer [1974] which are de\ufb01ned as probability\nmeasures on the design space X with \ufb01nite support. Using the limiting relation\nlim\nri\u2192\u221e\nri\nr = \u03c9i > 0,\ni = 1, . . . , n,\nwhere r = Pn\ni=1 ri denotes the total sample size, an approximate design is of the form\n\u03be =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nx1 . . . xn\n\u03c91 . . . \u03c9n\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8f4\n\uf8fe\n,\n0 < \u03c9i \u22641,\nn\nX\ni=1\n\u03c9i = 1,\nwhere the xi\u2019s and \u03c9i\u2019s are called support points and weights of the design, respectively. The\ngoal of the experiment is to estimate the parameters of the underlying model in (2.1) involving\nthe true covariates. In an error-in-variables models set-up however, the true covariate values are\nunobservable and thus unknown. Therefore, an approximate design provides the experimenter\nwith target values for the true covariates xi, i = 1, . . . , n which he would then try to achieve\nthrough the observed covariate values Xij, i = 1, . . . , n; j = 1, . . . , ri.\n3\nFollowing the methodology in Fuller [1987], Konstantinou and Dette [2015] derived the asymp-\ntotic properties of the maximum likelihood and least squares estimators for the parameter\nvector denoted by \u02c6\u03b8ML and \u02c6\u03b8LS respectively. In particular, under assumptions of regularity,\n\u221ar(\u02c6\u03b8ML \u2212\u03b8true)\nL\u2212\u2192N(0, M\u22121\nML(\u03be, \u03b8)),\nand\n\u221ar(\u02c6\u03b8LS \u2212\u03b8true)\nL\u2212\u2192N(0, M\u22121\nLS(\u03be, \u03b8)),\nwhere\nL\u2212\u2192denotes convergence in distribution and the information matrices MML(\u03be, \u03b8) and\nMLS(\u03be, \u03b8) are given by\nMML(\u03be, \u03b8) =\nZ\nX\n1\n\u03c31(x, \u03b8)\n\u0010\u2202m(x, \u03b8)\n\u2202\u03b8\n\u0011\u0010\u2202m(x, \u03b8)\n\u2202\u03b8\n\u0011T\nd\u03be(x)\n(2.2)\nMLS(\u03be, \u03b8) = D0(\u03be, \u03b8)D\u22121\n1 (\u03be, \u03b8)D0(\u03be, \u03b8),\n(2.3)\nwith\nDk(\u03be, \u03b8) =\nZ\nX\n[\u03c31(x, \u03b8)]k\n\u03c30(x, \u03b8)\n\u0010\u2202m(x, \u03b8)\n\u2202\u03b8\n\u0011\u0010\u2202m(x, \u03b8)\n\u2202\u03b8\n\u0011T\nd\u03be(x),\nk = 0, 1,\n(2.4)\n\u03c3k(x, \u03b8) =\n\u0012\n1,\n\u0010\u2202m(x, \u03b8)\n\u2202x\n\u0011T\u0013\n(\u03a3\u03b7\u03b5)k\n\u0012\n1,\n\u0010\u2202m(x, \u03b8)\n\u2202x\n\u0011T\u0013T\n,\nk = 0, 1.\n(2.5)\n3\nBayesian D-optimal saturated designs\nA locally optimal design maximises an appropriate concave functional of the information matrix,\nhere MML(\u03be, \u03b8) or MLS(\u03be, \u03b8), called an optimality criterion. In general, locally optimal designs\ndepend on the unknown parameter vector \u03b8 which must be speci\ufb01ed for their implementation.\nThe Bayesian approach on the other hand, takes into account any prior information available\nfor \u03b8 leading to more robust optimality criteria.\nWe consider the construction of Bayesian D-optimal designs introduced by Pronzato and Walter\n[1985] and Chaloner and Larntz [1989]. Let \u03b8 \u2208\u0398, where \u0398 \u2282Rp+1 and also let \u03c0 denote a\nprior distribution on the parameter space \u0398. A design \u03be\u2217\n\u03c0 is called Bayesian D-optimal with\nrespect to the prior \u03c0 for models of the form (2.1) if it maximises the function\n\u03a6\u03c0(\u03be) =\nZ\n\u0398\nlog |M(\u03be, \u03b8)| \u03c0(d\u03b8),\n(3.1)\nwhere the information matrix M(\u03be, \u03b8) is that corresponding to maximum likelihood or least\nsquares estimation, given in equations (2.2) and (2.3) respectively, according to the preferable\nestimation method for the parameter vector.\nIn the case of maximum likelihood estimation the criterion (3.1) is concave with respect to the\ndesign \u03be. Hence using Theorem 3.3 in Dette et al. [2007], the general equivalence theorem for\n4\ncharacterising and checking Bayesian D-optimality of a candidate design for models of the form\n(2.1) is given below.\nTheorem 3.1. A design \u03be\u2217\n\u03c0 is Bayesian D-optimal with respect to the prior \u03c0 for maximum\nlikelihood estimation in model (2.1) if and only if the inequality\nZ\n\u0398\ndML(x, \u03be\u2217\n\u03c0, \u03b8) \u03c0(d\u03b8) :=\nZ\n\u0398\n\u0010\u2202m(x, \u03b8)\n\u2202\u03b8\n\u0011T M\u22121\nML(\u03be\u2217\n\u03c0, \u03b8)\n\u03c31(x, \u03b8)\n\u0010\u2202m(x, \u03b8)\n\u2202\u03b8\n\u0011\n\u03c0(d\u03b8) \u2264p + 1,\nholds for all x \u2208X . Furthermore, the maximum is achieved at the support points of \u03be\u2217\n\u03c0.\nOn the other hand, when the vector of model parameters \u03b8 is estimated via least squares the\nmapping \u03be \u2192MLS(\u03be, \u03b8) and thus the optimality criterion (3.1) is not concave. However, the\nfollowing theorem provides a necessary condition for Bayesian D-optimality. That is, a design\nthat does not satisfy this condition cannot be Bayesian D-optimal.\nTheorem 3.2. If the design \u03be\u2217\n\u03c0 is Bayesian D-optimal with respect to the prior \u03c0 for least\nsquares estimation in model (2.1), then the inequality\nZ\n\u0398\ndLS(x, \u03be\u2217\n\u03c0, \u03b8) \u03c0(d\u03b8) :=\nZ\n\u0398\n2d0(x, \u03be\u2217\n\u03c0, \u03b8) \u2212\u03c31(x, \u03b8)d1(x, \u03be\u2217\n\u03c0, \u03b8) \u03c0(d\u03b8) \u2264p + 1,\nholds for all x \u2208X , where\ndk(x, \u03be\u2217\n\u03c0, \u03b8) =\n\u0010\u2202m(x, \u03b8)\n\u2202\u03b8\n\u0011T D\u22121\nk (\u03be\u2217\n\u03c0, \u03b8)\n\u03c30(x, \u03b8)\n\u0010\u2202m(x, \u03b8)\n\u2202\u03b8\n\u0011\n\u03c0(d\u03b8),\nk = 0, 1.\nFurthermore, the maximum is achieved at the support points of \u03be\u2217\n\u03c0.\nFor purposes of comparison with the corresponding locally D-optimal designs found in Konstantinou and Det\n[2015], in what follows we study saturated designs. These are designs that have the same num-\nber of support points as the dimension p + 1 of the parameter vector \u03b8. Lemma 3.1 shows\nthat regardless of the estimation method the Bayesian D-optimal saturated design is equally\nweighted.\nLemma 3.1. The Bayesian D-optimal saturated design with respect to the prior \u03c0 for maximum\nlikelihood or least squares estimation in model (2.1), puts equal weights at its support points.\nRemark 3.1. If we also take into account uncertainty on the response and covariate errors\nwhich are assumed to be (\u03b7ij, \u03b5ij)T\niid\n\u223cN(0, \u03a3\u03b7\u03b5) (i = 1, . . . , n; j = 1, . . . , ri), the Bayesian\nD-optimality criterion becomes\n\u03a6\u03c01,\u03c02(\u03be) =\nZ\n\u03a3\nZ\n\u0398\nlog |M(\u03be, \u03b8)| \u03c01(d\u03b8) \u03c02(d\u03a3\u03b7\u03b5),\n(3.2)\nwhere \u03c01 is a prior distribution on the parameter space \u0398 and \u03c02 is a prior distribution on\nthe space of positive de\ufb01nite covariance matrices \u03a3. Then for a Bayesian D-optimal design\nwith respect to the priors \u03c01 and \u03c02 maximising (3.2), Lemma 3.1 still holds. Furthermore, the\nstatements of Theorems 3.1 and 3.2 are also true, where the integration has to be performed\nalso with respect to the prior \u03c02.\n5\n4\nApplication to speci\ufb01c nonlinear models\nIn this section we specify the underlying regression function m(xi, \u03b8) and in particular, we\nconsider three nonlinear models widely used in applications for the modelling of the dose-\nresponse relationship. Namely, we consider we consider the Michaelis-Menten and Emax models\nspeci\ufb01ed by\nm1(x, \u03b8) =\n\u03b81x\n(\u03b82 + x),\n(x \u2208[0, xu] \u2282R+\n0 ),\n(4.1)\nand\nm2(x, \u03b8) = \u03b80 +\n\u03b81x\n\u03b82 + x,\n(x \u2208[0, xu] \u2282R+\n0 ),\n(4.2)\nrespectively, and also the three-parameter exponential regression model given by\nm3(x, \u03b8) = \u03b80 + \u03b81e\u2212\u03b82x\n(x \u2208[0, xu] \u2282R+\n0 ).\n(4.3)\nIn the Michaelis-Menten model the parameter \u03b81 > 0 is the maximum achievable response and\n\u03b82 > 0 is the dose x where the response is half-maximal. Similarly in the Emax model, \u03b81 and\n\u03b82 are the asymptotic maximum increase of the response and the dose producing half of the\nasymptotic maximum e\ufb00ect respectively and \u03b80 \u22650 is the placebo e\ufb00ect, that is, the response\nat dose x = 0. Finally, for the three-parameter exponential regression model the parameter\n\u03b81 > 0 is involved in the placebo e\ufb00ect along with \u03b80 \u22650 and \u03b82 \u2208R\\{0} describes the rate\nof the dose e\ufb00ect. The construction of optimal designs in the case of no measurement error in\nthe covariates has been discussed by Dette et al. [2010], Rasch [1990] and Han and Chaloner\n[2003] for the Michaelis-Menten, the Emax and the exponential regression model, respectively,\namong others.\nFor the sake of simplicity we further assume that for these univariate models the response\nand covariate errors are uncorrelated. Therefore, the variance-covariance matrix of the vector\nmeasurement errors given in (2.1) and the \u03c3-functions de\ufb01ned in (2.5) become\n\u03a3\u03b7\u03b5 =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\u03c32\n\u03b7\n0\n0 \u03c32\n\u03b5\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8,\n\u03c30(x, \u03b8) = 1 +\n\u0010\u2202m(x, \u03b8)\n\u2202x\n\u00112\n,\n\u03c31(x, \u03b8) = \u03c32\n\u03b7 +\n\u0010\u2202m(x, \u03b8)\n\u2202x\n\u00112\n\u03c32\n\u03b5.\nIn the following two theorems we derive the Bayesian D-optimal saturated designs for maximum\nlikelihood estimation in each of the aforementioned nonlinear models with measurement errors\nas in (2.1). Using these analytical characterisations the design problem is reduced to \ufb01nding\nthe solution of an equation in one variable and therefore, the numerical e\ufb00ort for design search\nreduces substantially.\nTheorem 4.1. The Bayesian D-optimal saturated design with respect to a prior \u03c0 for maximum\nlikelihood estimation in the Michaelis-Menten model (4.1) with measurement errors as in (2.1)\n6\nis equally supported at points x\u2217\n1 and xu, whereas for the Emax model (4.2) with measurement\nerrors as in (2.1) it is equally supported at points 0, x\u2217\n1 and xu. The non-trivial support point\nx\u2217\n1 \u2208(0, xu) is a solution of the equation\nZ\n\u0398\n1\nx1\n\u2212\n1\nxu \u2212x1\n\u2212\n2(\u03b82 + x1)3\n(\u03b82 + x1)4 + \u03b82\n1\u03b82\n2\u033a2\n\u03b5\u03b7\n\u03c0(d\u03b8) = 0,\n(4.4)\nin the interval (0, xu), where \u033a2\n\u03b5\u03b7 = \u03c32\n\u03b5/\u03c32\n\u03b7.\nTheorem 4.2. The Bayesian D-optimal saturated design with respect to a prior \u03c0 for maximum\nlikelihood estimation in the exponential regression model (4.3) with measurement errors as in\n(2.1) is equally supported at points 0, x\u2217\n1 and xu. The non-trivial support point x\u2217\n1 \u2208(0, xu) is\na solution of the equation\nZ\n\u0398\n1 \u2212e\u03b82xu + \u03b82xue\u03b82x1\nx1 \u2212xu + xue\u03b82x1 \u2212x1e\u03b82xu \u2212\n\u03b82e2\u03b82x1\ne2\u03b82x1 + \u03b82\n1\u03b82\n2\u033a2\u03b5\u03b7\n\u03c0(d\u03b8) = 0,\n(4.5)\nin the interval (0, xu), where \u033a2\n\u03b5\u03b7 = \u03c32\n\u03b5/\u03c32\n\u03b7.\nThe corresponding analytical results for Bayesian D-optimal saturated designs for least squares\nestimation are given below. We note that in the case of Theorem 4.4 the two non-trivial support\npoints of the design can only be evaluated using numerical optimisation.\nTheorem 4.3. The Bayesian D-optimal saturated design with respect to a prior \u03c0 for least\nsquares estimation in the Michaelis-Menten model (4.1) with measurement errors as in (2.1)\nputs equal masses at points x\u2217\n1 and xu, whereas for the Emax model (4.2) with measurement\nerrors as in (2.1) it puts equal masses at points 0, x\u2217\n1 and xu. The non-trivial support point\nx\u2217\n1 \u2208(0, xu) is a solution of the equation\nZ\n\u0398\n1\nx1\n\u2212\n1\nxu \u2212x1\n\u2212\n2(\u03b82 + x1)3\n(\u03b82 + x1)4 + \u03b82\n1\u03b82\n2\u033a2\u03b5\u03b7\n+\n2\u03b82\n1\u03b82\n2\n(\u03b82 + x1)[(\u03b82 + x1)4 + \u03b82\n1\u03b82\n2] \u03c0(d\u03b8) = 0,\n(4.6)\nin the interval (0, xu) and \u033a2\n\u03b5\u03b7 = \u03c32\n\u03b5/\u03c32\n\u03b7.\nTheorem 4.4. The Bayesian D-optimal saturated design with respect to a prior \u03c0 for least\nsquares estimation in the exponential regression model (4.3) with measurement errors as in\n(2.1) is always supported at the larger end-point xu of the design space.\nRemark 4.1. With the assumption of uncorelated response and covariate errors, the prior \u03c02\non the space of matrices \u03a3 reduces to a prior on the space of the corresponding error variances\n\u03c32\n\u03b7 and \u03c32\n\u03b5, which is a subset of R2. Using the Bayesian D-optimality criterion (3.2) instead\nof (3.1), does not a\ufb00ect the characteristics of the Bayesian D-optimal saturated designs with\nrespect to \u03c01 and \u03c02 which remain the same as described in Theorems 4.1-4.4. In each case,\n7\nthe value of the non-trivial support point does change however, as it is a solution of a di\ufb00erent\nequation. For example, in the case of Theorem 4.1, the analogous of equation (4.4) is\nZ\nR+\nZ\n\u0398\n1\nx1\n\u2212\n1\nxu \u2212x1\n\u2212\n2(\u03b82 + x1)3\n(\u03b82 + x1)4 + \u03b82\n1\u03b82\n2\u033a2\u03b5\u03b7\n\u03c01(d\u03b8) \u03c02(d\u033a2\n\u03b5\u03b7) = 0,\nwhere here \u03c02 is a prior on the ratio \u033a2\n\u03b5\u03b7 = \u03c32\n\u03b5/\u03c32\n\u03b7 which is induced by the given prior on the\nspace of error variances.\n5\nData example\nThe theoretical results of the previous section are now illustrated via several data examples. In\nwhat follows, a uniform prior distribution is used on the parameter space corresponding to the\ncase of no preference for speci\ufb01c parameter values. Under this concept there is no need for the\nexperimenter to specify a prior, thereby avoiding a step that is often di\ufb03cult in practice. A\nnumber \u03bd of equally spaced values are taken from each of the parameter\u2019s uncertainty intervals\nand the resulting prior points on the entire parameter space are equally likely to be observed.\nWe note that when the \u201ctrue\u201d parameter values are not speci\ufb01ed (Tables 1 and 4) the e\ufb03ciency\nof a design \u03be is calculated via\ne\ufb00(\u03be) =\nexp{\n1\np+1\u03a6\u03c0(\u03be)}\nexp{\n1\np+1\u03a6\u03c0(\u03be\u2217\nBay)} = exp\nn\n1\np + 1[\u03a6\u03c0(\u03be) \u2212\u03a6\u03c0(\u03be\u2217\nBay)]\no\n,\n(5.1)\nwhere \u03be\u2217\nBay is the Bayesian D-optimal saturated design for the prior \u03c0 and a speci\ufb01c error-ratio\nvalue \u033a2\n\u03b5\u03b7. Finally, when nominal values for the parameters are considered for the calculations\n(Tables 2 and 3), we use the D-e\ufb03ciency de\ufb01ned for a design \u03be as\ne\ufb00D(\u03be) =\n\u0012 det{M(\u03be, \u03b8)}\ndet{M(\u03be\u2217\n\u03b8, \u03b8)}\n\u00131/p+1\n,\n(5.2)\nwhere p + 1 is the number of model parameters and \u03be\u2217\n\u03b8 is the locally D-optimal design for\nerrors-in-variables models with classical errors using the parameter values vector \u03b8, explicitly\nde\ufb01ned in Konstantinou and Dette [2015].\nWe begin with an investigation of how the Bayesian D-optimal saturated designs change in\nthe presence of error in the covariates. For this purpose we consider an example discussed\nin Mihara et al. [2000].\nThese authors model the velocity of a biochemical reaction (CSD-\nplus pyrovate) with respect to the concentration of a substrate (L-cysteine sul\ufb01nate) via the\nMichaelis-Menten model. The design space in this example is X = [0, 80] and the obtained\nparameter estimates are (\u03b81, \u03b82) = (16, 3.5). However, in their set-up Mihara et al. [2000] do\nnot take into account possible errors in the measurement of the substrate concentration. Such\n8\nerrors are the result of instrument recording errors which, as mentioned in the introduction,\ncorrespond to the classical error structure.\nTo study the case where a parameter space is provided by the experimenter, we use the estimates\nstated above as a starting point for the choice of an uncertainty space.\nIn particular, we\nconsider the parameter space \u0398 = [8, 24] \u00d7 [1.75, 5.25] which corresponds to the \u00b150% region\naround the point of parameter estimates. Using Theorems 4.1 and 4.3 we \ufb01nd the Bayesian\nD-optimal two-point designs for maximum likelihood and least squares estimation respectively\nin the Michaelis-Menten model (4.1) with classical errors as in (2.1). Table 1 presents the\nsupport points of these designs for various values of the error-variances ratio \u033a2\n\u03b5\u03b7 = \u03c32\n\u03b5/\u03c32\n\u03b7. For\nthese calculations two uniform priors are considered, using \u03bd = 5 and \u03bd = 11 equally-spaced\nvalues from each of the parameter space intervals [8, 24] and [1.75, 5.25]. Note that the point\n(\u03b81, \u03b82) = (16, 3.5) of the parameter estimates is included in the resulting prior points for both\npriors. We also calculate the e\ufb03ciencies (5.1) of the Bayesian designs assuming no error in the\ncovariates, that is, \u033a2\n\u03b5\u03b7 = 0 which are also given in Table 1. Using a uniform prior with \u03bd = 11\nthis design is equally supported at points 3.06 and 80 for maximum likelihood estimation and\nat points 5.82 and 80 when the parameters are estimated via least squares.\nAs for the case of locally D-optimal designs discussed in Konstantinou and Dette [2015], taking\nthe error in the covariate into account results in the non-trivial support point of the Bayesian\nD-optimal design to move further away from x = 0 compared to its value when \u033a2\n\u03b5\u03b7 is assumed\nto be zero. As the \u033a2\n\u03b5\u03b7-value becomes larger, the value of the non-trivial support point increases\nfurther. The choice of estimation method also seems to have an e\ufb00ect on the Bayesian D-optimal\ndesign with the non-trivial support point of the design for least squares estimation always being\nlarger. Furthermore, the Bayesian D-optimal design assuming no error in the covariate has\ne\ufb03ciency less that 90% for some error-variances ratio values \u033a2\n\u03b5\u03b7 > 0. Most importantly, even\nif the covariate error variance \u03c32\n\u03b5 is small but equal to the response error variance \u03c32\n\u03b7 (hence\n\u033a2\n\u03b5\u03b7 = 1), the e\ufb03ciency of the Bayesian design ignoring the covariate error is 82.44% if the\nparameters are estimated via maximum likelihood. Therefore, the usual strategy of ignoring\nthe covariate error if it is believed to be small can result in ine\ufb03cient designs. We \ufb01nally note\nthat the Bayesian design using \u03bd = 11 turned out to have e\ufb03ciencies of approximately 100%\nwhen compared with the corresponding Bayesian design for \u03bd = 101. Hence for the rest of this\nsection the uniform prior with \u03bd = 11 is used.\nWe now assess the robustness of Bayesian D-optimal saturated designs against misspeci\ufb01cations\nof the model parameters. Konstantinou and Dette [2015] use data discussed in Frisillo and Stewart\n[1980] on how the wave velocity of ultrasonic signals relates to the percent gas-brine saturation.\nIn this example the use of error-in-variables models is justi\ufb01ed by the possible false measure-\nment of the intensity of an X-ray beam, which is then used to determine the percentage of\ngas-brine saturation (see Frisillo and Stewart [1980] for more details). Konstantinou and Dette\n[2015] \ufb01t the exponential regression model (4.3) to the data and obtain the parameter estimates\n9\nTable 1: Left part: Support points of the Bayesian D-optimal two-point designs using a uniform\nprior with \u03bd = 5 or \u03bd = 11 values for each parameter. Right part: E\ufb03ciencies (%) of the\nBayesian D-optimal two-point design (\u03bd = 11) for \u033a2\n\u03b5\u03b7 = 0.\nSupport points\n\u033a2\n\u03b5\u03b7\nEstimation Method\n\u03bd = 5\n\u03bd = 11\nE\ufb03ciencies (%)\n4/1\nMLE\n(8.02,80)\n(8.12,80)\n62.92\nLSE\n(9.14,80)\n(9.21,80)\n84.68\n2/1\nMLE\n(6.79,80)\n(6.86,80)\n72.96\nLSE\n(8.14,80)\n(8.19,80)\n91.48\n1/1\nMLE\n(5.77,80)\n(5.82,80)\n82.44\nLSE\n(7.36,80)\n(7.40,80)\n95.97\n1/2\nMLE\n(4.94,80)\n(4.99,80)\n90.11\nLSE\n(6.78,80)\n(6.82,80)\n98.38\n1/4\nMLE\n(4.30,80)\n(4.34,80)\n95.26\nLSE\n(6.37,80)\n(6.42,80)\n99.44\n(\u03b80, \u03b81, \u03b82) = (1210, 66.07, 0.0696). Then using these estimates and assuming that \u033a2\n\u03b5\u03b7 = 1, they\n\ufb01nd the locally D-optimal designs on the design space X = [0, 35] for model (4.3) with errors\nas in (2.1), which are given by\n\u03beMLE\nloc\n=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n0\n17.23 35\n1/3\n1/3\n1/3\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8,\n\u03beLSE\nloc\n=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n1.26 21.54 35\n1/3\n1/3\n1/3\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8,\n(5.3)\nfor maximum likelihood and least squares estimation respectively.\nFor the construction of the corresponding Bayesian D-optimal designs we consider the param-\neter space \u0398 = {1210} \u00d7 [33, 100] \u00d7 [0.01, 0.3]. The estimate for \u03b80 is used since this parameter\ndoes not a\ufb00ect the design and the interval used for \u03b81 corresponds roughly to the \u00b150% interval\naround its estimate. The estimate for the parameter \u03b82 = 0.0696 suggests a very small dose ef-\nfect. Hence the corresponding uncertainty interval is chosen such that larger rates of dose e\ufb00ect\n10\nare considered. Theorem 4.2 provides a complete analytical characterisation of the Bayesian\nD-optimal three-point design for maximum likelihood estimation. In the case of least squares\nestimation Theorem 4.4 only provides us with the larger support point and the weights of the\ndesign and thus the other two support points were found numerically using the Particle Swarm\nOptimisation (PSO) algorithm (see, for example, Clerc [2006]). Assuming that \u033a2\n\u03b5\u03b7 = 1, the\nBayesian D-optimal three-point designs for maximum likelihood and least squares estimation\nin model (4.3) with errors as in (2.1) are given by\n\u03beMLE\nBay\n=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n0\n11.59 35\n1/3\n1/3\n1/3\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8,\n\u03beLSE\nBay =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n6.79 16.33 35\n1/3\n1/3\n1/3\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8.\n(5.4)\nThe following two tables present the e\ufb03ciencies of the locally and Bayesian D-optimal designs,\ngiven in (5.3) and (5.4) respectively, along with the e\ufb03ciencies of the uniform design \u03beuni\nallocating equal weights at points 0, 17.5 and 35, for the four end-points of the parameter\nspace.\nWe thus examine the e\ufb03ciencies (5.2) of these designs when the \u201ctrue\u201d parameter\nvalues are equal to either one of the extreme values of their corresponding uncertainty interval.\nTable 2 presents the e\ufb03ciencies for the case of maximum likelihood estimation whereas the\ncorresponding results for least squares estimation are given in Table 3.\nTable 2: E\ufb03ciencies (%) of the locally and Bayesian D-optimal three-point designs for maxi-\nmum likelihood estimation assuming \u033a2\n\u03b5\u03b7 = 1 and of the uniform design on X = [0, 35].\nE\ufb03ciencies (%)\n(\u03b81, \u03b82)\n\u03beMLE\nloc\n\u03beMLE\nBay\n\u03beuni\n(33, 0.01)\n99.91\n94.25\n99.82\n(33, 0.3)\n31.57\n73.09\n30.20\n(100, 0.01)\n100\n93.09\n99.97\n(100, 0.3)\n49.30\n96.45\n47.23\nAverage\n70.20\n89.22\n69.31\nWe observe that for both estimation methods the e\ufb03ciencies of the designs \ufb02uctuate, with the\nBayesian D-optimal design always being more robust. Although in the case of least squares\nestimation the Bayesian design has e\ufb03ciencies below 90%, it has the largest average e\ufb03ciency.\n11\nTable 3: E\ufb03ciencies (%) of the locally and Bayesian D-optimal three-point designs for least\nsquares estimation assuming \u033a2\n\u03b5\u03b7 = 1 and of the uniform design on X = [0, 35].\nE\ufb03ciencies (%)\n(\u03b81, \u03b82)\n\u03beLSE\nloc\n\u03beLSE\nBay\n\u03beuni\n(33, 0.01)\n88.61\n59.16\n99.86\n(33, 0.3)\n15.17\n58.82\n24.40\n(100, 0.01)\n90.94\n61.03\n99.99\n(100, 0.3)\n15.39\n75.17\n24.27\nAverage\n52.53\n63.55\n62.13\nThis is due to the fact that the choice of a uniform prior for the construction of the Bayesian\nD-optimal designs results in the average, over the parameter space, of the values for the deter-\nminant of the information matrix to be maximised. Therefore, when there is no preference for\nspeci\ufb01c parameter values, the use of the Bayesian design is a consistently more e\ufb03cient choice\navoiding the risk of having an extremely ine\ufb03cient design if the parameters are misspeci\ufb01ed.\nThroughout this paper the response and covariate errors are assumed to be known at least up\nto the value of the error-variances ratio \u033a2\n\u03b5\u03b7. However, \u033a2\n\u03b5\u03b7 does a\ufb00ect the optimal choice of the\nBayesian design (see Table 1). We thus conclude this section with a robustness assessment of the\nBayesian D-optimal designs with respect to misspeci\ufb01cation of the \u033a2\n\u03b5\u03b7-value. In particular, we\nuse equation (5.1) to calculate the e\ufb03ciencies of the Bayesian D-optimal designs for \u033a2\n\u03b5\u03b7 = 1,\ngiven in (5.4), and of the uniform design three-point design \u03beuni on X = [0, 35] when the\nBayesian D-optimal design \u03be\u2217\nBay corresponds to various other \u033a2\n\u03b5\u03b7-values. The results are given\nin Table 4\nIt is evident that under both estimation methods the Bayesian D-optimal design assuming equal\nresponse and covariate errors is extremely robust and e\ufb03cient against misspeci\ufb01cation of the\nerror-variances ratio value \u033a2\n\u03b5\u03b7. On the contrary the \u201co\ufb00the shelf\u201d uniform design commonly\nused in practice has e\ufb03ciencies well below 90%.\n12\nTable 4: E\ufb03ciencies (%) of the Bayesian D-optimal three-point designs assuming \u033a2\n\u03b5\u03b7 = 1 and\nof the uniform three-point designs on X = [0, 35].\nE\ufb03ciencies\n\u033a2\n\u03b5\u03b7\nEstimation Method\n\u03beBay\n\u03beuni\n4/1\nMLE\n97.48\n91.51\nLSE\n97.66\n74.02\n2/1\nMLE\n99.32\n86.93\nLSE\n99.37\n75.13\n1/1\nMLE\n100\n81.77\nLSE\n100\n76.08\n1/2\nMLE\n99.30\n76.43\nLSE\n99.28\n76.99\n1/4\nMLE\n97.34\n71.35\nLSE\n96.95\n78.04\nAcknowledgements This work has been supported in part by the Collaborative Research\nCenter \u201cStatistical modeling of nonlinear dynamic processes\u201d (SFB 823, Teilprojekt C2) of the\nGerman Research Foundation (DFG).\nReferences\nI. Burghaus and H. Dette. Optimal designs for nonlinear regression models with respect to\nnon-informative priors. Journal of Statistical Planning and Inference, 154:12\u201325, 2014.\nR.J. Carroll, D. Ruppert, and L.A. Stefanski. Measurement Error in Nonlinear Models. Taylor\n& Francis, 1995.\nK. Chaloner. Bayesian design for estimating the turning point of a quadratic regression. Comm.\nStatist., Theory Methods, 18(4):1385\u20131400, 1989.\nK. Chaloner. A note on optimal bayesian design for nonlinear problems. Journal of Statistical\n13\nPlanning and Inference, 37(2):229 \u2013 235, 1993.\nK. Chaloner and K. Larntz. Optimal bayesian design applied to logistic regression experiments.\nJournal of Statistical Planning and Inference, 21(2):191 \u2013 208, 1989.\nK. Chaloner and K. Larntz. Bayesian design for accelerated life testing. Journal of Statistical\nPlanning and Inference, 33(2):245 \u2013 259, 1992.\nK. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science, 10\n(3):273\u2013304, 1995.\nH. Cherno\ufb00. Locally optimal designs for estimating parameters. The Annals of Mathematical\nStatistics, 24(4):586\u2013602, 1953.\nM. Clerc. Particle Swarm Optimization. Iste Publishing Company, London, 2006.\nH. Dette and H. M. Neugebauer. Bayesian d-optimal designs for exponential regression models.\nJournal of Statistical Planning and Inference, 60:331\u2013349, 1997.\nH. Dette, L. M. Haines, and L. A. Imhof. Maximin and bayesian optimal designs for regression\nmodels. Statistica Sinica, 17:463\u2013480, 2007.\nH. Dette, C. Kiss, M. Bevanda, and F. Bretz. Optimal designs for the EMAX, log-linear and\nexponential models. Biometrika, 97(2):513\u2013518, 2010.\nV. G. Dovi, A. P. Reverberi, and L. Maga.\nOptimal design of sequential experiments for\nerror-in-variables models. Computers & Chemical Engineering, 17(1):111 \u2013 115, 1993.\nA. L. Frisillo and T. J. Stewart. E\ufb00ect of partial gas/brine saturation on ultrasonic absorption\nin sandstone. Journal of Geophysical Research, 85(B10):5209\u20135211, 1980.\nW. A. Fuller. Measurement Error Models. Wiley, New York, 1987.\nC. Han and K. Chaloner. D-and c-optimal designs for exponential regression models used in\npharmacokinetics and viral dynamics. Journal of Statistical Planning and Inference, 115:\n585\u2013601, 2003.\nS. Keeler and P. Reilly. The design of experiments when there are errors in all the variables.\nCanadian Journal of Chemical Engineering, 70:774\u2013778, 1992.\nJ. Kiefer. General equivalence theory for optimum designs (approximate theory). The Annals\nof Statistics, 2(5):849\u2013879, 1974.\nM. Konstantinou and H. Dette.\nLocally optimal designs for errors-in-variables models.\nBiometrika, 102(4):951\u2013958, 2015.\nH. Mihara, T. Kurihara, T. Yoshimura, and N. Esaki. Kinetic and mutational studies of three\nNifS homologs from escherichia coli: Mechanistic di\ufb00erence between L-Cysteine Desulfurase\nand L-Selenocysteine Lyase Reactions. Journal of Biochemistry, 127(4):559\u2013567, 2000.\nL. Pronzato and E. Walter. Robust experiment design via stochastic approximation. Mathe-\nmatical Biosciences, 75(1):103\u2013120, 1985.\nD. Rasch. Optimum experimental design in nonlinear regression. Comm. Statist. Theory Meth-\nods, 19(12):4789\u20134806, 1990.\n14\nAppendix\nProof of of Theorem 3.2\nProof. Let \u03be\u2217\n\u03c0 be a Bayesian D-optimal design with respect to the prior \u03c0 for least squares\nestimation in any functional model of the form (2.1). For any other design \u03be and a \u2208[0, 1] also\nlet \u03bea = (1 \u2212a)\u03be\u2217\n\u03c0 + a\u03be. Then the Frechet derivative of the criterion function \u03a6\u03c0(\u03be) at \u03be\u2217\n\u03c0 in the\ndirection of \u03be \u2212\u03be\u2217\n\u03c0 is\nd\nda\u03a6\u03c0(\u03bea) |a=0 =\nZ\n\u0398\nd\nda\n\u0000log |MLS(\u03bea, \u03b8)|\n\u0001\n|a=0 \u03c0(d\u03b8)\n=\nZ\n\u0398\nd\nda\n\u00002 log |D0(\u03bea, \u03b8)|\n\u0001\n|a=0 \u2212d\nda\n\u0000log |D1(\u03bea, \u03b8)|\n\u0001\n|a=0 \u03c0(d\u03b8)\n=\nZ\n\u0398\n2tr{D\u22121\n0 (\u03be\u2217\n\u03c0, \u03b8)[D0(\u03be, \u03b8) \u2212D0(\u03be\u2217\n\u03c0, \u03b8)]} \u2212tr{D\u22121\n1 (\u03be\u2217\n\u03c0, \u03b8)[D1(\u03be, \u03b8) \u2212D1(\u03be\u2217\n\u03c0, \u03b8)]} \u03c0(d\u03b8)\n=\nZ\n\u0398\n2tr{D\u22121\n0 (\u03be\u2217\n\u03c0, \u03b8)D0(\u03be, \u03b8)} \u2212tr{D\u22121\n1 (\u03be\u2217\n\u03c0, \u03b8)D1(\u03be, \u03b8)} \u03c0(d\u03b8) \u2212(p + 1).\nNow using Dirac measures \u03b4x with weight 1 at the support points x \u2208X of the design \u03be we\nhave that\ntr{D\u22121\nk (\u03be\u2217\n\u03c0, \u03b8)Dk(\u03b4x, \u03b8)} = tr\nn\nD\u22121\nk (\u03be\u2217\n\u03c0, \u03b8)(\u03c31(x, \u03b8))k\n\u03c30(x, \u03b8)\n\u0010\u2202m(x, \u03b8)\n\u2202\u03b8\n\u0011\u0010\u2202m(x, \u03b8)\n\u2202\u03b8\n\u0011To\n=\n\u0010\u2202m(x, \u03b8)\n\u2202\u03b8\n\u0011T (\u03c31(x, \u03b8))k\n\u03c30(x, \u03b8) D\u22121\nk (\u03be\u2217\n\u03c0, \u03b8)\n\u0010\u2202m(x, \u03b8)\n\u2202\u03b8\n\u0011\n= (\u03c31(x, \u03b8))kdk(x, \u03be\u2217\n\u03c0, \u03b8),\nk = 0, 1.\nSince \u03be\u2217\n\u03c0 is Bayesian D-optimal with respect to the prior \u03c0, d\u03a6\u03c0(\u03bea)/da |a=0 is non-positive for\nall designs \u03be, and the inequality\nR\n\u0398 dLS(x, \u03be\u2217\n\u03c0, \u03b8) \u03c0(d\u03b8) \u2264p + 1 for all x \u2208X follows.\nNow let us assume for the Bayesian D-optimal design \u03be\u2217\n\u03c0 that maxx\u2208X\nR\n\u0398 dLS(x, \u03be\u2217\n\u03c0, \u03b8) \u03c0(d\u03b8) <\np + 1. This yields that\nZ\nX\nZ\n\u0398\ndLS(x, \u03be\u2217\n\u03c0, \u03b8) \u03c0(d\u03b8) d\u03be\u2217\n\u03c0(x) < (p + 1)\nZ\nX\nd\u03be\u2217\n\u03c0(x) = p + 1.\nOn the other hand, it follows from the de\ufb01nition of the functions d0 and d1 and a straightforward\ncalculation that for any design \u03be\nZ\nX\nZ\n\u0398\nd0(x, \u03be, \u03b8) \u03c0(d\u03b8) d\u03be(x) =\nZ\nX\nZ\n\u0398\n\u03c31(x, \u03be, \u03b8)d1(x, \u03be, \u03b8) \u03c0(d\u03b8) d\u03be(x) =\nZ\n\u0398\np+1 \u03c0(d\u03b8) = p+1,\nwhich yields\nR\nX\nR\n\u0398 dLS(x, \u03be\u2217\n\u03c0, \u03b8) \u03c0(d\u03b8) d\u03be\u2217\n\u03c0(x) = p + 1 and contradicts our initial assumption.\nHence maxx\u2208X\nR\n\u0398 dLS(x, \u03be\u2217\n\u03c0, \u03b8) \u03c0(d\u03b8) = p + 1 and from\nR\nX\nR\n\u0398 dLS(x, \u03be\u2217\n\u03c0, \u03b8) \u03c0(d\u03b8) d\u03be\u2217\n\u03c0(x) = p + 1\nit follows that this maximum is attained at each support point of the Bayesian D-optimal design\n\u03be\u2217\n\u03c0.\n15\nProof of Lemma 3.1\nProof. Let \u03be = {x0, . . . , xp; \u03c90, . . . , \u03c9p} be any saturated design. Also let X be the (p + 1) \u00d7\n(p + 1) matrix with ith row given by\n\u0010\n\u2202m(xi,\u03b8)\n\u2202\u03b8\n\u0011T\n, i = 0, . . . , p, W = diag(\u03c90, . . . , \u03c9p) and\nLk = diag(\u03c3k(x0, \u03b8), . . . , \u03c3k(xp, \u03b8)) for k = 0, 1. Under this notation the determinants of the\ninformation matrices MML(\u03be, \u03b8) and MLS(\u03be, \u03b8), given in (2.2) and (2.3) respectively, become\ndet {MML(\u03be, \u03b8)} = [det X]2 [det L1]\u22121 [det W]\ndet {MLS(\u03be, \u03b8)} = det\n\b\nD0(\u03be, \u03b8)D\u22121\n1 (\u03be, \u03b8)D0(\u03be, \u03b8)\n\t\n= [det X]2 [det L0]\u22121 [det L1]\u22121 [det W] .\nHence the criterion (3.1) becomes\nZ\n\u0398\nlog |W| + 2 log |X| \u2212log |L1| \u03c0(d\u03b8),\nand\nZ\n\u0398\nlog |W| + 2 log |X| \u2212log |L0| \u2212log |L1| \u03c0(d\u03b8),\nfor maximum likelihood and least squares estimation respectively.\nMaximising any of the\nabove expressions with respect to the weights gives \u03c9i = 1/(p + 1), for all i = 0, . . . , p since\ndet W = \u03c90 \u03c91 . . . \u03c9p, which proves the assertion.\nProof of Theorem 4.1\nProof. From Lemma 3.1 it follows that a Bayesian D-optimal saturated design on X = [0, xu]\nfor maximum likelihood estimation in the Michaelis-Menten model with measurement errors as\nin (2.1), puts equal weights at its support points points. For any design \u03be\u03c0 = {x1, x2; 0.5, 0.5}\n(0 < x1 < x2 \u2264xu), the criterion de\ufb01ned in (3.1) for the information matrix given in (2.2)\nbecomes\n\u03a6\u03c0(\u03be\u03c0) =\nZ\n\u0398\n2 log \u03b81 + 2 log x1 + 2 log x2 + 2 log(x2 \u2212x1) \u2212log 4\n\u2212log(\u03c32\n\u03b7(\u03b82 + x1)4 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u2212log(\u03c32\n\u03b7(\u03b82 + x2)4 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u03c0(d\u03b8).\nIt is easy to check that for \ufb01xed x1, this is increasing with x2 and therefore maximized for x\u2217\n2 =\nxu. The smaller support point of the optimal design is found by solving \u2202\u03a6\u03c0(\u03be\u03c0)/\u2202x1 |x2=xu= 0\nfor x1 \u2208(0, xu). This is equivalent to solving\nZ\n\u0398\n1\nx1\n\u2212\n1\nxu \u2212x1\n\u2212\n2\u03c32\n\u03b7(\u03b82 + x1)3\n\u03c32\n\u03b7(\u03b82 + x1)4 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5\n\u03c0(d\u03b8) = 0,\n(.5)\nfor x1 \u2208(0, xu).\nSimilarly using Lemma 3.1, a Bayesian D-optimal saturated design on X = [0, xu] for maxi-\nmum likelihood estimation in the Emax model with measurement errors as in (2.1), is equally\n16\nweighted. Thus for the three-point design \u03be\u03c0 = {x0, x1, x2; 1/3, 1/3, 1/3} (0 < x0 < x1 < x2 \u2264\nxu) we have that\n\u03a6\u03c0(\u03be\u03c0) =\nZ\n\u0398\n2 log \u03b81 + 4 log \u03b82 + 2 log(x0 \u2212x1) + 2 log(x0 \u2212x2) + 2 log(x1 \u2212x2) \u2212log 27\n\u2212log(\u03c32\n\u03b7(\u03b82 + x0)4 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u2212log(\u03c32\n\u03b7(\u03b82 + x1)4 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u2212log(\u03c32\n\u03b7(\u03b82 + x2)4 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u03c0(d\u03b8).\nThe criterion above is decreasing with x0 and increasing with x2 and therefore, it is maximised\nat x\u2217\n0 = 0 and x\u2217\n2 = xu. The equation \u2202\u03a6\u03c0(\u03be\u03c0)/\u2202x1 |x0=0;x2=xu= 0 gives again equation (.5).\nProof of Theorem 4.2\nProof. Following similar arguments as in the proof of Theorem 4.1, for an equally weighted\nthree-point design \u03be\u03c0 = {x0, x1, x2; 1/3, 1/3, 1/3} (0 \u2264x0 < x1 < x2 \u2264xu), the criterion\nde\ufb01ned in (3.1) using the information matrix given in (2.2) for the Emax model (4.3), becomes\n\u03a6\u03c0(\u03be\u03c0) =\nZ\n\u0398\n2 log \u03b81 + 2 log(e\u03b82x2(x0 \u2212x1) + e\u03b82x0(x1 \u2212x2) + e\u03b82x1(x2 \u2212x0)) \u2212log 27\n\u2212log(\u03c32\n\u03b7e2\u03b82x0 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u2212log(\u03c32\n\u03b7e2\u03b82x1 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u2212log(\u03c32\n\u03b7e2\u03b82x2 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u03c0(d\u03b8).\nIt is easy to check that e\u03b82x2(x0 \u2212x1) + e\u03b82x0(x1 \u2212x2) + e\u03b82x1(x2 \u2212x0) is increasing with x0,\ndecreasing with x2 and negative for all x0 < x1 < x2. Therefore, the criterion is decreasing\nwith x0 and increasing with x2 and thus it is maximized at x\u2217\n0 = 0 and x\u2217\n2 = xu. The middle\nsupport point of the Bayesian D-optimal design is found by solving \u2202\u03a6\u03c0(\u03be\u03c0)/\u2202x1 |x0=0;x2=xu= 0\nfor x1 \u2208(0, xu) which is equivalent to solving\nZ\n\u0398\n1 \u2212e\u03b82xu + \u03b82xue\u03b82x1\nx1 \u2212xu + xue\u03b82x1 \u2212x1e\u03b82xu \u2212\n\u03b82e2\u03b82x1\ne2\u03b82x1 + \u03b82\n1\u03b82\n2\u033a2\n\u03b5\u03b7\n\u03c0(d\u03b8) = 0,\nfor x1 \u2208(0, xu), where \u033a2\n\u03b5\u03b7 = \u03c32\n\u03b5/\u03c32\n\u03b7.\nProof of Theorem 4.3\nProof. From Lemma 3.1, it follows that a Bayesian D-optimal saturated design for least squares\nestimation assigns equal weights at its support points. For the Michaelis-Menten model with\nmeasurement errors as in (2.1) and a two-point equally weighted design \u03be\u03c0 = {x1, x2; 0.5, 0.5}\n(0 < x1 < x2 \u2264xu), the criterion de\ufb01ned in (3.1) for the information matrix given in (2.3)\nbecomes\n\u03a6\u03c0(\u03be\u03c0) =\nZ\n\u0398\n2 log \u03b81 + 2 log(x2 \u2212x1) + 4 log(\u03b82 + x1) + 4 log(\u03b82 + x2) \u2212log 4\n\u2212log(\u03c32\n\u03b7(\u03b82 + x1)4 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u2212log(\u03c32\n\u03b7(\u03b82 + x2)4 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5)\n\u2212log((\u03b82 + x1)4 + \u03b82\n1\u03b82\n2) \u2212log((\u03b82 + x2)4 + \u03b82\n1\u03b82\n2) \u03c0(d\u03b8).\n17\nFor \ufb01xed x1, the above expression is increasing with x2 and therefore maximized at x\u2217\n2 = xu.\nThe smaller support point of the optimal design is found by solving \u2202\u03a6\u03c0(\u03be\u03c0)/\u2202x1 |x2=xu= 0 for\nx1 \u2208(0, xu), which is equivalent to solving\nZ\n\u0398\n1\nx1\n\u2212\n1\nxu \u2212x1\n\u2212\n2(\u03b82 + x1)3\n(\u03b82 + x1)4 + \u03b82\n1\u03b82\n2\u033a2\n\u03b5\u03b7\n+\n2\u03b82\n1\u03b82\n2\n(\u03b82 + x1)[(\u03b82 + x1)4 + \u03b82\n1\u03b82\n2] \u03c0(d\u03b8) = 0,\n(.6)\nfor x1 \u2208(0, xu).\nIn the case of the Emax model with errors as in (2.1), the criterion for a three-point equally\nweighted design \u03be\u03c0 = {x0, x1, x2; 1/3, 1/3, 1/3} (0 \u2264x0 < x1 < x2 \u2264xu) becomes\n\u03a6\u03c0(\u03be\u03c0) =\nZ\n\u0398\n2 log \u03b81 + 4 log \u03b82 + 2 log(x2 \u2212x1) + 2 log(x2 \u2212x0) + 2 log(x1 \u2212x0) + 4 log(\u03b82 + x0)\n+ 4 log(\u03b82 + x1) + 4 log(\u03b82 + x2) \u2212log 4 \u2212log(\u03c32\n\u03b7(\u03b82 + x0)4 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5)\n\u2212log(\u03c32\n\u03b7(\u03b82 + x1)4 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u2212log(\u03c32\n\u03b7(\u03b82 + x2)4 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u2212log((\u03b82 + x0)4 + \u03b82\n1\u03b82\n2)\n\u2212log((\u03b82 + x1)4 + \u03b82\n1\u03b82\n2) \u2212log((\u03b82 + x2)4 + \u03b82\n1\u03b82\n2) \u03c0(d\u03b8).\nIt is easy to check, following similar arguments as before, that the above expression is decreasing\nwith x0, increasing with x2 and thus maximized at x\u2217\n0 = 0, x\u2217\n2 = xu and x\u2217\n1 is the solution of\nthe same equation as for the Michaelis-Menten model, that is, equation (.6).\nProof of Theorem 4.4\nProof. For the three-parameter exponential regression model with measurement errors as in\n(2.1) and a three-point equally weighted design \u03be\u03c0 = {x0, x1, x2; 1/3, 1/3, 1.3} (0 \u2264x0 < x1 <\nx2 \u2264xu), the criterion de\ufb01ned in (3.1) for the information matrix given in (2.3) becomes\n\u03a6\u03c0(\u03be\u03c0) =\nZ\n\u0398\n2 log \u03b81 + 2 log(e\u03b82x2(x0 \u2212x1) + e\u03b82x0(x1 \u2212x2) + e\u03b82x1(x2 \u2212x0)) \u2212log 27\n\u2212log(\u03c32\n\u03b7e2\u03b82x0 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u2212log(\u03c32\n\u03b7e2\u03b82x1 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5) \u2212log(\u03c32\n\u03b7e2\u03b82x2 + \u03b82\n1\u03b82\n2\u03c32\n\u03b5)\n\u2212log(1 + \u03b82\n1\u03b82\n2e\u22122\u03b82x0) \u2212log(1 + \u03b82\n1\u03b82\n2e\u22122\u03b82x1) \u2212log log(1 + \u03b82\n1\u03b82\n2e\u22122\u03b82x2) \u03c0(d\u03b8).\nFollowing the proof of Theorem 4.2, the criterion is increasing with x2 and thus it is maximized\nat x\u2217\n2 = xu.\n18\n"}