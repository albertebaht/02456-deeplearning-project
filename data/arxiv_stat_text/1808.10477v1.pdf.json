{"text": "Simulation\u2013Selection\u2013Extrapolation: Estimation in\nHigh\u2013Dimensional Errors\u2013in\u2013Variables Models\nLinh Nghiem & Cornelis J. Potgieter\nDepartment of Statistical Science, Southern Methodist University\nAbstract\nThis paper considers errors-in-variables models in a high-dimensional setting where the number of\ncovariates can be much larger than the sample size, and there are only a small number of non-zero co-\nvariates. The presence of measurement error in the covariates can result in severely biased parameter\nestimates, and also a\ufb00ects the ability of penalized methods such as the lasso to recover the true sparsity\npattern. A new estimation procedure called SIMSELEX (SIMulation-SELection-EXtrapolation) is pro-\nposed. This procedure augments the traditional SIMEX approach with a variable selection step based on\nthe group lasso. The SIMSELEX estimator is shown to perform well in variable selection, and has signif-\nicantly lower estimation error than naive estimators that ignore measurement error. SIMSELEX can be\napplied in a variety of errors-in-variables settings, including linear models, generalized linear models, and\nCox survival models. It is furthermore shown how SIMSELEX can be applied to spline-based regression\nmodels. SIMSELEX estimators are compared to the corrected lasso and the conic programming estima-\ntor for a linear model, and to the conditional scores lasso for a logistic regression model. Finally, the\nmethod is used to analyze a microarray dataset that contains gene expression measurements of favorable\nhistology Wilms tumors.\nKeywords: Gene expressions; High-dimensional data; Measurement error; Microarray data; SIMEX;\nSparsity.\n1\nIntroduction\nErrors-in-variables models arise in settings where some covariates cannot be measured with great accuracy.\nAs such, the observed covariates have larger variance than the true underlying variables, obscuring the\nrelationship between the covariates and the outcome. More formally, let it be of interest to model a re-\nsponse variable Y as a function of covariates X. However, the observed sample consists of measurements\n(W1, Y1), . . . , (Wn, Yn), with Wi = Xi+Ui, i = 1, . . . , n where the Ui are i.i.d. Gaussian measurement error\nvectors with mean zero and covariance matrix \u03a3u. The Ui are assumed to be independent from the true\ncovariates Xi, and the matrix \u03a3u is assumed known. However, the methodology can still but applied when\n\u03a3u is estimated from from auxiliary data. This paper will consider models that specify (at least partially)\na distribution for Y conditional on X, with said distribution involving unknown parameters \u03b8. Such models\ninclude (but are not limited to) linear and generalized linear models, Cox survival models, and spline-based\nregression models. Not accounting for measurement error when \ufb01tting these models can result in biased\nparameter estimates as well as a loss of power when detecting relationships between variables, see Carroll\net al. (2006). The e\ufb00ects of measurement error have mostly been studied in the low-dimensional setting\nwhere the number of observation n is greater than the number of covariates p, see Armstrong (1985) for\ngeneralized linear models and Prentice (1982) for Cox survival models.\n1\narXiv:1808.10477v1  [stat.ME]  30 Aug 2018\nIn this paper, these models are considered in the high-dimensional setting, where the dimension p can\nbe much larger than the sample size n. Typically, the true \u03b8 is sparse, meaning that it has only s non-zero\ncomponents with s < min(n, p). In this setting, it is of interest to both recover the true sparsity pattern\nof the vector \u03b8 as well as estimate the non-zero components of \u03b8 accurately. When the covariates X are\nobserved without error, the lasso and its generalizations as proposed by Tibshirani (1996) can be employed\nfor estimating a sparse \u03b8. The lasso adds an \u21131 constraints on \u03b8 to a loss function L(\u03b8; Y, X). That is,\n\u02c6\u03b8 = argmin\n\u03b8\n[L(\u03b8; Y, X) + \u03be1 \u2225\u03b8\u22251]\n(1)\nwhere \u03be1 is a tuning parameter and \u2225\u03b8\u22251 = Pp\nj=1 |\u03b8p| is the \u21131 norm of the \u03b8. For the generalized linear\nmodel, L(\u03b8; Y, X) is often chosen as the negative log-likelihood function, while for the Cox survival model,\nL(\u03b8; Y, X) is the log of the partial likelihood function, see Hastie et al. (2015) for details on how the lasso is\napplied in both of these settings.\nThe presence of measurement error introduces an added layer of complexity and can have severe conse-\nquences on the lasso estimator: the number of non-zero estimates can be in\ufb02ated, sometimes dramatically,\nand as such the true sparsity pattern of the model is not recovered, see Rosenbaum et al. (2010). Several\nmethods have been proposed that correct for measurement error in high-dimensional setting. Rosenbaum\net al. (2010) proposed a matrix uncertainty selector (MU) for additive measurement error in the linear model.\nRosenbaum et al. (2013) proposed an improved version of the MU selector, and Belloni et al. (2017) proved\nits near-optimal minimax properties and developed a conic programming estimator that can achieve the\nminimax bound. The conic estimators require selection of three tuning parameters, a di\ufb03cult task in prac-\ntice. Another approach for handling measurement error is to modify the loss and conditional score functions\nused with the lasso, see S\u00f8rensen et al. (2015) and Datta et al. (2017). Additionally, S\u00f8rensen et al. (2018)\ndeveloped the generalized matrix uncertainty selector (GMUS) for the errors-in-variables generalized linear\nmodels. Both the conditional score approach and GMUS require the subjective choice of tuning parameters.\nThis paper proposes a new method of estimation called Simulation-Selection-Extrapolation (SIMSELEX).\nThis method is based on the simulation\u2013extrapolation (SIMEX) procedure of Cook and Stefanski (1994).\nSIMEX has been well-studied and applied extensively for correcting measurement error in low-dimensional\nmodels, see for example Stefanski and Cook (1995), K\u00a8uchenho\ufb00et al. (2006) and Apanasovich et al. (2009).\nThe classic application of SIMEX does not work well when model sparsity is required, often resulting in\nan estimate of \u03b8 with a large number of non-zero components. This breakdown is illustrated in Appendix\nA. The SIMSELEX approach overcomes this di\ufb03culty by augmenting SIMEX with a variable selection step\nperformed after the simulation step and before the extrapolation step. The variable selection step is based\non an application of the group lasso. SIMSELEX inherits the \ufb02exibility of SIMEX and can be applied to a\nlarge number of di\ufb00erent high-dimensional errors-in-variables models.\nThe remainder of this paper proceeds as follows. Section 2 provides an overview of the SIMEX procedure.\nSection 3 proposes the SIMSELEX procedure for the high-dimensional setting. In Section 4, application of\nSIMSELEX is illustrated for linear, logistic, and Cox regression models. Section 5 discusses the application of\nSIMSELEX in the context of spline regression. In Section 6, the methodology is illustrated with a microarray\ndataset. Section 7 contains concluding remarks.\n2\n2\nA Review of Simulation-Extrapolation (SIMEX)\nLet Xi denote a vector of model covariates, let Wi = Xi + Ui denote a version of Xi contaminated by\nmeasurement error Ui, and let Yi denote an outcome variable depending on Xi in a known way through\nparameter vector \u03b8. Assume that the observed data are (Wi, Yi), i = 1, . . . , n. The measurement error Ui is\nassumed to be multivariate Gaussian with mean zero and known covariance matrix \u03a3u. While the outcomes\nYi depend on the true covariates Xi, only the observed Wi are available for model estimation.\nNow, let S denote an estimator of \u03b8 calculated from the observed data. If the uncontaminated covariates\nXi had been observed, it would be possible to calculate the true estimator \u02c6\u03b8true = S({Xi, Yi}i=1,...,n).\nHowever, as the covariates Xi are unobserved, it is not possible to calculate this estimator.\nThe naive\nestimator of \u03b8 based on the observed sample is \u02c6\u03b8naive = S({Wi, Yi}i=1,...,n). This estimator treats the Wi\nas if no measurement error is present. Generally, the naive estimator is neither consistent nor unbiased for\n\u03b8.\nSIMEX is one of the most prominent methods developed to deal with the potential bias introduced by the\npresence of measurement error in a variety of models. SIMEX estimates the e\ufb00ect of measurement error on\nan estimator through simulation, after which the estimator is calculated by extrapolating simulation results,\nsee Carroll et al. (2006) and Stefanski and Cook (1995). The SIMEX procedure can be implemented as\nfollows:\n1. Simulation step:\n(a) Choose a grid of \u03bb = 0 < \u03bb1 < . . . < \u03bbM\n(b) For each \u03bbm in the grid:\n(b.1) Generate B sets of pseudodata by adding random error to Wi, i = 1, . . . , n,\nW(b)\ni (\u03bbm) = Wi +\np\n\u03bbmU(b)\ni ,\nb = 1, . . . , B,\nwith\nU (b) \u223cNp(0, \u03a3u).\n(b.2) Calculate the naive estimator for each set of pseudodata,\n\u02c6\u03b8(b)(\u03bbm) = S({W(b)\ni (\u03bbm), Yi}i=1,...,n).\n(b.3) Average these estimators,\n\u02c6\u03b8(\u03bbm) = 1\nB\nB\nX\nb=1\n\u02c6\u03b8(b)(\u03bbm).\n2. Extrapolation step:\n(a) Model \u02c6\u03b8(\u03bb) as a function of \u03bb.\n(b) Extrapolate the model to \u03bb = \u22121 to obtain \u02c6\u03b8simex.\nHeuristically, SIMEX adds new random error \u221a\u03bbmU(b)\ni\nto Wi to obtain W(b)\ni (\u03bbm) with increased measure-\nment error. The naive estimator is then computed based on the pseudodata (W (b)\ni\n(\u03bbm), Yi), i = 1, . . . , n.\nThat is, \u02c6\u03b8(b)(\u03bbm) = S({W(b)\ni (\u03bbm), Yi}i=1,...,n).\nFor a given value \u03bbm, the naive estimator has inherent\nvariability due to the simulated errors U(b)\ni . The e\ufb00ect of this variability is reduced by generating a large\nnumber of sets of pseudodata and averaging the naive estimators from all the sets of pseudodata for a given\nvalue of \u03bb to obtain \u02c6\u03b8(\u03bb) = (1/B) PB\nb=1 \u02c6\u03b8(b)(\u03bb). For a given set of pseudodata, the covariance matrix of\n3\nthe measurement error component is (1 + \u03bb)\u03a3u. As such, the case with \u03bb = \u22121 corresponds to the case\nwhen no measurement error exists. Therefore, after the simulation step calculates \u02c6\u03b8(\u03bb) on a grid of \u03bb, the\nextrapolation step regresses \u02c6\u03b8(\u03bb) on \u03bb by an extrapolation function \u0393(\u03bb) and extrapolates to \u03bb = \u22121 to\nobtain the estimator \u02c6\u03b8simex.\nIn low-dimensional data settings where sparsity is not desired, S is usually computed based maximum\nlikelihood or an \u21132 distance metric such as least squares. A commonly used extrapolation function is the\nquadratic function \u0393(\u03bb) = \u03b30 + \u03b31\u03bb + \u03b32\u03bb2 which usually results in an estimator with good mean squared\nerror (MSE) properties, see Stefanski and Cook (1995). Other popular choices of extrapolation functions\nare the linear function \u0393lin(\u03bb) = \u03b30 + \u03b31\u03bb, and the nonlinear means model, \u0393nonlin(\u03bb) = \u03b30 + \u03b31/(\u03b32 + \u03bb).\nAlthough the theory of SIMEX was developed assuming all measurement errors are normal, implementation\ntends to be robust against departure from normality, see Section 5.3 of Carroll et al. (2006). In order to\napply the SIMEX procedure, the covariance matrix \u03a3u needs to be known or accurately estimable from\nauxiliary data. The latter scenario is often true when repeated measurement data are available.\nUnfortunately, as illustrated in Appendix A, SIMEX as outlined above cannot be applied to the high-\ndimensional setting without some adjustments. Even if the estimator S is constructed to ensure sparsity of\nthe estimator \u02c6\u03b8naive, direct application of the extrapolation step does not ensure that the estimator \u02c6\u03b8simex is\nalso sparse. That is, for each value of the parameter \u03bbm used in the simulation step, the obtained solution\n\u02c6\u03b8(\u03bbm) is sparse, but may not have the same sparsity pattern as \u02c6\u03b8(\u03bbm\u2032), m \u0338= m\u2032. More speci\ufb01cally, let \u03b8j\ndenote the jth component of the parameter vector and assume this true value is equal to 0. When viewing a\nsolution path for this jth component, say (\u03bbm, \u02c6\u03b8j(\u03bbm)), m = 1, . . . , M, even a single \u03bbi for which \u02c6\u03b8j(\u03bbi) \u0338= 0\nwill result in an extrapolated value \u02c6\u03b8j(\u22121) \u0338= 0. If extrapolation is therefore applied indiscriminately to each\n\u03b8j, many components of the extrapolated solution vector will be non-zero. As such, a modi\ufb01ed algorithm\nreferred to as SIMSELEX (SIMulation-SELection-EXtrapolation) is proposed in the next section.\n3\nThe SIMSELEX Estimator\nA common assumption when analyzing high-dimensional data is sparsity of the solution.\nThe lasso is\na popular method both for enforcing model sparsity and for estimating the nonzero model coe\ufb03cients.\nTherefore, when measurement error is present in the covariates, it is a natural idea to \ufb01nd a way of combining\nthe lasso with the SIMEX procedure. In this section, a method for doing so is developed. The resulting\nsimulation-selection-extrapolation (SIMSELEX) estimator augments SIMEX by adding a variable selection\nstep after the simulation step but before the extrapolation step. The three steps of SIMSELEX are presented\nand discussed below.\n3.1\nSimulation step\nThe simulation step of the SIMSELEX procedure is identical to the simulation step of SIMEX. However,\nthe criterion function being minimized for each set of pseudodata now incorporates a lasso-type penalty\non the model parameters. Speci\ufb01cally, for given value of \u03bb and corresponding pseudodata (W (b)\ni\n(\u03bb), Yi),\ni = 1, . . . , n, the estimator \u02c6\u03b8(b)(\u03bb) is calculated according to a criterion of the form in (1) with the tuning\nparameter \u03be1, typically chosen based on cross-validation. Two versions of the tuning parameter are popular\nin practice: \u03bemin is the value that minimizes the estimated prediction risk on the test sets, whereas \u03be1se is the\nvalue that makes the estimated prediction risk fall within one standard error of the minimum (one-standard-\nerror-rule), see Friedman et al. (2001). Note that cross-validation is implemented separately for each set of\n4\npseudodata. Even so, the simulation step of the SIMSELEX procedure inherits fast computation of the lasso\nestimator for many models (including linear and logistic regression). The simulation step results in pairs\n(\u03bbi, \u02c6\u03b8(\u03bbi)), i = 1, . . . , M. This is the data used in the selection and extrapolation steps described next.\n3.2\nSelection step\nTo perform variable selection, a lasso-based approach is applied to the data (\u03bbm, \u02c6\u03b8(\u03bbm)). Assume that the\nquadratic function \u0393(\u03bb) serves as a good approximation to the relationship for this data. Speci\ufb01cally,\n\u02c6\u03b8mj = \u03b30j + \u03b31j\u03bbm + \u03b32j\u03bb2\nm + emj,\nm = 1, . . . , M,\nj = 1, . . . , p\n(2)\nwith \u02c6\u03b8mj = \u02c6\u03b8j(\u03bbm) and emj denoting zero-mean error terms.\nTo achieve model sparsity, it is desirable\nto shrink (as a group) the parameters (\u03b30j, \u03b31j, \u03b32j) to the vector (0, 0, 0) for many of the components \u03b8j.\nExtrapolation will then only be applied to the variables with non-zero solutions (\u02c6\u03b30j, \u02c6\u03b31j, \u02c6\u03b32j), with all other\ncoe\ufb03cients being set equal to 0.\nThe discussion in the preceding paragraph suggests an approach to overcome the challenge of the extrap-\nolation step resulting in too many non-zero estimated coe\ufb03cients. By applying a group lasso (Hastie et al.,\n2015, Section 4.3) simultaneously to all the solution paths (\u03bbj, \u03b8mj), m = 1, . . . , M, j = 1, . . . , p, shrinking\ncan be applied to groups of coe\ufb03cients corresponding to individual variables. If the true model is sparse,\nmany of the solutions (\u02c6\u03b30j, \u02c6\u03b31j, \u02c6\u03b32j) will be set to the zero vector. The p equations in (2) can be written in\nmatrix form, \u0398 = \u039b\u0393 + E, where\n\u039b =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n\u03bb1\n\u03bb2\n1\n...\n...\n...\n1\n\u03bbM\n\u03bb2\nM\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\u0398 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u02c6\u03b811\n. . .\n\u02c6\u03b81p\n...\n...\n\u02c6\u03b8M1\n. . .\n\u02c6\u03b8Mp\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\u0393 =\n\uf8ee\n\uf8ef\uf8f0\n\u03b301\n. . .\n\u03b30p\n\u03b311\n. . .\n\u03b31p\n\u03b321\n. . .\n\u03b32p\n\uf8f9\n\uf8fa\uf8fb\nand\nE =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\ne11\n. . .\ne1p\n...\n...\neM1\n. . .\neMp.\n\uf8f9\n\uf8fa\uf8fa\uf8fb.\nWhen the kth column of the estimated matrix \u02c6\u0393 is a zero vector, the corresponding kth row of \u02c6\u0398 = \u02c6\u039b\u0393 will\nalso be a zero vector and the kth variable is not selected for inclusion in the model.\nIn the present context, the group lasso has the penalized discrepancy function\nD(\u0393) = 1\n2\nM\nX\nm=1\np\nX\nj=1\n\u0010\n\u02c6\u03b8mj \u2212\u03b30j \u2212\u03b31j\u03bbm \u2212\u03b32j\u03bb2\nm\n\u00112\n+ \u03be2\n\uf8eb\n\uf8ed\np\nX\nj=1\nq\n\u03b32\n0j + \u03b32\n1j + \u03b32\n2j\n\uf8f6\n\uf8f8\nwhere \u03be2 is a penalty parameter. This function can be written in matrix form,\nD(\u0393) = 1\n2\np\nX\nj=1\n\u0010\n\u2225\u0398j \u2212\u039b\u0393j\u22252\n2 + \u03be2 \u2225\u0393j\u22252\n\u0011\n(3)\nwhere \u0398j and \u0393j denote the jth column of \u0398 and \u0393 respectively, and \u2225.\u22252 denotes the \u21132 norm. To \ufb01nd\n\u02c6\u0393 that minimizes D, standard subgradient methods can be used for numerical optimization. Equation (3)\nis block-separable and convex, so subgradient methods are guaranteed to converge to the global minimum.\n5\nThe subgradient equations, which are generalization of derivatives for non-di\ufb00erentiable functions, (Hastie\net al., 2015, Section 5.2.2) are\n\u2212\u039bT \u0010\n\u0398j \u2212\u039b\u02c6\u0393j\n\u0011\n+ \u03be2\u02c6sj = 0,\nj = 1, . . . , p\n(4)\nwhere \u02c6sj \u2208R3 is an element of the subdi\ufb00erential of the norm\n\r\r\r\u02c6\u0393j\n\r\r\r\n2.\nAs a result, if \u02c6\u0393j \u0338= 0, then\n\u02c6sj = \u02c6\u0393j/\n\r\r\r\u02c6\u0393j\n\r\r\r\n2. On the other hand, if \u02c6\u0393j = 0, then \u02c6sj is any vector with \u2225\u02c6sj\u22252 \u22641. Therefore, \u02c6\u0393j must\nsatisfy\n\u02c6\u0393j =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n0\nif\n\r\r\u039b\u22a4\u0398j\n\r\r\n2 \u2264\u03be2\n\uf8ee\n\uf8f0\u039b\u22a4\u039b +\n\u03be2\n\r\r\r\u02c6\u0393j\n\r\r\r\n2\nI\n\uf8f9\n\uf8fb\n\u22121\n\u039b\u22a4\u0398j\notherwise.\n(5)\nThe \ufb01rst equation of (5) gives a simple rule for when to set the all elements of a speci\ufb01c column of \u02c6\u0393\nequal to 0 for a speci\ufb01c value of the penalty \u03be2. Therefore \u02c6\u0393 can be computed using the proximal gradient\ndescent, which is a generalization of gradient descent for functions that are decomposed into the sum of\na di\ufb00erentiable and a non-di\ufb00erentiable part (Hastie et al., 2015, Section 5.3). At the kth iteration, each\ncolumn \u02c6\u0393j can be updated as follows. First calculate\n\u03c9(k)\nj\n= \u02c6\u0393(k\u22121)\nj\n+ \u03bd\u039b\u22a4(\u0398j \u2212\u039b\u02c6\u0393(k\u22121)\nj\n)\n(6)\nand then use this quantity to update\n\u02c6\u0393(k)\nj\n=\n\uf8eb\n\uf8ed1 \u2212\n\u03bd\u03be2\n\r\r\r\u03c9(k)\nj\n\r\r\r\n2\n\uf8f6\n\uf8f8\n+\n\u03c9(k)\nj\n.\n(7)\nfor all j = 1, . . . , p. Here, \u03bd is the step size that needs to be speci\ufb01ed for the algorithm and (z)+ = max(z, 0).\nThe convergence of the algorithm is guaranteed if the step size \u03bd \u2208(0, 1/L), where L is the maximum\neigenvalue of the matrix \u039bT \u039b/M. The tuning parameter \u03be2 can be chosen using cross-validation.\nNote that implementation of selection as discussed is based on the used of the quadratic function \u0393(\u03bb).\nThe linear function \u0393lin(\u03bb), as de\ufb01ned in Section 2, could alternatively be used for the selection step, but\nthe means model \u0393nonlin(\u03bb) results in a non-convex loss function and is computationally very expensive to\nimplement when paired with a lasso-type penalty.\n3.3\nExtrapolation step\nFinally, the extrapolation step of the SIMSELEX procedure is applied only to the components of \u02c6\u03b8 that are\nselected in the preceding step. While one might be inclined to use the coe\ufb03cients \u02c6\u0393 found in the selection\nstep to perform extrapolation, these tend to perform poorly as they have been shrunk towards 0. Rather,\nwhen the jth variable has been selected in the previous step, extrapolation function \u0393new(\u03bb) (potentially\ndi\ufb00erent from the function \u0393(\u03bb) used in the selection step) is used to model (\u03bbm, \u02c6\u03b8j(\u03bbm)).\nSpeci\ufb01cally,\nindividual extrapolation functions are now \ufb01t to each selected component of the parameter vector and then\nextrapolated to \u03bb = \u22121 to obtain the SIMSELEX parameter estimates. No penalty term is used in the\nextrapolation step as variable selection has already been performed.\nThe need for this type of re\ufb01tting\npost-selection has been discussed in the literature, see Lederer (2013).\n6\n4\nModel Illustration and Simulation Results\nThe performance of the SIMSELEX approach to high-dimensional errors-in-variables models is discussed in\nthis section with reference to speci\ufb01c underlying models. Where applicable, the performance of competitor\nestimators is also included. Extensive simulation studies have been performed, with selected (representative)\nresults reported in this paper. Several performance metrics were employed for evaluating method performance\nin the simulations.\nThese include metrics related to the recovery of the sparsity pattern and also the\nestimation error associated with parameter recovery. In all the simulations done, it was assumed that all\ncovariates are measured with error, and that the measurement error covariance matrix is known.\n4.1\nLinear Regression\nAssume the observed data are of the form (Wi, Yi), i = 1, . . . , n where Yi = X\u22a4\ni \u03b8 + \u03b5i and Wi = Xi + Ui.\nFor linear models with high-dimensional covariates subject to measurement error, three solutions have been\nproposed in the literature.\nFirstly, Rosenbaum et al. (2010) proposed the Matrix Uncertainty Selection\n(MUS) method, which does not require that the measurement error covariance matrix \u03a3u be known or\nestimable. Secondly, there are two approaches that do make use of \u03a3u; S\u00f8rensen et al. (2015) considered a\ncorrection to the lasso resulting in an unbiased loss function in the linear model framework, while Belloni\net al. (2017) proposed a conic programming estimator. The method of S\u00f8rensen et al. (2015) requires the\nselection of one tuning parameter, while that of Belloni et al. (2017) requires three tuning parameters. A\nbrief overview of these last two approaches is given in Appendix B. Furthermore, the results of a simulation\nstudy comparing these two methods to the proposed SIMSELEX method are reported.\nFor the simulation, data pairs (Wi, Yi) were generated according to the linear model Yi = X\u22a4\ni \u03b8 + \u03b5i\nwith observations contaminated by additive measurement error, Wi = Xi + Ui. Both the true covariates\nXi and the measurement error components Ui were generated to be i.i.d. p-variate normal. Speci\ufb01cally,\nXi \u223cNp(0, \u03a3), with \u03a3 having entries \u03a3ij = \u03c1|i\u2212j| with \u03c1 = 0.25, and Ui \u223cNp(0, \u03a3u) with \u03a3u = \u03c32\nuIp\u00d7p\nand \u03c3u = 0.45. The error components \u03b5i were simulated to be i.i.d. univariate normal, \u03b5 \u223cN(0, \u03c32\n\u03b5) with\n\u03c3\u03b5 = 0.128. The sample size was \ufb01xed at n = 300, and simulations were done for number of covariates\np \u2208{100, 500, 600}.\nTwo choice of the true \u03b8 were considered, namely \u03b81 = (1, 1, 1, 1, 1, 0, . . . , 0)\u22a4and\n\u03b82 = (1, 1/2, 1/3, 1/4, 1/5, 0, . . . , 0)\u22a4. Both cases have s = 5 non-zero coe\ufb03cients and p \u22125 zero coe\ufb03cients.\nUnder each simulation con\ufb01guration considered, N = 500 samples were generated.\nNote that above simulation settings corresponds to a noise-to-signal ratio of approximately 20% for each\nindividual covariate. However, if one uses a metric such as the proportional increase in total variability,\n\u2206V = (det(\u03a3W) \u2212det(\u03a3)) / det(\u03a3), the di\ufb03culty presented by the high-dimensional setting becomes clear.\nIf one were to only observe the s = 5 non-zero covariates, \u2206V = 1.73, while for p = 100, this metric is\n\u2206V = 7.6 \u00d7 108. This changes what one would heuristically label \u201csmall\u201d and \u201clarge\u201d measurement error.\nIn the simulation study, \ufb01ve di\ufb00erent estimators were computed: the true lasso estimator using the\nuncontaminated X-data, the naive lasso estimator treating the W -data as if it were uncontaminated, the\nconic estimator with tuning parameters as implemented in Belloni et al. (2017), the corrected lasso estimator\nwith the tuning parameter R chosen based on 10-fold cross-validation, and the SIMSELEX estimator.\nFor the simulation step of SIMSELEX, the grid of \u03bb contains M = 5 values equally spaced from 0.01 to\n2. For each value of \u03bb, a total of B = 100 sets of pseudodata were generated. The tuning parameter of the\nlasso was chosen using the one-standard-error rule and 10-fold cross-validation. For the group lasso selection\nstep, the step size \u03bd was chosen to be (20L)\u22121, where L is the maximum eigenvalue of the matrix \u039b\u22a4\u039b/M.\n7\nTable 1: Comparison estimators for linear regression based on \u21132 estimation error and ability to recover\nsparsity pattern based on the average number of false positives (FP) and false negatives (FN) across 500\nsimulations. The standard error are included in parentheses.\n\u03b8\nEstimator\np = 100\np = 500\np = 600\n\u21132\nFP\nFN\n\u21132\nFP\nFN\n\u21132\nFP\nFN\n1\nTrue\n0.04\n0.77\n0.00\n0.05\n0.58\n0.00\n0.05\n0.73\n0.00\n(0.01)\n(1.56)\n(0.00)\n(0.01)\n(1.05)\n(0.00)\n(0.01)\n(1.53)\n(0.00)\nNaive\n0.54\n0.80\n0.00\n0.57\n1.41\n0.00\n0.58\n1.16\n0.00\n(0.06)\n(1.52)\n(0.00)\n(0.06)\n(3.03)\n(0.00)\n(0.06)\n(2.71)\n(0.00)\nConic\n0.24\n-\n0.00\n0.26\n-\n-\n0.26\n-\n-\n(0.05)\n-\n-\n(0.05)\n-\n-\n(0.05)\n-\n-\nCorrected Lasso\n0.30\n1.18\n0.00\n0.32\n2.76\n0.00\n0.32\n2.64\n0.00\n(0.06)\n(2.15)\n(0.00)\n(0.06)\n(4.57)\n(0.00)\n(0.06)\n(5.10)\n(0.00)\nSIMSELEX\n0.23\n0.00\n0.00\n0.25\n0.00\n0.00\n0.25\n0.00\n0.00\n(0.08)\n(0.00)\n(0.00)\n(0.08)\n(0.00)\n(0.00)\n(0.08)\n(0.00)\n(0.00)\n2\nTrue\n0.04\n0.72\n0.00\n0.04\n1.14\n0.00\n0.04\n1.26\n0.00\n(0.01)\n(1.62)\n(0.00)\n(0.01)\n(2.42)\n(0.00)\n(0.01)\n(2.75)\n(0.00)\nNaive\n0.30\n0.72\n0.00\n0.31\n1.18\n0.00\n0.32\n1.39\n0.00\n(0.03)\n(1.82)\n(0.06)\n(0.03)\n(2.50)\n(0.06)\n(0.03)\n(3.52)\n(0.00)\nConic\n0.13\n-\n-\n0.15\n-\n-\n0.15\n-\n-\n(0.03)\n-\n-\n(0.03)\n-\n(-\n(0.03)\n-\n-\nCorrected Lasso\n0.17\n0.89\n0.00\n0.18\n1.90\n0.01\n0.19\n2.00\n0.00\n(0.03)\n(1.70)\n(0.06)\n(0.04)\n(3.73)\n(0.08)\n(0.03)\n(3.36)\n(0.04)\nSIMSELEX\n0.23\n0.00\n0.96\n0.25\n0.00\n1.13\n0.25\n0.00\n1.12\n(0.06)\n(0.00)\n(0.47)\n(0.06)\n(0.00)\n(0.49)\n(0.06)\n(0.00)\n(0.48)\nThe lasso estimators were computed using the glmnet function in MATLAB, see Qian et al. (2013). The\ngroup lasso was implementing using our own code, available online with this paper.\nThe \ufb01ve estimators are compared using the average estimation error \u21132 =\nqPp\nj=1(\u02c6\u03b8j \u2212\u03b8j)2. Furthermore,\nthe ability of the method to recover the true sparsity pattern is evaluated by considering the average number\nof false positive (FP) and false negative (FN) estimates per simulated dataset.\nNote that although the\nconic estimator does perform coe\ufb03cient shrinkage, it generally does not set any estimates exactly equal to 0.\nTherefore, one would need to impose a threshold-type method to perform variable selection using the conic\nestimator. This idea is proposed in Belloni et al. (2017), but no implementation guidelines are provided. As\nsuch, variable selection using the conic estimator was not considered in this simulation study. The simulation\nresults are presented in Tables 1.\nTable 1 shows the severe consequence of measurement error on the estimates when performance metrics\n\u21132, false positives, and false negatives are considered. The naive estimator which ignores measurement error\ncompletely has the worst performance \u2014 it has \u21132 error often twice that of either the conic or SIMSELEX\nmethods. The conic and corrected lasso have comparable performance to the SIMSELEX estimators, with\nSIMSELEX having slightly smaller \u21132 error for the case \u03b81, and the conic estimator has slightly smaller \u21132\nerror for the case \u03b82.\nRegarding the ability of these methods to recover the true sparsity pattern, Table 1 demonstrates that the\nSIMSELEX estimator performs very well. In terms of average number of false positives, the naive estimator\nperforms poorly in the settings considered. For the case \u03b81, the SIMSELEX estimator performs the best;\nit is able to recover true sparsity pattern in all the cases considered. The corrected lasso estimator still has\nsome false positives for the case \u03b81. For the case \u03b82, the SIMSELEX estimator still has estimated FP equal\n8\nTable 2: Comparison of estimators for logistic regression based on \u21132 estimation error and ability to recover\nsparsity pattern based on the average number of false positives (FP) and false negatives (FN) across 500\nsimulations. The standard error are included in parentheses.\n\u03b8\nEstimator\np = 100\np = 500\np = 600\n\u21132\nFP\nFN\n\u21132\nFP\nFN\n\u21132\nFP\nFN\n1\nTrue\n1.16\n2.85\n0.00\n1.30\n5.09\n0.00\n1.31\n5.75\n0.00\n(0.15)\n(3.31)\n(0.00)\n(0.15)\n(6.59)\n(0.04)\n(0.14)\n(6.99)\n(0.06)\nNaive\n1.44\n2.36\n0.00\n1.53\n5.04\n0.02\n1.54\n5.40\n0.01\n(0.13)\n(2.93)\n(0.06)\n(0.13)\n(6.50)\n(0.13)\n(0.12)\n(7.03)\n(0.10)\nConditional scores\n2.24\n1.92\n1.40\n2.33\n1.84\n2.14\n2.36\n4.47\n1.65\n(0.71)\n(3.82)\n(1.11)\n(0.65)\n(4.48)\n(1.26)\n(0.67)\n(7.60)\n(1.18)\nSIMSELEX\n1.25\n0.06\n0.11\n1.35\n0.05\n0.22\n1.37\n0.05\n0.24\n(0.23)\n(0.26)\n(0.32)\n(0.23)\n(0.24)\n(0.46)\n(0.22)\n(0.23)\n(0.47)\n2\nTrue\n1.81\n4.81\n0.00\n2.00\n8.99\n0.01\n2.02\n9.52\n0.01\n(0.23)\n(4.32)\n(0.06)\n(0.21)\n(9.94)\n(0.12)\n(0.21)\n(10.32)\n(0.09)\nNaive\n2.32\n3.25\n0.02\n2.44\n6.28\n0.06\n2.46\n6.71\n0.06\n(0.16)\n(3.62)\n(0.15)\n(0.15)\n(7.00)\n(0.24)\n(0.15)\n(7.98)\n(0.24)\nConditional scores\n2.23\n0.90\n1.69\n2.34\n2.66\n1.83\n2.34\n2.09\n2.11\n(0.69)\n(2.26)\n(1.18)\n(0.68)\n(5.95)\n(1.22)\n(0.63)\n(4.88)\n(1.23)\nSIMSELEX\n2.00\n0.06\n0.21\n2.15\n0.07\n0.38\n2.16\n0.05\n0.45\n(0.29)\n(0.23)\n(0.41)\n(0.28)\n(0.26)\n(0.50)\n(0.27)\n(0.23)\n(0.52)\nto 0, but selects on average around one false negative variable. In this same setting, the corrected lasso has\nlower average number of false negatives but higher average number of false positives.\n4.2\nLogistic Regression\nAssume the observed data are of the form (Wi, Yi), i = 1, . . . , n where Yi \u223cBernoulli\n\u0002\nF(X\u22a4\ni \u03b8)\n\u0003\nand\nWi = Xi+Ui. The choice F(x) = logit(x) results in a logistic regression model. Two solutions for performing\nlogistic regression in a sparse high-dimensional setting with errors-in-variables exist in the literature. The\nconditional scores lasso approach of S\u00f8rensen et al. (2015) can be applied to GLMs. This method requires the\ncovariance matrix \u03a3u be known or estimable. Additionally, S\u00f8rensen et al. (2018) proposed a Generalized\nMatrix Uncertainty Selector (GMUS) for sparse high-dimensional models with measurement error.\nThe\nconditional scores lasso is directly comparable to our proposed solution and is reviewed in the supplementary\nmaterial.\nFor the logistic model simulation, data pairs (Wi, Yi) were generated according to the model Yi|Xi \u223c\nBernoulli(pi) where logit(pi) = X\u22a4\ni \u03b8, and covariates are subject to additive measurement error, Wi =\nXi + Ui. Simulation of the true covariates Xi and the measurement error components Ui were done as\noutlined in the linear model simulation (see Section 4.1).\nThe sample size was \ufb01xed at n = 300, and\nsimulations were done for number of covariates p \u2208{100, 500, 600}. Two choice of the true \u03b8 were considered,\n\u03b81 = (1, 1, 1, 1, 1, 0, . . . , 0)\u22a4and \u03b82 = (2, 1.75, 1.50, 1.25, 1, 0, . . . , 0)\u22a4.\nBoth cases have s = 5 non-zero\ncoe\ufb03cients. The true estimator, naive estimator, conditional scores lasso estimator, and the SIMSELEX\nestimator were computed for each simulated dataset. The tuning parameter of the conditional scores lasso\nneeds to be chosen with some care. For brevity, the details are contained in Appendix B. The performance\nmetrics \u21132, and average numbe of false positives (FP) and false negatives (FN) were calculated to compare\nthe estimators. The results are presented in Table 2.\nTable 2 shows that in terms of \u21132 estimation error, the SIMSELEX estimator always performs better\n9\nthan the naive estimator and in many con\ufb01gurations, the SIMSELEX estimator has performance close to\nthe true estimator. The conditional scores lasso has a much higher \u21132 error than the three other estimators\nfor the case of \u03b81, notably performing worse than even the naive estimator. It performs just slightly better\nthan the naive estimator for the case of \u03b82. It should be noted that this could be attributed to inherent\ndi\ufb03culty in choosing the tuning parameter for this approach.\nIn terms of variable selection, SIMSELEX also performs well. SIMSELEX has the lowest average number\nof false positives in all the cases considered, and has only slightly higher average number of false negatives\nthan the true and naive estimator. The conditional scores lasso performs worse than the SIMSELEX across\nall the performance metrics.\n4.3\nCox Proportional Hazard Model\nThe Cox proportional hazard model is commonly used for the analysis of survival data. It is assumed that the\nrandom failure time T has conditional hazard function h(t|X) = h0(t) exp(X\u22a4\u03b8) where h0(t) is the baseline\nhazard function. Survival data is frequently subject to censoring in practice. It is therefore assumed that\nthe observed data are of the form (Wi, Yi, Ii), i = 1, . . . , n where Yi = min(Ti, Ci), Ci being the censoring\ntime for observation i, and Ii = I(Ti < Ci) being an indicator of whether failure occurred in subject i before\nthe censoring time.\nFor the simulation study, the true covariates Xi and the measurement error Ui were simulated as in the\nlinear model simulation (see Section 4.1). The survival times Ti were simulated using the Weibull hazard as\nbaseline, h0(t) = \u03bbT \u03c1t\u03c1\u22121 with shape parameter \u03c1 = 1 and scale parameter \u03bbT = 0.01. The censoring times\nCi were randomly drawn from an exponential distribution with rate \u03bbC = 0.001. Two choice of the true\n\u03b8 were considered, \u03b81 = (1, 1, 1, 1, 1, 0, . . . , 0)\u22a4and \u03b82 = (2, 1.75, 1.50, 1.25, 1, 0, . . . , 0)\u22a4. For \u03b81, the model\ncon\ufb01guration resulted in samples with between 20% and 25% of the observations being censored, while for\n\u03b82, between 25% and 30% of the observations were censored. The sample size was \ufb01xed at n = 300, and\nsimulations were done for number of covariates p \u2208{100, 500, 600}.\nFor the Cox model, implementation of SIMSELEX is much more computationally intensive than the\nlinear and logistic models.\nThis can be attributed to computation of the generalized lasso for the Cox\nmodel, see (Hastie et al., 2015, Section 3.5). As such, only B = 20 replicates were used for each \u03bb value\nin the extrapolation step of the SIMSELEX algorithm. It should further be noted that, to the best of our\nknowledge, the Cox model with high-dimensional data subject to measurement error has not been considered\nin by any other authors. As such, there is no competitor method for use in the simulation study. However,\nthe model using the true covariates not subject to measurement error can be viewed as a gold standard\nmeasure of performance. The naive model was also implemented. The simulation result are reported in\nTable 3.\nSimilar to the case of logistic regressions, the SIMSELEX estimator has a signi\ufb01cantly lower \u21132 error than\nthe naive estimator for both \u03b81 and \u03b82. With regards to recovery of the sparsity pattern, the SIMSELEX\nestimator has average number of false positives and false negatives equal to 0 for parameter vector \u03b81. In\nthis same setting, both the true estimator and the naive estimator result in the selection of between 2 and 4\nfalse positive covariates on average. For the case \u03b82, the true estimator has average number of false positives\nas high as 9, while it is as high as 3 for the naive approach. Neither of these approaches result in false\nnegatives, while the SIMSELEX estimator has average number of false negatives around 0.13 but zero false\npositive in all the considered cases.\n10\nTable 3: Comparison of estimators for Cox survival models based on \u21132 estimation error and ability to recover\nsparsity pattern based on the average number of false positives (FP) and false negatives (FN) across 500\nsimulations.\n\u03b8\nEstimator\np = 100\np = 500\np = 600\n\u21132\nFP\nFN\n\u21132\nFP\nFN\n\u21132\nFP\nFN\n1\nTrue\n0.78\n2.57\n0.00\n0.89\n3.78\n0.00\n0.89\n4.12\n0.00\n(0.11)\n(2.72)\n(0.00)\n(0.11)\n(3.78)\n(0.00)\n(0.11)\n(4.20)\n(0.00)\nNaive\n1.34\n1.65\n0.00\n1.41\n2.27\n0.00\n1.42\n2.34\n0.00\n(0.09)\n(2.21)\n(0.00)\n(0.09)\n(2.75)\n(0.00)\n(0.09)\n(2.73)\n(0.00)\nSIMSELEX\n1.00\n0.00\n0.00\n1.09\n0.00\n0.00\n1.10\n0.00\n0.00\n(0.18)\n(0.00)\n(0.00)\n(0.17)\n(0.00)\n(0.00)\n(0.18)\n(0.00)\n(0.00)\n2\nTrue\n1.20\n5.24\n0.00\n1.37\n8.92\n0.00\n1.38\n9.19\n0.00\n(0.16)\n(4.16)\n(0.00)\n(0.15)\n(6.71)\n(0.00)\n(0.16)\n(6.60)\n(0.00)\nNaive\n2.32\n1.89\n0.00\n2.39\n3.33\n0.00\n2.40\n3.36\n0.00\n(0.10)\n(2.25)\n(0.00)\n(0.11)\n(4.00)\n(0.00)\n(0.11)\n(4.07)\n(0.00)\nSIMSELEX\n1.86\n0.00\n0.05\n1.94\n0.00\n0.13\n1.96\n0.00\n0.14\n(0.22)\n(0.00)\n(0.22)\n(0.23)\n(0.00)\n(0.34)\n(0.23)\n(0.00)\n(0.34)\n5\nSIMSELEX for Spline-Based Regression\n5.1\nSpline Model Estimation\nThe proposed SIMSELEX algorithm can also be adapted for used for more \ufb02exible models such as regression\nusing splines. Assume that the data (Wi, Yi) are generated by an additive model Yi = Pp\nj=1 fj(Xij)+\u03f5i with\nWi = Xi + Ui and Ui having known covariance matrix \u03a3U. Also assume that E[Yi] = 0, i = 1, . . . , n. In\npractice, this can be achieved by centering the observed outcome variable. Furthermore, each of the functions\nfj(x) is assumed su\ufb03ciently smooth so that it can be well-approximated by an appropriately chosen set of\nbasis functions. In this paper, the focus will be on an approximation using cubic B-splines with K knots.\nThis model will have p(K + 3) regression coe\ufb03cients that need to be estimated.\nNow, assume that the true covariates Xi have been observed without measurement error. Let \u03c6jk(x),\nj = 1, . . . , p, k = 1, . . . , K + 3 denote the resulting set of cubic B-spline basis functions where the knots\nfor the jth covariate have been chosen as the (100k)/(K + 1)th percentiles, k = 1, . . . , K, of said covariate.\nThe model to be estimated is then of the form Yi = Pp\nj=1\nPK+3\nk=1 \u03b2jk\u03c6jk(Xij) + \u03f5i. In this setting, the jth\ncovariate is selected if at least one of the coe\ufb03cients \u03b2jk, k = 1, . . . , K is nonzero. Therefore, it is natural\nto delineate all the coe\ufb03cients \u03b2jk into p groups, each corresponding to a covariate and containing K + 3\nparameters. The model parameters are estimated by minimizing the penalized loss function\nR(\u03b2) =\nn\nX\ni=1\n\uf8ee\n\uf8f0Yi \u2212\np\nX\nj=1\nK+3\nX\nk=1\n\u03b2jk\u03c6jk(Xij)\n\uf8f9\n\uf8fb\n2\n+ (1 \u2212\u03b1)\u03ba\np\nX\nj=1\nv\nu\nu\nt\nK+3\nX\nk=1\n\u03b22\njk + \u03b1\u03ba\np\nX\nj=1\nK+3\nX\nk=1\n\u2225\u03b2jk\u2225.\n(8)\nThis loss function has been considered in Simon et al. (2013) for the sparse group lasso estimator. Let \u02c6\u03b2true\ndenote the estimated coe\ufb03cients from this model. The loss function (8) combines the lasso and group lasso\npenalties. The tuning parameter \u03b1 \u2208[0, 1] balances overall parameter sparsity and within-group sparsity.\nWhile it is expected that only a few covariates will be selected, the nonlinear e\ufb00ect of each selected covariate\nmay require a large number of basis functions to be accurately modeled. Therefore, strong overall sparsity\nbut only mild within-group sparsity is expected. As per Simon et al. (2013), \u03b1 = 0.05 is used. The estimator\n11\nof each function fj is \u02c6f true\nj\n(x) = PK+3\nk=1 \u02c6\u03b2true\njk \u03c6jk(x) for all j = 1, . . . , p.\nNow, using the contaminated data Wi, a similar procedure can be followed to obtain the naive es-\ntimator.\nAgain, evaluate the knots of the model as equally spaced percentiles, this time of the covari-\nates contaminated by measurement error. The corresponding cubic B-spline basis functions are denoted\n\u03c6W\njk(x). The naive estimator \u02c6\u03b2naive can be obtained by minimizing a function analogous to (8), but with\ntrue data Xij replaced by contaminated data Wij in the loss function. The naive estimator for function fj\nis \u02c6f naive\nj\n(x) = PK+3\nk=1 \u02c6\u03b2naive\njk\n\u03c6W\njk(x) for all j = 1, . . . , p.\nTo compute the SIMSELEX estimator, for each of the added noise level \u03bbm, generate B pseudodata\n\u02dc\nW (b)(\u03bbm), b = 1, . . . , B as before. The same set of basis functions obtained for the naive estimate is used.\nThen, the estimate \u02c6\u03b2(b)\njk (\u03bbm) for each set of pseudodata is obtained by minimizing a function analogous to\n(8), but with true data Xij replaced by pseudodata \u02dcW (b)\nij (\u03bbm) in the loss function. The estimates \u02c6\u03b2(b)\njk (\u03bbm)\nare averaged across B samples to obtain \u02c6\u03b2jk(\u03bbm) for each \u03bbm in the grid.\nImplementation of the selection step is based on considering the norm of the coe\ufb03cients \u03b2jk, k =\n1, . . . , K + 3, corresponding to the jth covariate instead of modeling each coe\ufb03cient \u03b2jk separately. Specif-\nically, after the simulation step is performed, let \u02c6\u03b2j(\u03bbm) = [\u02c6\u03b2j1(\u03bbm), . . . , \u02c6\u03b2j,K+3(\u03bbm)]\u22a4, m = 1, . . . , M,\nj = 1, . . . , p, and let \u02c6\u03b7mj =\n\r\r\r\u02c6\u03b2j(\u03bbm)\n\r\r\r\nq denote the corresponding \u2113q norm, q = 1, 2. The norm is modeled\nquadratically as\n\u02c6\u03b7mj = \u03930j + \u03931j\u03bbm + \u03932j\u03bb2\nm + \u03b5jm,\nm = 1, . . . , M\nwith \u03b5mj zero-mean error terms. The jth covariate is zeroed out if all the elements of the vector (\u03930j, \u03931j, \u03932j)\nare set to zero. The group lasso loss function to be minimized is\n\u02dcR = 1\n2\nM\nX\ni=1\np\nX\nj=1\n\u0000\u02c6\u03b7mj \u2212\u03930j \u2212\u03931j\u03bbm \u2212\u03932j\u03bb2\nm\n\u00012 + \u03be4\np\nX\nj=1\nq\n\u03932\n0j + \u03932\n1j + \u03932\n2j\n(9)\nsimilar to the loss function de\ufb01ned in Section 3.2. Equation (9) is convex and block-separable, so can be\nminimized e\ufb03ciently through proximal gradient descent methods. The tuning parameter \u03be4 can be chosen\nthrough cross-validation.\nAn alternative approach to selection based on the individual coe\ufb03cients rather than the norm of the\ncoe\ufb03cients was also considered. This latter approach is described in greater detail in Appendix C. Further-\nmore, a simulation study was done to compare the all-coe\ufb03cient approach to the norm-based approach with\nq = 1 and q = 2, see Table 1 in Appendix C. It was concluded that selection based on the \u21132 was fastest to\nimplement and gave best results using all performance metrics considered.\nIf the jth covariate is chosen in the selection step, extrapolation is performed separately on each \u03b2jk to\nget the SIMSELEX estimate for each coe\ufb03cient, denoted by \u02c6\u03b2ssx\njk . Then, the SIMSELEX estimate for each\nfunction fj is computed as \u02c6f s\nj (x) = PK+3\nk=1 \u02c6\u03b2ssx\njk \u03c6W\njk(x).\n5.2\nSimulation\nData pairs (Wi, Yi) were generated according to the additive model Yi = Pp\nj=1 fj(Xij)+\u03f5i, and Wi = Xi+Ui\nwith f1(t) = 3 sin(2t) + sin(t), f2(t) = 3 cos(2\u03c0/3t) + t, f3(t) = (1 \u2212t)2 \u22124, f4(t) = 3t, and fj(t) = 0,\nj = 5, . . . , p.\nThe s = 4 non-zero functions have all been centered at 0.\nThe true covariates Xij were\ngenerated from a Gaussian copula model with correlation structure \u03a3ij = 0.25|i\u2212j|, see Xue-Kun Song\n(2000) for more details.\nThe covariates marginal were then rescaled to have a uniform distribution on\n[\u22123, 3]. The measurement errors Ui were generated to be i.i.d. p-variate normal, Ui \u223cNp(0, \u03c32\nuIp), with\n12\nTable 4: Comparison of estimators for high-dimensional spline-based regression models based on estimation\nerror (MISE) and ability to recover sparsity pattern based on the average number of false positives (FP) and\nfalse negatives (FN) across 500 simulations. The standard errors are included in the parentheses.\n\u03c32\nu\nEstimator\np = 100\np = 500\np = 600\nMISE\nFP\nFN\nMISE\nFP\nFN\nMISE\nFP\nFN\n0.15\nTrue\n16.08\n3.77\n0.00\n18.05\n12.11\n0.000\n18.32\n13.41\n0.00\n(3.20)\n(2.61)\n(0.00)\n(3.28)\n(6.47)\n(0.00)\n(3.21)\n(7.06)\n(0.00)\nNaive\n37.10\n9.48\n0.00\n47.62\n16.00\n0.00\n48.35\n16.37\n0.00\n(7.13)\n(5.78)\n(0.00)\n(8.41)\n(10.16)\n(0.00)\n(7.74)\n(10.20)\n(0.00)\nSIMSELEX\n16.76\n4.62\n0.00\n21.71\n5.49\n0.00\n21.97\n5.42\n0.00\n(4.92)\n(2.90)\n(0.00)\n(6.46)\n(3.41)\n(0.00)\n(5.98)\n(3.25)\n(0.00)\n0.3\nTrue\n16.07\n3.77\n0.00\n18.05\n12.11\n0.00\n18.32\n13.41\n0.000\n(3.20)\n(2.61)\n(0.00)\n(3.28)\n(6.47)\n(0.00)\n(3.21)\n(7.06)\n(0.00)\nNaive\n70.40\n8.70\n0.01\n87.73\n13.26\n0.08\n89.12\n13.43\n0.11\n(12.06)\n(6.04)\n(0.10)\n(13.20)\n(10.84)\n(0.28)\n(13.40)\n(11.15)\n(0.32)\nSIMSELEX\n37.79\n2.96\n0.03\n53.85\n3.27\n0.23\n55.10\n3.15\n0.26\n(11.26)\n(2.31)\n(0.18)\n(15.00)\n(2.74)\n(0.43)\n(15.54)\n(2.69)\n(0.45)\nIp the p \u00d7 p identity matrix. Two values of \u03c32\nu were considered, \u03c32\nu = 0.15 and \u03c32\nu = 0.3, corresponding\nto 5% and 10% noise-to-signal ratios for each individual covariate. Simulations were also done for number\nof covariates p \u2208{100, 500, 600}. Although the NSR look small in each covariate, recall from Section 4.1\nthat the change in total proportion of variability \u2206V increases rapidly in multivariate space.\nFor each\ncon\ufb01guration, N = 500 samples were generated.\nFor each simulated dataset, the true, naive, and SIMSELEX estimators were computed. We are unaware\nof any other method in the literature dealing with spline-based regression in the high-dimensional setting\nwhen covariates are subject to measurement error. For each covariate, the number of knots was chosen to\nbe K = 6. As such, each function fj is modeled by K + 3 = 9 basis functions. In the simulation step\nof SIMSELEX, B = 20 sets of pseudodata are generated for each level of added measurement error. The\nfunction estimators are evaluated using integrated squared error, ISE = Pp\nj=1\nR \u0010\n\u02c6fij(x) \u2212fij(x)\n\u00112\ndx, as\nwell as the number of false positive (FP) and false negative (FN) covariates selected. The simulation results\nare summarized in Table 4.\nTable 4 demonstrates that SIMSELEX has a signi\ufb01cantly lower estimation error (MISE) than the naive\nestimator in all the con\ufb01gurations considered.\nParticularly, in the case of \u03c32\nu = 0.15, the SIMSELEX\nestimator has MISE close to the true estimator. In the case of \u03c32\nu = 0.3, compared to the naive estimator,\nthe SIMSELEX estimator reduces MISE signi\ufb01cantly. For example, in the case of p = 500, the reduction in\nMISE resulting from using the SIMSELEX over the naive estimator is more than 38%. Even so, it is clear\nthat measurement error has a signi\ufb01cant e\ufb00ect on the recovery of the functions fj for the case \u03c32\nu = 0.3.\nRegarding variable selection, the SIMSELEX estimator performs very well in the case of \u03c32\nu = 0.15. In\nthis case, SIMSELEX is always able to select the true non-zero functions by having false negatives equal 0\nin all samples, while having almost the same average number of false positives as the true estimator with\np = 100 and lowest average number of of false positives with p = 500 and p = 600. In the case of \u03c32\nu = 0.3,\nSIMSELEX gives considerably fewer false positives on averages than both the true and naive estimators.\nSIMSELEX does have the highest average number of false negatives for this setting, but this is still below\n0.3 in all the cases considered.\nFinally, Figure 1 shows plots of the estimators corresponding to the \ufb01rst, second, and third quantiles\n13\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22124\n\u22122\n0\n2\n4\nx\nf\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22124\n\u22122\n0\n2\n4\n6\nx\nf\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n0\n5\n10\nx\nf\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22125\n0\n5\nx\nf\n(a)\n(b)\n(c)\n(d)\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22124\n\u22122\n0\n2\n4\nx\nf\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22124\n\u22122\n0\n2\n4\n6\nx\nf\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n0\n5\n10\nx\nf\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22125\n0\n5\nx\nf\n(e)\n(f)\n(g)\n(h)\nFigure 1: Curves Q1 (\n), Q2 (\n), Q3 (\n), and true function (\n) for the esimated\nfunctions from the naive estimators (top) and the SIMSELEX estimators (bottom) corresponding to p = 600\nand \u03c32\nu = 0.15. For (a),(e): f1(x) = 3 sin(2x) + sin(x); for (b),(f): f2(x) = 3 cos(2\u03c0x/3) + x; for (c), (g):\nf3(x) = (1 \u2212x)2 \u22124; for (d), (h): f4(x) = 3x.\n(Q1, Q2, and Q3) of ISE for the naive estimator and the SIMSELEX estimator in the case of \u03c32\nu = 0.15\nand p = 600. The SIMSELEX estimator captures the shape of the functions considerably better, especially\naround the peaks of f1 and f2. Particularly, in the case of \u03c32\nu = 0.15, the SIMSELEX estimator is able to\ncapture the shape of all the nonzero functions very well.\nComparable \ufb01gures for the case \u03c32\nu = 0.3 and p = 600 are given in Figure 2 in Appendix C. As one would\nanticipate there, the increase in measurement error variance results in poorer recovery of the underlying\nfunctions. Even so, SIMSELEX has notably better performance than the naive approach.\n6\nMicroarray Analysis\nIn microarray studies, it is common to take measurements for a large number of genes. At the same time, it\nis often assumed that only a small number of these genes are related to the outcome of interest. Furthermore,\nmicroarray studies tend to have both noisy measurements and small sample sizes (relative to the number of\ngenes measured). As such, SIMSELEX is well-suited for identifying genes related to the outcome of interest.\nIn this data application, an A\ufb00ymetrix microarray dataset containing gene expression measurements of 144\nfavorable histology Wilms tumors is analyzed. The dataset is publicly available on the ArrayExpress website\nunder access number E-GEOD-10320. In these Wilms tumors, the cancer cell\u2019s nuclei is not very large or\ndistorted, so a high proportion of patients are successfully treated. Relapse is a possibility after treatment,\nmeaning that these tumors can recur. It is of interest to identify the genes associated with relapse. In the\nWilms tumors dataset, out of n = 144 samples, 53 patients experienced a relapse, and 91 patients had no\nrelapse with a minimum of three years follow-up. The data collection process also made use of multiple\n14\nprobes per patient, i.e. replicate data are available for each patient. This allows for the measurement error\nvariance for each gene to be estimated. The gene expression measurements are transformed to a logarithmic\nscale for analysis.\nThese data were previously analyzed by S\u00f8rensen et al. (2015). To make their analysis comparable to\nthe SIMSELEX approach, data preprocessing is done as described by them. Speci\ufb01cally, the raw data was\nprocessed using the Bayesian Gene Expression (BGX) Bioconductor of Hein et al. (2005). This analysis\ncreates a posterior distribution for the log-scale expression level of each gene in each sample. For gene j in\npatient i, the posterior mean \u02c6\u00b5ij was then taken as an estimates of the true gene expression level.\nNow, let \u02c6\u00b5j = (\u02c6\u00b51j, . . . , \u02c6\u00b5nj)\u22a4denote the estimated vector of gene expression levels for gene j for the\nn patients. Furthermore, let \u00af\u00b5j = (1/n) Pn\nj=1 \u02c6\u00b5ij and \u02c6\u03c32\nj = (1/n) Pn\nj=1(\u02c6\u00b5ij \u2212\u00af\u00b5j)2 denote the mean and\nvariance of each gene. Standardized measurements Wi = (Wi1, . . . , Wip), i = 1, . . . , n can then be calculated\nas Wij = (\u02c6\u00b5ij \u2212\u00af\u00b5j)/\u02c6\u03c3j,\ni = 1, . . . , n,\nj = 1, . . . , p. To quantify the measurement error present in the data,\nit is assumed that the measurement error variance is constant across patients (samples) for a given gene and\nthat the measurement error itself is independent across all genes for a given patient. The measurement\nerror variance need not be equal across genes. Let var(\u02c6\u00b5ij) denote the posterior variance of the estimated\ndistribution of gene j, patient i. These estimates are then combined as \u02c6\u03c32\nu,j = (1/n) Pn\ni=1 var(\u02c6\u00b5ij). The\nmeasurement error covariance matrix of the standardized data W is then estimated by matrix with diagonal\nelements ( \u02c6\u03a3u)j,j = \u02c6\u03c32\nuj/\u02c6\u03c32\nj , j = 1, . . . , p and o\ufb00-diagonal elements equal to 0. Finally, only the p = 2074\ngenes with \u02c6\u03c32\nu,j < (1/2)\u02c6\u03c32\nj were retained. That is, only genes with estimated noise-to-signal ratio less than\n1 were retained for the analysis.\nUsing the data (Wi, Yi), i = 1, . . . , n, with Yi an indicator of relapse, four di\ufb00erent procedures were used\nto \ufb01t a logistic regression model to the data. These procedures are a naive model with lasso penalty, the\nconditional scores lasso of S\u00f8rensen et al. (2015), the SIMSELEX model proposed in this paper, and \ufb01nally\na SIMEX model (i.e. no selection step is implemented). For the naive, SIMSELEX and SIMEX models, 10-\nfold cross-validations using the one-standard-error rule was used to select the tuning parameter. The elbow\nmethod was used for choosing the tuning parameters in computing the conditional scores lasso. The SIMEX\nmodel without selection identi\ufb01ed 1699 out of 2074 genes. Even though many of the estimated coe\ufb03cients\nare close to zero, 17 of the estimated coe\ufb03cients exceed 0.1, and a further 41 exceed 0.01. This result is not\nof much practical value if one assumes that only a small number of the genes are associated with relapse.\nThe results of the other three analyses are presented in Table 5.\nThe naive approach identi\ufb01ed 26 non-zero genes, while conditional scores identi\ufb01ed 13 non-zero genes.\nSIMSELEX identi\ufb01ed only 3 non-zero genes. Note that all the genes chosen by the SIMSELEX were also\nchosen by the conditional scores estimator and the naive estimator. However, the magnitude of the esti-\nmated coe\ufb03cients were much larger for SIMSELEX compared to the naive and conditional scores estimators.\nInterpreting these results in the context of the simulation results presented, both the naive and conditional\nscores approaches tend to have false positives, potentially accounting for the larger number of genes selected.\nIt is possible that SIMSELEX misses some genes as it is more prone to false negatives. However, in the\nsimulation scenarios considered, the false negative rate of SIMSELEX was considerably lower than the false\npositive rate of conditional scores.\n15\nTable 5: Gene symbols and estimated coe\ufb03cients from the naive lasso, the conditional scores lasso, and the\nSIMSELEX estimator applied to the Wilms tumors data. Genes selected by SIMSELEX are printed in bold.\nGene\nNaive\nConditional scores\nSIMSELEX\n202016 at\n-0.2216\n-0.0348\n-0.3758\n205132 at\n-0.1997\n-0.2127\n-0.3739\n213089 at\n0.2096\n0.0575\n0.3886\n209466 x at\n-0.0310\n-0.2425\n218678 at\n-0.1256\n-0.1600\n209259 s at\n-0.1038\n-0.1599\n209281 s at\n-0.0511\n-0.1054\n204710 s at\n-0.2004\n-0.0958\n202766 s at\n-\n-0.0740\n208905 at\n-\n-0.0463\n201194 at\n-\n-0.0448\n211737 x at\n-\n-0.0279\n203156 at\n-0.1090\n-0.0128\n213779 at\n0.1142\n201859 at\n-0.1087\n208965 s at\n0.1388\n205933 at\n0.0913\n(12 more\nnon-zero genes)\n| \u00b7 | < 0.06\n7\nConclusion\nThe paper presents a modi\ufb01ed SIMEX algorithm with a selection step for sparse models estimation in high-\ndimensional settings when measurement error is present. This algorithm, referred to as the SIMSELEX, is\nconsidered in various modeling settings, including linear regression, logistic regression, the Cox proportional\nhazards model, and spline-based regression.\nIn the linear model setting, it is seen to have performance\ncomparable to the corrected lasso. In the logistic model setting, it has much better performance than the\ncorrected scores lasso. In the Cox model and spline-model settings, no other estimators have been proposed in\nthe literature. For these, it is shown that the method leads to much better performance than a naive approach\nthat ignores measurement error, and compares favorably to estimators obtained using the uncontaminated\ndata.\nReferences\nApanasovich, T. V., Carroll, R. J., and Maity, A. (2009). Simex and standard error estimation in semipara-\nmetric measurement error models. Electronic journal of statistics 3, 318.\nArmstrong, B. (1985). Measurement error in the generalised linear model. Communications in Statistics-\nSimulation and Computation 14, 529\u2013544.\nBelloni, A., Rosenbaum, M., and Tsybakov, A. B. (2017). Linear and conic programming estimators in\nhigh dimensional errors-in-variables models. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology) 79, 939\u2013956.\n16\nCarroll, R. J., Ruppert, D., Stefanski, L. A., and Crainiceanu, C. M. (2006). Measurement error in nonlinear\nmodels: a modern perspective. CRC press.\nCook, J. R. and Stefanski, L. A. (1994). Simulation-extrapolation estimation in parametric measurement\nerror models. Journal of the American Statistical association 89, 1314\u20131328.\nDatta, A., Zou, H., et al. (2017). Cocolasso for high-dimensional error-in-variables regression. The Annals\nof Statistics 45, 2400\u20132426.\nFriedman, J., Hastie, T., and Tibshirani, R. (2001). The elements of statistical learning, volume 1. Springer\nseries in statistics New York.\nHastie, T., Tibshirani, R., and Wainwright, M. (2015).\nStatistical learning with sparsity: the lasso and\ngeneralizations. CRC press.\nHein, A.-M. K., Richardson, S., Causton, H. C., Ambler, G. K., and Green, P. J. (2005). Bgx: a fully\nbayesian integrated approach to the analysis of a\ufb00ymetrix genechip data. Biostatistics 6, 349\u2013373.\nK\u00a8uchenho\ufb00, H., Mwalili, S. M., and Lesa\ufb00re, E. (2006). A general method for dealing with misclassi\ufb01cation\nin regression: the misclassi\ufb01cation simex. Biometrics 62, 85\u201396.\nLederer, J. (2013). Trust, but verify: bene\ufb01ts and pitfalls of least-squares re\ufb01tting in high dimensions. arXiv\npreprint arXiv:1306.0113 .\nPrentice, R. (1982). Covariate measurement errors and parameter estimation in a failure time regression\nmodel. Biometrika 69, 331\u2013342.\nQian, J., Hastie, T., Friedman, J., Tibshirani, R., and Simon, N. (2013). Glmnet for matlab 2013. URL\nhttp://www. stanford. edu/\u02dc hastie/glmnet matlab .\nRosenbaum, M., Tsybakov, A. B., et al. (2010). Sparse recovery under matrix uncertainty. The Annals of\nStatistics 38, 2620\u20132651.\nRosenbaum, M., Tsybakov, A. B., et al. (2013). Improved matrix uncertainty selector. In From Probability to\nStatistics and Back: High-Dimensional Models and Processes\u2013A Festschrift in Honor of Jon A. Wellner,\npages 276\u2013290. Institute of Mathematical Statistics.\nSimon, N., Friedman, J., Hastie, T., and Tibshirani, R. (2013). A sparse-group lasso. Journal of Computa-\ntional and Graphical Statistics 22, 231\u2013245.\nS\u00f8rensen, \u00d8., Frigessi, A., and Thoresen, M. (2015). Measurement error in lasso: Impact and likelihood bias\ncorrection. Statistica Sinica pages 809\u2013829.\nS\u00f8rensen, \u00d8., Hellton, K. H., Frigessi, A., and Thoresen, M. (2018). Covariate selection in high-dimensional\ngeneralized linear models with measurement error. Journal of Computational and Graphical Statistics .\nStefanski, L. A. and Cook, J. R. (1995). Simulation-extrapolation: the measurement error jackknife. Journal\nof the American Statistical Association 90, 1247\u20131256.\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical\nSociety. Series B (Methodological) pages 267\u2013288.\n17\nXue-Kun Song, P. (2000). Multivariate dispersion models generated from gaussian copula. Scandinavian\nJournal of Statistics 27, 305\u2013320.\nAppendix A: Failure of SIMEX for lasso\nIn both Sections 1 and 2 of the main paper, it was mentioned that simulation-extrapolation (SIMEX) fails\nwhen applied to high-dimensional errors-in-variables models without suitable modi\ufb01cation to the procedure.\nHere, a simulated example is presented to demonstrate said failure. Speci\ufb01cally, standard SIMEX in\ufb02ates\nthe number of estimated nonzero components considerably, even when combined with a procedure such as\nthe lasso.\nFor the simulation, data pairs (Wi, Yi) were generated according to the linear model Yi = X\u22a4\ni \u03b8 + \u03b5i\nwith additive measurement error Wi = Xi + Ui. Both the true covariates Xi and the measurement error\ncomponents Ui were generated to be i.i.d. p-variate normal. Speci\ufb01cally, Xi \u223cNp(0, \u03a3), with \u03a3 having\nentries \u03a3ij = \u03c1|i\u2212j| with \u03c1 = 0.25, and Ui \u223cNp(0, \u03a3u) with \u03a3u = \u03c32\nuIp\u00d7p with \u03c32\nu = 0.45. The error\ncomponents \u03b5i were simulated to be i.i.d. univariate normal, \u03b5 \u223cN(0, \u03c32\n\u03b5) with \u03c3\u03b5 = 0.128. The sample\nsizes was \ufb01xed at n = 300, and the number of covariates was p = 500. The parameter vector was taken to\nbe \u03b8 = {1, 1, 1, 1, 1, 0, . . . , 0} with s = 5 nonzero coe\ufb03cients and p \u2212s = 495 zero coe\ufb03cients.\nThe SIMEX procedure was implemented as outlined in Section 2 of the main paper. In the simulation\nstep, the grid of \u03bb-values contained M = 13 equally spaced values ranging from 0.2 to 2. For each value of\n\u03bb, a total of B = 100 sets of pseudo-data were generated. In applying the lasso, the tuning parameter was\nchosen based on the one-standard-error rule based on 10-fold cross-validation. The lasso was implemented\nusing the glmnet package in R. For the extrapolation step, a quadratic function was used.\nThe analysis of the simulated data shows that SIMEX applied to the lasso results in 174 nonzero parameter\nestimates. Of the 169 false positives, 156 are fairly small (less than 0.001 in absolute value), with 13 false\npositives being larger (greater than 0.001 in absolute value). Comparitively, a naive application of the lasso\n(not correcting for measurement error) gives only 5 non-zero parameter estimates.\nThe failure of SIMEX in performing variable selection is intuitive \u2013 consider a \ufb01xed value of \u03bb and a set\nof simulated pseudo-data. In any set of pseudo-data, it is possible that a new false positive detection occurs.\nThus, given B sets of pseudo-data, there can be multiple di\ufb00erent false positives, all corresponding to the\nsame value of \u03bb. In the averaged estimate \u02c6\u03b8(\u03bb), there are potentially several di\ufb00erent non-zero estimates that\nonly showed up in a small fraction of the sets of pseudo-data. This, of course, occurs for every value of \u03bb. For\nthe ith variable, when the extrapolation step is applied to the simulated data (\u03bbi, \u02c6\u03b8j(\u03bbi)), i = 1, . . . , M, the\nextrapolated estimate will be non-zero \ufb01nal even if there is only a single value of \u02c6\u03b8j(\u03bbi) that is non-zero. The\nproposed SIMSELEX procedure augmenting SIMEX with a variable selection step is speci\ufb01cally designed to\novercome this di\ufb03culty.\nAppendix B: Review of exiting methodology\nIn Section 4 of the main paper, the SIMSELEX estimator is compared to several existing methods for \ufb01tting\nerrors-in-variables models in high-dimensional settings. For the linear model, SIMSELEX is compared with\nthe corrected lasso estimator of S\u00f8rensen et al. (2015) and the conic estimator of Belloni et al. (2017). For\nthe logistic model, the SIMSELEX estimator is compared with the conditional scores lasso of S\u00f8rensen et al.\n(2015). These approaches are brie\ufb02y reviewed in this section.\n18\nLinear Model\nThe corrected lasso estimator of S\u00f8rensen et al. (2015) is the solution to the optimization problem\nmin\n\u03b8\nL(\u03b8) = \u2225Y \u2212W \u03b8\u22252\n2 \u2212\u03b8\u22a4\u03a3u\u03b8\ns.t. \u2225\u03b8\u22251 \u2264R\nwhere for p-dimensional vector x, \u2225x\u22251 = Pp\nj=1 |xj| and \u2225x\u22252\n2 = Pp\nj=1 x2\nj. Here, R is a tuning parameter\nthat can be chosen based on cross-validation using an estimate of the unbiased loss function. Speci\ufb01cally,\nif the data are partitioned into random subset P1, . . . , PJ, each subset having size n/J, let (W(Pj), Y(Pj))\ndenote the data in the jth partition and let (W(\u2212Pj), Y(\u2212Pj)) denote the data excluding the jth partition.\nAlso let \u02c6\u03b8j denote the estimated parameter vector based on (W(\u2212Pj), Y(\u2212Pj)). Then the tuning parameter\nR can be chosen using cross-validation loss function\nLCV (R) =\nJ\nX\nj=1\n\r\r\rYPj \u2212WPj \u02c6\u03b8j\n\r\r\r\n2\n2 \u2212\nJ\nX\nj=1\n\u02c6\u03b8\u22a4\nj \u03a3u \u02c6\u03b8j.\nThe optimal tuning parameter R can be chosen either to minimize LCV , or according to the one standard\nerror rule (see Friedman et al. (2001)).\nS\u00f8rensen et al. (2015) prove that the corrected lasso performs\nsign-consistent covariate selection in large samples.\nThe conic estimator of Belloni et al. (2017) is also the solution to an optimization problem,\nmin\n\u03b8,t \u2225\u03b8\u22251 + \u03bbt\ns.t\n\r\r\r\r\n1\nnW\u22a4(Y \u2212W\u03b8 + \u03a3u\u03b8)\n\r\r\r\r\n\u221e\n\u2264\u00b5t + \u03c4,\nt \u22650,\n\u2225\u03b8\u22252 \u2264t.\nwhere for p-dimensional vector x, \u2225x\u2225\u221e= maxj=1,...,p |xj|. This method requires the selection of three\ntuning parameters, here denoted \u00b5, \u03c4 and \u03bb. The optimal choices of these tuning parameters depend on the\nunderlying model structure, including the rate at which the number of nonzero model coe\ufb03cients increases\nwith sample size. Belloni et al. (2017) do suggest tuning parameter values for application. Furthermore,\nthese authors also proved that under suitable sparsity conditions, their conic estimator has smaller minimax\ne\ufb03ciency bound than the Matrix Uncertainty Selection estimator of Rosenbaum et al. (2010). We are not\naware of any comparison, numerical or otherwise, of the corrected lasso estimator and the conic estimator.\nThis comparison is presented as part of our simulation study in Section 4.1 of the main paper.\nLogistic Regression\nFor the logistic regression model, the SIMSELEX estimator is compared with the conditional scores lasso\nestimator developed by S\u00f8rensen et al. (2015). The conditional scores lasso estimator is computed by solving\nthe set of estimating equations\nn\nX\ni=1\n\u0012\nYi \u2212F\n\u001a\n\u03b7i \u22121\n2\u03b8\u22a4\u03a3u\u03b8\n\u001b\u0013  \n1\nWi + Yi\u03a3u\u03b8\n!\n= 0 subject to \u2225\u03b8\u22251 \u2264R\n19\nwhere \u03b7i = \u00b5 + \u03b8\u22a4(Wi + Yi\u03a3u\u03b8). Note that this is a system of p + 1 estimating equations. S\u00f8rensen et al.\n(2015) also illustrate how the conditional scores lasso can be applied to other GLMs.\nSince there is no well-de\ufb01ned loss function associated with the conditional scores lasso, the tuning pa-\nrameter R can\u2019t be chosen based on cross-validation as in the linear case. Instead, the authors suggest using\nthe elbow method as in Rosenbaum et al. (2010). First, a grid of R-values is chosen. For each value of R\nin the grid, the conditional score lasso estimator is computed. Finally, the number of non-zero coe\ufb03cients\nis plotted as a function of R, and the optimal R is chosen as the point at which the plot elbows i.e. starts\nto become \ufb02at. Note that \ufb01nding this elbow for the conditional scores lasso is somewhat subjective and the\nauthors do not provide an automated way of performing this selection.\nFor the simulation study in Section 4.2 of the main paper, the tuning parameter R was chosen in a manner\nidentical to the simulation study presented in S\u00f8rensen et al. (2015). First, N0 = 100 samples were simulated\nusing the data generation mechanism outlined. For the jth simulated dataset, let R = \u03b4\n\r\r\r\u02c6\u03b8naive\n\r\r\r\n1, where\n\r\r\r\u02c6\u03b8naive\n\r\r\r\n1 denotes the \u21131 norm of the naive lasso estimator. Let (\u03b4, NZj(\u03b4)) denote the curve of the number\nof non-zero coe\ufb03cients as a function of \u03bb. These curves were then averaged, resulting in curve (\u03b4, NZ(\u03b4))\nwhere NZ(\u03b4) = N \u22121\n0\nP\nj NZj(\u03b4). The value of \u03b4 used subsequently to evaluate the conditional scores lasso\nestimators in the simulation study was the point at which the curve NZ(\u03b4) elbows. For each given simulation\ncon\ufb01guration, a di\ufb00erent value of \u03b4 was calculated. The elbow plots for this simulation study are presented\nbelow.\nFigure 2 below illustrates the shape of the curve (\u03b4, NZ(R)) for the six given simulation con\ufb01gurations,\nwhere the dashed lines indicate the (subjective) point where the curves elbow.\nAppendix C: Additional Methods and Results for Spline Regression\nVariable Selection\nRecall that for the spline regression, the data (Wi, Yi) are assumed to be generated by an additive model\nYi =\np\nX\nj=1\nfj(Xij) + \u03f5i\nwith Wi = Xi + Ui and Ui having known covariance matrix \u03a3U. It is assumed that E[Yi] = 0, i = 1, . . . , n,\nand each function fj is assumed to be su\ufb03ciently smooth and well-approximated by K + 3 basis functions\n\u03c6j1(x), . . . , \u03c6j,K+3(x). The model to be estimated is then of the form\nYi =\np\nX\nj=1\nK+3\nX\nk=1\n\u03b2jk\u03c6jk(Xij) + \u03f5i.\nAfter the simulation step of SIMSELEX, the jth covariate is associated with K+3 \u201cpaths\u201d (\u03bbi, \u02c6\u03b2j1(\u03bbi)), . . . , ...(\u03bbi, \u02c6\u03b2j,K+3(\u03bbi)),\neach of which needs to be extrapolated to \u03bb = \u22121. This is di\ufb00erent from the parametric model setting con-\nsidered in Section 4 of the main paper, where each covariate j is associated with only one parameter path\n\u03b8j(\u03bbi) that needs to be extrapolated to \u03bb = \u22121. Therefore, the selection step for spline-based regression\nneeds to be approached with some care. Here, two di\ufb00erent approaches for selection step are considered.\nThe \ufb01rst approach for selection considered applies a variation of the group lasso to all p(K+3) coe\ufb03cients\n20\n5\n4\n3\n2\n1\n0\n0\n5\n10\n15\n20\n25\n30\n\u03b4\nAvgNZ\n(a) p = 100 with \u03b81\n5\n4\n3\n2\n1\n0\n0\n10\n20\n30\n40\n\u03b4\nAvgNZ\n(b) p = 500 with \u03b81\n5\n4\n3\n2\n1\n0\n0\n10\n20\n30\n40\n\u03b4\nAvgNZ\n(c) p = 600 with \u03b81\n5\n4\n3\n2\n1\n0\n0\n10\n20\n30\n40\n\u03b4\nAvgNZ\n(d) p = 100 with \u03b82\n5\n4\n3\n2\n1\n0\n0\n20\n40\n60\n80\n\u03b4\nAvgNZ\n(e) p = 500 with \u03b82\n5\n4\n3\n2\n1\n0\n0\n20\n40\n60\n80\n\u03b4\nAvgNZ\n(f) p = 600 with \u03b82\nFigure 2: Elbow plots choosing tuning parameters in implementation of conditional scores lasso estimator\nin the logistic regression simulation.\n21\n\u03b2jk. This is done using a quadratic extrapolation function. Speci\ufb01cally, it is assumed that\n\u02c6\u03b2jk(\u03bbi) = \u03930jk + \u03931jk\u03bbi + \u03932jk\u03bb2\ni + \u03b5ijk,\ni = 1, . . . , M,\nj = 1, . . . , p,\nk = 1, . . . , K + 3\nwith \u03b5ijk zero-mean error terms. With this approach, the jth covariate is zeroed out if all the parameter\nestimates {\u02c6\u0393ijk}i=0,1,2, k=1,...,K equal zero. Applying the group lasso, the loss function to be minimized is\nR =\np\nX\nj=1\n\u0010\n\u2225\u0398j \u2212\u039b\u0393j\u22252\n2 + \u03be3 \u2225\u0393j\u22252\n\u0011\n(10)\nwhere\n\u0393j =\n\uf8ee\n\uf8ef\uf8f0\n\u03930j1\n. . .\n\u03930jK\n\u03931j1\n. . .\n\u03931jK\n\u03932j1\n. . .\n\u03932jK\n\uf8f9\n\uf8fa\uf8fb, \u0398j =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u02c6\u03b2j1(\u03bb1)\n. . .\n\u02c6\u03b2jK(\u03bb1)\n...\n...\n\u02c6\u03b2j1(\u03bbM)\n. . .\n\u02c6\u03b2jK(\u03bbM)\n\uf8f9\n\uf8fa\uf8fa\uf8fb, \u039b =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n\u03bb1\n\u03bb2\n1\n...\n...\n...\n1\n\u03bbM\n\u03bb2\nM\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\nand \u2225.\u22252 denotes the Frobenius norm (matrix version of the \u21132 norm). This is a very natural extension of the\napproach considered in Section 4. The tuning parameter \u03be4 can be chosen through cross-validation. Even\nthough (10) is convex and block-separable, the minimization is computationally very expensive due to the\nnumber of model parameters. As such, an alternative approach intended to speed up computation was also\nconsidered.\nThe alternative approach considered for selection applies the group lasso not to each individual coe\ufb03cient,\nbut to the norm of each group of coe\ufb03cients \u03b2jk, k = 1, . . . , K + 3 corresponding to the jth covariate. This\nis motivated by noting that the norm of a group of coe\ufb03cients will only equal 0 if all the coe\ufb03cients in said\ngroup are equal to 0. More speci\ufb01cally, let \u02c6\u03b2j(\u03bbi) = [\u02c6\u03b2j1(\u03bbi), . . . , \u02c6\u03b2jK(\u03bbi)]\u22a4, i = 1, . . . , M, j = 1, . . . , p, and\nlet \u02c6\u03b7ij =\n\r\r\r\u02c6\u03b2j(\u03bbi)\n\r\r\r\nq denote the corresponding \u2113q norm. The two scenarios considered are q = 1 and 2. The\nnorm is modeled quadratically as\n\u02c6\u03b7ij = \u03930j + \u03931j\u03bbi + \u03932j\u03bb2\ni + \u03b5ij,\ni = 1, . . . , M,\nwith \u03b5ij zero-mean error terms. The jth covariate is not selected if all the elements of the estimated vector\n(\u02c6\u03930j, \u02c6\u03931j, \u02c6\u03932j) are equal to zero. The group lasso loss function to be minimized is\n\u02dcR = 1\n2\nM\nX\ni=1\np\nX\nj=1\n\u0000\u02c6\u03b7ij \u2212\u03930j \u2212\u03931j\u03bbi \u2212\u03932j\u03bb2\ni\n\u00012 + \u03be4\np\nX\nj=1\nq\n\u03932\n0j + \u03932\n1j + \u03932\n2j.\n(11)\nEquation (11) is convex and block-separable, and can be minimized e\ufb03ciently through proximal gradient\ndescent methods. The tuning parameter \u03be4 can be chosen through cross-validation.\nTable 6 compares the performance of the SIMSELEX estimator with three methods of doing variable\nselection in the case of p = 100 and with \u03c32\nu = 0.15. Other simulation parameters are as speci\ufb01ed in Section\n5.2. Firstly, selection approach (10) using individual models for all the coe\ufb03cients \u03b2jk was implemented.\nSecondly, approach (11) was applied both for the \u21131 norm and for the \u21132 norm, calculated based on the\ngroups of parameters corresponding to speci\ufb01c variables. The table reports the MISE, the number of false\npositives (FP) and false negatives, and also the average time (in seconds), all calculated for 500 simulated\nsamples. The average time was recorded based on running the simulations on one node (memory 7GB) of\nManeFrame II (M2), the high-performance computing cluster of Southern Methodist University in Dallas,\n22\nTable 6: Comparison of SIMSELEX variable selection methods for spline regression with p = 100.\nSelection\nMISE\nFP\nFN\nTime (second)\nAll coe\ufb03cients\n17.32\n21.50\n0.00\n819.00\n\u21131 norm\n17.17\n10.06\n0.00\n59.70\n\u21132 norm\n16.76\n4.62\n0.00\n56.68\nTX.\nConsidering the results in Table 1, selection based on the \u21132 norm gives the best result, while selection\nbased on individually considering all the coe\ufb03cients gives the worst results. The latter also takes more\nthan 14 times longer to compute (on average) than the \u21132 approach. The \u21131 approach is comparable to \u21132\nin terms of MISE and average computation time, but has a much higher average number of false positive\nselections. Therefore, the SIMSELEX estimator with selection using \u21132 norm for parameter groups is used\nfor the simulation study in the main paper.\nAdditional Plots for Estimated Functions\nIn the simulation study of spline regressions, the SIMSELEX and the naive estimator is compared based\non estimation error, ability to recover the true sparsity pattern, and ability to capture the true shape of\nnonzero functions. Similar to the Figure 1 of the main paper, Figure 3 below shows plots of the estimators\ncorresponding to the \ufb01rst, second, and third quantiles (Q1, Q2, and Q3) of ISE for the naive estimator\n(top) and the SIMSELEX estimator (bottom) in the case of \u03c32\nu = 0.30 and p = 600. It can be seen that\nthe SIMSELEX estimator is able to capture the shape of the functions considerably better than the naive\nestimator.\n23\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22124\n\u22122\n0\n2\n4\nx\nf\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22124\n\u22122\n0\n2\n4\n6\nx\nf\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n0\n5\n10\nx\nf\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22125\n0\n5\nx\nf\n(a)\n(b)\n(c)\n(d)\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22124\n\u22122\n0\n2\n4\nx\nf\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22124\n\u22122\n0\n2\n4\n6\nx\nf\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n0\n5\n10\nx\nf\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22125\n0\n5\nx\nf\n(e)\n(f)\n(g)\n(h)\nFigure 3: Curves Q1 (\n), Q2 (\n), Q3 (\n), and true function (\n) for the esimated\nfunctions from the naive estimators (top) and the SIMSELEX estimators (bottom) corresponding to p = 600\nand \u03c32\nu = 0.30. For (a),(e): f1(x) = 3 sin(2x) + sin(x); for (b),(f): f2(x) = 3 cos(2\u03c0x/3) + x; for (c), (g):\nf3(x) = (1 \u2212x)2 \u22124; for (d), (h): f4(x) = 3x.\n24\n"}