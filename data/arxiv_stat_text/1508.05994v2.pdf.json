{"text": "arXiv:1508.05994v2  [math.ST]  29 Jan 2016\nImproved estimation in a general multivariate elliptical model\nTatiane F. N. Melo\nInstitute of Mathematics and Statistics, Federal University of Goi\u00b4as, Brazil\nemail: tmelo@ufg.br\nSilvia L. P. Ferrari\nDepartment of Statistics, University of S\u02dcao Paulo, Brazil\nemail: silviaferrari@usp.br\nAlexandre G. Patriota\nDepartment of Statistics, University of S\u02dcao Paulo, Brazil\nemail: patriota@ime.usp.br\nAbstract\nThe problem of reducing the bias of maximum likelihood estimator in a general multivariate elliptical regression\nmodel is considered. The model is very \ufb02exible and allows the mean vector and the dispersion matrix to have parame-\nters in common. Many frequently used models are special cases of this general formulation, namely: errors-in-variables\nmodels, nonlinear mixed-effects models, heteroscedastic nonlinear models, among others. In any of these models, the\nvector of the errors may have any multivariate elliptical distribution. We obtain the second-order bias of the maximum\nlikelihood estimator, a bias-corrected estimator, and a bias-reduced estimator. Simulation results indicate the effective-\nness of the bias correction and bias reduction schemes.\nKeywords: Bias correction; bias reduction; elliptical model; maximum likelihood estimation; general parameteri-\nzation.\n1\nIntroduction\nIt is well known that, under some standard regularity conditions, maximum-likelihood estimators (MLEs) are consistent\nand asymptotically normally distributed. Hence, their biases converge to zero when the sample size increases. However,\nfor \ufb01nite sample sizes, the MLEs are in general biased and bias correction plays an important role in the point estimation\ntheory.\nA general expression for the term of order O(n\u22121) in the expansion of the bias of MLEs was given by Cox and\nSnell (1968). This term is often called second-order bias and can be useful in actual problems. For instance, a very high\nsecond-order bias indicates that other than maximum-likelihood estimation procedures should be used. Also, corrected\nestimators can be formulated by subtracting the estimated second-order biases from the respective MLEs. It is expected\nthat these corrected estimators have smaller biases than the uncorrected ones, especially in small samples.\n1\nCox and Snell\u2019s formulae for second-order biases of MLEs were applied in many models. Cordeiro and McCullagh\n(1991) use these formulae in generalized linear models; Cordeiro and Klein (1994) compute them for ARMA models;\nCordeiro et al. (2000) apply them for symmetric nonlinear regression models; Vasconcellos and Cordeiro (2000) obtain\nthem for multivariate nonlinear Student t regression models. More recently, Cysneiros et al. (2010) study the univariate\nheteroscedastic symmetric nonlinear regression models (which are an extension of Cordeiro et al. 2000) and Patriota\nand Lemonte (2009) obtain a general matrix formula for the bias correction in a multivariate normal model where the\nmean and the covariance matrix have parameters in common.\nAn alternative approach to bias correction was suggested by Firth (1993). The idea is to adjust the estimating function\nso that the estimate becomes less biased. This approach can be viewed as a \u201cpreventive\u201d method, since it modi\ufb01es the\noriginal score function, prior to obtaining the parameter estimates. In this paper, estimates obtained from Cox and\nSnell\u2019s approach and Firth\u2019s method will be called bias-corrected estimates and bias-reduced estimates, respectively.\nFirth showed that in generalized linear models with canonical link function the preventive method is equivalent to\nmaximizing a penalized likelihood that is easily implemented via an iterative adjustment of the data. The bias reduction\nproposed by Firth has received considerable attention in the statistical literature. For models for binary data, see Mehrabi\nand Matthews (1995); for censored data with exponential lifetimes, see Pettitt et al. (1998). In Bull et al. (2002) bias\nreduction is obtained for the multinomial logistic regression model. In Kosmidis and Firth (2009) a family of bias-\nreducing adjustments was developed for a general class of univariate and multivariate generalized nonlinear models. The\nbias reduction in cumulative link models for ordinal data was studied in Kosmidis (2014). Additionally, Kosmidis and\nFirth (2011) showed how to obtain the bias-reducing penalized maximum likelihood estimator by using the equivalent\nPoisson log-linear model for the parameters of a multinomial logistic regression.\nIt is well-known and was noted by Firth (1993) and Kosmidis and Firth (2009) that the reduction in bias may\nsometimes be accompanied by in\ufb02ation of variance, possibly yielding an estimator whose mean squared error is bigger\nthan that of the original one. Nevertheless, published empirical studies such as those mentioned above show that, in some\nfrequently used models, bias-reduced and bias-corrected estimators can perform better than the unadjusted maximum\nlikelihood estimators, especially when the sample size is small.\nOur goal in this paper is to obtain bias correction and bias reduction to the maximum likelihood estimators for\nthe general multivariate elliptical model. We extend the work of Patriota and Lemonte (2009) to the elliptical class of\ndistributions de\ufb01ned in Lemonte and Patriota (2011). We focus on analytical methods only, because simulations for a\ngeneral multivariate normal model suggests that analytical bias corrections outperforms the computationally intensive\nbootstrap methods (Lemonte, 2011).\nIn order to illustrate the ampleness of the general multivariate elliptical model, we mention some of its submodels:\nmultiple linear regression, heteroscedastic multivariate nonlinear regressions, nonlinear mixed-effects models (Patriota\n2011), heteroscedastic errors-in-variables models (Patriota et al. 2009a,b), structural equation models, multivariate\nnormal regression model with general parametrization (Lemonte, 2011), simultaneous equation models and mixtures of\nthem. It is important to note that the usual normality assumption of the error is relaxed and replaced by the assumption of\nelliptical errors. The elliptical family of distributions includes many important distributions such as multivariate normal,\nStudent t, power exponential, contaminated normal, Pearson II, Pearson VII, and logistic, with heavier or lighter tails\nthan the normal distribution; see Fang et al. (1990).\nThe paper is organized as follows. Section 2 presents the notation and general results for bias correction and bias\nreduction. Section 3 presents the model and our main results, namely the general expression for the second-order bias\nof MLEs, in the general multivariate elliptical model. Section 4 applies our results in four important special cases:\nheteroscedastic nonlinear (linear) model, nonlinear mixed-effects models, multivariate errors-in-variables models and\nlog-symmetric regression models. Simulations are presented in Section 5. Applications that use real data are presented\n2\nand discussed in Section 6. Finally, Section 7 concludes the paper. Technical details are collected in one appendix.\n2\nBias correction and bias reduction\nLet \u03b8 be the p-vector of unknown parameters and \u03b8r its rth element. Also, let U(\u03b8) be the score function and Ur(\u03b8) = Ur\nits rth element. We use the following tensor notation for the cumulants of the log-likelihood derivatives introduced by\nLawley (1956):\n\u03bars = E\n\u0012\u2202Ur\n\u2202\u03b8s\n\u0013\n,\n\u03bar,s = E(UrUs),\n\u03bars,t = E\n\u0012\u2202Ur\n\u2202\u03b8s\nUt\n\u0013\n,\n\u03barst = E\n\u0012 \u22022Ur\n\u2202\u03b8s\u2202\u03b8t\n\u0013\n,\n\u03ba(t)\nrs = \u2202\u03bars\n\u2202\u03b8t\n,\n\u03bar,s,t = E(UrUsUt),\nand so on. The indices r, s and t vary from 1 to p. The typical (r, s)th element of the Fisher information matrix\nK(\u03b8) is \u03bar,s and we denote by \u03bar,s the corresponding element of K(\u03b8)\u22121. All \u03ba\u2019s refer to a total over the sample and\nare, in general, of order n. Under standard regular conditions, we have that \u03bars = \u2212\u03bar,s, \u03bars,t = \u03ba(t)\nrs \u2212\u03barst and\n\u03bar,s,t = 2\u03barst \u2212\u03ba(t)\nrs \u2212\u03ba(s)\nrt \u2212\u03ba(r)\nst . These identities will be used to facilitate some algebraic operations.\nLet Bb\u03b8(\u03b8) be the second-order bias vector of b\u03b8 whose jth element is Bb\u03b8j(\u03b8), j = 1, 2, . . . , p. It follows from the\ngeneral expression for the multiparameter second-order biases of MLEs given by Cox and Snell (1968) that\nBb\u03b8j(\u03b8) =\np\nX\nr,s,t=1\n\u03baj,r\u03bas,t\n\u001a1\n2\u03barst + \u03bars,t\n\u001b\n.\n(1)\nThe bias corrected MLE is de\ufb01ned as\nb\u03b8BC = b\u03b8 \u2212Bb\u03b8(b\u03b8).\nThe bias-corrected estimator b\u03b8BC is expected to have smaller bias than the uncorrected estimator, b\u03b8.\nFirth (1993) proposed an alternative method to partially remove the bias of MLEs. The method replaces the score\nfunction by its modi\ufb01ed version\nU \u2217(\u03b8) = U(\u03b8) \u2212K(\u03b8)Bb\u03b8(\u03b8),\nand a modi\ufb01ed estimate, b\u03b8BR, is given as a solution to U \u2217(\u03b8) = 0. It is noticeable that, unlike Cox and Snell\u2019s approach,\nFirth\u2019s bias reduction method does not depend on the \ufb01niteness of b\u03b8.\n3\nModel and main results\nWe shall follow the same notation presented in Lemonte and Patriota (2011). The elliptical model as de\ufb01ned in Fang et\nal. (1990) follows. A q \u00d7 1 random vector Y has a multivariate elliptical distribution with location parameter \u00b5 and a\nde\ufb01nite positive scale matrix \u03a3 if its density function is\nfY (y) = |\u03a3|\u22121/2g\n\u0000(y \u2212\u00b5)\u22a4\u03a3\u22121(y \u2212\u00b5)\n\u0001\n,\n(2)\n3\nTable 1: Generating functions of some multivariate elliptical distributions.\nDistribution\nGenerating function g(u)\nnormal\n1\n(\n\u221a\n2\u03c0)q e\u2212u/2\nCauchy\n\u0393( 1+q\n2 )\n\u0393( 1\n2) \u03c0\u2212q/2(1 + u)\u2212(1+q)/2\nStudent t\n\u0393( \u03bd+q\n2 )\n\u0393( \u03bd\n2 ) \u03c0\u2212q/2\u03bd\u2212q/2 \u00001 + u\n\u03bd\n\u0001\u2212(\u03bd+q)/2, \u03bd > 0\npower exponential\n\u03bb\u0393( q\n2)\n\u0393( q\n2\u03bb)2\u2212q/(2\u03bb)\u03c0\u2212q/2e\u2212u\u03bb/2, \u03bb > 0\nwhere g : [0, \u221e) \u2192(0, \u221e) is called the density generating function, and it is such that\nR \u221e\n0\nu\nq\n2 \u22121g(u)du < \u221e. We will\ndenote Y \u223cElq(\u00b5, \u03a3, g) \u2261Elq(\u00b5, \u03a3). It is possible to show that the characteristic function is \u03c8(t) = E(exp(it\u22a4Y )) =\nexp(it\u22a4\u00b5)\u03d5(t\u22a4\u03a3t), where t \u2208Rq and \u03d5 : [0, \u221e) \u2192R. Then, if \u03d5 is twice differentiable at zero, we have that E(Y ) = \u00b5\nand Var(Y ) = \u03be\u03a3, where \u03be = \u03d5\u2032(0). We assume that the density generating function g does not have any unknown\nparameter, which implies that \u03be is a known constant. From (2), when \u00b5 = 0 and \u03a3 = Iq, where Iq is a q \u00d7 q identity\nmatrix, we obtain the spherical family of densities. A comprehensive exposition of the elliptical multivariate class of\ndistributions can be found in Fang et al. (1990). Table 1 presents the density generating functions of some multivariate\nelliptical distributions.\nLet Y1, Y2, ..., Yn be n independent random vectors, where Yi has dimension qi \u2208N, for i = 1, 2, ..., n. The general\nmultivariate elliptical model (Lemonte and Patriota 2011) assumes that\nYi = \u00b5i(\u03b8, xi) + ei,\ni = 1, . . . , n,\nwith ei\nind\n\u223cElqi(0, \u03a3i(\u03b8, wi)), where \u201c\nind\n\u223c\u201d means \u201cindependently distributed as\u201d, xi and wi are mi \u00d7 1 and ki \u00d7 1\nnonstochastic vectors of auxiliary variables, respectively, associated with the ith observed response Yi, which may have\ncomponents in common. Then,\nYi\nind\n\u223cElqi(\u00b5i, \u03a3i),\ni = 1, . . . , n,\n(3)\nwhere \u00b5i = \u00b5i(\u03b8, xi) is the location parameter and \u03a3i = \u03a3i(\u03b8, wi) is the de\ufb01nite positive scale matrix. Both \u00b5i and \u03a3i\nhave known functional forms and are twice differentiable with respect to each element of \u03b8. Additionally, \u03b8 is a p-vector\nof unknown parameters (where p < n and it is \ufb01xed). Since \u03b8 must be identi\ufb01able in model (3), the functions \u00b5i and \u03a3i\nmust be de\ufb01ned to accomplish such restriction.\nSeveral important statistical models are special cases of the general formulation (3), for example, linear and nonlinear\nregression models, homoscedastic or heteroscedastic measurement error models, and mixed-effects models with normal\nerrors. It is noteworthy that the normality assumption for the errors may be relaxed and replaced by any distribution\nwithin the class of elliptical distributions, such as the Student t and the power exponential distributions. The general\n4\nformulation allows a wide range of different speci\ufb01cations for the location and the scale parameters, coupled with a large\ncollection of distributions for the errors. Section 4 presents four important particular cases of the main model (3) that\nshow the applicability of the general formulation.\nFor the sake of simplifying the notation, let zi = Yi \u2212\u00b5i and ui = z\u22a4\ni \u03a3\u22121\ni zi. The log-likelihood function associated\nwith (3), is given by\n\u2113(\u03b8) =\nn\nX\ni=1\n\u2113i(\u03b8),\n(4)\nwhere \u2113i(\u03b8) = \u22121\n2 log |\u03a3i| + log g(ui). It is assumed that g(\u00b7), \u00b5i and \u03a3i are such that \u2113(\u03b8) is a regular log-likelihood\nfunction (Cox and Hinkley 1974, Ch. 9) with respect to \u03b8. To obtain the score function and the Fisher information matrix,\nwe need to derive \u2113(\u03b8) with respect to the unknown parameters and to compute some moments of such derivatives. We\nassume that such derivatives exist. Thus, we de\ufb01ne\nai(r) = \u2202\u00b5i\n\u2202\u03b8r\n,\nai(sr) =\n\u22022\u00b5i\n\u2202\u03b8s\u2202\u03b8r\n,\nCi(r) = \u2202\u03a3i\n\u2202\u03b8r\n,\nCi(sr) =\n\u22022\u03a3i\n\u2202\u03b8s\u2202\u03b8r\nand\nAi(r) = \u2212\u03a3\u22121\ni Ci(r)\u03a3\u22121\ni ,\nfor r, s = 1, . . . , p. We make use of matrix differentiation methods (Magnus and Neudecker 2007) to compute the\nderivatives of the log-likelihood function. The score vector and the Fisher information matrix for \u03b8 can be shortly\nwritten as\nU(\u03b8) = F \u22a4Hs\nand\nK(\u03b8) = F \u22a4eHF,\n(5)\nrespectively, with F =\n\u0000F \u22a4\n1 , . . . , F \u22a4\nn\n\u0001\u22a4, H = block-diag{H1, . . . , Hn}, s = (s\u22a4\n1 , . . . , s\u22a4\nn )\u22a4, eH = HMH and\nM = block-diag\n\b\nM \u22a4\n1 , . . . , M \u22a4\nn\n\t\n, wherein\nFi =\n\u0012\nDi\nVi\n\u0013\n,\nHi =\n\u0014\n\u03a3i\n0\n0\n2\u03a3i \u2297\u03a3i\n\u0015\u22121\n,\nsi =\n\u0014\nvizi\n\u2212vec(\u03a3i \u2212viziz\u22a4\ni )\n\u0015\n,\nwhere the \u201cvec\u201d operator transforms a matrix into a vector by stacking the columns of the matrix, Di = (ai(1), . . . , ai(p)),\nVi = (vec(Ci(1)), . . . , vec(Ci(p))), vi = \u22122Wg(ui) and Wg(u) = d log g(u)/du. Here, we assume that F has rank p\n(i.e., \u00b5i and \u03a3i must be de\ufb01ned to hold such condition). The symbol \u201c\u2297\u201d indicates the Kronecker product. Following\nLange et al. (1989) we have, for the q-variate Student t distribution with \u03bd degrees of freedom, tq(\u00b5, \u03a3, \u03bd), that Wg(u) =\n\u2212(\u03bd + q)/{2(\u03bd + u)}. Following G\u00b4omez et al. (1998) we have, for the q-variate power exponential PEq(\u00b5, \u03b4, \u03bb) with\nshape parameter \u03bb > 0 and u \u0338= 0, that Wg(u) = \u2212\u03bbu\u03bb\u22121/2, \u03bb \u0338= 1/2. In addition, we have\nMi =\n\"\n4\u03c8i(2,1)\nqi\n\u03a3i\n0\n0\n2ci\u03a3i \u2297\u03a3i\n#\n+ (ci \u22121)\n\u00140\n0\n0\nvec(\u03a3i)vec(\u03a3i)\u22a4\n\u0015\n,\nwhere\nci\n=\n4\u03c8i(2,2)/{qi(qi\n+\n2)},\n\u03c8i(2,1)\n=\nE(W 2\ng (ri)ri)\nand\n\u03c8i(2,2)\n=\nE(W 2\ng (ri)r2\ni ), with ri = ||Li||2, Li \u223cElqi(0, Iqi). Here, we assume that g(u) is such that \u03c8i(2,1) and \u03c8i(2,2) ex-\nist and are \ufb01nite for all i = 1, . . . , n. One can verify these results by using standard differentiation techniques and some\nstandard matrix operations.\nThe values of \u03c8i(l,k) are obtained from solving the following one-dimensional integrals (Lange et al. 1989):\n\u03c8i(l,k) =\nZ \u221e\n0\nWg(s2)lg(s2)rqi+2k\u22121cqids,\n(6)\n5\nwhere cqi = 2\u03c0\nqi\n2 /\u0393( qi\n2 ) is the surface area of the unit sphere in Rqi and \u0393(a) is the well-known gamma function. One\ncan \ufb01nd these quantities for many distributions simply solving (6) algebraically or numerically. Table 2 shows these\nquantities for the normal, Cauchy, Student t and power exponential distributions.\nTable 2: Functions \u03c8i(2,1), \u03c8i(2,2), \u03c8i(3,2), \u03c8i(3,3) for normal, Cauchy, Student t and power exponential (P.E.) distribu-\ntions.\n\u03c8i(2,1)\n\u03c8i(2,2)\n\u03c8i(3,2)\n\u03c8i(3,3)\nnormal\nqi\n4\nqi(qi+2)\n4\n\u2212qi(qi+2)\n8\n\u2212qi(qi+2)(qi+4)\n8\nqi \u22651\nCauchy\nqi(qi+1)\n4(qi+3)\nqi(qi+2)(qi+1)\n4(qi+3)\n\u2212qi(qi+2)(qi+1)2\n8(qi+3)(qi+5)\n\u2212qi(qi+2)(qi+4)(qi+1)2\n8(qi+3)(qi+5)\nqi \u22651\nStudent t\nqi(qi+\u03bd)\n4(qi+\u03bd+2)\nqi(qi+2)(qi+\u03bd)\n4(qi+\u03bd+2)\n\u2212\nqi(qi+2)(qi+\u03bd)2\n8(qi+2+\u03bd)(qi+4+\u03bd)\n\u2212qi(qi+2)(qi+4)(qi+\u03bd)2\n8(qi+2+\u03bd)(qi+4+\u03bd)\nqi \u22651\nP.E.\n\u03bb2\u0393( 4\u03bb\u22121\n2\u03bb\n)\n21/\u03bb\u0393( 1\n2\u03bb )\n2\u03bb+1\n4\n\u2212\u03bb3\u0393( 6\u03bb\u22121\n2\u03bb\n)\n21/\u03bb\u0393( 1\n2\u03bb )\n\u2212(2\u03bb+1)(4\u03bb+1)\n8\nqi = 1, \u03bb > 1\n4\nP.E.\n\u03bb2\u0393( qi\u22122\n2\u03bb +2)\n21/\u03bb\u0393( qi\n2\u03bb )\nqi(2\u03bb+qi)\n4\n\u2212\u03bb3\u0393( qi\u22122\n2\u03bb +3)\n21/\u03bb\u0393( qi\n2\u03bb )\n\u2212qi(2\u03bb+qi)(4\u03bb+qi)\n8\nqi \u22652, \u03bb > 0\nIt is important to remark that the \u03c8i(l,k)\u2019s may involve unknown quantities (for instance, the degrees of freedom\n\u03bd of the Student t distribution and the shape parameter \u03bb of the power exponential distribution). One may want to\nestimate these quantities via maximum likelihood estimation. Here, we consider these as known quantities for the\npurpose of keeping the robustness property of some distributions. Lucas (1997) shows that the protection against \u201clarge\u201d\nobservations is only valid when the degrees of freedom parameter is kept \ufb01xed for the Student t distribution. Therefore,\nthe issue of estimating these quantities is beyond of the main scope of this paper. In practice, one can use model selection\nprocedures to choose the most appropriate values of such unknown parameters.\nNotice that, in the Fisher information matrix K(\u03b8), the matrix M carries all the information about the adopted\ndistribution, while F and H contain the information about the adopted model. Also, K(\u03b8) has a quadratic form that can\nbe computed through simple matrix operations. Under the normal case, vi = 1, M = H\u22121 and hence eH = H.\nThe Fisher scoring method can be used to estimate \u03b8 by iteratively solving the equation\n(F (m)\u22a4eH(m)F (m))\u03b8(m+1) = F (m)\u22a4eH(m)s\u2217(m),\nm = 0, 1, . . .,\n(7)\nwhere the quantities with the upper index \u201c(m)\u201d are evaluated at b\u03b8, m is the iteration counter and\ns\u2217(m) = F (m)\u03b8(m) + H\u22121(m)M \u22121(m)s(m).\nEach loop, through the iterative scheme (7), consists of an iterative re-weighted least squares algorithm to optimize the\nlog-likelihood (4). Thus, (5) and (7) agree with the corresponding equations derived in Patriota and Lemonte (2009).\nObserve that, despite the complexity and generality of the postulated model, expressions (5) and (7) are very simple and\nfriendly.\n6\nNow, we can give the main result of the paper.\nTheorem 3.1. The second-order bias vector Bb\u03b8(\u03b8) under model (3) is given by\nBb\u03b8(\u03b8) = (F \u22a4eHF)\u22121F \u22a4eH\u03be,\n(8)\nwhere \u03be = (\u03a61, . . . , \u03a6p)vec((F \u22a4eHF)\u22121), \u03a6r = (\u03a6\u22a4\n1(r), . . . \u03a6\u22a4\nn(r))\u22a4, and \u03a6i(r) is given in the Appendix.\nProof: See the Appendix.\nIn many models the location vector and the scale matrix do not have parameters in common, i.e., \u00b5i = \u00b5i(\u03b81, xi)\nand \u03a3i = \u03a3i(\u03b82, wi), where \u03b8 = (\u03b8\u22a4\n1 , \u03b8\u22a4\n2 )\u22a4. Therefore, F = block\u2013diag{F\u03b81, F\u03b82} and the parameter vectors \u03b81 and\n\u03b82 will be orthogonal (Cox and Reid 1987). This happens in mixed models, nonlinear models, among others. However,\nin errors-in-variables and factor analysis models orthogonality does not hold. Model (3) is general enough to encompass\na large number of models even those that do not have orthogonal parameters.\nCorollary 3.1. When \u00b5i = \u00b5i(\u03b81, xi) and \u03a3i = \u03a3i(\u03b82, wi), where \u03b8 = (\u03b8\u22a4\n1 , \u03b8\u22a4\n2 )\u22a4the second-order bias vector of b\u03b81\nand b\u03b82 are given by\nBb\u03b81(\u03b8) = (F \u22a4\n\u03b81 eH1F\u03b81)\u22121F \u22a4\n\u03b81 eH1\u03be1\nand\nBb\u03b82(\u03b8) = (F \u22a4\n\u03b82 eH2F\u03b82)\u22121F \u22a4\n\u03b82 eH2\u03be2,\nrespectively. The quantities F\u03b81, F\u03b82, eH1, eH2, \u03be1 and \u03be2 are de\ufb01ned in the Appendix.\nProof: See the Appendix.\nFormula (8) says that, for any particular model of the general multivariate elliptical class of models (3), it is always\npossible to express the bias of b\u03b8 as the solution of an weighted least-squares regression. Also, if zi \u223cNqi(0, \u03a3i) then\nci = \u2212e\u03c9i = 1, \u03b71i = 0, \u03b72i = \u22122, eH = H,\nJi(r) =\n\u0012\n0\n2(Iqi \u2297ai(r))Di\n\u0013\n,\nand formula (8) reduces to the one obtained by Patriota and Lemonte (2009).\nTheorem 3.1 implies that all one needs to compute bias-corrected and bias-reduced MLEs in the general elliptical\nmodel is: (i) the \ufb01rst and second derivatives of the location vector \u00b5i and the scale matrix \u03a3i with respect to all the\nparameters; (2) the derivatives Wg(u); (3) some moments involving the chosen elliptical distribution (these moments\nare given in Table 2 for some elliptical distributions). With these quantities, the matrices in (8) can be computed and the\nbias vector can be computed through an weighted least-squares regression.\n7\n4\nSpecial models\nIn this section, we present four important particular cases of the main model (3). All special cases presented in Patriota\nand Lemonte (2009) are also special cases of the general multivariate elliptical model de\ufb01ned in this paper.\n4.1\nHeteroscedastic nonlinear models\nConsider the univariate heteroscedastic nonlinear model de\ufb01ned by\nYi = f(xi, \u03b1) + ei, i = 1, 2, . . . , n,\nwhere Yi is the response, xi is a column vector of explanatory variables, \u03b1 is a column vector p1 \u00d7 1 of unknown\nparameters and f is a nonlinear function of \u03b1. Assume that e1, e2, . . . , en are independent, with ei \u223cEl(0, \u03c32\ni ). Here\n\u03c32\ni = \u03c32\ni (\u03b3) = h(\u03c9\u22a4\ni \u03b3), where \u03b3 is a p2 \u00d7 1 vector of unknown parameters. Then\nYi\nind\n\u223cEl(f(xi, \u03b1), \u03c32\ni ),\nwhich is a special case of (3) with \u03b8 = (\u03b1\u22a4, \u03b3\u22a4)\u22a4, \u00b5i = f(xi, \u03b1) and \u03a3i = \u03c32\ni . Here El stands for El1. Notice that for\nthe heteroscedastic linear model f(xi, \u03b1) = x\u22a4\ni \u03b1.\nThe second-order bias vector Bb\u03b8(\u03b8) comes from (8), which depends on derivatives of f(xi, \u03b1) and \u03c32\ni with respect\nto the parameter vector \u03b8. Also, it depends on the quantities \u03c8i(2,1), \u03c8i(2,2), \u03c8i(3,2), \u03c8i(3,3) (see Table 2) and Wg(ui)\ncontaining information about the adopted distribution.\n4.2\nNonlinear mixed-effects model\nOne of the most important examples is the nonlinear mixed-effects model introduced by Lange et al. (1989) and studied\nunder the assumption of a Student t distribution. Let\nYi = \u00b5i(xi, \u03b1) + Zibi + ui,\nwhere Yi is the qi \u00d7 1 vector response, \u00b5i is a qi-dimensional nonlinear function of \u03b1, xi is a vector of nonstochastic\ncovariates, Zi is a matrix of known constants, \u03b1 is a p1 \u00d7 1 vector of unknown parameters and bi is an r \u00d7 1 vector of\nunobserved random regression coef\ufb01cients. Assume that,\n\u0012 bi\nui\n\u0013\n\u223cElr+qi\n\u0012\u0014 0\n0\n\u0015\n,\n\u0014 \u03a3b(\u03b31)\n0\n0\nRi(\u03b32)\n\u0015\u0013\n,\nwhere \u03b31 is a p2-dimensional vector of unknown parameters and \u03b32 is a p3 \u00d7 1 vector of unknown parameters. Fur-\nthermore, the vectors (b1, u1)\u22a4, (b2, u2)\u22a4, . . ., (bn, un)\u22a4are independent. Therefore, the marginal distribution of the\nobserved vector is\nYi \u223cElqi (\u00b5i(xi, \u03b1); \u03a3i(Zi, \u03b3)) ,\n(9)\nwhere \u03b3 = (\u03b3\u22a4\n1 , \u03b3\u22a4\n2 )\u22a4and \u03a3i(Zi, \u03b3) = Zi\u03a3b(\u03b31)Z\u22a4\ni\n+ Ri(\u03b32). Equation (9) is a special case of (3) with \u03b8 =\n(\u03b1\u22a4, \u03b3\u22a4)\u22a4, \u00b5i = \u00b5i(xi, \u03b1) and \u03a3i = \u03a3i(Zi, \u03b3). From (8) one can compute the bias vector Bb\u03b8(\u03b8).\n8\n4.3\nErrors-in-variables model\nConsider the model\nx1i = \u03b20 + \u03b21x2i + qi,\ni = 1, . . . , n,\nwhere x1i is a v \u00d71 latent response vector, x2i is a m\u00d71 latent vector of covariates, \u03b20 is a v \u00d71 vector of intercepts, \u03b21\nis a v \u00d7 m matrix of slopes, and qi is the equation error having a multivariate elliptical distribution with location vector\nzero and scale matrix \u03a3q. The variables x1i and x2i are not directly observed, instead surrogate variables X1i and X2i\nare measured with the following additive structure:\nX1i = x1i + \u03b4x1i\nand\nX2i = x2i + \u03b4x2i.\n(10)\nThe random quantities x2i, qi, \u03b4x1i and \u03b4x2i are assumed to follow an elliptical distribution given by\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\nx2i\nqi\n\u03b4x1i\n\u03b4x2i\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8\nind\n\u223cEl2v+2m\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\u00b5x2\n0\n0\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8,\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\u03a3x2\n0\n0\n0\n0\n\u03a3q\n0\n0\n0\n0\n\u03c4x1i\n0\n0\n0\n0\n\u03c4x2i\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\nwhere the matrices \u03c4xi and \u03c4zi are known for all i = 1, . . . , n. These \u201cknown\u201d matrices may be attained, for example,\nthrough an analytical treatment of the data collection mechanism, replications, machine precision, etc (Kulathinal et al.\n(2002)).\nTherefore, the observed vector Yi = (X\u22a4\n1i, X\u22a4\n2i)\u22a4has marginal distribution given by\nYi\nind\n\u223cElv+m(\u00b5(\u03b8), \u03a3i(\u03b8))\n(11)\nwith\n\u00b5(\u03b8) =\n\u0012\u03b20 + \u03b21\u00b5x2\n\u00b5x2\n\u0013\nand\n\u03a3i(\u03b8) =\n\u0012\u03b21\u03a3x2\u03b2\u22a4\n1 + \u03a3q + \u03c4x1i\n\u03b21\u03a3x2\n\u03a3x2\u03b2\u22a4\n1\n\u03a3x2 + \u03c4x2i\n\u0013\n,\nwhere \u03b8 = (\u03b2\u22a4\n0 , vec(\u03b21)\u22a4, \u00b5\u22a4\nx2, vech(\u03a3x2)\u22a4, vech(\u03a3q)\u22a4)\u22a4, \u201cvech\u201d operator transforms a symmetric matrix into a vec-\ntor by stacking into columns its diagonal and superior diagonal elements. The mean vector (\u03b8) and the covariance-\nvariance matrix \u03a3i(\u03b8) of observed variables have the matrix \u03b21 in common, i.e., they share mv parameters. Kulathinal\net al. (2002) study the linear univariate case (v = 1, m = 1).\nEquation (11) is a special case of (3) with qi = v + m, \u03b8 = (\u03b1\u22a4, \u03b3\u22a4)\u22a4, \u00b5i = \u00b5i(\u03b8) and \u03a3i = \u03a3i(\u03b8). In this case, a\nprogramming language or software that can perform operations on vectors and matrices, e.g. Ox (Doornik, 2013) and R\n(Ihaka and Gentleman, 1996), can be used to obtain the bias vector Bb\u03b8(\u03b8) from (8).\n4.4\nLog-symmetric regression models\nLet T be a continuous positive random variable with probability density function\nfT (t; \u03b7, \u03c6, g) =\n1\n\u221a\u03c6tg\n \nlog2\n\"\u0012 t\n\u03b7\n\u0013\n1\n\u221a\u03c6\n#!\n, \u03b7 > 0, \u03c6 > 0,\n(12)\n9\nwhere g is the density generating function of a univariate elliptical distribution, and we write T \u223cLS(\u03b7, \u03c6, g). Vanegas\nand Paula (2014) called the class of distribution in (12) the log-symmetric class of distributions. It includes log-normal,\nlog-Student t, log-power-exponential distributions, among many others, as special cases. It is easy to verify that log(T )\nhas a univariate elliptical distribution (i.e., symmetric distribution) with location parameter \u00b5 = log(\u03b7) and scale param-\neter \u03c6. The parameter \u03b7 is the median of T , and \u03c6 can be interpreted as a skewness or relative dispersion parameter.\nVanegas and Paula (2015) de\ufb01ned and studied semi-parametric regression models for a set T1, T2, . . . , Tn with\nTi \u223cLS(\u03b7i, \u03c6i, g) with \u03b7i > 0 and \u03c6i > 0 following semi-parametric regression structures. Here we assume parametric\nspeci\ufb01cation for \u03b7i and \u03c6i as \u03b7i = \u03b7i(xi, \u03b1) and \u03c6i = \u03c6i(\u03c9i, \u03b3).\nHence,\nYi = log(Ti)\nind\n\u223cEl (\u00b5i(xi, \u03b1), \u03c6i(\u03c9i, \u03b3)) ,\n(13)\nwhere \u00b5i(xi, \u03b1) = log(\u03b7(xi, \u03b1)). Therefore, (13) is a special case of the general elliptical model (3), and formula (8)\napplies.\n5\nSimulation results\nIn this section, we shall present the results of Monte Carlo simulation experiments in which we evaluate the \ufb01nite sample\nperformances of the original MLEs and their bias-corrected and bias-reduced versions. The simulations are based on the\nunivariate nonlinear model without random effects (Section 4.1) and the errors-in-variables model presented in Section\n4.2, when Yi follows a normal distribution, a Student t distribution with \u03bd degrees of freedom, or a power exponential\ndistribution with shape parameter \u03bb. For all the simulations, the number of Monte Carlo replications is 10,000 (ten\nthousand) and they have been performed using the Ox matrix programming language (Doornik, 2013).\nFirst consider the model described in (9) with qi = 1, Zi = 0, \u03a3i = \u03c32 and\n\u00b5i(\u03b1) = \u00b5i(xi, \u03b1) = \u03b11 +\n\u03b12\n1 + \u03b13x\u03b14\ni\n,\ni = 1, . . . , n.\n(14)\nHere the unknown parameter vector is \u03b8 = (\u03b11, \u03b12, \u03b13, \u03b14, \u03c32)\u22a4. The values of xi were obtained as random draws from\nthe uniform distribution U(0, 100). The sample sizes considered are n = 10, 20, 30, 40 and 50. The parameter values\nare \u03b11 = 50, \u03b12 = 500, \u03b13 = 0.50, \u03b14 = 2 and \u03c32\ni = 200. For the Student t distribution, we \ufb01xed the degrees of\nfreedom at \u03bd = 4, and for the power exponential model the shape parameter is \ufb01xed at \u03bb = 0.8.\nTables 3-4 present the bias, and the root mean squared errors (\n\u221a\nMSE) of the maximum likelihood estimates, the\nbias-corrected estimates and the bias-reduced estimates for the nonlinear model with normal and Student t distributed\nerrors, respectively. To save space, the corresponding results for the power exponential model are not shown.1 We note\nthat the bias-corrected estimates and the bias-reduced estimates are less biased than the original MLE for all the sample\nsizes considered. For instance, when n = 20 and the errors follow a Student t distribution (see Table 4) the estimated\nbiases of b\u03c32 are \u221241.24 (MLE), \u221212.30 (bias-corrected) and \u22124.55 (bias-reduced). For the normal case with n = 10\n(see Table 3), the estimated biases of b\u03b12 are 2.16 (MLE), 0.70 (bias-corrected) and \u22120.27 (bias-reduced). We also\nobserve that the bias-reduced estimates are less biased than the bias-corrected estimates in most cases. As n increases,\nthe bias and the root mean squared error of all the estimators decrease, as expected. Additionally, we note that the MLE\n1All the omitted tables in this paper are presented in a supplement available from the authors upon request.\n10\nof \u03b12 has\n\u221a\nMSE larger than those of the modi\ufb01ed versions. For the estimation of \u03c32,\n\u221a\nMSE is smaller for the original\nMLE. In other cases, we note that the estimators have similar root mean squared errors.\nTable 3: Biases and\n\u221a\nMSE of the maximum likelihood estimate and its adjusted versions; nonlinear model; normal\ndistribution.\nMLE\nBias-corrected MLE\nBias-reduced MLE\nn\n\u03b8\nBias\n\u221a\nMSE\nBias\n\u221a\nMSE\nBias\n\u221a\nMSE\n\u03b11\n\u22120.29\n6.69\n\u22120.13\n6.67\n\u22120.01\n6.67\n\u03b12\n2.16\n20.07\n0.70\n19.40\n\u22120.27\n19.06\n10\n\u03b13\n0.01\n0.13\n0.00\n0.12\n0.00\n0.12\n\u03b14\n0.03\n0.30\n0.01\n0.29\n\u22120.00\n0.29\n\u03c32\n\u221280.05\n106.44\n\u221232.06\n103.32\n9.09\n128.72\n\u03b11\n\u22120.08\n4.07\n\u22120.01\n4.07\n0.01\n4.07\n\u03b12\n0.66\n17.94\n\u22120.08\n17.84\n\u22120.27\n17.82\n20\n\u03b13\n0.00\n0.09\n0.00\n0.09\n\u22120.00\n0.09\n\u03b14\n0.02\n0.21\n0.01\n0.20\n0.00\n0.20\n\u03c32\n\u221240.07\n69.73\n\u22128.09\n68.95\n0.86\n72.02\n\u03b11\n\u22120.10\n3.11\n\u22120.04\n3.10\n\u22120.02\n3.10\n\u03b12\n0.71\n17.24\n\u22120.05\n17.15\n\u22120.18\n17.13\n30\n\u03b13\n0.00\n0.09\n\u22120.00\n0.09\n\u22120.00\n0.09\n\u03b14\n0.02\n0.20\n0.00\n0.19\n0.00\n0.19\n\u03c32\n\u221226.41\n55.26\n\u22123.26\n55.11\n0.82\n56.32\n\u03b11\n\u22120.08\n2.69\n\u22120.02\n2.69\n\u22120.01\n2.69\n\u03b12\n0.83\n16.80\n0.09\n16.70\n0.01\n16.69\n40\n\u03b13\n0.00\n0.09\n0.00\n0.09\n0.00\n0.09\n\u03b14\n0.02\n0.19\n0.00\n0.18\n\u22120.00\n0.18\n\u03c32\n\u221220.04\n47.26\n\u22122.04\n47.13\n0.33\n47.74\n\u03b11\n\u22120.08\n2.39\n\u22120.03\n2.38\n\u22120.02\n2.38\n\u03b12\n1.07\n14.25\n0.30\n14.12\n0.23\n14.11\n50\n\u03b13\n0.00\n0.08\n0.00\n0.08\n0.00\n0.08\n\u03b14\n0.01\n0.19\n0.00\n0.18\n\u22120.00\n0.18\n\u03c32\n\u221215.93\n41.41\n\u22121.21\n41.30\n0.36\n41.67\nWe now consider the errors-in-variables model described in (10). The sample sizes considered are n = 15, 25, 35 and\n50. The parameter values are \u03b20 = 0.70 1v\u00d71, \u03b21 = 0.40 1v\u00d7m, \u00b5x2 = 70 1m\u00d71, \u03a3q = 40 Iv\u00d71 and \u03a3x2 = 250 Im\u00d71.\nHere, 1r\u00d7s is as r \u00d7s matrix of ones and Ir\u00d7s is the identity matrix with dimension r \u00d7s. For the Student t distribution,\nwe \ufb01xed the degrees of freedom at \u03bd = 4 and, for power exponential model, the shape parameter was \ufb01xed at \u03bb = 0.7.\nWe consider v \u2208{1, 2} and m = 1.\nIn Tables 5-6, we present the MLE, the bias-corrected estimates, the bias-reduced estimates, and corresponding\n11\nTable 4: Biases and\n\u221a\nMSE of the maximum likelihood estimate and its adjusted versions; nonlinear model; Student t\ndistribution.\nMLE\nBias-corrected MLE\nBias-reduced MLE\nn\n\u03b8\nBias\n\u221a\nMSE\nBias\n\u221a\nMSE\nBias\n\u221a\nMSE\n\u03b11\n\u22120.51\n8.66\n\u22120.31\n8.63\n\u22120.20\n8.56\n\u03b12\n3.34\n28.47\n1.39\n27.34\n2.05\n27.67\n10\n\u03b13\n0.01\n0.17\n0.00\n0.16\n0.01\n0.16\n\u03b14\n0.06\n0.42\n0.03\n0.39\n\u22120.01\n0.38\n\u03c32\n\u221293.18\n127.60\n\u221254.24\n130.73\n\u221217.40\n170.35\n\u03b11\n\u22120.17\n5.03\n\u22120.07\n5.02\n\u22120.04\n5.01\n\u03b12\n2.01\n25.64\n0.91\n25.11\n1.29\n24.98\n20\n\u03b13\n0.01\n0.14\n0.01\n0.14\n0.01\n0.13\n\u03b14\n0.04\n0.29\n0.01\n0.28\n0.00\n0.27\n\u03c32\n\u221241.24\n85.51\n\u221212.30\n89.41\n\u22124.55\n93.08\n\u03b11\n\u22120.10\n3.81\n\u22120.01\n3.80\n0.01\n3.82\n\u03b12\n2.25\n25.75\n1.13\n25.34\n1.61\n25.41\n30\n\u03b13\n0.01\n0.14\n0.01\n0.14\n0.01\n0.14\n\u03b14\n0.04\n0.29\n0.01\n0.27\n0.00\n0.26\n\u03c32\n\u221227.15\n70.02\n\u22126.15\n72.64\n\u22121.78\n107.53\n\u03b11\n\u22120.10\n3.27\n\u22120.02\n3.26\n\u22120.01\n3.26\n\u03b12\n1.82\n24.94\n0.75\n24.67\n1.18\n24.78\n40\n\u03b13\n0.01\n0.12\n0.00\n0.12\n0.01\n0.12\n\u03b14\n0.03\n0.26\n0.01\n0.25\n0.00\n0.25\n\u03c32\n\u221220.38\n60.43\n\u22124.01\n62.21\n\u22121.82\n62.98\n\u03b11\n\u22120.13\n2.86\n\u22120.05\n2.85\n\u22120.03\n2.85\n\u03b12\n1.48\n18.86\n0.38\n18.59\n0.24\n18.46\n50\n\u03b13\n0.01\n0.11\n0.00\n0.11\n0.00\n0.11\n\u03b14\n0.02\n0.24\n0.00\n0.23\n0.00\n0.23\n\u03c32\n\u221215.40\n53.99\n\u22121.94\n55.56\n\u22120.43\n56.11\n12\nestimated root mean squared errors for the Student t and power exponential distributions, for the errors-in-variables\nmodel. The results for the normal distribution are not shown to save space. We observe that, in absolute value, the\nbiases of the bias-corrected estimates and bias-reduced estimates are smaller than those of the original MLE for different\nsample sizes. Furthermore, the bias-reduced estimates are less biased than the bias-corrected estimates in most cases.\nThis can be seen e.g. in Table 6 when v = 1, m = 1, Yi follows a power exponential distribution and n = 15. In this\ncase, the bias of the MLE, the bias-corrected estimate and the bias-reduced estimate of \u03a3q are \u22124.92, \u22120.66 and \u22120.17,\nrespectively. When Yi follows a Student t distribution, n = 15, v = 1 and m = 1 we observe the following biases of the\nestimates of \u03a3x2: 5.18 (MLE), 2.91 (bias-corrected) and 2.66 (bias-reduced); see Table 5. We note that the root mean\nsquared errors decrease with n.\nFor the sake of saving space, the simulation results for the normal, Student t and power exponential errors-in-variable\nmodels with v = 2 and m = 1 are not presented. Overall, our \ufb01ndings are similar to those reached for the other models.\nTable 5: Biases and\n\u221a\nMSE of the maximum likelihood estimate and its adjusted versions; errors-in-variables model;\nv = 1 and m = 1; Student t distribution.\nMLE\nBias-corrected MLE\nBias-reduced MLE\nn\n\u03b8\nBias\n\u221a\nMSE\nBias\n\u221a\nMSE\nBias\n\u221a\nMSE\n\u03b20\n\u22120.00\n9.90\n0.01\n9.90\n0.01\n9.89\n\u03b21\n0.00\n0.14\n0.00\n0.14\n0.00\n0.14\n15\n\u00b5x2\n0.05\n4.82\n0.05\n4.82\n0.05\n4.82\n\u03a3x2\n5.18\n129.34\n2.91\n128.12\n2.66\n127.86\n\u03a3q\n\u22123.64\n19.52\n\u22120.68\n20.72\n\u22120.42\n20.85\n\u03b20\n\u22120.02\n7.14\n\u22120.01\n7.14\n\u22120.01\n7.14\n\u03b21\n0.00\n0.10\n0.00\n0.10\n0.00\n0.10\n25\n\u00b5x2\n0.03\n3.69\n0.03\n3.69\n0.03\n3.69\n\u03a3x2\n3.61\n97.32\n2.25\n96.76\n2.17\n96.69\n\u03a3q\n\u22122.31\n14.87\n\u22120.47\n15.41\n\u22120.38\n15.44\n\u03b20\n\u22120.02\n5.93\n\u22120.02\n5.93\n\u22120.02\n5.93\n\u03b21\n0.00\n0.08\n0.00\n0.08\n0.00\n0.08\n35\n\u00b5x2\n\u22120.01\n3.12\n\u22120.01\n3.12\n\u22120.01\n3.12\n\u03a3x2\n1.94\n79.78\n0.98\n79.45\n0.94\n79.44\n\u03a3q\n\u22121.65\n12.63\n\u22120.31\n12.96\n\u22120.26\n12.97\n\u03b20\n\u22120.01\n4.92\n\u22120.01\n4.92\n\u22120.01\n4.92\n\u03b21\n0.00\n0.07\n0.00\n0.07\n0.00\n0.07\n50\n\u00b5x2\n0.01\n2.59\n0.01\n2.59\n0.01\n2.59\n\u03a3x2\n1.04\n65.50\n0.37\n65.33\n0.36\n65.33\n\u03a3q\n\u22121.18\n10.53\n\u22120.24\n10.78\n\u22120.21\n10.79\n13\nTable 6: Biases and\n\u221a\nMSE of the maximum likelihood estimate and its adjusted versions; errors-in-variables model;\nv = 1 and m = 1; power exponential distribution.\nMLE\nBias-corrected MLE\nBias-reduced MLE\nn\n\u03b8\nBias\n\u221a\nMSE\nBias\n\u221a\nMSE\nBias\n\u221a\nMSE\n\u03b20\n\u22120.12\n9.25\n\u22120.11\n9.25\n\u22120.11\n9.24\n\u03b21\n0.00\n0.13\n0.00\n0.13\n0.00\n0.13\n15\n\u00b5x2\n\u22120.02\n6.47\n\u22120.02\n6.47\n\u22120.02\n6.47\n\u03a3x2\n\u22129.27\n103.32\n0.52\n107.51\n0.82\n107.64\n\u03a3q\n\u22124.92\n15.67\n\u22120.66\n17.55\n\u22120.17\n17.76\n\u03b20\n0.02\n6.83\n0.03\n6.83\n0.03\n6.83\n\u03b21\n0.00\n0.09\n\u22120.00\n0.09\n\u22120.00\n0.09\n25\n\u00b5x2\n\u22120.02\n4.98\n\u22120.02\n4.98\n\u22120.02\n4.98\n\u03a3x2\n\u22125.60\n80.20\n0.36\n81.95\n0.47\n81.99\n\u03a3q\n\u22123.04\n12.94\n\u22120.36\n13.49\n\u22120.18\n13.54\n\u03b20\n0.01\n5.59\n0.02\n5.58\n0.02\n5.58\n\u03b21\n\u22120.00\n0.08\n\u22120.00\n0.08\n\u22120.00\n0.08\n35\n\u00b5x2\n\u22120.04\n4.21\n\u22120.04\n4.21\n\u22120.04\n4.21\n\u03a3x2\n\u22123.53\n68.01\n0.77\n69.10\n0.82\n69.12\n\u03a3q\n\u22122.14\n11.11\n\u22120.18\n11.46\n\u22120.08\n11.49\n\u03b20\n0.03\n4.67\n0.03\n4.67\n0.03\n4.67\n\u03b21\n\u22120.00\n0.06\n\u22120.00\n0.06\n\u22120.00\n0.06\n50\n\u00b5x2\n\u22120.03\n3.52\n\u22120.03\n3.52\n\u22120.03\n3.52\n\u03a3x2\n\u22122.83\n56.89\n0.18\n57.51\n0.21\n57.52\n\u03a3q\n\u22121.51\n9.21\n\u22120.12\n9.41\n\u22120.07\n9.42\n14\n6\nApplications\n6.1\nRadioimmunoassay data\nTiede and Pagano (1979) present a dataset, referred here as the radioimmunoassay data, obtained from the Nuclear\nMedicine Department at the Veterans Administration Hospital, Buffalo, New York. Lemonte and Patriota (2011) ana-\nlyzed the data to illustrate the applicability of the elliptical models with general parameterization. Following Tiede and\nPagano (1979) we shall consider the nonlinear regression model (14), with n = 14. The response variable is the observed\nradioactivity (count in thousands), the covariate corresponds to the thyrotropin dose (measured in micro-international\nunits per milliliter) and the errors follow a normal distribution or a Student t distribution with \u03bd = 4 degrees of free-\ndom. We assume that the scale parameter is unknown for both models. In Table 7 we present the maximum likelihood\nestimates, the bias-corrected estimates, the bias-reduced estimates, and the corresponding estimated standard errors are\ngiven in parentheses. We note that all the estimates present smaller standard errors under the Student t model than under\nthe normal model (Table 7).\nFor all parameters, the original MLEs are very close to the bias-corrected MLE and the bias-reduced MLE when the\nStudent t model is used. However, under the normal model, signi\ufb01cant differences in the estimates of \u03b11 are noted. The\nestimates for \u03b11 are 0.44 (MLE), 0.65 (bias-corrected MLE) and 1.03 (bias-reduced MLE).\nTable 7: Estimates and standard errors (given in parentheses); radioimmunoassay data.\nNormal distribution\n\u03b8\nMLE\nBias-corrected MLE\nBias-reduced MLE\n\u03b11\n0.44 (0.80)\n0.65 (0.99)\n1.03 (1.06)\n\u03b12\n7.55 (0.95)\n7.34 (1.16)\n6.91 (1.25)\n\u03b13\n0.13 (0.06)\n0.13 (0.06)\n0.13 (0.08)\n\u03b14\n0.96 (0.24)\n0.93 (0.28)\n0.95 (0.34)\n\u03c32\n0.31 (0.12)\n0.40 (0.15)\n0.50 (0.19)\nStudent t distribution\n\u03b8\nMLE\nBias-corrected MLE\nBias-reduced MLE\n\u03b11\n0.90 (0.12)\n0.91 (0.13)\n0.90 (0.15)\n\u03b12\n7.09 (0.17)\n7.08 (0.19)\n7.07 (0.22)\n\u03b13\n0.09 (0.01)\n0.09 (0.01)\n0.09 (0.02)\n\u03b14\n1.31 (0.08)\n1.31 (0.09)\n1.29 (0.10)\n\u03c32\n0.02 (0.01)\n0.02 (0.01)\n0.03 (0.01)\n6.2\nFluorescent lamp data\nRosillo and Chivelet (2009) present a dataset referred here as the \ufb02uorescent lamp data. The authors analyze the lifetime\nof \ufb02uorescent lamps in photovoltaic systems using an analytical model whose goal is to assist in improving ballast design\n15\nand extending the lifetime of \ufb02uorescent lamps. Following Rosillo and Chivelet (2009) we shall consider the nonlinear\nregression model (9) with qi = 1, Zi = 0, \u03a3i = \u03c32, \u03b8 =\n\u0000\u03b1\u22a4, \u03c32\u0001\u22a4=\n\u0000\u03b10, \u03b11, \u03b12, \u03b13, \u03c32\u0001\u22a4and\n\u00b5i(\u03b1) =\n1\n1 + \u03b10 + \u03b11xi1 + \u03b12xi2 + \u03b13x2\ni2\n,\ni = 1, . . . , 14,\nwhere the response variable is the observed lifetime/advertised lifetime (Y ), the covariates correspond to a measure of\ngas discharge (x1) and the observed voltage/ad- vertised voltage (measure of performance of lamp and ballast - x2) and\nthe errors are assumed to follow a normal distribution. Here we also assume a Student t distribution with \u03bd = 4 degrees\nof freedom for the errors.\nIn Table 8 we present the maximum likelihood estimates, the bias-corrected estimates, the bias-reduced estimates,\nand the corresponding estimated standard errors. As in the previous application, the estimates present smaller standard\nerrors under the Student t model than under the normal model.\nThe original MLEs for \u03b10 and \u03b13 are bigger than the corresponding corrected and reduced versions by approximately\none unit (normal and Student t models). The largest differences are among the estimates of \u03b12; for example, for the\nnormal model we have \u221256.33 (MLE), \u221254.45 (bias-corrected MLE) and \u221253.86 (bias-reduced MLE).\nWe now use the Akaike Information Criterion (AIC, Akaike, 1974), the Schwarz Bayesian criterion (BIC, Schwarz,\n1978) and the \ufb01nite sample AIC (AICC, Hurvich and Tsai, 1989) to evaluate the quality of the normal and Student t\n\ufb01ts. For the normal model we have AIC = \u22129.98, BIC = \u22126.79 and AICC = \u22122.48. For the t model we have\nAIC = \u221211.24, BIC = \u22128.04 and AICC = \u22123.74. Therefore, the t model presents the best \ufb01t for this dataset, since\nthe values of the AIC, BIC and AICC are smaller.\nLet\nbD =\n14\nX\nj=1\n(bY \u2212bY(j))\u22a4(bY \u2212bY(j)),\nwhere bY and bY(j) are the vectors of predicted values computed from the model \ufb01t for the whole sample and the sample\nwithout the jth observation, respectively. The quantity bD measures the total effect of deleting one observation in the\npredicted values. For a \ufb01xed sample size, it tends to be high if a single observation can highly in\ufb02uence the prediction\nof new observations. We have bD = 0.119, 0.120, and 0.123 (normal model) and bD = 0.101, 0.100, and 0.095 (Student\nt model) when using the MLE, the bias-corrected estimate, and the bias-reduced estimate, respectively. Notice that bD\nis smaller for the Student t model regardless of the estimate used. This is evidence that the Student t model is more\nsuitable than the normal model for predicting lifetime of \ufb02uorescent lamps in this study.\n6.3\nWHO MONICA data\nWe now turn to a dataset from the WHO MONICA Project that was considered in Kulathinal et al. (2002). This dataset\nwas \ufb01rst analyzed under normal distributions for the marginals of the random errors (Kulathinal et al. 2002; Patriota\net al. 2009a). Thereafter, it was studied under a scale mixture of normal distributions for the marginals of the random\nerrors (Cao et al., 2012). The approach used in the present paper is different from the others because here we consider a\njoint elliptical distribution for the vector of random errors. The other authors assumed that the distributions of the errors\nwere independent, while we assume that they are uncorrelated but not independent. For our proposal, the errors will\nonly be independent under normality.\n16\nTable 8: Estimates and standard errors (given in parentheses); \ufb02uorescent lamp data.\nNormal distribution\n\u03b8\nMLE\nBias-corrected MLE\nBias-reduced MLE\n\u03b10\n29.49\n(5.21)\n28.54\n(5.66)\n28.25\n(5.84)\n\u03b11\n9.99\n(4.69)\n9.68\n(5.21)\n9.62\n(5.42)\n\u03b12\n\u221256.33\n(10.10)\n\u221254.45\n(10.93)\n\u221253.86\n(11.26)\n\u03b13\n26.53\n(4.89)\n25.61\n(5.28)\n25.31\n(5.43)\n\u03c32\n1.40 \u00d7 10\u22122\n(5.00 \u00d7 10\u22123)\n1.80 \u00d7 10\u22122\n(7.00 \u00d7 10\u22123)\n1.90 \u00d7 10\u22122 (7.00 \u00d7 10\u22123)\nStudent t distribution\n\u03b8\nMLE\nBias-corrected MLE\nBias-reduced MLE\n\u03b10\n30.66\n(4.64)\n29.94\n(5.05)\n29.85\n(5.20)\n\u03b11\n8.48\n(4.00)\n8.24\n(4.42)\n8.46\n(4.57)\n\u03b12\n\u221258.20\n(8.94)\n\u221256.79\n(9.71)\n\u221256.67\n(10.00)\n\u03b13\n27.27\n(4.30)\n26.58\n(4.66)\n26.55\n(4.80)\n\u03c32\n7.30 \u00d7 10\u22123 (3.60 \u00d7 10\u22123)\n9.20 \u00d7 10\u22123 (4.60 \u00d7 10\u22123)\n9.80 \u00d7 10\u22123 (4.90 \u00d7 10\u22123)\nThe dataset considered here corresponds to the data collected for men (n = 38). As describe in Kulathinal et al.\n(2002), the data are trends of the annual change in the event rate (y) and trends of the risk scores (x). The risk score is\nde\ufb01ned as a linear combination of smoking status, systolic blood pressure, body mass index, and total cholesterol level.\nA follow-up study using proportional hazards models was employed to derive its coef\ufb01cients, and provides the observed\nrisk score and its estimated variance. Therefore, the observed response variable, X1, is the average annual change in\nevent rate (%) and the observed covariate, X2, is the observed risk score (%). We use the heteroscedastic model (10)\nwith v = m = 1 and zero covariance between the errors \u03b4x1i and \u03b4x2i.\nTable 9 gives the MLE and the bias-corrected/reduced estimates (standard errors are given in parentheses). We\nconsidered the full sample (n = 38) and randomly chosen sub-samples of n = 10, 20 and 30 observations.\nThe original MLEs for \u03b20, \u03b21 and \u00b5x2 are practically the same as their bias-corrected and bias-reduced versions for\nall sample sizes. The largest differences are among the estimates of \u03a3q; for example, for n = 10 we have 6.17 (MLE),\n8.14 (bias-corrected MLE) and 8.81 (bias-reduced MLE). In general, as expected, larger sample sizes correspond to\nsmaller standard errors.\n7\nConcluding remarks\nWe studied bias correction and bias reduction for a multivariate elliptical model with a general parameterization\nthat uni\ufb01es several important models (e.g., linear and nonlinear regressions models, linear and nonlinear mixed models,\nerrors-in-variables models, among many others). We extend the work of Patriota and Lemonte (2009) to the elliptical\nclass of distributions de\ufb01ned in Lemonte and Patriota (2011). We express the second order bias vector of the maximum\nlikelihood estimates as an weighted least-squares regression.\n17\nTable 9: Estimates and standard errors (given in parentheses); WHO MONICA data.\nn\n\u03b8\nMLE\nBias-corrected MLE\nBias-reduced MLE\n\u03b20\n\u22122.58 (1.34)\n\u22122.58 (1.44)\n\u22122.45 (1.47)\n\u03b21\n0.05 (0.60)\n0.05 (0.63)\n0.07 (0.64)\n10\n\u00b5x2\n\u22121.54 (0.58)\n\u22121.54 (0.61)\n\u22121.53 (0.62)\n\u03a3x2\n2.89 (1.50)\n3.22 (1.65)\n3.29 (1.69)\n\u03a3q\n6.17 (3.99)\n8.14 (4.93)\n8.81 (5.25)\n\u03b20\n\u22122.68 (0.65)\n\u22122.69 (0.68)\n\u22122.69 (0.69)\n\u03b21\n0.48 (0.30)\n0.47 (0.31)\n0.43 (0.31)\n20\n\u00b5x2\n\u22121.29 (0.44)\n\u22121.29 (0.46)\n\u22121.29 (0.46)\n\u03a3x2\n3.53 (1.25)\n3.73 (1.31)\n3.76 (1.32)\n\u03a3q\n3.00 (1.66)\n3.59 (1.87)\n3.73 (1.92)\n\u03b20\n\u22122.22 (0.54)\n\u22122.22 (0.55)\n\u22122.20 (0.55)\n\u03b21\n0.43 (0.24)\n0.43 (0.25)\n0.42 (0.25)\n30\n\u00b5x2\n\u22120.77 (0.42)\n\u22120.77 (0.42)\n\u22120.77 (0.42)\n\u03a3x2\n4.71 (1.34)\n4.88 (1.39)\n4.89 (1.39)\n\u03a3q\n4.36 (1.86)\n4.89 (2.01)\n4.88 (2.01)\n\u03b20\n\u22122.08 (0.53)\n\u22122.08 (0.54)\n\u22122.08 (0.54)\n\u03b21\n0.47 (0.23)\n0.47 (0.24)\n0.46 (0.24)\n38\n\u00b5x2\n\u22121.09 (0.36)\n\u22121.09 (0.36)\n\u22121.09 (0.36)\n\u03a3x2\n4.32 (1.10)\n4.44 (1.13)\n4.45 (1.13)\n\u03a3q\n4.89 (1.78)\n5.34 (1.89)\n5.30 (1.88)\n18\nAs can be seen in our simulation results, corrected-bias estimators and reduced-bias estimators form a basis of\nasymptotic inferential procedures that have better performance than the corresponding procedures based on the original\nestimator. We further note that, in general, the bias-reduced estimates are less biased than the bias-corrected estimates.\nComputer packages that perform simple operations on matrices and vectors can be used to compute bias-corrected and\nbias-reduced estimates.\nAppendix\nLemma A.1. Let zi \u223cElqi(0, \u03a3i, g), and ci and \u03c8i(2,1) as previously de\ufb01ned. Then,\nE\n\u0000vizi\n\u0001\n= 0,\nE\n\u0000v2\ni ziz\u22a4\ni\n\u0001\n= 4\u03c8i(2,1)\nqi\n\u03a3i,\nE\n\u0000v2\ni vec(ziz\u22a4\ni )z\u22a4\ni\n\u0001\n= 0,\nE\n\u0000v2\ni vec(ziz\u22a4\ni )vec(ziz\u22a4\ni )\u22a4\u0001\n= ci\n\u0000vec(\u03a3i)vec(\u03a3i)\u22a4+ 2\u03a3i \u2297\u03a3i\n\u0001\n,\nE\n\u0000v3\ni vec(ziz\u22a4\ni )vec(ziz\u22a4\ni )\u22a4\u0001\n= \u2212c\u2217\ni\n\u0000vec(\u03a3i)vec(\u03a3i)\u22a4+ 2\u03a3i \u2297\u03a3i\n\u0001\n,\nE\n\u0000v3\ni z\u22a4\ni Ai(t)ziz\u22a4\ni Ai(s)ziz\u22a4\ni Ai(r)zi\n\u0001\n= \u22128e\u03c9i\n\u0000tr{Ai(t)\u03a3i}tr{Ai(s)\u03a3i}tr{Ai(r)\u03a3i}\n+ 2tr{Ai(t)\u03a3i}tr{Ai(s)\u03a3iAi(r)\u03a3i}\n+ 2tr{Ai(s)\u03a3i}tr{Ai(t)\u03a3iAi(r)\u03a3i}\n+ 2tr{Ai(r)\u03a3i}tr{Ai(t)\u03a3iAi(s)\u03a3i})\n+ 8tr{Ai(t)\u03a3iAi(s)\u03a3iAi(r)\u03a3i}\n\u0001\n,\nwhere c\u2217\ni = 8\u03c8i(3,2)/{qi(qi + 2)}, \u03c8i(3,2) = E(W 3\ng (ri)r2\ni ), e\u03c9i = \u03c8i(3,3)/{qi(qi + 2)(qi + 4)} and \u03c8i(3,3) =\nE(W 3\ng (ri)r3\ni ).\nProof: The proof can be obtained by adapting the results of Mitchell (1989) for a matrix version.\nFrom Lemma A.1, we can \ufb01nd the cumulants of the log-likelihood derivatives required to compute the second-order\nbiases.\nProof of Theorem 3.1: Following Cordeiro and Klein (1994), we write (1) in matrix notation to obtain the second-order\nbias vector of b\u03b8 in the form\nBb\u03b8(\u03b8) = K(\u03b8)\u22121Wvec(K(\u03b8)\u22121),\n(15)\nwhere W = (W (1), . . . , W (p)) is a p \u00d7 p2 partitioned matrix, each W (r), referring to the rth component of \u03b8, being a\np \u00d7 p matrix with typical (t, s)th element given by\nw(r)\nts = 1\n2\u03batsr + \u03bats,r = \u03ba(r)\nts \u22121\n2\u03batsr = 3\n4\u03ba(r)\nts \u22121\n4(\u03bat,s,r + \u03ba(t)\nsr + \u03ba(s)\nrt ).\n19\nBecause K(\u03b8) is symmetric and the tth element of Wvec(K(\u03b8)\u22121) is w(1)\nt1 \u03ba1,1 + (w(1)\nt2 + w(2)\nt1 )\u03ba1,2 + \u00b7 \u00b7 \u00b7 + (w(s)\ntr +\nw(r)\nts )\u03bas,r + \u00b7 \u00b7 \u00b7 + (w(p\u22121)\ntp\n+ w(p)\nt(p\u22121))\u03bap\u22121,p + w(p)\ntp \u03bap,p, we may write\nw(r)\nts = 1\n2(w(s)\ntr + w(r)\nts ) = 1\n4(\u03ba(r)\nts + \u03ba(s)\ntr \u2212\u03ba(t)\nsr \u2212\u03bat,s,r).\n(16)\nComparing (15) and (8) we note that for the proof of this theorem it suf\ufb01ces to show that F \u22a4eH\u03be = Wvec((F \u22a4eHF)\u22121),\ni.e.,\nW = F \u22a4HMH(\u03a61, . . . , \u03a6p).\nNotice that\n\u03basr =\nn\nX\ni=1\n\u001aci\n2 tr{Ai(r)Ci(s)} \u22124\u03c8i(2,1)\nqi\na\u22a4\ni(s)\u03a3\u22121\ni ai(r)\n\u2212(ci \u22121)\n4\ntr{Ai(s)\u03a3i}tr{Ai(r)\u03a3i}\n\u001b\n.\n(17)\nThe quantities \u03c8i(2,1) and \u03c8i(2,2) do not depend on \u03b8 and hence, the derivative of (17) with respect to \u03b8t is\n\u03ba(t)\nsr =\nn\nX\ni=1\n\u001aci\n2 tr{Ai(t)\u03a3iAi(s)Ci(r) + Ai(s)\u03a3iAi(t)Ci(r) + Ci(ts)Ai(r)\n+ Ci(tr)Ai(s)}\n\u001b\n\u2212\nn\nX\ni=1\n\u001a4\u03c8i(2,1)\nqi\n\u0000a\u22a4\ni(ts)\u03a3\u22121\ni ai(r) + a\u22a4\ni(s)Ai(t)ai(r)\n+ a\u22a4\ni(s)\u03a3\u22121\ni ai(tr)\n\u0001\u001b\n+\nn\nX\ni=1\n\u001a(ci \u22121)\n4\ntr{Ai(t)Ci(s) + \u03a3\u22121\ni Ci(ts)}tr{Ai(r)\u03a3i}\n\u001b\n+\nn\nX\ni=1\n\u001a(ci \u22121)\n4\ntr{Ai(t)Ci(r) + \u03a3\u22121\ni\nCi(tr)}tr{Ai(s)\u03a3i}\n\u001b\n.\nTherefore,\n\u03ba(r)\nst + \u03ba(s)\ntr \u2212\u03ba(t)\nsr =\nn\nX\ni=1\n\u001aci\n2 tr{Ai(r)\u03a3iAi(s)Ci(t) + Ai(s)\u03a3iAi(r)Ci(t)\n+ 2Ci(rs)Ai(t)}\n\u001b\n\u2212\nn\nX\ni=1\n\u001a4\u03c8i(2,1)\nqi\n\u00002a\u22a4\ni(t)\u03a3\u22121\ni ai(sr)\n+ a\u22a4\ni(t)Ai(s)ai(r) + a\u22a4\ni(s)Ai(r)ai(t) \u2212a\u22a4\ni(s)Ai(t)ai(r)\n\u0001\u001b\n+\nn\nX\ni=1\n\u001a(ci \u22121)\n2\ntr{Ai(r)Ci(s) + \u03a3\u22121\ni Ci(rs)}tr{Ai(t)\u03a3i}\n\u001b\n.\n20\nNow, the only quantity that remains to obtain is \u03bat,s,r = E(UtUsUr). Noting that zi is independent of zj for i \u0338= j,\nwe have\n\u03bat,s,r = 1\n8\nn\nX\ni=1\nE\n\u001a\u0002\ntr{Ai(t)(\u03a3i \u2212viziz\u22a4\ni )}tr{Ai(s)(\u03a3i \u2212viziz\u22a4\ni )}\ntr{Ai(r)(\u03a3i \u2212viziz\u22a4\ni )}\n\u0003\n+ 4tr{Ai(t)(\u03a3i \u2212viziz\u22a4\ni )}(v2\ni a\u22a4\ni(r)\u03a3\u22121\ni\nziz\u22a4\ni \u03a3\u22121ai(s)) + 4tr{Ai(r)(\u03a3i \u2212viziz\u22a4\ni )}(v2\ni a\u22a4\ni(t)\u03a3\u22121\ni\nziz\u22a4\ni \u03a3\u22121\nai(s)) + 4tr{Ai(s)(\u03a3i \u2212viziz\u22a4\ni )}(v2\ni a\u22a4\ni(t)\u03a3\u22121\ni ziz\u22a4\ni \u03a3\u22121ai(r))\n\u001b\n.\nThen, by using Lemma A.1 and from (16), we have, after lengthy algebra, that\nW (r) =\nn\nX\ni=1\nF \u22a4\ni HiMiHi\u03a6i(r),\n(18)\nwhere\n\u03a6i(r) = \u22121\n2\n\u0012\nH\u22121\ni\nM \u22121\ni\nBi(r)HiFi + \u2202Fi\n\u2202\u03b8r\n\u0013\n,\nand\nBi(r) = \u22121\n2\n\u0012\n\u03b71iCi(r)\n2\u03b71i\u03a3i \u2297a\u22a4\ni(r)\n2\u03b72i\u03a3i \u2297ai(r)\n2(ci \u22121)S1i(r)\n\u0013\n\u22121\n4\n\u0012\u03b71i\u03a3itr{Ci(r)\u03a3\u22121\ni }\n2\u03b71iai(r)vec(\u03a3i)\u22a4\n2\u03b71ivec(\u03a3i)a\u22a4\ni(r)\n2(ci + 8e\u03c9i)S2i(r)\n\u0013\n,\nwith\n\u03b71i = c\u2217\ni + 4\u03c8i(2,1)/qi, \u03b72i = c\u2217\ni \u22124\u03c8i(2,1)/qi,\nS1i(r) = vec(\u03a3i)vec(Ci(r))\u22a4+ 1\n2vec(\u03a3i)vec(\u03a3i)\u22a4tr{Ci(r)\u03a3\u22121\ni } and\nS2i(r) = vec(Ci(r))vec(\u03a3i)\u22a4+ vec(\u03a3i)vec(Ci(r))\u22a4+ 4\u03a3i \u2297Ci(r)\n+\n\u0002\n\u03a3i \u2297\u03a3i + 1\n2vec(\u03a3i)vec(\u03a3i)\u22a4\u0003\ntr{Ci(r)\u03a3\u22121\ni }.\nUsing (18) and (15) the theorem is proved.\nProof of Corollary 3.1: It follows from Theorem 3.1, eq. (8), when\nF = block\u2013diag{F\u03b81, F\u03b82}, eH = block\u2013diag{ eH1, eH2} and \u03be = (\u03be\u22a4\n1 , \u03be\u22a4\n2 )\u22a4,\n21\nwhere F\u03b8j =\nh\nF \u22a4\n\u03b8j(1), . . . , F \u22a4\n\u03b8j(n)\ni\u22a4\nand eHj = block-diag{ eHj(1), . . . , eHj(n)} for j = 1, 2, with F\u03b81(i) = \u2202\u00b5i/\u2202\u03b8\u22a4\n1 ,\nF\u03b82(i) = \u2202[vec(\u03a3i)]/\u2202\u03b8\u22a4\n2 , eH1(i) =\n4\u03c8i(2,1)\nqi\n\u03a3\u22121\ni\nand eH2(i) = ci (2\u03a3i \u2297\u03a3i)\u22121 + (ci \u22121)vec(\u03a3\u22121\ni )vec(\u03a3\u22121\ni\n)\u22a4. Further-\nmore, \u03be1 =\nh\n\u03be\u22a4\n1(1), . . . , \u03be\u22a4\n1(n)\ni\u22a4\nand \u03be2 =\nh\n\u03be\u22a4\n2(1), . . . , \u03be\u22a4\n2(n)\ni\u22a4\nwith\n\u03be1(i) = \u22121\n2\n\u02d9F\u03b81(i) vec((F \u22a4\n\u03b81 eH(1)F\u03b81)\u22121),\n\u03be2(i) = 1\n4M \u2217\ni P \u2217\ni vec((F \u22a4\n\u03b81 eH(1)F\u03b81)\u22121) + 1\n8\n\u0010\nM \u2217\ni Q\u2217\ni \u22124 \u02d9F\u03b82(i)\n\u0011\nvec((F \u22a4\n\u03b82 eH(2)F\u03b82)\u22121).\nAlso,\n\u02d9F\u03b81(i)\n=\n[F 1\n\u03b81(i), . . . , F p1\n\u03b81(i)],\n\u02d9F\u03b82(i)\n=\n[F 1\n\u03b82(i), . . . , F p2\n\u03b82(i)],\nQ\u2217\ni\n=\n[Q\u2217\ni(1),\n. . . , Q\u2217\ni(p2)], P \u2217\ni = [P \u2217\ni(1), . . . , P \u2217\ni(p1)], F r\n\u03b81(i) =\n\u2202F\u03b81(i)\n\u2202\u03b81(r) , F s\n\u03b82(i) =\n\u2202F\u03b82(i)\n\u2202\u03b82(s) , where \u03b81(r) and \u03b82(s) are the rth and sth\nelements of \u03b81 and \u03b82, respectively, r = 1, . . . , p1, s = 1, . . . , p2 and\nM \u2217\ni = 1\nci\n\u0012\nIq2\ni \u2212\nvec(\u03a3i)vec(\u03a3i)\u22a4\n2ci + vec(\u03a3i)\u22a4vec(\u03a3i)\n\u0013\n,\nQ\u2217\ni(s) =\n\u0012\n(ci \u22121)S1i(s) + 1\n2(ci + 8e\u03c9i)S2i(s)\n\u0013 \u0000\u03a3\u22121\ni\n\u2297\u03a3\u22121\ni\n\u0001\nF\u03b82(i),\nP \u2217\ni(r) =\n\u0010\n2\u03b72i(Iqi \u2297ai(r)) + \u03b71ivec(\u03a3i)a\u22a4\ni(r)\u03a3\u22121\ni\n\u0011\nF\u03b81(i).\nAcknowledgement\nWe gratefully acknowledge the \ufb01nancial the support from CNPq and FAPESP.\nReferences\n[1] Akaike, H. (1974). A new look at the statistical model identi\ufb01cation. IEEE Transactions on Automatic Control., 19\n716\u2013723.\n[2] Bull, S.B., Mak, C. and Greenwood, C. (2002). A modi\ufb01ed score function estimator for multinomial logistic regres-\nsion in small samples. Computational Statistical Data Analysis, 39 57\u201374.\n[3] Cao, C\u2013Z, Lin, J\u2013G and Zhu, X\u2013X. (2012). On estimation of a heteroscedastic measurement error model under\nheavy-tailed distributions. Computational Statistics and Data Analysis, 56 438\u2013448.\n[4] Cordeiro, G.M., Ferrari, S.L.P., Uribe-Opazo, M.A. and Vasconcellos, K.L.P. (2000). Corrected maximum-\nlikelihood estimation in a class of symmetric nonlinear regression models. Statistics and Probability Letters, 46\n317\u2013328.\n[5] Cordeiro, G.M. and Klein, R. (1994). Bias correction in ARMA models. Statistics and Probability Letters, 19 169\u2013\n176.\n22\n[6] Cordeiro, G.M. and McCullagh, P. (1991). Bias correction in generalized linear models. Journal of the Royal Statis-\ntical Society B, 53, 629\u2013643.\n[7] Cox, D.R. and Hinkley, D.V. (1974). Theoretical Statistics. London: Chapman and Hall.\n[8] Cox, D.R. and Reid, N. (1987). Parameter orthogonality and approximate conditional inference (with discussion).\nJournal of the Royal Statistical Society B, 40, 1\u201339.\n[9] Cox, D.R. and Snell, E. (1968). A general de\ufb01nition of residuals (with discussion). Journal of the Royal Statistical\nSociety B, 30 248\u2013275.\n[10] Cysneiros, F.J.A., Cordeiro, G.M. and Cysneiros, A.H.M.A. (2010). Corrected maximum likelihood estimators in\nheteroscedastic symmetric nonlinear models, Journal of Statistical Computation and Simulation, 80 451\u2013461.\n[11] Doornik, J.A. (2013). Object-Oriented Matrix Programming using Ox. London: Timberlake Consultants Press\n(ISBN 978-0-9571708-1-0).\n[12] Fang, K.T., Kotz, S. and Ng, K.W. (1990). Symmetric Multivariate and Related Distributions. London: Chapman\nand Hall.\n[13] Firth, D. (1993). Bias reduction of maximum likelihood estimates. Biometrika, 80 27\u201338.\n[14] G\u00b4omez, E., G\u00b4omez-Villegas, M.A. and Mart\u00b4\u0131n, J.M. (1998). A multivariate generalization of the power exponential\nfamily of distributions. Communications in Statistics. Part A: Theory and Methods, 27 589\u2013600.\n[15] Hurvich, C.M. and Tsai, C.L. (1989). Regression and Time Series Model Selection in Small Samples. Biometrika,\n76 297\u2013307.\n[16] Ihaka, R. and Gentleman, R. (1996). R: A language for data analysis and graphics. Journal of Computational\nGraphics and Statistics, 5 299\u2013314.\n[17] Kosmidis I. (2014). Improved estimation in cumulative link models. Journal of the Royal Statistical Society, 76\n169\u2013196.\n[18] Kosmidis I. and Firth, D. (2009). Bias reduction in exponential family nonlinear models. Biometrika, 96 793\u2013804.\n[19] Kosmidis I. and Firth, D. (2011). Multinomial logit bias reduction via the Poisson log-linear model. Biometrika,\n98 755\u2013759.\n[20] Kulathinal, S.B., Kuulasmaa, K. and Gasbarra, D. (2002). Estimation of an errors-in-variables regression model\nwhen the variances of the measurement error vary between the observations. Statistics in Medicine, 21 1089\u20131101.\n[21] Lange, K.L., Little, R.J.A. and Taylor, J.M.G. (1989). Robust statistical modeling using the t distribution. Journal\nof the American Statistical Association, 84 881\u2013896.\n[22] Lawley, D.N. (1956). A general method for approximating to the distribution of likelihood ratio criteria. Biometrika,\n43 295\u2013303.\n[23] Lemonte, A.J. (2011). Improved maximum-likelihood estimation in a regression model with general parametriza-\ntion. Journal of Statistical Computation and Simulation, 81 1027\u20131037.\n23\n[24] Lemonte, A.J. and Patriota, A.G. (2011). Multivariate elliptical models with general parameterization. Statistical\nMethodology, 8 389\u2013400.\n[25] Lucas, A.(1997). Robustness of the Student t based M-estimator. Communications in Statistics, Theory and Meth-\nods, 26 1165\u20131182.\n[26] Magnus, J.R. and Neudecker, H. (2007). Matrix Differential Calculus with Applications in Statistics and Econo-\nmetrics. Chichester: Wiley, third edition.\n[27] Mehrabi, Y. and Matthews, J.N.S. (1995). Likelihood-based methods for bias reduction in limiting dilution assays.\nBiometrics, 51 1543\u20131549.\n[28] Mitchell, A.F.S. (1989). The information matrix, skewness tensor and \u03b1-connections for the general multivariate\nelliptical distribution. Annals of the Institute of Statistical Mathematics, 41 289\u2013304.\n[29] Patriota, A.G. (2011). A note on in\ufb02uence in nonlinear mixed-effects elliptical models. Computational Statistics\nand Data Analysis, 51 218\u2013225.\n[30] Patriota, A.G., Bolfarine, B. and de Castro, M. (2009a). A heteroscedastic structural errors-in-variables model with\nequation error. Statistical Methodology, 6 408\u2013423.\n[31] Patriota, A.G. and Lemonte, A.J. (2009). Bias correction in a multivariate normal regression model with general\nparameterization. Statistics & Probability Letters, 79 1655\u20131662.\n[32] Patriota, A.G., Lemonte, A.J. and Bolfarine, H. (2009b). Improved maximum likelihood estimators in a het-\neroskedastic errors-in-variables model. Statistical Papers, 52 455\u2013467.\n[33] Pettitt, A.N., Kelly, J.M. and Gao, J.T. (1998). Bias correction for censored data with exponential lifetimes. Statis-\ntica Sinica, 8 941\u2013964.\n[34] Rosillo, F.G., Chivelet, N.M. (2009). Lifetime prediction of \ufb02uorescent lamps used in photovoltaic systems. Light-\ning Research and Technology, 41 (2) 183\u2013197.\n[35] Schwarz, G. (1978). Estimating the dimensional of a model. Annals of Statistics, 6 461\u2013464.\n[36] Tiede, J.J. and Pagano, M. (1979). Application of robust calibration to radioimmunoassay. Biometrics, 35 567\u2013574.\n[37] Vanegas, L.H. and Paula, G.A. (2014). Log\u2013symmetric distributions:\nstatistical properties and parame-\nter estimation. Brazilian Journal of Probability and Statistics. Accepted for publication. Available online at\nhttp://www.imstat.org/bjps/papers/BJPS272.pdf; accessed on 2015-06-23.\n[38] Vanegas, L.H. and Paula, G.A. (2015). A semiparametric approach for joint modeling of median and skewness.\nTest, 24 110\u2013135.\n[39] Vasconcellos, K.L.P. and Cordeiro, G.M. (2000). Bias corrected estimates in multivariate Student t regression\nmodels. Communications in Statistics, Theory and Methods, 29 797\u2013822.\n24\n"}