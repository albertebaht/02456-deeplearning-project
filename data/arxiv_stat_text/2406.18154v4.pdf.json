{"text": "Errors-In-Variables Model Fitting for Partially\nUnpaired Data Utilizing Mixture Models\nWolfgang Hoegele1 and Sarah Brockhaus1\n1 Munich University of Applied Sciences HM\nDepartment of Computer Science and Mathematics\nLothstra\u00dfe 64, 80335 M\u00a8unchen, Germany\nwolfgang.hoegele@hm.edu\nNovember 19, 2024\nAbstract\nWe introduce a general framework for regression in the errors-in-variables regime, allowing for\nfull flexibility about the dimensionality of the data, observational error probability density types,\nthe (nonlinear) model type and the avoidance of ad-hoc definitions of loss functions.\nIn this\nframework, we introduce model fitting for partially unpaired data, i.e. for given data groups the\npairing information of input and output is lost (semi-supervised). This is achieved by constructing\nmixture model densities, which directly model the loss of pairing information allowing inference.\nIn a numerical simulation study linear and nonlinear model fits are illustrated as well as a real data\nstudy is presented based on life expectancy data from the world bank utilizing a multiple linear\nregression model. These results show that high quality model fitting is possible with partially\nunpaired data, which opens the possibility for new applications with unfortunate or deliberate loss\nof pairing information in data.\nKeywords: Errors-In-Variables; Mixture Models; Model Fitting; Semi-Supervised; Total Least\nSquares\nAbout the Authors\nDr. H\u00a8ogele is a Full Professor of Applied Mathematics and Computational Science at the Depart-\nment of Computer Science and Mathematics at the Munich University of Applied Sciences HM,\nGermany.\nDr. Brockhaus is a Full Professor of Applied Mathematics and Statistics at the Department of\nComputer Science and Mathematics at the Munich University of Applied Sciences HM, Germany.\nThis manuscript is accepted and will be published in STATISTICS (Taylor & Francis), 2024\nThis manuscript appears also on ArXiv.org\narXiv:2406.18154 [stat.ME], https://doi.org/10.48550/arXiv.2406.18154\narXiv:2406.18154v4  [stat.ME]  17 Nov 2024\nContents\n1\nIntroduction\n3\n2\nMethods\n5\n2.1\nGeneral Nomenclature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nModel Fit with Completely Paired Data . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2.1\nExample: Fitting a Line and Gaussian Disturbance . . . . . . . . . . . . . .\n7\n2.2.2\nExample: Fitting a Hyperplane and Gaussian Disturbance (Errors-In-Variables\nMultiple Linear Regression) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2.3\nConnection to Interval Data Regression\n. . . . . . . . . . . . . . . . . . . .\n8\n2.2.4\nExample: Linear Interval Data Regression . . . . . . . . . . . . . . . . . . .\n9\n2.3\nModel Fit with Completely Unpaired Data\n. . . . . . . . . . . . . . . . . . . . . .\n9\n2.4\nModel Fit with Partially Unpaired Data . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.4.1\nExample: Fitting a Line and Gaussian Disturbance . . . . . . . . . . . . . .\n11\n2.4.2\nExample: Fitting a Hyperplane and Gaussian Disturbance (Errors-In-Variables\nMultiple Linear Regression) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.4.3\nExample: Linear Interval Data Regression . . . . . . . . . . . . . . . . . . .\n12\n2.5\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.5.1\nNumerical Implementation of the General Formula for Partially Unpaired Data 12\n2.5.2\nEvaluation of Unpaired Data Subgroups . . . . . . . . . . . . . . . . . . . .\n13\n2.5.3\nSimultaneous Estimation of the Underlying Density Functions . . . . . . . .\n13\n2.5.4\nBayesian Extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3\nSimulation Study\n14\n3.1\nDemonstration for a Line Fit\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.2\nNonlinear Model with Anisotropic Observation Errors\n. . . . . . . . . . . . . . . .\n16\n4\nReal Data Study: Life Expectancy\n17\n5\nDiscussion and Conclusion\n19\nA Completely Paired Data: Derivations for Errors in y only\n20\nB Completely Paired Data: Relation to Deming Regression\n21\nC Demonstration for a Plane Fit with Gaussian Disturbance for Partially Unpaired\nData\n22\nD Application of R2\n\u03b4\n24\n1\nIntroduction\nParametric model fitting is a standard task in many applications starting from problem specific\nmodels with a few meaningful parameters to huge, flexible models (such as in the training phase\nof ANNs) [Zhang, 1997, Bishop, 2006]. The general idea is that a defined parametric model family\nis fitted to given input / output data (i.e. supervised learning) with the purpose to estimate the\nbest fit parameters of the model representing the data. Typically, loss functions are defined for this\npurpose, such as the squared or absolute losses [Wang et al., 2022a]. Practical difficulties of model\nfitting are i) finding the appropriate model family and their parameters, ii) defining an adequate\nloss function, iii) applying an efficient optimization algorithm and iv) dealing with data deficiencies\n(such as outliers, strong noise, incompleteness, partially lost pairing information, etc.).\nA possibility to avoid the definition of an ad-hoc loss function is the modeling of known uncertainties\nin the data and applying Maximum Likelihood (ML) approaches, which, in consequence, lead\ninherently to data driven loss functions. For example, data uncertainties can appear only in the\noutput data (such as for ordinary least squares) or in input and output data, also known as the\nerrors-in-variables approaches. A well-known connection is between a normal distributed error in\nthe output and the squared loss function. These approaches can always be extended to Bayesian\nestimations if prior distributions are assumed for the model parameters leading to Maximum A\nPosteriori (MAP) or Minimum Mean Squared Error (MMSE) algorithms based on the posterior\ndistribution [Hoegele et al., 2013].\nUnpaired / unlabeled data (also broken sampling in regression) can occur in many applications and\nis focus of current research, e.g. [Bai and Hsing, 2005, Liang et al., 2007, Wang et al., 2022b]. In this\nwork, we understand by partially unpaired data (under the term semi-supervised data) specifically\nthat for parts of the data the one-to-one pairing of input and output data is missing, but we still\nhave paired subgroups in the data. Semi-supervised data often is understood as having additional\ninput data without output data, e.g. [Kostopoulos et al., 2018, Qi and Luo, 2022]. This is different\nto the partially unpaired data in this paper and we regard this as a different flavor of semi-supervised\ndata since we lose significant information compared to a fully supervised framework, but we still\nhave (possibly weak) input/output relations in contrast to unsupervised learning. See Figure 1 for\nan illustration of different pairing information between input x and output y. It is demonstrated\nthat mixtures of labeled and unlabeled data can improve the predictive performance in regression\nproblems [Liang et al., 2007]. There are different strategies to deal with such deficiencies, such\nas altering the data set by data imputation approaches, e.g. see [Bennett, 2001, Sterne et al.,\n2009], or focusing on unordered subsets in hypothesis testing [Wang et al., 2022b]. We regard\nthe work of Liang et al. [Liang et al., 2007] considering a general predictive Bayesian frameworks\nfor mixed labeled and unlabeled data as closest to our goal. Although the work provides a very\ngeneral framework for regression and classification, it misses the configurational complexity of\npartially unpaired data.\nIn consequence, one task of this paper is to incorporate the largest\npossible variety of missing pairing information transparently to model fitting by avoiding data\nalteration (deletion or imputation) and actively constructing mixture model probability densities\naccurately representing the partial pairing.\nFigure 1: Nomenclature of data configurations: completely paired, completely unpaired and semi-\nsupervised with the classical and the partially unpaired configuration.\nMixture Models are probability density functions, which are composed of the weighted sum of\nelementary probability densities. They are applied in a variety of applications, especially Gaussian\nMixture Models (GMM) are very popular, which are the weighted sum of Gaussian densities, e.g.\n[Liang et al., 2007]. A main task is in the literature to find the GMM description of a given data set\nutilizing expectation maximization (EM) algorithms, also for the task of missing data deficiencies\n3\n[Michael et al., 2020, McCaw et al., 2022]. To the knowledge of the authors, a general framework\nfor constructing mixture models for dealing with the high complexity of corrupt / incomplete\npairing information of the data with the goal to support a model fitting problem is missing in the\nliterature. Closely related approaches of utilizing mixture models in applied mathematics with\nlost pairing information are presented for computer vision in order to deal with unknown feature\ncorrespondences [Hoegele, 2024a] and for random equations with high combinatorial possibilities\nfor the stochastic parameters [Hoegele, 2024b].\nIn this paper, we propose to consider the problem of model fitting in a new conclusive way. A\ngeneral probabilistic framework for fitting models in data based purely on observational error prob-\nability density functions including errors-in-variables is presented, which has direct relations to\nwell-known standard methods for completely paired data, such as ordinary least squares, Dem-\ning regression [Deming, 1964], total least squares[Markovsky and Van Huffel, 2007], interval data\nregression as well as multiple linear regression. Observational error densities can have different\nreasons, for example, errors only in the output can be classical Gaussian measurement errors, but\nif regression is performed additionally with measured input data then both, input and output er-\nrors (errors-in-variables), are typically described by Gaussians. Another example is interval data,\nwhich can be represented by uniform densitiy functions and which can occur, e.g., in survey data,\nin particular, when asking for sensitive information like income. This framework will be general-\nized from supervised to semi-supervised model fitting by including (partially) unpaired data in one\ncommon line of stochastic argumentation. It is a key point that the presented derivations allow\nfor full flexibility about i) the number and dimensions of the input / output data, ii) the type of\nindividual error characteristics of each data point with errors-in-variables utilizing general density\nfunctions, iii) the type of (linear or nonlinear) models which should be fitted and iv) the pairing\ninformation level. In the schematic Figure 2 the concepts of this work are explained in an overview\nstarting from the well\u2013known ordinary least squares application of a line fit in Subfigure A to the\nmost general concept of this paper in Subfigure D with a nonlinear fit in partially unpaired data.\nThis presentation utilizes a one-dimensional input and output for illustrative purposes, which is\ngeneralized in the paper to arbitrary dimensions.\nIn summary, there are two main goals of this paper: I) presenting a general stochastic argu-\nmentation framework for model fitting (without ad-hoc loss functions) (see Figure 2A\u2013C) and II)\npresenting an extension of model fitting within this framework to partially unpaired data utilizing\nmixture models (see Figure 2D).\nIn I) the main ideas of the stochastic argumentation are: a) Formulating the fitting problem as a\nMaximum Likelihood (ML) problem of difference random variables. b) Applying the law of total\nprobability for densities wherever necessary to make sure that correct stochastic dependencies are\nutilized and identifying the density functions of the basic random variables in the ML problem. c)\nPresentation of the optimization problems by the resulting objective functions. Points a) to c) are\npresented repeatedly for different fitting scenarios and their generality is a first main result of this\npaper.\nIn II) the extension of model fitting to partially unpaired data is structured in the following three-\nstep-approach: Presenting the cases for\n\u2022 completely paired data sets as the standard case in model fitting (i.e., supervised learning)\n(Section 2.2).\n\u2022 completely unpaired data sets are introduced mathematically utilizing mixture model random\nvariables to model fitting (Section 2.3). In this extreme case it is impossible to model the\nrelation between input and output.\n\u2022 partially unpaired data sets, which lie between the two previous extremes and include different\nlevels of pairing (i.e., semi-supervised learning) (Section 2.4).\nThis structure is chosen in order to allow clear and separated lines of argumentation which even-\ntually conclude in the second main result of the paper.\nIn the results section, we demonstrate the applicability of this framework by simulation studies\nwith Gaussian and uniform mixture models as observational error densities for a line fit (Section\n3.1) and a fit of anisotropic noisy data with a cubic polynomial (Section 3.2). Further, in Section\n4, we will demonstrate how this argumentation can be applied to multiple linear regression for life\n4\nFigure 2: Schematic illustration of the model fitting concepts in this paper for an input x \u2208R\nand output y \u2208R, presented as steps of generalizations. A) Well\u2013known ordinary least squares for\na line fit model, which corresponds to Maximum Likelihood estimation with a constant Gaussian\nobservational error density f\u03b5 in y. B) Generalization of case (A) to a general nonlinear model\nM(x; \u03b1) and general observational error density f\u03b5l in y individual for each data point l = 1, .., L.\nThis is presented in Supplement A. C) Generalization of (B) to the errors-in-variables approach\nincluding observational error density functions f\u03b7l in x.\nThis is presented in Section 2.2.\nD)\nGeneralization of (C) to also include partially unpaired data subgroups, which contains all possible\npairings of data within a subgroup (in this case 3 \u00b7 3 = 9 pairings). This is realized with mixture\nmodels, which are the weighted sum of the data point probability densities. This is presented in\nSection 2.4.\nexpectancy data from the world bank. These results demonstrate the importance of modeling the\ninherent uncertainties and the use of different levels of pairing information in data.\n2\nMethods\n2.1\nGeneral Nomenclature\nThroughout the paper, we utilize the following nomenclature:\n\u2022 Observations are presented by input data xl \u2208Rk and output data yl \u2208Rm for l = 1, .., L\nas independent observations.\n\u2022 We call the data set completely paired if for every l = 1, .., L there is a unique correspondence\nbetween xl and yl, typically written as tuples (xl, yl).\nWe call the data completely unpaired if there is no pairing at all, i.e. there is a set of xh for\nh = 1, .., H and independently a set of yl for l = 1, .., L and there is no information which\nxh corresponds to which yl.\nIn consequence, partially unpaired data are a mix of both extremes, i.e. we have R subgroups\nof the data xh and yl and inside each subgroup there is no pairing information of the data\n(no correspondences) but it is guaranteed that no xh, or yl respectively, of one subgroup\n5\ncorresponds to a xh, or yl respectively, of another subgroup. This means, we have pairing\ninformation only on the level of subgroups. These data configurations are illustrated in Figure\n1.\n\u2022 Probability density functions are denoted by X \u223cfX(x), which are Lebesgue integrable\nfX(x) \u2208L1 and contain standard cases such as the normal or uniform distribution. Dirac\ndistributions are used for theoretical discussions to show the connection between undisturbed\nand disturbed observations.\n\u2022 We denote explicit models to be fitted from x to y by functions M( \u00b7 ; \u03b1) : Rk 7\u2192Rm\ndepending on the model parameters \u03b1 \u2208RN. A toy example for an explicit model is the\none-dimensional affine model of linear regression R 7\u2192R (k = 1, m = 1, N = 2): M(x; \u03b1) =\n\u03b11 + \u03b12 \u00b7 x.\n\u2022 Following a flexible Bayesian view on random variables is essential in this work.\nIn the\nclassical perspective we have an undisturbed variable y \u2217(the true value), which is disturbed\nby an error random variable \u03b5 leading to the observation y, in short: y := y \u2217+ \u03b5. We\ninterpret y as a new random variable of observations which has a shifted density function of\n\u03b5 (we interpret y \u2217as a random variable with Dirac distribution at the true value). In this\nwork, we consequently take an alternative point of view and introduce the definition by the\nreformulation: y \u2217:= y \u2212\u03b5. This time we interpret y \u2217as a new random variable of the true\nvalues (as typical in Bayesian frameworks) which has a shifted density function of \u03b5 (this\ntime, we interpret y as the random variable with Dirac distribution at the observed value).\nThe meaning of \u03b5 in these two perspectives is different but related, capturing the uncertainty\nof observation with a different center. We consequently utilize the latter notation in the rest\nof the paper.\n\u2022 In the stochastic argumentation, we utilize the notation\nf L\nT\nl=1\nZl\n(z) := fZ1,...,ZL(z)\nwith the meaning of the common density function of all individual random variables Z1, . . . , ZL.\n\u2022 In the following derivations, we focus on the argmax/argmin of an expression. Since the\nargmax/argmin is independent of the application of strictly monotonic increasing functions,\nwe neglect those in the course of argumentation, i.e. for c \u2208R+ we will write\nargmax \u03b1 c \u00b7 f(\u03b1) = argmax \u03b1 f(\u03b1) = argmax \u03b1 ln (f(\u03b1)) .\n2.2\nModel Fit with Completely Paired Data\nThe observations in this section are of type (xl, yl) as tuples for l = 1, .., L. Standard approaches,\nsuch as ordinary least squares (errors in y only) in this argumentation framework are presented in\nSupplement A.\nWe introduce disturbances in the input and output data with the notation (which is referred in\nliterature to errors-in-variables [Markovsky and Van Huffel, 2007]):\nx \u2217\nl := xl \u2212\u03b7l\n(1)\ny \u2217\nl := yl \u2212\u03b5l\n(2)\nwith x \u2217\nl \u2208Rk and y \u2217\nl \u2208Rm the random variables of true values, and the uncertainty random\nvariables \u03b7l \u223cf\u03b7l(s) : Rk 7\u2192R and \u03b5l \u223cf\u03b5l(s) : Rm 7\u2192R independent for all l = 1, .., L. We are\ninterested in the case where the model is correctly chosen so that the true x \u2217\nl predicts the true y \u2217\nl :\nM(x \u2217\nl ; \u03b1)\nd= y \u2217\nl\n\u2200l = 1, .., L\nM(xl \u2212\u03b7l; \u03b1)\nd= yl \u2212\u03b5l\n\u2200l = 1, .., L ,\nwith\nd= meaning equality in distribution. Due to \u03b7l and \u03b5l being random variables, the differences\nof right and left side M(xl \u2212\u03b7l; \u03b1) \u2212yl + \u03b5l are interpreted as difference random variables for\n6\nall l = 1, .., L whose density function values should have the highest possible value at 0 \u2208Rm to\nachieve the most probable equality leading to the Maximum Likelihood approach:\n\u21d2argmax \u03b1 f L\nT\nl=1\n[ M(xl\u2212\u03b7l;\u03b1)\u2212yl+\u03b5l ](0)\n= argmax \u03b1\nL\nY\nl=1\nfM(xl\u2212\u03b7l;\u03b1)\u2212yl+\u03b5l(0)\n(independency of \u03b7l, \u03b5l \u2200l)\n= argmax \u03b1\nL\nY\nl=1\nZ\nRk\nfM(xl\u2212s;\u03b1)\u2212yl+\u03b5l(0) \u00b7 f\u03b7l(s) ds\n(law of total probility)\n= argmax \u03b1\nL\nY\nl=1\nZ\nRk\nf\u03b5l(yl \u2212M(xl \u2212s; \u03b1)) \u00b7 f\u03b7l(s) ds\n(shifted \u03b5l)\n(3)\n= argmax \u03b1\nL\nY\nl=1\nZ\nRk\nf\u03b5l(yl \u2212M(s; \u03b1)) \u00b7 f\u03b7l(xl \u2212s) ds\n(integral shift)\n(4)\nApplying the law of total probability allows recovering the observation density functions in the final\nML expression. In the following, we present examples of this general formula (4) (or equivalently\nEquation (3) if beneficial).\nRemark: By setting f\u03b7l(s) = \u03b4(s) (the Dirac distribution), we allow no variation of the x \u2217\nl -values\nand, in consequence, get the equation of ordinary least squares (Supplement A) by applying the\nsifting property. In consequence, this can be regarded as a true generalization of errors in y only.\n2.2.1\nExample: Fitting a Line and Gaussian Disturbance\nIn the line of total least squares [Markovsky and Van Huffel, 2007], we are introducing Gaussian\ndisturbances by \u03b7l \u223cN(0, \u03c32\n\u03b7)(s) \u2200l = 1, .., L and \u03b5l \u223cN(0, \u03c32\n\u03b5)(s) \u2200l = 1, .., L. Utilizing the\none-dimensional affine model M(x; \u03b1) = \u03b11 + \u03b12 \u00b7 x and inserting it in Equation (4), we get\n\u21d2\nargmax \u03b1\nL\nY\nl=1\nZ\nR\ne\n\u2212\n1\n2 \u03c32\u03b5 (yl\u2212\u03b11\u2212\u03b12\u00b7s)2\u2212\n1\n2 \u03c32\u03b7 (xl\u2212s)2\nds\n(5)\n=\nargmax \u03b1\nL\nY\nl=1\ne\n\u2212\n(\u03b11+\u03b12\u00b7xl\u2212yl)2\n2 (\u03b12\n2 \u03c32\u03b7+\u03c32\u03b5)\nq\n\u03b12\n2 \u03c32\u03b7 + \u03c32\u03b5\n=\nargmin \u03b1\nL\n2 ln\n\u0000\u03b12\n2 \u03c32\n\u03b7 + \u03c32\n\u03b5\n\u0001\n+\nL\nX\nl=1\n(\u03b11 + \u03b12 \u00b7 xl \u2212yl)2\n2 (\u03b12\n2 \u03c32\u03b7 + \u03c32\u03b5)\n.\n(6)\nSince this is a Deming regression type of problem, the solution to this minimization is also closely\nrelated to the classical Deming regression. The similarities and differences are presented in Sup-\nplement B.\n2.2.2\nExample: Fitting a Hyperplane and Gaussian Disturbance (Errors-In-Variables\nMultiple Linear Regression)\nFurther extending this argumentation to hyperplanes for xl \u2208Rk and yl \u2208R, and \u03b7l \u223cN(0, diag(\u03c32\n\u03b7,1, .., \u03c32\n\u03b7,k))(s)\n\u2200l = 1, .., L and \u03b5l \u223cN(0, \u03c32\n\u03b5)(s) \u2200l = 1, .., L for fitting an affine hyperplane model M(x; \u03b1) =\n7\n\u03b11 +\nkP\nn=1\n\u03b1n+1 \u00b7 xn leads (utilizing Equation (4)) to the general optimization problem\n\u21d2\nargmax \u03b1\nL\nY\nl=1\nZ\nRk\ne\n\u2212\n1\n2 \u03c32\u03b5\n \nyl\u2212\u03b11\u2212\nk\nP\nn=1\n\u03b1n+1 sn\n!2\n\u22121\n2\n \nk\nP\nn=1\n(xl,n\u2212sn)2\n\u03c32\u03b7,n\n!\nds\n=\nargmax \u03b1\nL\nY\nl=1\ne\n\u2212\n \n\u03b11+\nk\nP\nn=1\n\u03b1n+1\u00b7xl,n\u2212yl\n!2\n2\n  \nk\nP\nn=1\n\u03b12\nn+1 \u03c32\u03b7,n\n!\n+\u03c32\u03b5\n!\ns\u0012 kP\nn=1\n\u03b12\nn+1 \u03c32\u03b7,n\n\u0013\n+ \u03c32\u03b5\n=\nargmin \u03b1\nL\n2 ln\n  k\nX\nn=1\n\u03b12\nn+1 \u03c32\n\u03b7,n\n!\n+ \u03c32\n\u03b5\n!\n+\nL\nX\nl=1\n\u0012\n\u03b11 +\nkP\nn=1\n\u03b1n+1 \u00b7 xl,n \u2212yl\n\u00132\n2\n\u0012\u0012 kP\nn=1\n\u03b12\nn+1\u03c32\u03b7,n\n\u0013\n+ \u03c32\u03b5\n\u0013 .\n2.2.3\nConnection to Interval Data Regression\nInterval data is defined as data for which only the borders of an interval in which the true data\npoint lies are observed. Performing model fitting for such data is an active field of research, e.g.\nfor multilinear linear regression models [Lima Neto and De Carvalho, 2008, Souza et al., 2017].\nFirst, interval data for regression is defined the following way: For each data point coordinate\nxl,i (i = 1, .., k) and yl,j (j = 1, .., m), respectively, we only know the interval borders, i.e.\nxl,i \u2208[xl,i, xl,i] and yl,i \u2208[yl,j, yl,j], which are independently measured. This information can\nbe interpreted as a uniform distribution with probability mass inside the interval and zero out-\nside.\nBy introducing xl :=\n1\n2\n\u0000xl + xl\n\u0001\nand yl :=\n1\n2\n\u0010\nyl + yl\n\u0011\n, and vl,i :=\n1\n2\n\u0010\nxl,i \u2212xl,i\n\u0011\nand\nwl,j := 1\n2\n\u0010\nyl,j \u2212yl,j\n\u0011\n, this is equivalent to the general description of Equations (1) and (2) with\n\u03b7l \u223c\nk\nY\ni=1\nU[\u2212vl,i,vl,i](si)\n\u03b5l \u223c\nm\nY\nj=1\nU[\u2212wl,j,wl,j](sj) ,\nwith U[a,b](s) the density function of the uniform distribution on the interval [a, b]. This is obvious,\nsince we can define an interval by either the two interval borders or the midpoint and its half\nwidth.\nSecond, this means we can apply Equation (3) for fitting a model M into that interval data, leading\nto\n\u21d2argmax \u03b1\nL\nY\nl=1\nZ\nRk\nm\nY\nj=1\nU[\u2212wl,j,wl,j]\n\u00121\n2\n\u0010\nyl,j + yl,j\n\u0011\n\u2212Mj\n\u00121\n2\n\u0000xl + xl\n\u0001\n\u2212s; \u03b1\n\u0013\u0013\n\u00b7\nk\nY\ni=1\nU[\u2212vl,i,vl,i](si) ds .\nThis can be further simplified to the argmax \u03b1 of\nL\nY\nl=1\n1\nkQ\ni=1\nvl,i\n\u00b7\nZ\n[\u2212vl,1,vl,1]\u00d7\n\u00b7\u00b7\u00b7\u00d7[\u2212vl,k,vl,k]\nm\nY\nj=1\nU[\u2212wl,j,wl,j]\n\u00121\n2\n\u0010\nyl,j + yl,j\n\u0011\n\u2212Mj\n\u00121\n2\n\u0000xl + xl\n\u0001\n\u2212s; \u03b1\n\u0013\u0013\nds .\nFor each s the integrand is either zero or the positive normalization constant of the density of\n\u03b5l, leading to a k-dimensional constant region for the integrand, whose volume is integrated over\n8\nthe k-dimensional box [\u2212vl,1, vl,1] \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [\u2212vl,k, vl,k]. This means, the resulting optimization is\nsearching for \u03b1 which maximizes the overlapping volume of the k-dimensional region with the\nk-dimensional box for all data points l = 1, .., L under consideration of the weights vl,i and wl,i.\nThis is an intuitive general understanding of model fitting with interval data.\n2.2.4\nExample: Linear Interval Data Regression\nUtilizing the one-dimensional affine model M(x; \u03b1) = \u03b11 + \u03b12 \u00b7 x, we can further derive\n\u21d2argmax \u03b1\nL\nY\nl=1\n1\nvl\n\u00b7\nZ\n[\u2212vl,vl]\nU[\u2212wl,wl]\n\u00121\n2\n\u0000yl + yl\n\u0001\n\u2212\n\u0012\n\u03b11 + \u03b12\n\u00121\n2\n\u0000xl + xl\n\u0001\n\u2212s\n\u0013\u0013\u0013\nds\n\u03b12\u0338=0\n\u21d2argmax \u03b1\nL\nY\nl=1\n1\nvl\n\u00b7\nZ\n[\u2212vl,vl]\n1\n2 wl\n\u03c7[cl,min(\u03b1),cl,max(\u03b1)] (s) ds\nwith the abbreviations cl,\u00b1(\u03b1) :=\n1\n2\n\u0000xl + xl\n\u0001\n+\n1\n\u03b12\n\u0000\u03b11 \u22121\n2\n\u0000yl + yl\n\u0001\n\u00b1 wl\n\u0001\n, cl,min = min(cl,\u00b1),\ncl,max = max(cl,\u00b1) and \u03c7[a,b](s) the characteristic function (1 if s \u2208[a, b], else 0). This can further\nbe simplified to\n\u21d2argmax \u03b1\nL\nY\nl=1\n1\nvl \u00b7 wl\n\u00b7 max [ min [vl, cl,max(\u03b1)] \u2212max [\u2212vl, cl,min(\u03b1)] , 0 ] .\nFor an example fit according to this formula, see the results Section 3.1\nRemark: With this framework of argumentation, we can also introduce uncertainty about the\nknowledge of the integral borders in a transparent way by not assuming a strict uniform distribu-\ntion, but a distribution blurred at the borders.\n2.3\nModel Fit with Completely Unpaired Data\nIn this section, we are losing the property of tuples, i.e. observations of type xh for h = 1, .., H and\nyl for l = 1, .., L are unpaired. Only a set of xh and a set of yl observations are available. Please\nnote, even the sizes H and L can be different. Although there is theoretical research about the\nusability of such broken sampling data sets, e.g. [Bai and Hsing, 2005], obviously such data will\nonly lead to very limited regression results if there are no further assumptions about the involved\nprobability densities since we only have marginal distributions. Nonetheless, we want to introduce\na formulation by mixture models for this case, which will later be utilized for partially unpaired\ndata directly. We start this argumentation with possible disturbances in input and output data:\nx \u2217\nh := xh \u2212\u03b7h\ny \u2217\nl := yl \u2212\u03b5l\nwith x \u2217\nh \u2208Rk and y \u2217\nl \u2208Rm the random variables of the true values, and the uncertainty random\nvariables \u03b7h \u223cf\u03b7h(s) : Rk 7\u2192R and \u03b5l \u223cf\u03b5l(s) : Rm 7\u2192R independent for all h = 1, .., H and\nl = 1, .., L. A new step is now to introduce the two mixture model random variables\nX\u2217\u223cfX\u2217(s) = 1\nH\nH\nX\nh=1\nfxh\u2212\u03b7h(s) = 1\nH\nH\nX\nh=1\nf\u03b7h(xh \u2212s)\nY \u2217\u223cfY \u2217(s) = 1\nL\nL\nX\nl=1\nfyl\u2212\u03b5l(s) = 1\nL\nL\nX\nl=1\nf\u03b5l(yl \u2212s) ,\nwhich exactly contain the ignorance of the pairing, i.e. all xh and yl observations are present\nin these mixture models at once. We follow the same technical argumentation as in the previous\nsection:\nM(X\u2217; \u03b1)\nd= Y \u2217.\n9\nDue to X\u2217and Y \u2217being random variables, the difference of right and left side M(X\u2217; \u03b1) \u2212Y \u2217is\nagain interpreted as a difference random variable, whose density function value should have highest\nvalue at 0 \u2208Rm. This leads to the ML approach:\n\u21d2argmax \u03b1 fM(X\u2217;\u03b1)\u2212Y \u2217(0)\n= argmax \u03b1\nZ\nRk\nfM(s;\u03b1)\u2212Y \u2217(0) \u00b7 fX\u2217(s) ds\n(law of total probility)\n= argmax \u03b1\nZ\nRk\nfY \u2217(M(s; \u03b1)) \u00b7 fX\u2217(s) ds\n(shifted Y \u2217)\n(7)\n= argmax \u03b1\nZ\nRk\n \n1\nL\nL\nX\nl=1\nf\u03b5l(yl \u2212M(s; \u03b1))\n!\n\u00b7\n \n1\nH\nH\nX\nh=1\nf\u03b7h(xh \u2212s)\n!\nds\n(def. of X\u2217, Y \u2217)\n= argmax \u03b1\n1\nLH\nL\nX\nl=1\nH\nX\nh=1\nZ\nRk\nf\u03b5l(yl \u2212M(s; \u03b1)) \u00b7 f\u03b7h(xh \u2212s) ds .\n(8)\nRemark: The double sum in Equation (8) takes care of all combinations of xh and yl coming\ndirectly from a strict stochastic derivation with these mixture model random variables. In the\ncompletely paired case with independent observations, a product appears in Equation (4) which\ncorresponds to this double sum for the completely unpaired case.\nRemark: This fit with completely unpaired data is practically useless. This means, there will be\nbroad, probably non-distinct or multiple maxima in this objective function. Still, this argumen-\ntation helps in a theoretical perspective since it is applied directly to the partially unpaired data\nwhere we have a range of different levels of pairing information.\n2.4\nModel Fit with Partially Unpaired Data\nThis section introduces the argumentation for partially unpaired data, which is a main result of this\npaper. For this case, we partition the H observations xh and L observations yl into r = 1, .., R\ndisjoint independent groups, i.e. observations of group r of type xh,r for h = 1, .., Hr and yl,r for\nl = 1, .., Lr are unpaired with Hr and Lr representing the number of elements in subgroup r. This\nmeans, we have a set of xh-values and a set of yl-values for each subgroup and we have pairing\ninformation on the group level. The number of input and output elements in each subgroup Hr\nand Lr are not necessarily the same. Again, we allow disturbances in xh and yl:\nx \u2217\nh := xh \u2212\u03b7h\ny \u2217\nl := yl \u2212\u03b5l\nwith x \u2217\nh \u2208Rk and y \u2217\nl \u2208Rm the random variables of the true values, and the independent un-\ncertaintiy random variables \u03b7h \u223cf\u03b7h(s) : Rk 7\u2192R and \u03b5l \u223cf\u03b5l(s) : Rm 7\u2192R (l = 1, .., L).\nThe main argument for dealing with unpaired data is presented by mixture models, i.e. we define\n(r = 1, .., R):\nX\u2217\nr \u223cfX\u2217\nr(s) = 1\nHr\nHr\nX\nh=1\nf\u03b7h,r(xh,r \u2212s)\nY \u2217\nr \u223cfY \u2217\nr(s) = 1\nLr\nLr\nX\nl=1\nf\u03b5l,r(yl,r \u2212s) .\nFollowing our standard line of argumentation, we get\nM(X\u2217\nr; \u03b1)\nd= Y \u2217\nr\n\u2200r = 1, .., R ,\n(9)\n10\nand again focus on the difference random variables M(X\u2217\nr; \u03b1) \u2212Y \u2217\nr for all r = 1, .., R at 0.\nFollowing the ML approach, we arrive at\n\u21d2argmax \u03b1 f R\nT\nr=1\n[ M(X\u2217\nr;\u03b1)\u2212Y \u2217\nr ](0)\n= argmax \u03b1\nR\nY\nr=1\nfM(X\u2217\nr;\u03b1)\u2212Y \u2217\nr(0)\n(group independency)\n= argmax \u03b1\nR\nY\nr=1\nZ\nRk\nfY \u2217\nr(M(s; \u03b1)) \u00b7 fX\u2217\nr(s) ds\n(cp. equ. (7))\n(10)\n= argmax \u03b1\nR\nY\nr=1\n\uf8ee\n\uf8f0\n1\nLrHr\nLr\nX\nl=1\nHr\nX\nh=1\nZ\nRk\nf\u03b5l,r(yl,r \u2212M(s; \u03b1)) \u00b7 f\u03b7h,r(xh,r \u2212s) ds\n\uf8f9\n\uf8fb\n(cp. equ. (8))\n(11)\nEquation (11) is the most general formula we derive in this paper, since it contains the previous\ncases (completely paired data R = H = L and completely unpaired data R = 1). Most importantly,\nall other possibilities of partial pairing are contained in this equation. For example, ordinary least\nsquares for paired data (a standard regression approach) is achieved by setting R = L = H (equals\ngroup size 1) and setting f\u03b7h(s) = \u03b4(s).\nRemark: In the partially unpaired setup, we always work with input / output correspondences,\nonly on a subgroup basis. A completely paired data subset (= supervised data) is represented by\nsubgroups of size one. An additional pure input data subset (= unsupervised) can be approximated\nby neglecting the information about Y \u2217\nr, which corresponds to extremely flat f\u03b5l,r, arriving at the\nclassical definition of semi-supervised. This means, in Equation (11) the first term in the integral\nf\u03b5l,r gets essentially constant (independent of the prediction M(s; \u03b1)) and, therefore, this part\nbecomes practically noninformative with respect to the optimization on \u03b1. Thus, as expected, the\ncompletely unsupervised part of the data only on the input side can be neglected since it contains\nno information about the model parameters \u03b1.\nRemark: An interesting point is that the same cannot be said about having unsupervised data on\nthe output side, e.g. neglecting information about X\u2217\nr. This time f\u03b7h,r becomes a flat distribution,\nessentially leaving f\u03b5l,r(yl,r \u2212M(s; \u03b1)) in the integral of Equation (11) evaluated for all possible\ns. This time the change of \u03b1 can have direct influence on the optimization, essentially taking\ncare that the output values yl,r are plausible / possible (i.e. in the probabilistically blurred image\nof M(s; \u03b1)) for a given parameter set \u03b1. This shows an asymmetry with respect the classical\nsemi-supervised setup [Liang et al., 2007].\n2.4.1\nExample: Fitting a Line and Gaussian Disturbance\nGaussian disturbances in input and output variables with \u03b7h \u223cN(0, \u03c32\n\u03b7)(s) \u2200h = 1, .., H and\n\u03b5l \u223cN(0, \u03c32\n\u03b5)(s) \u2200l = 1, .., L, and utilizing the one-dimensional affine model M(x; \u03b1) = \u03b11 + \u03b12 \u00b7 x\nand inserting it, we get by applying Equation (11)\n\u21d2\nargmax \u03b1\nR\nY\nr=1\n\uf8ee\n\uf8ef\uf8f0\n1\nLrHr\nLr\nX\nl=1\nHr\nX\nh=1\ne\n\u2212\n(\u03b11+\u03b12\u00b7xh,r\u2212yl,r)2\n2 (\u03b12\n2 \u03c32\u03b7+\u03c32\u03b5)\nq\n\u03b12\n2 \u03c32\u03b7 + \u03c32\u03b5\n\uf8f9\n\uf8fa\uf8fb,\nwhich represents a solution to the Deming type problem for partially unpaired data.\n2.4.2\nExample: Fitting a Hyperplane and Gaussian Disturbance (Errors-In-Variables\nMultiple Linear Regression)\nGaussian disturbances in input and output variables with\n\u03b7h \u223cN(0, diag(\u03c32\n\u03b7,1, .., \u03c32\n\u03b7,k))(s) \u2200h = 1, .., H and \u03b5l \u223cN(0, \u03c32\n\u03b5)(s) \u2200l = 1, .., L, and utilizing\nthe k-dimensional affine model M(x; \u03b1) = \u03b11 +\nkP\nn=1\n\u03b1n+1 \u00b7 xn and inserting it, we get by applying\n11\nEquation (11)\n\u21d2\nargmax \u03b1\nR\nY\nr=1\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\nLrHr\nLr\nX\nl=1\nHr\nX\nh=1\ne\n\u2212\n \n\u03b11+\nk\nP\nn=1\n\u03b1n+1\u00b7xh,r,n\u2212yl,r\n!2\n2\n  \nk\nP\nn=1\n\u03b12\nn+1 \u03c32\u03b7,n\n!\n+\u03c32\u03b5\n!\ns\u0012 kP\nn=1\n\u03b12\nn+1 \u03c32\u03b7,n\n\u0013\n+ \u03c32\u03b5\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\nrepresenting errors-in-variables multiple linear regression for partially unpaired data.\n2.4.3\nExample: Linear Interval Data Regression\nAs final example, we present interval data that are given with xh :=\n1\n2\n\u0000xh + xh\n\u0001\nand yl :=\n1\n2\n\u0010\nyl + yl\n\u0011\n, and vh,i := 1\n2\n\u0010\nxh,i \u2212xh,i\n\u0011\nand wl,j := 1\n2\n\u0010\nyl,j \u2212yl,j\n\u0011\n, and\n\u03b7h \u223c\nk\nY\ni=1\nU[\u2212vh,i,vh,i](si)\n\u03b5l \u223c\nm\nY\nj=1\nU[\u2212wl,j,wl,j](sj) .\nThe one-dimensional affine model M(x; \u03b1) = \u03b11 + \u03b12 \u00b7 x, leads to the argmax \u03b1 of\nR\nY\nr=1\n\"\n1\nLrHr\nLr\nX\nl=1\nHr\nX\nh=1\n1\nvh,r \u00b7 wl,r\n\u00b7 max [ min [vh,r, cl,h,r,max(\u03b1)] \u2212max [\u2212vh,r, cl,h,r,min(\u03b1)] , 0 ]\n#\n,\nwith cl,h,r,\u00b1(\u03b1) := 1\n2\n\u0010\nxh,r + xh,r\n\u0011\n+\n1\n\u03b12\n\u0010\n\u03b11 \u22121\n2\n\u0010\nyl,r + yl,r\n\u0011\n\u00b1 wl,r\n\u0011\n, cl,h,r,min = min(cl,h,r,\u00b1) and\ncl,h,r,max = max(cl,h,r,\u00b1).\n2.5\nExtensions\n2.5.1\nNumerical Implementation of the General Formula for Partially Unpaired Data\nWe want to stress that the implementation of the general formula (11) is not recommended if\navoidable, due to typically high computational costs. A typical way to avoid this, is to work with\nspecific probability density types or model families, such as presented in the examples following\nEquation (11). For the general case, we provide the following implementation recommendations:\nFirst, a beneficial numerical implementation strategy is to avoid the (possibly massive) multi-\nplication in the general Equation (11). In consequence, we rewrite this by applying the natural\nlogarithm and multiplying it by \u22121 in order to generate a practically useful minimization problem\nargmin \u03b1\n\u2212\nR\nX\nr=1\nln\n\uf8eb\n\uf8ed\n1\nLrHr\n\uf8ee\n\uf8f0\nLr\nX\nl=1\nHr\nX\nh=1\nZ\nRk\nf\u03b5l,r(yl,r \u2212M(s; \u03b1)) \u00b7 f\u03b7h,r(xh,r \u2212s) ds\n\uf8f9\n\uf8fb\n\uf8f6\n\uf8f8.\nSecond, although the formulation of Equation (11) shows the combinatorics of the possible cor-\nrespondences in unpaired data subsets, this is not an efficient way for implementation since it\ninvolves the approximation of Lr \u00b7 Hr integrals for each subgroup r. It is recommended to utilize\nEquation (10), by first evaluating the mixture models fY \u2217\nr and fX\u2217\nr for appropriate s for the nu-\nmerical integration and then solving only one integral numerically for each subgroup, leading to\nthe formula\nargmin \u03b1\n\u2212\nR\nX\nr=1\nln\n\uf8eb\n\uf8ed\nZ\nRk\nfY \u2217\nr(M(s; \u03b1)) \u00b7 fX\u2217\nr(s) ds\n\uf8f6\n\uf8f8.\nThird, numerical approximation of the integral is necessary.\nFor high dimensional input data\ndimensions xh \u2208Rk it is preferable to apply advanced Monte Carlo integration. For example, if\n12\nwe utilize p = 1, .., P samples sr,p drawn from the mixture model density fX\u2217\nr, then we can apply\nMonte Carlo integration with the formula\nargmin \u03b1\n\u2212\nR\nX\nr=1\nln\n \n1\nP\nP\nX\np=1\nfY \u2217\nr(M(sr,p; \u03b1))\n!\n,\n(12)\nwhich increases computational efficiency significantly.\nFourth, the choice of optimization algorithm depends strongly on the dimensionality of the pa-\nrameters \u03b1 \u2208RN. For low dimensions such as N < 10 standard minimization routines such as\nQuasi-Newton optimization are recommended. For high dimension optimization problems stochas-\ntic gradient descent or simulated annealing are certainly preferable approaches. As starting values\nof these iterative optimization routines the ordinary least squares solutions can be utilized, if\napplicable.\nFifth, due to the choice of f\u03b5l,r and f\u03b7h,r (in the best case representing the true data errors), the\noptimization problem can be more or less difficult. For example, selecting these densities with\ntoo small standard deviations, the objective function might contain a large number of non-distinct\nlocal extrema next to each other, which is difficult for local optimization algorithms. On the other\nside, selecting these densities with too large standard deviations may lead to very broad extrema,\nwhich can be helpful for the optimization algorithm but strongly reduces the information value of\nthe observed data.\n2.5.2\nEvaluation of Unpaired Data Subgroups\nPresenting the mathematical argumentation framework does not mean, that a practical model\nfitting problem at hand is well stated. Let us focus on the input set D = {xh, h = 1, .., H}. We\nare choosing R unpaired subgroups Sr (r = 1, .., R) which in total represents a partition of these\ninput values\nR\n[\nr=1\nSr = D\n\u2227\nSr \u2229Sm = \u2205\u2200r, m \u2208{1, .., R}, r \u0338= m .\nThe question arises which partitioning is beneficial for the fit and which is not. At this point, we\nonly want to discuss this problem by exploring the extremes:\nA) If all subgroups Sr (approximately) contain a representative sample of the whole data set D,\nthen the model fitting is qualitatively the same as if we would use the completely unpaired\ncase, which can be regarded as useless for a practical model fit, since no useful pairing\ninformation is contained in such a partitioning.\nB) If all subgroups Sr are presenting different, separated areas of the input data set, i.e. each\nsubgroup is very dissimilar to D.\nIn consequence, one way to judge about the practical usefulness of the partitioning is to look\nfor dissimilarity of each Sr to D and between subgroups Sr.\nThe question arises: What is a\ngood measure to determine the dissimilarity between Sr and D and between subgroups Sr for\nall r = 1, .., R? Only for high dissimilarity, the pairing inside the subgroups will not degrade the\nmodel fitting result strongly. In Supplement C an illustrative example for this effect is presented.\nFor designing data observation processes with deliberately partially unpaired data (maybe due to\nprivacy protection, or observational costs etc.) it could be helpful to measure such dissimilarities\ndirectly and we regard this as future work.\n2.5.3\nSimultaneous Estimation of the Underlying Density Functions\nIn Equation (11) we assumed known and fixed input and output density functions f\u03b7h,r and f\u03b5l,r for\ndata groups r = 1, .., R with their element indices h = 1, .., Hr and l = 1, .., Lr. An extension of the\nproposed estimation concept is that the density functions are depending on unknown parameters,\ni.e. \u03b2h,r for x- and \u03b3l,r for y-values, which we want to estimate simultaneously with the model\n13\nparameters \u03b1. We denote this by density functions f\u03b7h,r( \u00b7 ; \u03b2h,r) and f\u03b5l,r( \u00b7 ; \u03b3l,r). This leads to\nthe extended Maximum Likelihood problem:\nargmax (\u03b1,\u03b2,\u03b3)\nR\nY\nr=1\n\uf8ee\n\uf8f0\n1\nLrHr\nLr\nX\nl=1\nHr\nX\nh=1\nZ\nRk\nf\u03b5l,r(yl,r \u2212M(s; \u03b1); \u03b3l,r) \u00b7 f\u03b7h,r(xh,r \u2212s; \u03b2h,r) ds\n\uf8f9\n\uf8fb.\nRemark:\nOne challenge of this extension is that the optimization gets high-dimensional with\npossibly many local maxima, which might occur due to the flexible interplay of densities with large\nstandard deviations and the matching of the unpaired data groups. Further, too many parameters\nscaling with the data size may lead to problems of identifiability.\nIn consequence, we assume\nthat there will be the need to combine parameters and force the densities to smaller standard\ndeviations, e.g. by penalizing parameters (\u03b2, \u03b3) which correspond to large standard deviations.\nThere are many standard approaches for additive penalizing and we regard this out of the scope\nof the current presentation. A pragmatic approach for combining parameters could be to assume\nthat for each input and output coordinate the error densities are known and identical with zero\nmean and unknown standard deviations, i.e. \u03b2 := \u03c3\u03b7 \u2208Rk and \u03b3 := \u03c3\u03b5 \u2208Rm (independent of\nh, l and r). This allows a global estimation of the coordinate errors simultaneously to the model\nparameters and increases the degrees of freedom of the optimization problem only by k + m. For\nexample, this corresponds for the errors-in-variables multiple linear regression in Section 2.4.2 to\nadditionally maximize all \u03b2n := \u03c3\u03b7,n (n = 1, .., k) and \u03b3 := \u03c3\u03b5, increasing the degrees of freedom\nfrom k + 1 to 2 \u00b7 (k + 1).\n2.5.4\nBayesian Extension\nIn this derivation, we defined the likelihood function, which we need to maximize in the previous\nsections by\nL(0 | \u03b1) :=\nR\nY\nr=1\nfM(X\u2217\nr;\u03b1)\u2212Y \u2217\nr(0) .\nThe unusual perspective in this likelihood derivation is that our observation is 0 since we must\nfind the parameters \u03b1 of the difference random variable M(X\u2217\nr; \u03b1) \u2212Y \u2217\nr to make 0 most likely.\nThis can be extended to a classical Bayesian perspective by introducing a prior for the random\nvariable \u03b1 \u223c\u03c0(\u03b1). In consequence, the posterior density (utilizing Bayes\u2019 rule) gets\n\u03c0(\u03b1 | 0) = c \u00b7 L(0 | \u03b1) \u00b7 \u03c0(\u03b1)\n= c \u00b7\n\uf8eb\n\uf8ed\nR\nY\nr=1\nZ\nRk\nfY \u2217\nr(M(s; \u03b1)) \u00b7 fX\u2217\nr(s) ds\n\uf8f6\n\uf8f8\u00b7 \u03c0(\u03b1) ,\nwith c a normalization constant. By utilizing a non- or weakly informative prior \u03c0(\u03b1) (such as\n\u03c0(\u03b1) = const. on a large enough domain) the previously presented optimization problem is identical\nto the maximization of the posterior, leading to a Maximum A Posteriori (MAP) estimate. This\nallows to interpret the plotted objective functions in the results Section 3.1 as presentations of the\ndensity functions of \u03b1, containing directly the inherent estimation uncertainties about \u03b1 graphically\nas intensity maps.\n3\nSimulation Study\nThe purpose of the results section is to illustrate the presented general fitting approach by examples\nto improve understanding of the derived formulas.\n3.1\nDemonstration for a Line Fit\nAt first, a simple line fit is illustrated. For each of the following scenarios we vary the number of\nsubgroups R. The following scenarios are investigated:\n14\n\u2022 Base scenario A: The correct parameter values are \u03b11 = 0 and \u03b12 = 0.5. We utilize L =\nH = 300 data points and the Gaussian data point disturbances are drawn with \u03c3\u03b7 = \u03c3\u03b5 = 0.2\nand expectation 0.\n\u2022 Scenario B: same as A, but with increased Gaussian disturbances \u03c3\u03b7 = \u03c3\u03b5 = 0.6.\n\u2022 Scenario C: same as A, but with L = H = 36.\n\u2022 Scenario D: L = H = 100 data points in Figure 3 and L = H = 300 in Figure 4. Interval\ndata regression with uniform disturbances with standard deviations \u03c3\u03b7 = \u03c3\u03b5 = 0.2 and their\ncorresponding interval boxes.\nSee Figure 3 for presentation of example objective function and resulting line fits of scenario A\nand D. The columns correspond to different numbers of unpaired subgroups R \u2208{1, 3, 12, L}. For\nFigure 3: Illustration of example results for different line fit scenarios according to scenarios A\n(Gaussian error) and D (interval data). For each case: top row: objective functions with true\nparameters (red crosses) and maximum (green crosses), bottom row: data presentation and line fit\nresults. Inside the color-coded unpaired subgroups (green and blue) all possible correspondences\nare plotted. Columns: Four different scenarios of partial pairing with R = 1, R = 3, R = 12 and\nR = L = H groups.\nthe completeley unpaired case (R = 1) only the objective function is presented.\nSee Figure 4 for a systematic evaluation of the line fits for scenarios A to D utilizing 1000 simulated\nfits with random data errors. Presented are the box plots of the residual errors for \u03b11 and \u03b12.\nQualitative conclusions from Figures 3 and 4: First, utilizing completely unpaired data leads to\narbitrary insufficient results which can be observed by the non-distinct maxima of the objective\n15\nFigure 4: Evaluation of scenarios A to D (with L = 300) for 1000 fits. Presented are the box\nplots of the residual errors \u2206\u03b1 = \u03b1fit \u2212\u03b1truth of the intercept \u03b11 and slope \u03b12. For each scenario\ndifferent pairings are presented with R = 3, R = 12 and R = L = H = 300.\nfunctions. Second, the fewer subgroups R are utilized, the broader (and more uncertain) gets the\nmaximum in the objective function in Figure 3. Further, for very few subgroups, such as R = 3, a\nbias on the slope \u03b12 is introduced for all scenarios as presented in Figure 4. On the other side, it\nis obvious that reasonable estimation of the parameters is absolutely possible even if only partially\nunpaired data is available (comparing R = 12 and R = 300). Third, comparing scenarios A to\nB: The uncertainty of estimation increases with an increased noise level of the data for all cases\nof R. Fourth, comparing scenarios B to C: The higher noise level in the data leads to similar\nuncertainties in comparison to fewer data. Fifth, comparing scenarios A to D: The main difference\nutilizing interval data compared to Gaussian disturbances is that also for the completely paired\ncase no distinct maximum appears but a plateau of high intensity values are observable in the\nobjective function in Figure 3. In Figure 4 it can be seen that interval data leads to similar results\nwith a stronger bias on the slope for R = 3.\nIn Supplement C a plane fit is presented as an example for 2D input variables.\n3.2\nNonlinear Model with Anisotropic Observation Errors\nIn order to demonstrate the flexibility of this framework, the fitting of a nonlinear model R 7\u2192R\n(k = 1, m = 1, N = 4)\nM(x; \u03b1) = \u03b11 + \u03b12 \u00b7 x + \u03b13 \u00b7 x2 + \u03b14 \u00b7 x3 ,\nis presented with anisotropic Gaussian disturbances \u03c3\u03b7 = 0.2 and \u03c3\u03b5 = 0.1 for L = H = 300\ndata points. In this model, nonlinearity holds with respect to x and not with respect to \u03b1, which\nis deliberate and not necessary.\nIn Figure 5 the results are presented for a completely paired\ncase (R = L = H) (left) and a partially unpaired case (right) with two areas of lost pairing\ninformation.\nThe general algorithm was implement according to section 2.5.1 with the simple\ntrapezoidal rule for integration and the Nelder-Mead-optimization in Matlab.\nFor comparison\nof fitting results, Figure 5 shows the generating cubic function (black dashed line), the model\nfitting result according to Equation (11) (red line) and two simple comparison model fits (violet\ncontinuous and dotted lines). This comparison model fit is the ordinary least squares fit application\nof the cubic model (neglecting the noise in x\u2212direction) with two different simple but intuitive\n16\nFigure 5: Illustration of example results for cubic model fitting scenarios on data with Gaussian\ndisturbances. The generating cubic function (black dashed line), the partially unpaired data model\nfitting (red line) and two simple comparison model fits (violet continuous and dotted lines) are\npresented. Left: completely paired case (R = L = H). Right: partially unpaired case with two\nunpaired areas with 51 lost data correspondences each (R = 200). Inside the color-coded unpaired\ngroups (green or blue dots) all possible correspondences are plotted as dots.\nimputation treatments of the unpaired data: A) (= violet continuous line) Taking the average of\nthe x\u2212and y\u2212values as a new artificial data point in these two areas and else neglect the unpaired\ndata. B) (= violet dotted line) Including all possible combinations of the unpaired data directly\nin the least squares fit. Both comparison approaches are regarded as suboptimal, but intuitive\ndata imputations for an unexperienced practitioner utilizing ordinary least squares, and therefore,\npresented for demonstration.\nIn Figure 5 (left) the benefit of including an error model in x additionally to y is presented (ordinary\nleast squares does not contain an error model in x), showing clearly superior results of the fit (red\nline) compared to the overlapping violet lines. In Figure 5 (right) the performance of the both\nsimple comparison model fits (violet lines) decrease significantly compared to the case on the left\nwhile the model fit (red line) stays robust, dealing in a stable way with the lost pairing information.\n4\nReal Data Study: Life Expectancy\nIt will be demonstrated how this framework can be utilized for a errors-in-variables multiple linear\nregression problem with observational errors in x and y on real data. This means, the model\nutilized is\nM(x; \u03b1) = \u03b11 +\nk\nX\nn=1\n\u03b1n+1 \u00b7 xn .\nThe task will be to fit this model to life expectancy data for most countries in the world. The specific\ndata set is taken from the world bank databank 1 and utilizes k = 4 input variables x1 = Birth rate,\ncrude (per 1,000 people) [SP.DYN.CBRT.IN], x2 = Urban population (percent of total population)\n[SP.URB.TOTL.IN.ZS], x3 = Political Stability and Absence of Violence/Terrorism: Estimate\n[PV.EST], x4 = logarithm of Incidence of tuberculosis (per 100,000 people) [SH.TBS.INCD] and\nthe output variable y = Life expectancy at birth, total (years) [SP.DYN.LE00.IN]. The correspond-\ning plot matrix is presented in Figure 6 for all 192 countries for which the variables were available.\nAs we want to demonstrate the errors-in-variables approach, we need to define error densities for\neach variable. Since in the world bank data there are no error margins provided, we assume nor-\nmally distributed errors with mean zero and standard deviations of 15% of the standard deviation\nof the full data set for each variable x1 (1.49), x2 (3.52), x3 (0.14), x4 (0.24) and y (1.12).\n1Data taken from https://databank.worldbank.org/source/world-development-indicators\n(June 2024),\nDatabase: World Development Indicators, data year 2020.\n17\nFigure 6: Pair plot of the input data x1 = Birth rate, crude (per 1,000 people), x2 = Urban\npopulation (percent of total population), x3 = Political Stability and Absence of Violence/Terrorism:\nEstimate, x4 = logarithm of Incidence of tuberculosis (per 100,000 people) (blue plots) and the\noutput variable y = Life expectancy at birth, total (years) (green plots). The Pearson correlation\ncoefficient is presented in each correlation plot. On the diagonal are the histograms of each variable.\nFor evaluation purposes, we perform a train-test-split with 172 training countries and 20 test\ncountries in order to judge if we can learn from the training countries the life expectancy for the\ntest countries based on the input variables. A major challenge is that the goodness of fit cannot be\nmeasured with the classical R2 since it is only valid for cases with no errors in the input variables.\nIn consequence, we utilize the extension of R2 to the errors-in-variables approach R2\n\u03b4 [Cheng et al.,\n2014] for multiple linear regression (see Supplement D for more details).\nFirst, we consider the case of the model fit with perfect pairing (R = L = H). Since the proposed\nalgorithm was implemented with Monte Carlo methods as described in Section 2.5.1, it was verified\nwith an explicit solution for the multiple linear regression model [Cheng et al., 2014] and showed\nequivalent results. The results are presented in Figure 7A as correlation and error plots of predicted\nand real output values for the train and test data sets. They show that the training allows a\nhigh quality prediction for the test countries with the multiple linear regression model for life\nexpectancy.\nIt is noted that the scatter plots themselves are defective, since the prediction is\nperformed by taking the input values x assuming no errors. In order to illustrate also the errors\nin the input values, other plot types would need to be established.\nSecond, one major approach in this work is to investigate explanatory power of the model fit if\nthe train data is partially unpaired, i.e. if we only consider data of groups of countries rather\nthan individual countries for model training. To achieve this, we introduce the country group-\ning along an additional criteria which is not part of the input: GDP per capita (current USD)\n[NY.GDP.PCAP.CD] with increasing GDP per capita for each group. Please note, although the\ngrouping is performed by GDP, the GDP itself is not part of the predictors and the overall infor-\nmation level for the predictors is reduced by this grouping compared to the completely paired data\nset. We considered 3 different groupings with group sizes Lr = Hr (the last group always consists\nof residual countries) of 4 (R = 44), 8 (R = 22) and 16 (R = 11). The results of the model fit based\n18\nFigure 7: Plot results for real and predicted values of the model fit. A) Utilizing all 172 countries\n(completely paired) and B) utilizing only R = 11 groups of countries (partially unpaired) with the\nR2\n\u03b4 values as goodness of fit measure. Top: Correlation plots, Bottom: Residual error plots. Left:\nTrain data, Right: Test data.\non this country groups are presented in Table 1. The R2\n\u03b4 for train and test data is presented which\nnumber of groups\ngroup sizes\nR2\n\u03b4 (train)\nR2\n\u03b4 (test)\n172\n1\n0.91\n0.96\n44\n4\n0.74\n0.74\n22\n8\n0.70\n0.68\n11\n16\n0.60\n0.59\nTable 1: Coefficient of determination R2\n\u03b4 in train and test data for different group sizes.\nshow overall very high goodness-of-fit values but with decreasing values for increasing group sizes,\nas one might expect. Please note, the R2\n\u03b4 for testing contains a rather strong noise component\nsince it is calculated by only 20 data points in the test set. In addition, in Figure 7B (for group\nsize 16) it is demonstrated that the fit results still align well for training and testing even for 11\nsubgroups. It is noted, that the grouping is part of the training to get \u03b1, and not part of these\nscatter plots. This indicates that the model fit and the prediction for the test countries work quite\nwell based only on the country groups, confirming that the pairing information can be reduced and\nthis still provides a valuable model fit.\n5\nDiscussion and Conclusion\nIn this work, we presented a general framework for model fitting scenarios with stochastic un-\ncertainties for completely paired and partially unpaired data utilizing mixture models. The main\nadvantage of this approach is its generality allowing for full flexibility about i) the number and\ndimensions of the data points, ii) the (possibly) individual error characteristics of each data point\nin the errors-in-variables framework, iii) the type of (linear or nonlinear) model to be fitted, iv)\nthe specific level of pairing information, and v) completely avoiding ad-hoc loss functions. The\npresented loss function is derived from the data\u2019s pairing structure and data specific error char-\nacteristics, making it the most suitable fit for these problems. We present our framework as a\ngeneralization of total least squares [Markovsky and Van Huffel, 2007], extending it to a broader\nerrors-in-variables context. By employing Gaussian errors with a line model, classical results are\nreproduced, but our approach also accommodates other scenarios, such as interval data through\nuniform distributions. The primary random variables in our study are the uncertainty variables\n\u03b7 and \u03b5, with all stochastic derivations stemming directly from their definitions. This rigorous\nfoundation is a key advantage of our approach.\nThe results in simulations and the real data study indicate by examples that there can be a trade-off\nbetween the level of pairing information (number and shape of unpaired subgroups) and estimation\n19\naccuracy, leading to a problem specific practical saturation in the accuracy level one can achieve\nby utilizing partially paired data. This means, the information about the full pairing of data is\nnot as important for the fitting process as one might think, and consequently, accurate results can\nbe performed also with reduced pairing information. Reduced pairing information can be useful,\nfor example, in cases where the data is partially corrupt, or by deliberately leaving out pairing\ninformation due to data privacy policies (e.g. anonymizing data by building unpaired subgroups).\nWe advocate to broaden the meaning of semi-supervised learning, as we did in this paper for\npartially unpaired data, in order to capture different scenarios of loss of pairing information which\nare practically relevant.\nAlthough the presented framework might be general, the derived formulas lead only for specific\nselections of density function and model types to closed form solutions. The practical implementa-\ntion can still be challenging, especially in cases with a high number of parameters \u03b1, leading to a\nhigh dimensional optimization problem of an objective function with possibly non-distinct or non-\nunique extrema. Further, if the involved probability densities are not leading to expressions where\nthe integral in Equation (11) can be exactly solved, numerical approximations of these integrals\ncan be challenging expecially for high-dimensional input data.\nIn the paper, it is only briefly presented how the ML approach can be directly extended applying\nMAP approaches.\nThis was done to directly interpret the plotted likelihood functions in the\nresults section as posterior densities by utilizing a non- or weakly informative prior. With this\ninterpretation, we are able to directly quantify the uncertainties of the parameters \u03b1 of the model\nfit, allowing the calculation of credibility intervals or regions.\nIn general, Maximum Likelihood estimators are asymptotically consistent under some conditions,\nlike identifiabilty. This extends to our framework, for fully paired data. However, for partially\nunpaired data, one would first need a meaningful definition of how the data and the data subgroups\ngrow towards infinity. This is strongly related to the question of how the dissimilarity of unpaired\nsubgroups of the data can be measured, as discussed in Section 2.5.2. This remains an interesting\nopen question with certainly a differentiated answer which we direct to future work on this topic.\nAlthough the presentation of model fitting in this paper had regression problems in mind, the same\nargumentation can be applied to classification tasks. The adaption is that the output data y and\nthe image of M(\u00b7; \u03b1) is discrete and finite. Further discretization, such as discrete density functions\nf \u03b5l can be modeled by Dirac distributions in order to directly apply the presented equations, e.g.\napplying the sifting property for the obtained integral in Equation (11).\nIn total, this is a general argumentation framework for model fitting with many possible appli-\ncations and an introduction to the specific treatment for partially unpaired data. The focus of\nthis presentation is on the applied researcher, explaining all derivations and results in detail as\nwell as providing numerical implementation strategies and interpretations of numerical examples.\nFurther work is encouraged in order to extend this framework or provide further examples (e.g.,\nbenchmarking compared to alternative fitting methods) of expressive applications.\nA\nCompletely Paired Data: Derivations for Errors in y only\nIn this case, the stochastic disturbances are only present in the output data:\ny \u2217\nl := yl \u2212\u03b5l\n(13)\nwith y \u2217\nl \u2208Rm the random variable of the true value, and the uncertainty random variable \u03b5l \u223c\nf\u03b5l(s) : Rm 7\u2192R independent for all l = 1, .., L. In a Bayesian context, the observed and true\ninput values are the same xl = x \u2217\nl . With this, we introduce the technical argumentation of model\nfitting by\nM(xl; \u03b1) = y \u2217\nl\n\u2200l = 1, .., L\n(14)\nM(xl; \u03b1) = yl \u2212\u03b5l\n\u2200l = 1, .., L ,\n(15)\ni.e. for given (undisturbed) xl we want to predict the true value y \u2217\nl . The first step in this technical\npresentation is to bring all basic random variables to the left side and equal this to 0:\nM(xl; \u03b1) \u2212yl + \u03b5l = 0\n\u2200l = 1, .., L .\n(16)\n20\nWe follow the interpretation: due to \u03b5l being a random variable, the left side is interpreted as\na shifted random variable which density function value should have highest value at 0 \u2208Rm,\nfollowing the idea of Maximum Likelihood for the parameters \u03b1.\n\u21d2\nargmax \u03b1\nf L\nT\nl=1\n[ M(xl;\u03b1)\u2212yl+\u03b5l ](0)\n(17)\n=\nargmax \u03b1\nL\nY\nl=1\nfM(xl;\u03b1)\u2212yl+\u03b5l(0)\n(independence of \u03b5l)\n(18)\n=\nargmax \u03b1\nL\nY\nl=1\nf\u03b5l(yl \u2212M(xl; \u03b1)) .\n(shifted \u03b5l)\n(19)\nWe recognize this as the common standard result of Maximum Likelihood (ML) in this new way\nof technical argumentation and we present standard examples in the following.\nExample: Gaussian Disturbance\nIntroducing Gaussian disturbances, we get \u03b5l \u223cN(0, \u03c32\n\u03b5 \u00b7 Im\u00d7m)(s) \u2200l = 1, .., L this results in\n\u21d2\nargmax \u03b1\nL\nY\nl=1\ne\n\u2212\n1\n2 \u03c32\u03b5 ||yl\u2212M(xl;\u03b1)||2\n(inserting pdf)\n(20)\n=\nargmax \u03b1\ne\n\u2212\n1\n2 \u03c32\u03b5\nL\nP\nl=1\n||yl\u2212M(xl;\u03b1)||2\n(21)\n=\nargmin \u03b1\nL\nX\nl=1\n||yl \u2212M(xl; \u03b1)||2\n(22)\nwhich is the case of multivariate (nonlinear) ordinary least squares.\nExample: Fitting a Line and Gaussian Disturbance (Linear Regression)\nFurther utilizing the one-dimensional affine model M(x; \u03b1) = \u03b11 + \u03b12 \u00b7 x and inserting it, we get\n\u21d2\nargmin \u03b1\nL\nX\nl=1\n(yl \u2212\u03b11 \u2212\u03b12 \u00b7 xl)2\n(23)\nwhich has the classical unique solution of the normal equations of ordinary least squares leading\nto a fitted line with parameters\n\u03b11 = x2 \u00b7 y \u2212x \u00b7 xy\nx2 \u2212x2\n,\n\u03b12 = xy \u2212x \u00b7 y\nx2 \u2212x2\n(24)\nwith\nx := 1\nL\nL\nX\nl=1\nxl,\ny := 1\nL\nL\nX\nl=1\nyl\n(25)\nx2 := 1\nL\nL\nX\nl=1\nx2\nl ,\nxy := 1\nL\nL\nX\nl=1\nxl \u00b7 yl .\n(26)\nB\nCompletely Paired Data: Relation to Deming Regression\nDeming regression is equivalent to the maximum likelihood for independent normally distributed\nobservation errors in xl and yl (l = 1, .., L), i.e.\n\u03b7l \u223cN(0, \u03c32\n\u03b7)(s) \u2200l = 1, .., L and \u03b5l \u223c\nN(0, \u03c32\n\u03b5)(s) \u2200l = 1, .., L with a line model. Since the true value sl for xl is unknown just like\nthe parameters \u03b1 they are estimated by maximizing them simultaneously with the parameters \u03b1,\n21\nwhich leads to the effect that the estimation of the true values influences the estimation of the\nparameters \u03b1:\nargmax \u03b1,s1,..,sL\nL\nY\nl=1\ne\n\u2212\n1\n2 \u03c32\u03b5 (yl\u2212\u03b11\u2212\u03b12\u00b7sl)2\u2212\n1\n2 \u03c32\u03b7 (xl\u2212sl)2\nIn the case of this paper, we are also estimating the best parameters \u03b1 but independently of any\nspecific true value sl: We are averaging over all possible true values by the use of the law of total\nprobability, compare Equation (5), which can be interpreted as an integrated Deming regression.\nThis is a valid alternative perspective leading to a slightly more defensive estimation of \u03b1 which\nis not influenced by the estimation of sl. An interesting observation is that the second part of\nderived objective function in Equation (6)\nargmin \u03b1\nL\nX\nl=1\n(\u03b11 + \u03b12 \u00b7 xl \u2212yl)2\n2 (\u03b12\n2 \u03c32\u03b7 + \u03c32\u03b5)\n,\n(27)\nactually leads to the classical Deming equations and the first part of Equation (6) L\n2 ln\n\u0000\u03b12\n2 \u03c32\n\u03b7 + \u03c32\n\u03b5\n\u0001\ncan be interpreted as a penalty added to the classical Deming regression, showing the more defensive\nestimation approach. Especially for large \u03c3\u03b7 the parameter \u03b12 will tend slightly more to zero. The\nderivation of the classical Deming regression in this context is directly the minimization of Equation\n(27) by setting the gradient to zero\n\u2207\u03b1\nL\nX\nl=1\n(\u03b11 + \u03b12 \u00b7 xl \u2212yl)2\n2 (\u03b12\n2 \u03c32\u03b7 + \u03c32\u03b5)\n= 0 ,\n(28)\nwhose solution results in the classical Deming regression coefficients\n\u03b11 = 1\nL\nL\nX\nl=1\nyl \u2212\u03b12 \u00b7 xl\n(29)\n\u03b12 =\nsyy \u2212\u03c32\n\u03b5\n\u03c32\u03b7 \u00b7 sxx +\nr\u0010\nsyy \u2212\u03c32\u03b5\n\u03c32\u03b7 \u00b7 sxx\n\u00112\n+ 4 \u03c32\u03b5\n\u03c32\u03b7 s2xy\n2 sxy\n(30)\nwith\nsxx = 1\nL\nL\nX\nl=1\n(xl \u2212x)2,\nsyy = 1\nL\nL\nX\nl=1\n(yl \u2212y)2,\nsxy = 1\nL\nL\nX\nl=1\n(xl \u2212x) \u00b7 (yl \u2212y) .\n(31)\nIn conclusion, the presented approach leads for Deming type problems to formulas, which we call\nintegrated Deming regression. These formulas can be interpreted as penalized classical Deming\nregression showing the more defensive approach by averaging over the true value during estimation\nof \u03b1 compared to estimating them simultaneously in classical Deming regression.\nC\nDemonstration for a Plane Fit with Gaussian Distur-\nbance for Partially Unpaired Data\nOne possible part of demonstrating the flexibility of this framework is to show how it works in\nhigher dimensions, which will be indicated by the previously introduced plane fit model.\nWe\nare using L = H = 1600 data points xh \u2208R2 and yl \u2208R and numbers of subgroups are R \u2208\n{6, 18, 100, L = H = 1600}. The data generation parameters are the same as for the base scenario\nA in the line fit section but with the true values \u03b11 = 0, \u03b12 = 0.2 and \u03b13 = 0.4.\nObviously the partitioning of the data has much more possibilities due to a much richer neighboring\ninformation for xh in 2D. In consequence, the discussed dissimilarity of the groups to the total\ndata set gets more difficult to study. Nonetheless, it can be demonstrated that the utilization with\nmore pairing information does increase accuracy and meaningful estimation can be performed even\nwith a low number of groups.\n22\nIn Figure 8 the fitting results are presented for separated groups (no overlapping of data groups in\nthe x-plane). In addition, in Figure 9 the same data is utilized but a random relabeling/switching\nof group labels is performed for approximately 30% of the data, which makes all groups slightly\nmore similar to each other. As proposed in Section 2.5.2, the plane fitting results get worse the\nmore similar the grouping gets.\nFigure 8: Illustration of example results for different plane fitting scenarios utilizing a partitioning\nof separated groups. Four different scenarios of partial pairing with R = 6, R = 18, R = 100\nand R = L = H unpaired subgroups. For each case: Left plot: representing the unpaired data\nsubgroups by color-coding. Right plot: presenting the fitted plane (red) in the full data (gray).\n23\nFigure 9: Illustration of example results for different plane fitting scenarios utilizing a partitioning\nof overlapping groups (i.e., randomly switching the group label for approximately 30% of the data\npoints compared to Figure 8). Three different scenarios of partial pairing with R = 6, R = 18 and\nR = 100 unpaired subgroups are demonstrated in the same type of presentation as in Figure 8.\nD\nApplication of R2\n\u03b4\nIn [Cheng et al., 2014] a consistent goodness of fit measure for errors-in-variables multiple linear\nregression is presented if the standard deviations of the input variables are known. We show its\ndirect application in this appendix. With X the L \u00d7 k-Matrix of L observations and k predictor\nvariables, y the L \u00d7 1 vector of output observations, \u03a3\u03b4 the k \u00d7 k covariance matrix of predictor\nvariables, S =\n1\nL XT P X and P = IL\u00d7L \u22121\nL1L\u00d7L (1 being the matrix consisting of 1s) the\ngoodness of fit is defined by\nR2\n\u03b4 = min\n \nbT S b\n1\nL yT P y + bT \u03a3\u03b4 b\n, 1\n!\n,\nwhere b is the vector of fitted slopes, i.e., in the notation of the multiple linear regression model\nof this paper b = (\u03b12, \u03b13, . . . , \u03b1k)T being independent of the intercept \u03b11.\n24\nReferences\nZhengyou Zhang. Parameter estimation techniques: a tutorial with application to conic fitting.\nImage and Vision Computing, 15(1):59\u201376, January 1997. doi: 10.1016/S0262-8856(96)01112-2.\nChristopher M. Bishop. Pattern recognition and machine learning. Information science and statis-\ntics. Springer, New York, 2006. ISBN 978-0-387-31073-2.\nQi Wang, Yue Ma, Kun Zhao, and Yingjie Tian. A Comprehensive Survey of Loss Functions\nin Machine Learning.\nAnnals of Data Science, 9(2):187\u2013212, April 2022a.\ndoi:\n10.1007/\ns40745-020-00253-5.\nWolfgang Hoegele, Rainer Loeschel, Barbara Dobler, Oliver Koelbl, and Piotr Zygmanski. Bayesian\nEstimation Applied to Stochastic Localization with Constraints due to Interfaces and Bound-\naries. Mathematical Problems in Engineering, 2013:1\u201317, 2013. doi: 10.1155/2013/960421.\nZhidong Bai and Tailen Hsing. The broken sample problem. Probability Theory and Related Fields,\n131(4):528\u2013552, April 2005. doi: 10.1007/s00440-004-0384-5.\nFeng Liang, Sayan Mukherjee, and Mike West. The Use of Unlabeled Data in Predictive Modeling.\nStatistical Science, 22(2), May 2007. doi: 10.1214/088342307000000032.\nYudong Wang, Yanlin Tang, and Zhi-Sheng Ye. Paired or Partially Paired Two-sample Tests With\nUnordered Samples. Journal of the Royal Statistical Society Series B: Statistical Methodology,\n84(4):1503\u20131525, September 2022b. doi: 10.1111/rssb.12541.\nGeorgios Kostopoulos, Stamatis Karlos, Sotiris Kotsiantis, and Omiros Ragos. Semi-supervised\nregression: A recent review. Journal of Intelligent & Fuzzy Systems, 35(2):1483\u20131500, August\n2018. doi: 10.3233/JIFS-169689.\nGuo-Jun Qi and Jiebo Luo. Small Data Challenges in Big Data Era: A Survey of Recent Progress\non Unsupervised and Semi-Supervised Methods. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 44(4):2168\u20132187, April 2022. doi: 10.1109/TPAMI.2020.3031898.\nDerrick A. Bennett. How can I deal with missing data in my study? Australian and New Zealand\nJournal of Public Health, 25(5):464\u2013469, October 2001. doi: 10.1111/j.1467-842X.2001.tb00294.\nx.\nJ. A C Sterne, I. R White, J. B Carlin, M. Spratt, P. Royston, M. G Kenward, A. M Wood, and\nJ. R Carpenter. Multiple imputation for missing data in epidemiological and clinical research:\npotential and pitfalls. BMJ, 338(jun29 1):b2393\u2013b2393, September 2009. doi: 10.1136/bmj.\nb2393.\nSemhar Michael, Tatjana Miljkovic, and Volodymyr Melnykov. Mixture modeling of data with\nmultiple partial right-censoring levels.\nAdvances in Data Analysis and Classification, 14(2):\n355\u2013378, June 2020. doi: 10.1007/s11634-020-00391-x.\nZachary R. McCaw, Hugues Aschard, and Hanna Julienne.\nFitting Gaussian mixture mod-\nels on incomplete data.\nBMC Bioinformatics, 23(1):208, December 2022.\ndoi:\n10.1186/\ns12859-022-04740-9.\nWolfgang Hoegele. A Stochastic-Geometrical Framework for Object Pose Estimation Based on\nMixture Models Avoiding the Correspondence Problem. Journal of Mathematical Imaging and\nVision, June 2024a. doi: 10.1007/s10851-024-01200-2.\nWolfgang Hoegele. Combinatorial potential of random equations with mixture models: Modeling\nand simulation, March 2024b. arXiv:2403.20152 [cs, math, stat].\nW. Edwards Deming. Statistical adjustment of data. Dover publ, New York, unabridged and corr.\nrepublication edition, 1964. ISBN 978-0-486-64685-5.\nIvan Markovsky and Sabine Van Huffel. Overview of total least-squares methods. Signal Processing,\n87(10):2283\u20132302, October 2007. doi: 10.1016/j.sigpro.2007.04.004.\n25\nEufr\u00b4asio De A. Lima Neto and Francisco De A.T. De Carvalho. Centre and Range method for\nfitting a linear regression model to symbolic interval data. Computational Statistics & Data\nAnalysis, 52(3):1500\u20131515, January 2008. doi: 10.1016/j.csda.2007.04.014.\nLeandro C. Souza, Renata M.C.R. Souza, Get\u00b4ulio J.A. Amaral, and Telmo M. Silva Filho. A\nparametrized approach for linear regression of interval data. Knowledge-Based Systems, 131:\n149\u2013159, September 2017. doi: 10.1016/j.knosys.2017.06.012.\nC.-L. Cheng, Shalabh, and G. Garg. Coefficient of determination for multiple measurement error\nmodels. Journal of Multivariate Analysis, 126:137\u2013152, April 2014. doi: 10.1016/j.jmva.2014.\n01.006.\n26\n"}