{"text": "A Simple Correction Procedure for\nHigh-Dimensional Generalized Linear Models with\nMeasurement Error\nMichael Byrd and Monnie McGee\nJanuary 6, 2020\nAbstract\nWe consider high-dimensional generalized linear models when the covariates\nare contaminated by measurement error. Estimates from errors-in-variables\nregression models are well-known to be biased in traditional low-dimensional\nsettings if the error is unincorporated. Such models have recently become of\ninterest when regularizing penalties are added to the estimation procedure.\nUnfortunately, correcting for the mismeasurements can add undue computa-\ntional di\ufb03culties onto the optimization, which a new tool set for practitioners\nto successfully use the models. We investigate a general procedure that uti-\nlizes the recently proposed Imputation-Regularized Optimization algorithm\nfor high-dimensional errors-in-variables models, which we implement for con-\ntinuous, binary, and count response type. Crucially, our method allows for\no\ufb00-the-shelf linear regression methods to be employed in the presence of con-\ntaminated covariates. We apply our correction to gene microarray data, and\n1\narXiv:1912.11740v2  [stat.CO]  2 Jan 2020\nillustrate that it results in a great reduction in the number of false positives\nwhilst still retaining most true positives.\n1\nIntroduction\nComplex, high-dimensional data sets have become the norm for many \ufb01elds where\nit is often of interest to uncover underlying structures and to estimate the e\ufb00ect size\nof a given relationship between the observed variables. For instance, in a microarray\nexperiment it may be of value to identify which genes are related to some quantitative\noutcome or if a particular gene in\ufb02uences presence of a disease. Statistical regular-\nization procedures have been essential to addressing these fundamental problems. In\nparticular, when the number of variables p is larger than the sample size n, traditional\nmethods, such as least squares regression, can no longer be used due to identi\ufb01ability\nissues. Hence, regularization procedures, like the Lasso [Tibshirani, 1996] and the\nMinimax Concave Penalty (MCP) [Zhang et al., 2010], have become necessary tools\nfor practitioners to identify patterns in their studies for a wide variety of problems\n[Hastie et al., 2015].\nFor i = 1, . . . , n, consider the generalized linear model (GLM) for independent\nand identically distributed pairs of responses and covariates (yi, xi), such that\nE(yi) = f(xT\ni \u03b2)\n(1)\nfor covariates, \u03b2 \u2208Rp, and inverse-link function, f [McCullagh, 2019]. The regular-\nized GLM aims to minimize the objective\nQ(\u03b2; X, y, \u03bb) = L(y; x, \u03b2) + P(\u03b2; \u03bb)\n(2)\n2\nwith respect to \u03b2, where L(y; x, \u03b2) is the negative log-likelihood function and P(\u03b2; \u03bb)\nis a penalty function on the coe\ufb03cients. The regularization parameter \u03bb determines\nthe overall level of sparsity, and is typically tuned with cross-validation [Friedman et al., 2001].\nThis formulation captures most types of data, including continuous, categorical, and\ncount. Adaptations of the GLM have been well studied for many common penalties\n[Van de Geer et al., 2008], and the objective in Equation (2) has many implemented\nprocedures for a wide variety of problems [Wu et al., 2008] [Breheny and Huang, 2011].\nIn addition to regularized procedures\u2019 well documented empirical performance,\nfavorable theoretical properties, such as selection consistency, have been well stud-\nied [Hastie et al., 2015]. These properties, however, make the assumption that the\nobserved covariates are perfectly measured, which, in many contexts, is not a real-\nistic assumption. For instance, in microarray experiments there are many possible\nsources for random error to be incorporated naturally into the data collection process\n[Rocke and Durbin, 2001]. While the A\ufb00ymetrix microarray itself is manufactured\nunder controlled conditions according to precise speci\ufb01cations, the genetic material\nprepared as the microarray sample is subject to propagation of error. RNA prepa-\nration, for example, takes at least three days, and requires up 15 steps per day. At\nany one of these steps, error or contaminants leading to error could be introduced\n[Perez, 2006].\nWe consider high-dimensional variable selection and estimation for GLMs in the\ncontext of measurement error. In particular, we address the additive measurement\nerror setting, which is known to cause a decrease in selection and estimation quality if\nnot corrected [Loh and Wainwright, 2011] [S\u00f8rensen et al., 2015]. Error-in-variables\n(EIV) regression has been a known issue in statistics, and has been well studied in\na plethera of contexts [Carroll et al., 2006]. The e\ufb00ect of mismeasured covariates in\nEIV regression are biased estimates of the regression coe\ufb03cients and a higher Type I\n3\nerror rate. An analysis that incorporates and corrects for measurement error will aim\nto result in consistent estimates with fewer false positives. This correction, however,\nwill come with decreases in power and model e\ufb03ciency.\nTo overcome the limitation of not directly observing the variables of interest, but\nrather contaminated variations, we make use of the Imputation-Regularized Opti-\nmization (IRO) algorithm [Liang et al., 2018]. The IRO-algorithm was proposed as\na technique for missing data in the high-dimensional setting, which notably gives\na \ufb02exible framework with consistency guarantees for latent variables. Recently, the\nIRO-algorithm was used in the context of estimating Gaussian graphical models\nwith mismeasured observations [Byrd et al., 2019]. The procedure was shown to be\nasymptomatically consistent; in addition it greatly reduced the number of false pos-\nitives found in the selection process and reduced the overall estimation error. Our\ncontribution to the measurement error problem is an extension to the IRO-algorithm\nfor common types of generalized linear models in the presence of measurement error.\nFurther, the implementation is based on a simple framework for high-dimensional\nmeasurement error problems, and we implement the procedure using well established\ntools; thus making the procedure easy to use for practicioners.\nOur goal is to provide a simple framework for high-dimensional measurement\nerror problems that can be implemented using well established tools and procedures.\n1.1\nLiterature Review\nHigh-dimensional EIV regression procedures have accumulated much attention due to\nthe fact that the contaminated observations result in inconsistent estimates and poor\nvariable selection [S\u00f8rensen et al., 2015] [Nghiem and Potgieter, 2018] [Belloni et al., 2016].\nThese procedures typically correct for the contamination by incorporating the as-\n4\nsumed known or estimable measurement error variability into the optimization [Loh and Wainwright, 20\n[S\u00f8rensen et al., 2015] [Datta et al., 2017], or by some pivotal estimation without a\nwell de\ufb01ned likelihood [Belloni et al., 2016] [S\u00f8rensen et al., 2018]. Notably, these\nprocedures tend to make traditionally convex penalties into non-convex formula-\ntions, requiring special care in development of optimization routines to solve them.\nMoreover, even when these issues have been addressed, model tuning is known to\nbe more di\ufb03cult as standard cross-validation is not easily applied for contaminated\nobservations [Datta et al., 2017] [Datta and Zou, 2019].\nWhile many of these procedures o\ufb00er nice theoretical properties for symmet-\nric, continuous responses, few have explored a more general framework for di\ufb00erent\ntypes of response data. We focus on two well established methods for correcting\nfor measurement error: (1) the Corrected Lasso (CLasso) and (2) The General-\nized Matrix Uncertainty Selector (GMUS). Both CLasso and GMUS were origi-\nnally established in the Gaussian error case, see [Loh and Wainwright, 2011] and\n[Rosenbaum et al., 2010], but have been established in the GLM framework by [S\u00f8rensen et al., 2015]\nand [S\u00f8rensen et al., 2018], respectively. The CLasso attempts to account for the\nbias introduced with the measurement error by incorporating it into the optimization\nproblem with two hyperparameters controlling the size of coe\ufb03cients [S\u00f8rensen et al., 2015].\nGMUS takes a slightly di\ufb00erent approach, limiting the amount of correlation between\nthe response and covariates by bounding the score function with a Taylor series ex-\npansion of the residual [S\u00f8rensen et al., 2018].\nIn practice, both methods can be hard to tune due to not having a well de\ufb01ned\nlikelihood. In the GLM case, CLasso and GMUS both use an elbow-plot, as explained\nin [S\u00f8rensen et al., 2018], to determine the amount of regularization, which requires\nuser input and produces unclear results. While GMUS is a convex optimization, the\nCLasso is not. Originally, [Loh and Wainwright, 2011] show favorable convergence\n5\nproperties in the Gaussian residual case, we \ufb01nd that the GLM solution from CLasso\nin a popular implementation does not always share these nice properties. Finally, we\nnote that CLasso and the proposed method require some knowledge of the measure-\nment error variability, whereas GMUS does not. However, in many applications, like\ngene expressions, replicates are taken with common practice, which allows for estima-\ntion of the variability of the contamination. Hence, lacking the ability to incorporate\nthe measurement error variability could be a disadvantage for various settings.\n1.2\nOverview\nThe outline for the remainder of this work is as follows. In Section 2, we establish the\nadditive measurement error formulation and the IRO-algorithm. We show how the\nIRO-algorithm can be used in solutions pertaining to the context of contaminated\nlinear models and we give practical considerations for its usage. Section 3 establishes\nrequired imputation procedures for continuous, categorical, and count data. This is\ndone by assuming the response has parametric form of Gaussian, binomial, and\nnegative binomial distributions, respectively. A simulation study is then presented\nin Section 4, illustrating our method\u2019s performance in Gaussian and binomial linear\nregression. Finally, a data analysis is presented in Section 5, illustrating the proposed\nmethod with two other correction procedures on an experiment using microarray\ngene expressions to \ufb01nd underlying causes of a tumor relapsing. All derivations and\nfurther results can be found in the Appendix.\n6\n2\nThe IRO-Algorithm for EIV Regression\nConsider the following additive measurement error formulation that will persist for\nthe remainder of the paper.\nLet X = (x1, . . . , xn)T \u2208Rn\u00d7p be n independent\nand identical realizations of a p-dimensional random variable, where, for covariance\nmatrix \u03a3x, xi \u223cN(0p, \u03a3x). Instead of directly observing realization xi, we observe\nri \u22652 contaminated replicates. Assume the contaminated observation to be related\nadditively to the true realization, where\nwij = xi + uij\n(3)\nsuch that uij \u223cN(0p, \u03a3u) for i = 1, . . . , n and j = 1, . . . , ri; note that, by indepen-\ndence of xi and uij, that wij \u223cN(0p, \u03a3x+\u03a3u). Denote the collection of observation\ni\u2019s replicates as Wi = (w1, . . . , wri)T, and let \u00afwi to be the average of the replicates\nfor observation i. Without loss of generality, we assume the measurement error is\ncentered at 0.\n2.1\nThe Imputation Regularization Optimization Algorithm\nThe Imputation-Regularized Optimization (IRO) algorithm was recently introduced\nin the context of high-dimensional variable selection with missing data [Liang et al., 2018].\nThe IRO-algorithm provides a much needed procedure for imputation in case where\nn < p, as common methods, like the well known EM-algorithm [Dempster et al., 1977],\ncan fail due to inconsistent or non-unique likelihoods [Yi and Caramanis, 2015]. The\nIRO-algorithm consists of two iterative steps. At iteration t = 1, . . . , T, missing val-\nues, zm, are imputed through a predictive density that is conditioned on the observed\ndata, zo, and the estimated model parameters from the previous iteration, \u2206(t\u22121),\n7\nnamely\nz(t)\nm \u223c\u03c0\n\u0000zm|zo, \u2206(t\u22121)\u0001\n.\n(4)\nThe newly generated values z(t)\nm are then used with observed values zo to estimate\nthe model parameters with a regularized objective function\n\u2206(t) = argmin\n\u2206\n\b\nF(\u2206; z(t)\nm , zo) + P(\u2206; \u03bb)\n\t\n,\n(5)\nwhere F denotes the parameter\u2019s relationship to the data and P denotes the reg-\nularization function with sparsity parameter \u03bb. The two steps are iterated, and,\nwhen the optimization in (5) is asymptotically consistent, the IRO-algorithm forms\na valid Markov chain that provides an asymptotically consistent estimate of the\nhigh-dimensional variables under mild conditions [Liang et al., 2018].\n2.2\nThe I-Step for High-Dimensional EIV Regression\nMeasurement error is similar in nature to missing data in that the missing values\nare related by some underlying density like the contaminated variable. Regardless\nof procedure, the conditional density for xi|Wi, \u2126u, \u2126x must be estimated for all\ni = 1, . . . , n. Recently, the IRO-algorithm was used in a measurement error correction\nprocedure for Gaussian graphical models, which estimates the precision matrix \u2126x\nwith an assumed known or estimable \u2126u [Byrd et al., 2019]. Going forward we will\nrefer to a procedure using the IRO-algorithm to correct for measurement error as an\nIRO-adjusted procedure. Referring back to the aforementioned contaminated model\nin (3), the predictive density used to compute the imputation is the full conditional\n8\nfound in Normal-Normal models in Bayesian inference,\n\u03c0(xi|Wi, \u2126u, \u2126x) \u223cN(ri\u039b\u2126u \u00afwi, \u039b),\n(6)\nwhere \u039b = (\u2126x + \u2126u)\u22121, as shown in the Appendix A.1.1.\nWe consider the high-dimensional EIV regression problem for GLMs with re-\nsponse y \u223cD and nuissance parameters \u0398. Here, we assume a relationship exists\nbetween the expectation of the response, yi, and a function of the linear combination\nof covariates, xi. Namely, we formulate the model\nED(yi; \u0398) = f(xT\ni \u03b2)\n(7)\nfor the inverse-link function f and sparse coe\ufb03cients \u03b2 \u2208Rp. The sparsity of the\ncoe\ufb03cients implies that most are 0. Denote the number of non-zero coe\ufb03cients by\nq = ||\u03b2||0, where q \u2264n and, typically, q \u226ap. Instead of observing pair (yi, xi), the\ncovariate is observed with (replicated) contamination (yi, Wi). To implement the\nIRO-algorithm for the EIV regression problem the imputation step in (6) must be\nadjusted to include the response model. The imputation distribution is altered, up\nto a normalizing constant, as\n\u03c0(xi|yi, Wi, \u2126x, \u2126u, \u03b2, \u0398) \u221d\u03c0(yi|xi, \u03b2, \u0398)\u03c0(xi|Wi, \u2126x, \u2126u),\n(8)\nwhere the distribution of xi|Wi, \u2126u, \u2126x is as in (6).\nFor well speci\ufb01ed densities\nfor each function, the distribution for the imputation step in (8) is known and easily\nsampled, which will be explored in Section 3. Once X has been imputed, an estimate\nof \u2126x and \u03b2 is obtained, and the process repeated. The general procedure is presented\nin Algorithm 1.\n9\nAlgorithm 1 The IRO-adjusted Procedure for Contaminated GLMs\n1: Set number of IRO iterations, T\n2: Input known \u2126u or obtain \u02c6\u2126u using replicate data\n3: Obtain initial estimate for \u03b2(0) and \u2126(0)\nx\nusing \u00af\nW\n4: for t = 1, . . . , T do\n5:\nfor i = 1 , . . . , n do\n6:\ndraw x(t)\ni\n\u223c\u03c0(xi|yi, Wi, \u2126(t\u22121)\nx\n, \u02c6\u2126u, \u03b2(t\u22121), \u0398(t\u22121))\n\u25b7Impute\n7:\nEstimate \u2126(t)\nx with X(t)\n8:\nEstimate \u03b2(t) and \u0398(t)\n\u25b7Regularize\n2.3\nThe RO-Step for High-Dimensional EIV Regression\nOnce the imputation step has been performed, then the remaining parameters must\nbe estimated from the imputed realizations. Beginning with \u2126x, the precision matrix\nof the true underlying data, it is tempting to estimate the covariance directly and\nthen invert the estimated covariance.\nHowever in the setting where n < p, the\nestimated covariance is likely to not be of full-rank, and hence inversion would not\nbe possible due to \u02c6\u03a3x being singular. Additionally, even if one could reasonably\nestimate \u03a3x, inversion is computationally expensive. The Gaussian graphical model\nliterature has given several ways to estimate \u2126x directly with a regularization term to\nimpose sparsity, which could then estimate a full rank matrix [Friedman et al., 2008].\nWhile estimating the o\ufb00-diagonal elements of \u2126x is appealing, many regulariza-\ntion procedures assume independence among covariates. Even if the assumption is\nnot strictly made, few regularization procedures make use of the dependence struc-\nture among the covariates; though some exceptions do exist [Yu and Liu, 2016]. Dis-\nregarding the dependency between covariates allows for estimation of only the diag-\nonal of \u2126x. This results in computational gains in the imputation step, as explained\nin Section 3, and saves a costly optimization of \u2126x. We observe in our simulations\nwith dependent covariates that estimating only the diagonal of \u2126x performs well.\n10\nMany procedures have been developed to estimate coe\ufb03cients in regularized gen-\neral linear models [Park and Hastie, 2007]. Any method which is consistent will be\nadequate for the regularization step in estimating the coe\ufb03cients at each iteration.\nThe more accurate the regularization method, the better the imputation. Of par-\nticular note is the ability to estimate the nuisance parameter \u0398, which is required\nfor the imputation step. This is a known problem in, for instance, Gaussian lin-\near regression, where the underlying model variability a\ufb00ects the selection quality\n[Belloni et al., 2011]. The general IRO-adjusted procedure for mismeasured random\nvariables is to alternate between imputation, as in equation (8), and optimizing pa-\nrameters \u2126x, \u03b2, and \u0398.\n2.4\nComputational Considerations\nMost of the regularized optimization procedures for GLMs require some hyperpa-\nrameter tuning. For example, consider the Lasso penalty\u2019s Lagrangian form, then\nthe optimization in (2) will be such that P(\u03b2; \u03bb) = \u03bb||\u03b2||1. The hyperparameter \u03bb\ndirectly a\ufb00ects the output by controlling the amount of sparsity, and hence needs\nto be tuned. We handle this hyperparameter tuning at each iteration of the IRO-\nalgorithm via conventional procedures like k-fold cross-validation, which for many\ncompeting methods is unavailable. Using the standard tool set makes analysis for\na practitioner easier as measurement error invalidates traditional methods, and ad-\nditional methods like [Datta and Zou, 2019] are incorporated into the procedure.\nMoreover, competing methods are often in a position of tuning a grid of hyperpa-\nrameters [Belloni et al., 2016]. This adds to their computational costs to \ufb01nd an\noptimal solution, which may not be plausible depending on the di\ufb03cultly of the\noptimization. Moreover, this adds to the di\ufb03cultly of use for practitioners, and a\n11\nhigher chance of misapplication or misinterpretation.\nWe brie\ufb02y note the similarity of the IRO-algorithm and Gibbs samplers from\nBayesian literature [Smith and Roberts, 1993]. Gibbs samplers require obtaining the\ndistribution of each random variable conditioned on the all other random variables in\nthe model, known as the full conditional distribution. These distributions are then\nused to generate values of that random variable, conditioned on the most recently\ngenerated value of the other random variables. This is similar to the IRO-algorithm,\nwhich replaces sampling of some variables with an optimization step. As such, the\nmassive amount of literature that has been developed for Gibbs sampling is applicable\nto procedures using the IRO-algorithm. This was illustrated in [Liang et al., 2018],\nwhere the well-known Gelman-Rubin diagnostics [Gelman et al., 1992] were used to\nillustrate convergence of the IRO-algorithm. We note that both samplers can take\nsome time to reach reasonable areas of the posterior distribution. While a good\nstarting value helps, it is still often bene\ufb01cial to discard some initial amount of\niterates as burn-in. We make use of this practice in our implementation of IRO-\nalgorithm.\nThe IRO-algorithm estimates a set of coe\ufb03cients at each iteration, and the dif-\nferences in the estimated coe\ufb03cients may be interpreted as the amount of varia-\ntion added into the estimation process as a result of the contaminated observations\n[Liang et al., 2018]. With mild conditions on the regularization procedure and vari-\nability of the data, the \ufb01ndings of [Liang et al., 2018] show that the IRO-algorithm\ngives a consistent estimate of the optimized parameters in each iteration. Moreover,\nthe results of [Byrd et al., 2019] extend this result to the measurement error scenario.\nA typical \ufb01nal estimate would make use of all iterations, such as taking the average\nestimated parameter from each iteration. However, the average of multiple sparse\nvectors is not guaranteed to be sparse, which does not give an easy interpretation\n12\nof the variable selection. Intuitively, spurious coe\ufb03cients that appear in the model\nought to do so a few number of times; therefore a trimmed mean could be used.\nAlternatively, we \ufb01nd using the median of each estimated coe\ufb03cient as the \ufb01nal\nestimate to give reliable estimates, as illustrated in Section 4.\n3\nIRO-Adjustments for Some Contaminated GLMs\nIn this section we explore imputation steps for three common types of response\ndata: continuous, categorical, and count. This is done by determining the necessary\nform of Equation (8) for responses distributed as a Gaussian, binomial, and nega-\ntive binomial distribution. These distributions are standard for GLMs, and cover\nmost use cases. We illustrate that the imputation can be accomplished from known,\nparameterized distributions, which makes the sampling painless. Additionally, we\naddress computational considerations of the imputation step. While we focus here\non closed form distributions to be used in the imputation step, there may not always\nbe well-known distributional forms available for every class of model. Many proce-\ndures exist for approximating distributions, such as the Integrated Nested Laplace\nApproximation [Rue et al., 2009]. Samples drawn from the output of these methods\ncould be used to estimate unknown distributional forms given by other models. All\nderivations are deferred to the Appendix A.1.\n3.1\nGaussian Linear Regression\nThe natural starting point is continuous data with Gaussian linear regression. Here,\nwe assume the response follows the familiar model, yi \u223cN(xT\ni \u03b2, \u03c32) for all i =\n13\n1, . . . , n. The imputation step can be shown to be\nxi|yi, \u03b2, Wi, \u2126u, \u2126x \u223cN\n\u0010\n\u039bG\n\u0010\nri\u2126u \u00afwi + yi\n\u03c32\u03b2\n\u0011\n, \u039bG\n\u0011\n,\n(9)\nwhere \u039bG = (\u039b\u22121 + \u03c3\u22122\u03b2\u03b2T)\u22121 for \u039b as de\ufb01ned in (6). We note the impact of\nquality estimates for \u03b2 and \u03c32, which will be used iteratively for the imputations.\nMany regularization procedures do not incorporate the residual variability into the\nestimation.\nIf one is con\ufb01dent in the quality of the estimates directly from the\nregularized model, then the residual variance could be estimated as\n\u02c6\u03c32 = ||y \u2212X(t)\u03b2(t)||2\n2\nn \u2212\u02c6q\n,\n(10)\nwhere \u02c6q is the number of estimated, non-zero coe\ufb03cients. However, many proce-\ndures\u2019 performance is known to become worse when the model error variance is not\n1 [Belloni et al., 2011]. Hence, if using such a method, like Lasso, it is often bene\ufb01-\ncial to instead use variants that incorporate the error variance, like the scaled Lasso\n[Sun and Zhang, 2012], into the model during the optimization procedure.\nWe remark on the computation of the imputation step, which requires inverting\nthe sum of a full rank and rank-1 matrix. If the features are modeled as independent,\nimplying \u2126x and \u2126u are diagonal, then some computational gains can be found by\nnoting that \u03b2\u03b2T is a rank-1 matrix. A typical procedure for generating p-dimensional\nGaussian data is to generate p independent standard Normal variables, and then to\nmultiply this vector by the Cholesky decomposition of the covariance matrix. The\nCholesky decomposition of a diagonal matrix is simply the square root of the diagonal\nelements, which can then be updated by \u03b2\u03b2T in O(p2) time instead of O(p3) time\nif done directly. There is not an easy way to address the problem of generating the\n14\nimputation step when \u2126x or \u2126u is not diagonal without making assumptions on\nits form. However, this is a well known problem in Bayesian literature, and recent\nadvances, such as [Bhattacharya et al., 2016], may prove applicable to our situation\nin the future.\n3.2\nBinomial Linear Regression\nWe now consider the binomial linear regression setting where covariates are con-\ntaminated with measurement error. For each observation i, let yi \u223cBern(pi) such\nthat\npi =\nexT\ni \u03b2\n1 + exT\ni \u03b2 .\n(11)\nIncorporating binomial regression into the IRO-algorithm is not as immediate as in\nGaussian linear regression due to the logit function, which maps the linear combina-\ntion of the covariates to the success probability. The Gaussian setting is conjugate,\nand hence easily found as in Bayesian inference. Binomial linear regression is known\nto not have a closed form full conditional distribution due to the logit function, and\nhas been an long-time area of interest in Bayesian literature [Holmes et al., 2006].\nDue to the overlap in the IRO-algorithm and Gibbs sampling methodologies, we are\nable to utilize some of these \ufb01ndings to incorporate into the imputation step.\nSpeci\ufb01cally, we will make use of a recent advancement in a line of research us-\ning data-augmentation to achieve a well-known distribution for the imputation step.\nUsing a newly proposed P\u00b4olya-Gamma family of distributions, [Polson et al., 2013]\nhave been successful in implementing a procedure that allows for a closed-form bi-\nnomial regression Gibbs sampler. A random variable z is P\u00b4olya-Gamma distributed\n15\nwith parameters b \u2208R+ and c \u2208R if\nz =\n1\n2\u03c02\n\u221e\nX\nk=1\ngk\n(k \u22121/2)2 + c2/(4\u03c02)),\n(12)\nwhere gk \u223cGa(b, 1) are iid Gamma random variables; we denote the the P\u00b4olya-\nGamma distribution as z \u223cPG(b, c). The main result in [Polson et al., 2013] is\nthat\n(e\u03c8)a\n(1 + e\u03c8)b = 2\u2212be\u03ba\u03c8\nZ \u221e\n0\ne\u2212z\u03c82/2\u03c0(z)dz,\n(13)\nwhere \u03ba = a\u2212b/2 and z \u223cPG(b, 0). Note that when \u03c8 = xT\ni \u03b2, the integrand of (13)\nis the kernel of a Gaussian distribution with respect to xT\ni \u03b2. Hence, the inverse-logit\nfunction, as in (11), can be expressed as an in\ufb01nite convolutional mixture of normal\nand gamma distributions.\nExploiting the mixture representation of the logit function in (13), [Polson et al., 2013]\nshowed that a Gibbs sampler was possible by exploiting the Normal-Normal conju-\ngacy of the prior on the coe\ufb03cients and Gaussian kernel. This procedure is possible\nby including the P\u00b4olya-Gamma random variable into the sampler. Thus, in addition\nto needing to impute X, our imputation step must also sample z = (z1, . . . , zn)T,\nwhich requires the full conditional distribution of zi. Fortunately, sampling zi|xi, \u03b2\nis an easy task. [Polson et al., 2013] showed that\nzi|xi, \u03b2 \u223cPG(1, xT\ni \u03b2),\n(14)\nwhich has been illustrated to have an e\ufb03cient sampling routine [Polson et al., 2013].\nThe full conditional to sample each observation\u2019s true realization is then Gaussian,\nnamely\nxi|yi, zi, \u03b2, Wi, \u2126u, \u2126x \u223cN (\u039bB (\u03bai\u03b2 + ri\u2126u \u00afwi) , \u039bB)\n(15)\n16\nwhere \u03bai = yi \u22121/2 and \u039bB = (\u039b\u22121 + zi\u03b2\u03b2T)\u22121. This computation is facilitated by\nassuming X to be normally distributed, as in the Gaussian linear regression case.\nHence the IRO-algorithm in this context will alternate between imputing X and\nz, then optimizing regression coe\ufb03cients \u03b2 and the covariate\u2019s precision \u2126x. While\nwe have focused on the binomial case, the P\u00b4olya-Gamma augmentation can be ex-\ntended to the multinomial linear regression case [Polson et al., 2013] [Chen et al., 2013]\n[Linderman et al., 2015]. The inclusion of z was shown to create an uniformaly er-\ngodic Gibbs sampler [Choi et al., 2013], and similar logic should apply to the IRO-\nadjusted procedure. Additionally, when \u2126x and \u2126u are assumed to be diagonal, a\nsimilar procedure to the Gaussian linear case can be used to quickly sample from the\nNormal distribution in (15). Unfortunately, this procedure will need to be computed\nn times, for each coe\ufb03cient, as the inverse requires observation speci\ufb01c zi.\n3.3\nNegative Binomial Linear Regression\nFinally, we brie\ufb02y consider response data being observed as counts. In the GLM\nframework, the typical procedures for modeling count data are variants of Poisson\nand negative binomial regression. We opt for the more \ufb02exible of the two meth-\nods, negative binomial regression, which is less susceptible to overdispersion by not\nenforcing the mean and variance to be the same. Remarkably, the P\u00b4olya-Gamma\naugmentation works for any distribution in the binomial family, and hence the nega-\ntive binomial imputations can be implemented in similar nature to Section 3.2. The\nfull conditional for the imputing xi is exactly as in (15). However, the augmented\nvariable z is of slightly di\ufb00erent form. Appealing to the additive nature of the prior\n17\ndistribution, as in (13), for yi observed counts out of mi trials, then\nzi|xi, \u03b2, mi \u223cPG(mi, xT\ni \u03b2)\n(16)\nas shown in [Polson et al., 2013]. While sampling the full conditional density becomes\nmore costly as mi grows, e\ufb03cient routines have been explored to quickly generate\nsamples [Polson et al., ].\n4\nSimulation\nHere, we examine the numerical performance of the our proposed estimator for high-\ndimensional Gaussian and binomial linear regression under di\ufb00erent settings. In each\nsetting, \ufb01ve di\ufb00erent estimates are compared in terms of estimation quality and vari-\nable selection. The \ufb01rst two estimates come from running the same regularization\nprocedure used in the IRO-adjustment, the MCP penalty [Zhang et al., 2010], on (1)\nthe true realizations (Ideal) and (2) the average of the contaminated replicates for\neach realization (Naive). The MCP was also then used in (3) our implementation of\nthe IRO-algorithm for measurement error (IRO). In addition to comparing the per-\nformance to the ideal and naive model, we also inspect two other competing models:\n(4) the Corrected Lasso (CLasso) [Loh and Wainwright, 2011] and (5) the Gener-\nalized Matrix Uncertainty Selector (GMUS) [S\u00f8rensen et al., 2018], as described in\nSection 1.1.\nAll computations were performed in R. To illustrate the ease of incorporating\nestablished methodologies into the IRO-adjustment, we make use of a standard reg-\nularization package. The MCP penalty was implemented with the R package \u2018ncvreg\u2019.\nThis package has been developed using e\ufb03cient coordinate-descent algorithms cre-\n18\nated for non-convex regularization, and is built with care to appropriately handle\npossible numerical issues in the optimization. For model tuning, the MCP procedure\nused the package default 10-fold cross-validation. The Corrected Lasso and GMUS\nprocedures were implemented with the R package \u2018hdme\u2019. For model tuning, the\nCorrected Lasso is able to take advantage of cross-validation, and used 10-fold cross-\nvalidation for tuning. However, the GMUS procedure requires hand-tuning for each\nproblem by inspecting a scree-plot and choosing the point where the number of zero\ncoe\ufb03cients stabilizes. We automate this tuning for the simulation study by choosing\nthe the \ufb01rst tuning parameter such that the following two points in the grid give the\nsame number of non-zero coe\ufb03cients.\nFor each setting, one of two di\ufb00erent sets of coe\ufb03cients are inspected. The two\nsets of coe\ufb03cients are as follows:\n1. \u03b2\u2217\n1 = (1, . . . , 1, \u22121, . . . , \u22121, 0, . . . , 0)T where 1 and -1 are repeated 5 times with\nall p \u221210 remaining coe\ufb03cients set to 0,\n2. \u03b2\u2217\n2 = (1, 1/2, 1/3, . . . , 1/10, 0, . . . , 0)T where, again, p \u221210 coe\ufb03cients are set\nto 0.\nThe measurement error was generated from a 0 mean Gaussian distribution, with\ndiagonal covariance \u03a3u. To control for the signal-to-noise ratio, we use \u03b3 \u2208{0.5, 1} as\ndiag(\u03a3u) = \u03b3diag(\u03a3x). Each observation in every case was generated to have r = 3\nreplicates. Each setting, as described in the following sections, was implemented\nwith n = 400, p = {100, 500, 1000}, and 100 random instances. Additionally, the\nIRO-algorithm ran for T = 100 imputation steps. To inspect the performance of\neach model, we take the average of the \u21132-norm di\ufb00erence (L2) of the estimated and\n19\ntrue coe\ufb03cients from each replicate within each setting,\n\u21132( \u02c6\u03b2) =\n1\n100\n100\nX\ni=1\n|| \u02c6\u03b2i \u2212\u03b2\u2217||2\n2,\nwhich measures the quality of the estimated coe\ufb03cients.\nThe variable selection\nquality is reported by the average number of true positives (TP) and false positives\n(FP).\n4.1\nGaussian Linear Regression\nWe begin by examining Gaussian linear regression. In addition to the MCP penalty,\nwe also inspected the performance using the Scaled Lasso [Sun and Zhang, 2012],\nfor which we defer discussion and results to the Appendix A.3.\nThree di\ufb00erent\ndata generating processes were considered, where data is generated such that X \u223c\nN(0p, \u03a3x) and yi = xT\ni \u03b2 + \u03f5i for \u03f5i \u223cN(0, \u03c32). The three settings inspect di\ufb00erent\nvalues of \u03a3x, \u03b2, and \u03c32, and are given by the following:\n(G1) X \u223cN(0, \u03a3x) such that the covariance is diagonal where \u03a3x = I. We use \u03b2\u2217\n2\nto de\ufb01ne the relationship y = X\u03b2\u2217\n2 + \u03f5, where \u03f5 \u223cN(0, I).\n(G2) X \u223cN(0, \u03a3x) such that the covariance is diagonal where \u03a3x = I. We use \u03b2\u2217\n1\nto de\ufb01ne the relationship y = X\u03b2\u2217\n1 + \u03f5, where \u03f5 \u223cN(0, 3I).\n(G3) X \u223cN(0, \u03a3x) such that \u2126x = \u03a3\u22121\nx\nis generated with a band structure so\nthat the diagonal and super-diagonal elements are non-zero. This is generated\nusing the \u2018huge\u2019 package for the default \u201cband\u201d setting. The \ufb01nal covariance\nhas diag(\u03a3x) = 1p and a decreasing relation for variables that are further away\nfrom each other. The o\ufb00-diagonal elements have a magnitude starting between\n20\nSetting\np\nMetric\nIdeal\nNaive\nIRO\nCLasso\nGMUS\nG1\n500\nL2\n0.319\n0.405\n0.373\n0.61\n0.654\nTP\n7.39\n6.2\n5.68\n6.43\n3.83\nFP\n9.53\n8.47\n3.02\n15\n0.23\n1000\nL2\n0.338\n0.423\n0.391\n0.62\n0.676\nTP\n7.15\n6.32\n5.5\n6.07\n3.61\nFP\n11.62\n11.09\n3.49\n18.14\n0.34\nG2\n500\nL2\n0.363\n0.68\n0.458\n1.227\n1.89\nTP\n10\n10\n10\n10\n9.93\nFP\n3.43\n6.78\n1.02\n19.34\n0.21\n1000\nL2\n0.359\n0.679\n0.426\n1.14\n1.949\nTP\n10\n10\n10\n10\n9.93\nFP\n5.13\n10.54\n0.98\n22.44\n0.2\nG3\n500\nL2\n0.424\n1.138\n0.916\n3.24\n2.793\nTP\n10\n9.9\n9.79\n7.58\n5.37\nFP\n4.64\n14.87\n3.83\n9.87\n0.97\n1000\nL2\n0.445\n1.229\n1.047\n3.239\n2.836\nTP\n10\n9.81\n9.58\n7.27\n4.94\nFP\n8.07\n23.65\n4.5\n12.1\n1.49\nTable 1: Simulation results for Gaussian linear regression under the three speci\ufb01ed settings\nwith noise-to-signal ratio \u03b3 = 0.5. Ideal, Naive, and IRO use the MCP penalty for regular-\nization. Bold numbers illustrate the best method between the correction procedures for the\nsetting metric.\n0.4 and 0.55 depending on p. We use \u03b2\u2217\n1 to de\ufb01ne the relationship y = X\u03b2\u2217\n1+\u03f5,\nwhere \u03f5 \u223cN(0p, I).\nThese settings give potiential situations that arise in practice.\nWe display the results for p = 500 and p = 1000 for settings G1, G2, and G3\nwhen using the MCP penalty for \u03b3 = 0.5 in Table 1; results for p = 100 are similar\nand presented in Appendix A.3.1 To begin, we compare the results of the Ideal and\nNaive model to the results of the our IRO-adjusted model. Focusing on variable\nselection, it is easy to see that the Ideal model outperforms the Naive model in every\nsetting, as expected. When comparing the Naive and IRO-adjusted procedure, the\n21\nbiggest take-away is the di\ufb00erence in the number of false positives. In most every\nsetting the Naive model \ufb01nds about \ufb01ve times as many false positives. In all but\none setting, the Naive model has a precision considerably less than 0.5. The IRO-\nadjusted procedure, however, never falls below 0.6. The corrected procedure does\nhave more trouble identifying true positives, but the number of true positives never\ndecreases by more than 10%. Finally, the quality of the estimated coe\ufb03cients, as\nestimated by the norm di\ufb00erence, is always favorable to the IRO-adjusted procedure.\nNow, comparing the IRO-adjusted model with the Corrected Lasso and GMUS\ngives more varied results. The Corrected Lasso seems to generally have a higher false\npositive rate and lower true positive rate than both IRO and GMUS. Interestingly,\nin setting G2, the Corrected Lasso performs worse than the Naive model, suggesting\na lack of robustness to model assumptions. GMUS does not seem to have much issue\nat all with false positives, having the lowest amount for every setting. However, the\nIRO-adjustment always has more true positives identi\ufb01ed. This can be attributed to\nusing the covariance structure information that GMUS does not take into account.\nThe IRO-adjusted model and Corrected Lasso have comparable true positive iden-\nti\ufb01cation. The IRO-adjusted model appears to have the highest quality coe\ufb03cient\nestimates, as illustrated by the superior norm di\ufb00erence of the estimated coe\ufb03cients\nin every setting.\n4.2\nBinomial Linear Regression\nWe now consider the case of using binomial linear regression to measure the the\nrelationship of contaminated covariates with binary response. To this end we con-\nsider two settings for this scenario, easily described as the covariates being either\nindependent or dependent. These settings are:\n22\n(B1) X \u223cN(0, \u03a3x) such that the covariance is diagonal where \u03a3x = I. We use \u03b2\u2217\n2\nto generate the relationship yi \u223cBinom(f(xT\ni \u03b2)).\n(B2) X \u223cN(0, \u03a3x) such that dependencies exist between features, where \u03a3x is\ngenerated as in setting G3. We use \u03b2\u2217\n1 to generate yi \u223cBinom(f(xT\ni \u03b2)).\nIn both instances f de\ufb01nes the inverse logit function.\nIn Table 2 we display the results of these two settings for p = 100, 500, and 1000\nwhen signal-to-noise is speci\ufb01ed such that \u03b3 = 0.5. Again, we begin by comparing\nthe averaged results of the Ideal and Naive model with the IRO-adjusted procedure\nthat is proposed. Again, it should be of no surprise that the naive implementation\ngenerally performs worse than the ideal. The e\ufb00ect is extreme for this case, but\ntypically the presence of contaminated observations increases the number of false\npositives and decreases the number of true positives. The IRO-adjusted procedure is\nable to achieve nearly the same number of true positives as the naive method, while\nreducing the number of false positives by more than half in every case. Strangely, the\nIRO-adjusted procedure also has fewer false positives than the ideal model for every\ncase. This can be attributed to the removal of spurious e\ufb00ects when examining the\nmodel at each imputation iteration. Finally, the IRO-adjustment is able to either do\nas well or better than the naive model in terms of estimate quality, as measured by\nthe norm di\ufb00erence.\nComparing the results of the IRO-adjustment and alternative correction proce-\ndures we note similar results as to the Gaussian case. Beginning with the CLasso, we\n\ufb01rst note a general poor performance for each of the settings, having a relatively low\nnumber of true positives and poor estimation quality. As the model is less powerful,\nless identi\ufb01cation was done in total, as seen by the low number of false positives,\ntoo. It may be possible to achieve better performance with extensive tuning, but\n23\nthe defaults already search a well speci\ufb01ed grid. A more likely reason for the per-\nformance can be attributed to the non-convex optimization that is performed to\n\ufb01nd the solution. While the Gaussian case has been specially designed for \ufb01nding\nnear-optimal solutions, the GLM case in general is much harder, and would make\nthe elbow method used for tuning a challenge if only some tuning parameters found\ngood solutions.\nOn the other hand, GMUS was able to \ufb01nd reasonable results for each setting. We\nsee, again, that the IRO-adjustment performs better in identifying the true positives\nin the model. This e\ufb00ect is seen best when the covariates are correlated in Setting\nB2. However, GMUS does perform better in regards to the number of false positives\nin the model. The choice of method would be then given to the practitioner, as both\nmethods perform better than the naive method. In terms of estimation quality, the\nIRO-adjustment performs better in every case. This is consistent with the results\nfrom the Gaussian setting, and establishes a general bias from the GMUS procedure.\nWe note that, unlike CLasso and GMUS, the IRO-adjusted setting is easily estab-\nlished for other classi\ufb01cation methods, like Linear Discriminant Analysis, which could\nbe incorporated to improve the variable selection [Witten and Tibshirani, 2011].\n5\nData Analysis\nTo show the e\ufb03cacy of our proposed method, we illustrate it with an application\nto a microarray gene expression data set. For the sake of comparison, the data set\nand the preprocessing steps are the same as in [S\u00f8rensen et al., 2015]. This data\nset is comprised of n = 144 subjects\u2019 gene expressions for favorable histology Wilms\ntumors, of which 53 relapsed and 91 did not; the data set is accessible by the GEO\nwebsite, dataset GSE10320 [Huang et al., 2009]. Each subject was measured with\n24\nSetting\np\nMetric\nIdeal\nNaive\nIRO\nCLasso\nGMUS\nB1\n100\nL2\n0.754\n1.204\n0.971\n3.939\n2.576\nTP\n10\n10\n9.98\n2.71\n9.53\nFP\n4.16\n5.9\n2.45\n0.1\n0.51\n500\nL2\n0.898\n1.537\n1.207\n4.012\n2.739\nTP\n10\n9.97\n9.93\n2.39\n9.18\nFP\n8.75\n11.35\n4.31\n0.08\n0.55\n1000\nL2\n1.035\n1.682\n1.413\n4.088\n2.796\nTP\n9.99\n9.93\n9.86\n2.2\n8.66\nFP\n12.27\n15.59\n6.16\n0.08\n0.9\nB2\n100\nL2\n0.728\n1.528\n1.268\n3.853\n2.89\nTP\n9.97\n9.57\n9.45\n2.13\n5.86\nFP\n5\n8.31\n3.55\n0.14\n1.32\n500\nL2\n0.934\n2.132\n2.185\n3.716\n2.982\nTP\n9.81\n7.85\n7.33\n1.68\n4.62\nFP\n13.65\n15.15\n6.16\n0.11\n2.52\n1000\nL2\n1.206\n2.455\n2.523\n3.446\n3.008\nTP\n9.41\n6.77\n5.92\n1.52\n4.28\nFP\n18.82\n16.96\n6.71\n0.06\n2.99\nTable 2: Simulation results for Binomial linear regression under the two speci\ufb01ed settings\nwith signal-to-noise ratio \u03b3 = 0.5, as described in-line. The Ideal, Naive, and IRO proce-\ndures use the MCP penalty for regularization.\n10 or 11 probes, and hence replicates are available to estimate the measurement\nerror variability. The Bioconductor package \u2018bgx\u2019 is able to incorporate the subject-\nlevel replicates in the preprocessing step to obtain the estimated measurement error\ncovariance [Hein et al., 2005], which is assumed diagonal. To cut down the number\nof genes to be inspected, any gene that had estimated signal-to-noise value \u03b3 > 0.5\nwas discarded, the rational being that with too much noise, no discernible selection\nwould be possible, regardless of correction.\nWe make use of the already processed output for the same dataset found in\n[Nghiem and Potgieter, 2018] and [Byrd et al., 2019]. After removing genes with es-\n25\ntimated signal-to-noise ratio larger than 0.5, there were a remaining p = 2074 genes\nremaining. The goal is to determine any genes that have an impact on the tumor\nrelapsing.\nWe accomplish this with a binomial linear regression.\nSimilar to the\nanalysis done in Section 4, we compare the results of the Naive estimate, the IRO-\nadjusted estimate, the CLasso estimate, and the GMUS estimate. For the purposes\nof illustration, we use the Lasso procedure with 10-fold cross-validation for the Naive\nand IRO-adjustment. As the CLasso and GMUS procedures both lack a well-de\ufb01ned\nlikelihood, we utilize the elbow-plot method as in the Binomial Regression simulation\nin Section 4.2.\nWe present the results of the analysis in Table 3, which shows the total number of\ngenes selected for each procedure and the number of overlap between each procedure.\nBeginning with the results of the Naive analysis, the Lasso procedure selected a total\nof 35 genes in total. The IRO-adjusted Lasso procedure selected less than half that\nof the Naive implementation, \ufb01nding a total of 14 genes when taking the median\nof each iteration\u2019s estimated coe\ufb03cients. Additionally, all 14 variables found by the\nIRO-adjusted Lasso were also found by the Naive procedure. This is in line with\nthe results found in the simulation conducted in the previous section, where the\nNaive and IRO implementations typically had similar amounts of true positives and\na disparate amount of false positives.\nTurning to the competing methods, the CLasso selected a total of 3 genes, all\nof which are shared with the Naive and IRO-adjustment. Given the relatively low\nnumber of true and false positives in the simulation, this seems to indicate similar\nbehavior. Finally, the GMUS procedure selected a total of 7 genes. The behavior of\nthe of GMUS was odd in the sense that there was only one gene in common with the\nCLasso and two genes in common with the Naive and IRO-adjustment. We believe\nthat this is likely attributed to the dependencies between the genes, which had a\n26\nrelatively large negative impact on GMUS. The overall outcome seems to suggest\nthe legitimacy of the simulation study, which illustrated the the IRO-adjustment to\nbe a middle ground between true and false positives.\nNaive\nIRO\nCLasso\nGMUS\nNaive\n35\n14\n3\n2\nIRO\n14\n14\n3\n2\nCLasso\n3\n3\n3\n1\nGMUS\n2\n2\n1\n7\nTable 3: The total number of selected genes that overlapped between the Naive, IRO-\nadjusted, CLasso, and GMUS procedures. Note, the diagonal shows the total number of\ngenes found by each procedure.\n6\nConclusion\nWe have provided a new method of correction for high-dimensional generalized lin-\near models with regularization. We employed the recent Imputation Regularization\nOptimization algorithm in a general correction context, and showed explicitly how\nto correct for the three most common data types: continuous, categorical, and count.\nOur proposed methodology improves on a simple naive implementation, which ig-\nnores the measurement error, and is competitive with current existing measurement\nerror correction procedures in this context. The ease of use is the main draw of our\nproposal, and does not require special reformulation of existing methods. This is\nadvantageous for practitioners who can use existing, well designed software, as well\nas providing an easy way to incorporate new state-of-the-art procedures.\nFuture work could be to establish imputation procedures for other settings, such\nas survival analysis or non-parametric regression. Many of these settings will not\nhave a well-known density for the imputation step, and hence would require a way\n27\nof estimating that density for sampling purposes. Such problems are well-known\nto Bayesian statisticians, and methods such as the Integrated Nested Laplace Ap-\nproximation [Rue et al., 2009] could prove useful.\nAn alternative direction could\nbe towards establishing post-selection inference procedures on the estimated coef-\n\ufb01cients. This notion, termed selective inference, has become popular recently for\nmaking a valid inference with regularized models [Taylor and Tibshirani, 2015], and\ncould prove insightful for rigorously providing a \ufb01nal set of estimated coe\ufb03cients.\n28\nReferences\n[Belloni et al., 2011] Belloni, A., Chernozhukov, V., and Wang, L. (2011). Square-\nroot Lasso: pivotal recovery of sparse signals via conic programming. Biometrika,\n98(4):791\u2013806.\n[Belloni et al., 2016] Belloni, A., Rosenbaum, M., Tsybakov, A. B., et al. (2016). An\n{\u21131, \u21132, \u2113\u221e}-regularization approach to high-dimensional errors-in-variables mod-\nels. Electronic Journal of Statistics, 10(2):1729\u20131750.\n[Bhattacharya et al., 2016] Bhattacharya, A., Chakraborty, A., and Mallick, B. K.\n(2016).\nFast sampling with Gaussian scale mixture priors in high-dimensional\nregression. Biometrika, pages 985\u2013991.\n[Breheny and Huang, 2011] Breheny, P. and Huang, J. (2011). Coordinate descent\nalgorithms for nonconvex penalized regression, with applications to biological fea-\nture selection. The Annals of Applied Statistics, 5(1):232.\n[Byrd et al., 2019] Byrd, M., Nghiem, L., and McGee, M. (2019). Bayesian regu-\nlarization of Gaussian graphical models with measurement error. arXiv preprint\narXiv:1907.02241.\n[Carroll et al., 2006] Carroll, R. J., Ruppert, D., Stefanski, L. A., and Crainiceanu,\nC. M. (2006). Measurement Error in Nonlinear Models: a Modern Perspective.\nChapman and Hall/CRC.\n[Chen et al., 2013] Chen, J., Zhu, J., Wang, Z., Zheng, X., and Zhang, B. (2013).\nScalable inference for logistic-normal topic models. In Advances in Neural Infor-\nmation Processing Systems, pages 2445\u20132453.\n29\n[Choi et al., 2013] Choi, H. M., Hobert, J. P., et al. (2013). The Polya-Gamma Gibbs\nsampler for Bayesian logistic regression is uniformly ergodic. Electronic Journal\nof Statistics, 7:2054\u20132064.\n[Datta and Zou, 2019] Datta, A. and Zou, H. (2019). A note on cross-validation for\nlasso under measurement errors. Technometrics, (just-accepted):1\u201313.\n[Datta et al., 2017] Datta, A., Zou, H., et al. (2017). Cocolasso for high-dimensional\nerror-in-variables regression. The Annals of Statistics, 45(6):2400\u20132426.\n[Dempster et al., 1977] Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977).\nMaximum likelihood from incomplete data via the EM algorithm. Journal of the\nRoyal Statistical Society: Series B (Methodological), 39(1):1\u201322.\n[Friedman et al., 2001] Friedman, J., Hastie, T., and Tibshirani, R. (2001).\nThe\nElements of Statistical Learning, volume 1. Springer Series in Statistics New York.\n[Friedman et al., 2008] Friedman, J., Hastie, T., and Tibshirani, R. (2008). Sparse\ninverse covariance estimation with the Graphical Lasso. Biostatistics, 9(3):432\u2013\n441.\n[Gelman et al., 1992] Gelman, A., Rubin, D. B., et al. (1992). Inference from itera-\ntive simulation using multiple sequences. Statistical Science, 7(4):457\u2013472.\n[Hastie et al., 2015] Hastie, T., Tibshirani, R., and Wainwright, M. (2015).\nSta-\ntistical Learning with Sparsity: the Lasso and Generalizations.\nChapman and\nHall/CRC.\n[Hein et al., 2005] Hein, A.-M. K., Richardson, S., Causton, H., Ambler, G. K., and\nGreen, P. J. (2005). Bgx: a fully bayesian gene expression index for a\ufb00ymetrix\ngenechip data. Biostatistics, 6(3):349\u2013373.\n30\n[Holmes et al., 2006] Holmes, C. C., Held, L., et al. (2006). Bayesian auxiliary vari-\nable models for binary and multinomial regression. Bayesian Analysis, 1(1):145\u2013\n168.\n[Huang et al., 2009] Huang, C.-C., Gadd, S., Breslow, N., Cutcli\ufb00e, C., Sredni, S. T.,\nHelenowski, I. B., Dome, J. S., Grundy, P. E., Green, D. M., Fritsch, M. K.,\net al. (2009). Predicting relapse in favorable histology Wilms tumor using gene\nexpression analysis: a report from the Renal Tumor Committee of the Children\u2019s\nOncology Group. Clinical Cancer Research, 15(5):1770\u20131778.\n[Liang et al., 2018] Liang, F., Jia, B., Xue, J., Li, Q., and Luo, Y. (2018).\nAn\nimputation\u2013regularized optimization algorithm for high dimensional missing data\nproblems and beyond. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology), 80(5):899\u2013926.\n[Linderman et al., 2015] Linderman, S., Johnson, M., and Adams, R. P. (2015). De-\npendent multinomial models made easy: Stick-breaking with the P\u00b4olya-Gamma\naugmentation. In Advances in Neural Information Processing Systems, pages 3456\u2013\n3464.\n[Loh and Wainwright, 2011] Loh, P.-L. and Wainwright, M. J. (2011).\nHigh-\ndimensional regression with noisy and missing data: Provable guarantees with\nnon-convexity. In Advances in Neural Information Processing Systems, pages 2726\u2013\n2734.\n[McCullagh, 2019] McCullagh, P. (2019). Generalized Linear Models. Routledge.\n31\n[Nghiem and Potgieter, 2018] Nghiem, L. and Potgieter, C. (2018).\nSimulation-\nselection-extrapolation: Estimation in high-dimensional errors-in-variables mod-\nels. arXiv preprint arXiv:1808.10477.\n[Park and Hastie, 2007] Park, M. Y. and Hastie, T. (2007). L1-regularization path\nalgorithm for generalized linear models. Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology), 69(4):659\u2013677.\n[Perez, 2006] Perez, J. (2006). Preparation of rna for microarray analysis. Technical\nreport.\n[Polson et al., ] Polson, N. G., Scott, J. G., and Windle, J. Improved P\u00b4olya-Gamma\nsampling. Technical report.\n[Polson et al., 2013] Polson, N. G., Scott, J. G., and Windle, J. (2013). Bayesian\ninference for logistic models using P\u00b4olya\u2013Gamma latent variables. Journal of the\nAmerican Statistical Association, 108(504):1339\u20131349.\n[Rocke and Durbin, 2001] Rocke, D. M. and Durbin, B. (2001). A model for mea-\nsurement error for gene expression arrays.\nJournal of Computational Biology,\n8(6):557\u2013569.\n[Rosenbaum et al., 2010] Rosenbaum, M., Tsybakov, A. B., et al. (2010). Sparse\nrecovery under matrix uncertainty. The Annals of Statistics, 38(5):2620\u20132651.\n[Rue et al., 2009] Rue, H., Martino, S., and Chopin, N. (2009).\nApproximate\nBayesian inference for latent Gaussian models by using integrated nested Laplace\napproximations.\nJournal of the Royal Statistical Society: Series B (Statistical\nMethodology), 71(2):319\u2013392.\n32\n[Smith and Roberts, 1993] Smith, A. F. and Roberts, G. O. (1993). Bayesian com-\nputation via the Gibbs sampler and related Markov chain Monte Carlo methods.\nJournal of the Royal Statistical Society: Series B (Methodological), 55(1):3\u201323.\n[S\u00f8rensen et al., 2015] S\u00f8rensen, \u00d8., Frigessi, A., and Thoresen, M. (2015). Mea-\nsurement error in Lasso: Impact and likelihood bias correction. Statistica Sinica,\npages 809\u2013829.\n[S\u00f8rensen et al., 2018] S\u00f8rensen, \u00d8., Hellton, K. H., Frigessi, A., and Thoresen, M.\n(2018).\nCovariate selection in high-dimensional generalized linear models with\nmeasurement error. Journal of Computational and Graphical Statistics, 27(4):739\u2013\n749.\n[Sun and Zhang, 2012] Sun, T. and Zhang, C.-H. (2012). Scaled sparse linear re-\ngression. Biometrika, 99(4):879\u2013898.\n[Taylor and Tibshirani, 2015] Taylor, J. and Tibshirani, R. J. (2015).\nStatistical\nlearning and selective inference. Proceedings of the National Academy of Sciences,\n112(25):7629\u20137634.\n[Tibshirani, 1996] Tibshirani, R. (1996).\nRegression shrinkage and selection via\nthe lasso.\nJournal of the Royal Statistical Society: Series B (Methodological),\n58(1):267\u2013288.\n[Van de Geer et al., 2008] Van de Geer, S. A. et al. (2008). High-dimensional gen-\neralized linear models and the Lasso. The Annals of Statistics, 36(2):614\u2013645.\n[Witten and Tibshirani, 2011] Witten, D. M. and Tibshirani, R. (2011). Penalized\nclassi\ufb01cation using \ufb01sher\u2019s linear discriminant. Journal of the Royal Statistical\nSociety: Series B (Statistical Methodology), 73(5):753\u2013772.\n33\n[Wu et al., 2008] Wu, T. T., Lange, K., et al. (2008). Coordinate descent algorithms\nfor Lasso penalized regression. The Annals of Applied Statistics, 2(1):224\u2013244.\n[Yi and Caramanis, 2015] Yi, X. and Caramanis, C. (2015). Regularized em algo-\nrithms: A uni\ufb01ed framework and statistical guarantees. In Advances in Neural\nInformation Processing Systems, pages 1567\u20131575.\n[Yu and Liu, 2016] Yu, G. and Liu, Y. (2016). Sparse regression incorporating graph-\nical structure among predictors. Journal of the American Statistical Association,\n111(514):707\u2013720.\n[Zhang et al., 2010] Zhang, C.-H. et al. (2010). Nearly unbiased variable selection\nunder minimax concave penalty. The Annals of Statistics, 38(2):894\u2013942.\n34\nA\nAppendix\nA.1\nDerivations\nIn this section we provide the derivations used to obtain the resulting distributions\nfor the respective imputation steps.\nA.1.1\nCovariate Only Imputation Distribution Derivation\nTo impute missing true data xi, we wish to \ufb01nd the full conditional distribution of\nxi| \u00afwi, \u2126x, \u2126u for each i = 1, . . . , n. Standard calculations \ufb01nd\n\u03c0(xi|Wi, \u2126x, \u2126u) \u221dexp\n\u001a\n\u22121\n2xT\ni \u2126xxi\n\u001b ri\nY\nj=1\nexp\n\u001a\n\u22121\n2(wij \u2212xi)T\u2126u(wij \u2212xi)\n\u001b\n\u221dexp\n\u001a\n\u22121\n2\n\u0002\nxT\ni (ri\u2126u + \u2126x) xi \u22122rixT\ni \u2126u \u00afwi\n\u0003\u001b\n\u221dexp\n\u001a\n\u22121\n2(xi \u2212ri\u039b\u2126u \u00afwi)T\u039b\u22121(xi \u2212ri\u039b\u2126u \u00afwi)\n\u001b\n,\nwhere \u039b = (\u2126x + ri\u2126u)\u22121. This result is a kernel of a multivariate Gaussian distri-\nbution,\n\u03c0(xi|Wi, \u2126x, \u2126u) \u223cN(ri\u039b\u2126u \u00afwi, \u039b),\n(17)\nWe brie\ufb02y note that so long as ri = rj, then observations i and j share the same\ncovariance component. When generating large multivariate Gaussian distributions,\nmost of the computation comes from the matrix inversions. By grouping observations\nwith the same number of replicates, time can be saved by only needing compute the\nfull-conditional distributions\u2019 covariance once.\n35\nA.1.2\nGaussian Linear Regression Imputation Distribution Derivation\nThe distribution to impute missing data from a linear model with a Gaussian link,\nwhere \u039b is as in (17), is\n\u03c0(xi|Wi, yi, \u2126x, \u2126u, \u03b2, \u03c32) \u221d\u03c0(yi|wi, \u03b2, \u03c32)\u03c0(xi|Wi, \u2126x, \u2126u)\n\u221dexp\n\u001a\n\u22121\n2\u03c32(yi \u2212xT\ni \u03b2)2\n\u001b\nexp\n\u001a\n\u22121\n2(xi \u2212ri\u039b\u2126u \u00afwi)T\u039b\u22121(xi \u2212ri\u039b\u2126u \u00afwi)\n\u001b\n\u221dexp\n\u001a\n\u22121\n2\n\u0014\nxT\ni (ri\u2126u + \u2126x + 1\n\u03c32\u03b2\u03b2T)xi \u22122xT\ni\n\u0010 yi\n\u03c32\u03b2 + ri\u039b\u2126u \u00afwi\n\u0011\u0015\u001b\n= exp\n\u001a\n\u22121\n2\n\u0010\nxi \u2212\u039bG\n\u0010\nri\u2126u \u00afwi + yi\n\u03c32\u03b2\n\u0011\u0011T\n\u039b\u22121\nG\n\u0010\nxi \u2212\u039bG\n\u0010\nri\u2126u \u00afwi + yi\n\u03c32\u03b2\n\u0011\u0011\u001b\n,\nwhere \u039bG =\n\u0000ri\u2126u + \u2126x + \u03c3\u22122\u03b2\u03b2T\u0001\u22121. This result is, again, the kernel of a multi-\nvariate Gaussian distribution,\n\u03c0(xi|Wi, \u2126x, \u2126u) \u223cN\n\u0010\n\u039bG\n\u0010\nri\u2126u \u00afwi + yi\n\u03c32\u03b2\n\u0011\n, \u039bG\n\u0011\n.\n(18)\nAgain, by grouping observations with the same number of replicates, time can be\nsaved by computing each matrix inverse for each unique number of replicates.\nA.1.3\nBinomial Linear Regression Imputation Distribution Derivation\nTo impute xi when using the logit function we appeal to [Polson et al., 2013], which,\nas explained in Section 3.2, uses P\u00b4olya Gamma random variables to augment the data\ngenerating process. For the most recently generated zi, from [Polson et al., 2013] we\nnote that\n\u03c0(yi|zi, xi, \u03b2) =\nexp{xT\ni \u03b2}yi\n1 + exp{xT\ni \u03b2} \u221dexp\n(\n\u2212zi\n2\n\u0012\u03bai\nzi\n\u2212xT\ni \u03b2\n\u00132)\n,\n(19)\n36\nwhere \u03bai = yi \u22121/2. Hence, with \u039b as in (17), we have\n\u03c0(xi|Wi, \u2126x, \u2126u, yi, \u03b2, zi)\n\u221dexp\n(\n\u22121\n2(xi \u2212ri\u039b\u2126u \u00afwi)T\u039b\u22121(xi \u2212ri\u039b\u2126u \u00afwi) \u2212zi\n2\n\u0012\u03bai\nzi\n\u2212xT\ni \u03b2\n\u00132)\n\u221dexp\n\u001a\n\u22121\n2\n\u0002\nxT\ni (zi\u03b2\u03b2T + ri\u2126u + \u2126x)xi \u22122xT\ni (\u03bai\u03b2 + ri\u2126u \u00afwi)\n\u0003\u001b\n\u221dexp\n\u001a\n\u22121\n2(xi \u2212\u039bB(\u03bai\u03b2 + ri\u2126u \u00afwi))T\u039bB(xi \u2212\u039bB(\u03bai\u03b2 + ri\u2126u \u00afwi))\n\u001b\n,\nwhere \u039bB = (zi\u03b2\u03b2T+ri\u2126u+\u2126x)\u22121. As expected from the results of [Polson et al., 2013],\nwe have a Gaussian kernal, where\n\u03c0(xi|Wi, \u2126x, \u2126u, yi, \u03b2, zi) \u223cN(\u039bB(\u03bai\u03b2 + ri\u2126u \u00afwi), \u039bB).\n(20)\nThe derivation for the full conditional distribution of zi is exactly the same as in\n[Polson et al., 2013].\nA.2\nEstimating the Measurement Error Covariance with Repli-\ncates\nHere, we address an estimate of the measurement error\u2019s precision matrix, \u2126u, which\nis necessary for the imputation step. In some instances, it may be realistic to know\nthe amount of variability in the measurement process; for instance, a machine taking\nmeasurements where the output falls within some perturbation of the truth. How-\never, in many contexts the variability of the contamination process will not be known,\nand hence need to be estimated, typically with replicates. Estimating \u03a3u is di\ufb03-\ncult due to not directly observing the amount of contamination on each observation.\n37\nHowever, if one assumes the amount of contamination is independent between each\nvariable, then a procedure exists to get an empirical estimate of the diagonal of \u03a3x\nand, hence, \u2126u.\nAn estimate of \u03a3u is also necessary for the imputation. When data is observed\nwith replicates for each observation, then this covariance matrix is an estimable vari-\nable under independence assumptions. Consider the measurement error distribution\nas described in Section 2, where for each observation\u2019s replicates, uij \u223cN(0p, \u03a3u).\nEstimating \u03a3u is not trivial because n < p and uij is not directly observed. Note\nfor replicate j and k of observation i that\ndijk = wij \u2212wik = xi \u2212uij \u2212xi \u2212uik = uij \u2212uik.\n(21)\nAssuming the amount of contamination is independent for each covariate, then, for\ncovariate m,\nV ar(d(m)\nijk ) = V ar(u(m)\nij ) + V ar(u(m)\nik ) = 2[\u03a3u]m,m.\nHence, if one were willing to assume the same distribution governing the contamina-\ntion of each observation, the di\ufb00erences from all i = 1, . . . , n where j < k could be\nused and averaged for all ri(ri \u22121) possible di\ufb00erences per observation,\n[ \u02c6\u2126u]m,m = 1\n\u221a\n2\n1\nn\nn\nX\ni=1\n1\nri(ri \u22121)\nX\nj<k\nd(m)\nijk .\n(22)\nIf heterogenious measurement error is believed to exist between observations, then\nit can easily be incorporated into the imputation step by using observation speci\ufb01c\n\u2126u,i. Here, the averaged covariance diagonal element would only be between the\npair-wise replicates for the obervation.\n38\nA.3\nOther Gaussian Simulation Results\nA.3.1\nComplete MCP Results\nIn Tables 4 and 5 we display the complete results for the results found in Section\n4.1. Table 4 displays the results for p = 100, and all results are displayed for \u03b3 = 1\nin Table 5. All results are similar to the discussion presented in Section 4.1.\n39\nSetting\np\nMetric\nIdeal\nNaive\nIRO\nCLasso\nGMUS\nG1\n100\nL2\n0.266\n0.349\n0.319\n0.611\n0.616\nTP\n8.45\n7.61\n7.09\n7.57\n4.34\nFP\n5.44\n4.69\n2.75\n9.85\n0.09\n500\nL2\n0.319\n0.405\n0.373\n0.61\n0.654\nTP\n7.39\n6.2\n5.68\n6.43\n3.83\nFP\n9.53\n8.47\n3.02\n15\n0.23\n1000\nL2\n0.338\n0.423\n0.391\n0.62\n0.676\nTP\n7.15\n6.32\n5.5\n6.07\n3.61\nFP\n11.62\n11.09\n3.49\n18.14\n0.34\nG2\n100\nL2\n0.33\n0.602\n0.422\n1.293\n1.6\nTP\n10\n10\n10\n10\n10\nFP\n1.33\n3.15\n0.75\n11.06\n0.16\n500\nL2\n0.363\n0.68\n0.458\n1.227\n1.89\nTP\n10\n10\n10\n10\n9.93\nFP\n3.43\n6.78\n1.02\n19.34\n0.21\n1000\nL2\n0.359\n0.679\n0.426\n1.14\n1.949\nTP\n10\n10\n10\n10\n9.93\nFP\n5.13\n10.54\n0.98\n22.44\n0.2\nG3\n100\nL2\n0.407\n1.016\n0.768\n3.792\n2.665\nTP\n10\n9.97\n9.96\n8.07\n6.42\nFP\n2.03\n5.51\n2.02\n4.36\n0.61\n500\nL2\n0.424\n1.138\n0.916\n3.24\n2.793\nTP\n10\n9.9\n9.79\n7.58\n5.37\nFP\n4.64\n14.87\n3.83\n9.87\n0.97\n1000\nL2\n0.445\n1.229\n1.047\n3.239\n2.836\nTP\n10\n9.81\n9.58\n7.27\n4.94\nFP\n8.07\n23.65\n4.5\n12.1\n1.49\nTable 4: Simulation results for Gaussian linear regression under the three speci\ufb01ed set-\ntings with noise-to-signal ratio \u03b3 = 0.5. Ideal, Naive, and IRO use the MCP penalty for\nregularization.\n40\nSetting\np\nMetric\nIdeal\nNaive\nIRO\nCLasso\nGMUS\nG1\n100\nL2\n0.265\n0.464\n0.385\n1.576\n0.709\nTP\n8.38\n6.73\n6\n3.1\n3.94\nFP\n5.79\n4.95\n2.17\n1.37\n0.12\n500\nL2\n0.322\n0.503\n0.436\n1.45\n0.735\nTP\n7.64\n5.77\n4.41\n2.72\n3.62\nFP\n9.37\n7.69\n1.02\n1.8\n0.22\n1000\nL2\n0.342\n0.524\n0.456\n1.408\n0.762\nTP\n6.95\n5.45\n4.23\n2.41\n3.43\nFP\n11.22\n11.06\n1.09\n2.38\n0.4\nG2\n100\nL2\n0.351\n0.913\n0.535\n3.943\n1.937\nTP\n10\n10\n10\n7.26\n9.88\nFP\n1.82\n4.47\n0.35\n0.77\n0.24\n500\nL2\n0.371\n1.026\n0.606\n3.367\n2.195\nTP\n10\n10\n9.99\n7.28\n9.62\nFP\n3.9\n9.56\n0.56\n1.13\n0.15\n1000\nL2\n0.366\n1.055\n0.637\n3.372\n2.257\nTP\n10\n10\n9.99\n7.07\n9.62\nFP\n4.3\n13.49\n0.4\n1.23\n0.34\nG3\n100\nL2\n0.39\n1.527\n1.166\n6.744\n2.775\nTP\n10\n9.85\n9.65\n2.83\n5.87\nFP\n1.66\n8.69\n1.89\n0.05\n0.74\n500\nL2\n0.416\n1.845\n1.74\n5.891\n2.853\nTP\n10\n9.06\n8.18\n2.88\n5.24\nFP\n4.87\n19.58\n3.12\n0.14\n1.76\n1000\nL2\n0.42\n2.1\n2.105\n5.461\n2.885\nTP\n10\n8.18\n6.98\n3.02\n4.82\nFP\n6.92\n22.68\n3.27\n0.2\n1.94\nTable 5: Simulation results for Gaussian linear regression under the three speci\ufb01ed set-\ntings with noise-to-signal ratio \u03b3 = 1. Ideal, Naive, and IRO use the MCP penalty for\nregularization.\nA.3.2\nScaled Lasso Results\nTo illustrate the IRO-algorithm with another methodology, we opted to illustrate the\nincorporation of the Scaled Lasso penalty [Sun and Zhang, 2012]. The Scaled Lasso\n41\npenalty incorporates the residual error term into the Lasso estimation procedure,\nwhich is necessary for the imputation step. We display the results in Tables 6 and\n7. The overall results are similar to the MCP penalty, with a few di\ufb00erences. One\ndi\ufb00erence is that the Naive model had a di\ufb03cult time \ufb01nding convergence. This\noccurred in both results for \u03b3 = 0.5 and \u03b3 = 1. We believe this can be attributed to\nthe residual variability being confused with the covariate mismeasurement variability.\nThis would lead to poor estimation of \u03c32\n\u03f5, and hence \u03b2. The Second di\ufb00ernce is\nslight degredation of performance for the IRO-correct. This is likely due to the bias\nincorporated into the estimate from the \u21131 penalty.\nSetting\np\nMetric\nIdeal\nNaive\nIRO\nCLasso\nGMUS\nG1\n500\nL2\n0.422\nNA\n0.507\n0.61\n0.654\nTP\n6.87\n6.73\n5.45\n6.43\n3.83\nFP\n4.79\n53.81\n2.23\n15\n0.23\n1000\nL2\n0.448\nNA\n0.542\n0.62\n0.676\nTP\n6.55\n6.18\n5.12\n6.07\n3.61\nFP\n4.99\n13.18\n2.19\n18.14\n0.34\nG2\n500\nL2\n1.009\nNA\n1.425\n1.227\n1.89\nTP\n10\n9.99\n10\n10\n9.93\nFP\n5.25\n107.28\n2.34\n19.34\n0.21\n1000\nL2\n1.069\nNA\n1.516\n1.14\n1.949\nTP\n10\n10\n10\n10\n9.93\nFP\n4.87\n13.3\n2.17\n22.44\n0.2\nG3\n500\nL2\n2.315\nNA\n2.65\n3.24\n2.793\nTP\n8.99\n8.28\n6.72\n7.58\n5.37\nFP\n4.55\n42.47\n2.98\n9.87\n0.97\n1000\nL2\n2.548\nNA\n2.767\n3.239\n2.836\nTP\n7.87\n7.17\n5.52\n7.27\n4.94\nFP\n5.15\n24.31\n2.62\n12.1\n1.49\nTable 6: Simulation results for Gaussian linear regression under the three speci\ufb01ed settings\nwith noise-to-signal ratio \u03b3 = 0.5. Ideal, Naive, and IRO use the Scaled Lasso penalty\nfor regularization. Due to convergence issues, many L2 norms for the naive method are\nmissing and denoted NA.\n42\nSetting\np\nMetric\nIdeal\nNaive\nIRO\nCLasso\nGMUS\nG1\n500\nL2\n0.419\nNA\n0.574\n1.45\n0.735\nTP\n7.07\n6.67\n4.42\n2.72\n3.62\nFP\n5.01\n46.3\n1.42\n1.8\n0.22\n1000\nL2\n0.453\nNA\n0.614\n1.408\n0.762\nTP\n6.34\n5.72\n4.06\n2.41\n3.43\nFP\n4.79\n17.88\n1.22\n2.38\n0.4\nG1\n500\nL2\n0.983\nNA\n1.77\n3.367\n2.195\nTP\n10\n9.95\n9.96\n7.28\n9.62\nFP\n4.92\n91.09\n1.51\n1.13\n0.15\n1000\nL2\n1.057\nNA\n1.938\n3.372\n2.257\nTP\n10\n10\n9.91\n7.07\n9.62\nFP\n4.8\n33\n1.14\n1.23\n0.34\nG1\n500\nL2\n2.299\nNA\n2.78\n5.891\n2.853\nTP\n9.06\n7.98\n5.44\n2.88\n5.24\nFP\n5.03\n65.15\n1.76\n0.14\n1.76\n1000\nL2\n2.533\nNA\n2.839\n5.461\n2.885\nTP\n7.81\n6.79\n4.63\n3.02\n4.82\nFP\n4.63\n17.58\n1.58\n0.2\n1.94\nTable 7: Simulation results for Gaussian linear regression under the three speci\ufb01ed settings\nwith noise-to-signal ratio \u03b3 = 1. Ideal, Naive, and IRO use the Scaled Lasso penalty for\nregularization. Due to convergence issues, many L2 norms for the naive method are missing\nand denoted NA.\nA.4\nOther Binomial Regression Results\nWe display the results when \u03b3 = 1 for the binomial linear regression simulation found\nin Section 4.2. Besides Setting B2 being slightly more in favor of the IRO-adjusted\nprocedure, the results are similar.\nA.5\nDetails for Data Analysis\nHere we illustrate the ELBO-plots for both the CLasso and GMUS as performed in\nthe data analysis found in Section 5. These plots, generated by the \u2018hdme\u2019 package\n43\nSetting\np\nMetric\nIdeal\nNaive\nIRO\nCLasso\nGMUS\nB1\n100\nL2\n0.695\n1.748\n1.21\n3.613\n2.713\nTP\n10\n9.98\n9.91\n2.3\n9.22\nFP\n4.07\n5.56\n1.73\n0.05\n0.53\n500\nL2\n0.933\n1.987\n1.658\n3.702\n2.846\nTP\n10\n9.86\n9.62\n2.03\n8.53\nFP\n8.51\n13.07\n3.07\n0.07\n0.89\n1000\nL2\n1.026\n2.117\n1.938\n3.735\n2.887\nTP\n10\n9.83\n9.38\n2.07\n8.17\nFP\n11.86\n17.51\n4.08\n0.12\n1.41\nB2\n100\nL2\n0.731\n2.171\n2.059\n3.374\n2.949\nTP\n9.97\n8.82\n7.88\n1.95\n5.41\nFP\n4.99\n9.23\n3.11\n0.1\n1.6\n500\nL2\n0.967\n2.701\n2.726\n3.266\n3.009\nTP\n9.74\n6.18\n5\n1.43\n4.37\nFP\n12.8\n10.83\n3.35\n0.05\n2.38\n1000\nL2\n1.216\n2.825\n2.859\n3.276\n3.026\nTP\n9.38\n5.29\n4.1\n1.36\n4\nFP\n17.98\n12.35\n3.35\n0.06\n3.01\nTable 8: Simulation results for Binomial linear regression under the two speci\ufb01ed settings\nwith signal-to-noise ratio \u03b3 = 1.\nThe Ideal, Naive, and IRO procedures use the MCP\npenalty for regularization.\noutput, show the tuning parameter on the x-axis and the number of non-zero coef-\n\ufb01cients on the y-axis. The authors encourage picking where the number of non-zero\ncoe\ufb03cients stabilize. That is to say, pick the tuning parameter where the following\ntuning parameters give the same number of non-zero coe\ufb03cients. We present the\nplots in Figure 1, where the left and right plot is for CLasso and GMUS, respec-\ntively. As noted in Section 4.2, the optimality of the solution for CLasso only holds\nfor the Gaussian case, hence the bumpiness. Hence, for CLasso we opt to choose the\nlargest radius value in the grid that gives the number of non-zero coe\ufb03cients to be 3\nas this is the most common amount in a short succession. GMUS begins to stablize\n44\nat 0.2, and this is the value used for the analysis.\n5\n10\n0.0\n0.5\n1.0\nradius\nNonzero coefficients\nNumber of nonzero coefficients\n5\n10\n15\n20\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\ndelta\nNonzero coefficients\nElbow plot\nFigure 1: Outputted ELBO-plots for CLasso (left) and GMUS (right). Note that the in-\ncrease of the regularization parameter has varying a\ufb00ect, hence the opposing trend.\n45\n"}