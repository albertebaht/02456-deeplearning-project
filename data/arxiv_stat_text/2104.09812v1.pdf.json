{"text": "Screening methods for linear errors-in-variables models\nin high dimensions\nLinh H. Nghiem1,3, Francis K.C. Hui1, Samuel M\u00a8uller2,3, and A.H. Welsh1\n1Research School of Finance, Actuarial Studies and Statistics, Australian National University, Australia\n2Department of Mathematics and Statistics, Macquarie University, Australia\n3School of Mathematics and Statistics, University of Sydney, Australia\nAbstract\nMicroarray studies, in order to identify genes associated with an outcome of interest, usually pro-\nduce noisy measurements for a large number of gene expression features from a small number of sub-\njects. One common approach to analyzing such high-dimensional data is to use linear errors-in-variables\nmodels; however, current methods for \ufb01tting such models are computationally expensive. In this paper,\nwe present two ef\ufb01cient screening procedures, namely corrected penalized marginal screening and cor-\nrected sure independence screening, to reduce the number of variables for \ufb01nal model building. Both\nscreening procedures are based on \ufb01tting corrected marginal regression models relating the outcome to\neach contaminated covariate separately, which can be computed ef\ufb01ciently even with a large number of\nfeatures. Under mild conditions, we show that these procedures achieve screening consistency and re-\nduce the number of features considerably, even when the number of covariates grows exponentially with\nthe sample size. Additionally, if the true covariates are weakly correlated, corrected penalized marginal\nscreening can achieve full variable selection consistency. Through simulation studies and an analysis of\ngene expression data for bone mineral density of Norwegian women, we demonstrate that the two new\nscreening procedures make estimation of linear errors-in-variables models computationally scalable in\nhigh dimensional settings, and improve \ufb01nite sample estimation and selection performance compared\nwith estimators that do not employ a screening stage.\nKeywords: dimension reduction; forward regression; measurement error; penalized regression; regular-\nization; sure independence screening\n1\nIntroduction\nIn microarray studies, to identify genes that are associated with an outcome of interest, a large number\nof gene expressions (potentially tens of thousands) are measured from typically a much smaller number\nof subjects (often in the tens to hundreds). The gene expression measurements tend to be noisy, where\nmeasurement errors come from many sources such as sample preparation, labeling, and hybridization; for\nexample, see Rocke and Durbin (2001) and Zakharkin et al. (2005). The gene measurements are also often\n1\narXiv:2104.09812v1  [stat.ME]  20 Apr 2021\nanalyzed on the log scale, making the assumption of additive measurement errors more plausible (Nghiem\nand Potgieter, 2019). Furthermore, as in common genome wide association studies (Do et al., 2011; Zhou\net al., 2018, among others), it is usually assumed that only a few genes are related to the outcome of interest,\ni.e a sparsity assumption on the statistical model. As a speci\ufb01c motivating example, after some preprocessing\nsteps, our Bone Mineral Density data in Section 4 contains noisy measurements of p = 993 features (genes)\nfrom n = 84 observations (Norwegian women), and we are interested in identifying genes that are associated\nwith the total hip T-score.\nFor analyzing such data, a commonly used approach which we focus on in this paper is the classical\nlinear errors-in-variables (EIV) model\ny = X \u03b20 + \u03b5,\nW = X +U,\n(1)\nwhere y \u2208Rn is a random vector of outcomes from n independent and identically distributed (iid) ob-\nservations, X \u2208Rn\u00d7p is the deterministic true covariate matrix (typically the true gene expressions),\n\u03b20 = (\u03b201, . . . , \u03b20p)\u22a4\u2208Rp is the true coef\ufb01cient vector, and \u03b5 \u2208Rn is the model error vector term\nwhose components are assumed to be iid with zero mean and variance \u03c32. Due to the existence of measure-\nment error, the true covariate matrix X is not observed; instead, we observe the random matrix W \u2208Rn\u00d7p,\nwhich is a noisy version of X, contaminated by an additive random measurement error matrix U \u2208Rn\u00d7p\nindependent of \u03b5. The rows of U are assumed to be iid random vectors with zero mean and covariance\nmatrix \u03a3u. We focus on the model (1) in high dimensional settings, where the number of covariates p can\nbe bigger than and grow with the sample size n, potentially at an exponential rate. We also assume the true\ncoef\ufb01cient vector \u03b20 to be sparse, meaning that only a few components of \u03b20 are non-zero.\nWhen the true covariate matrix X is observed, penalized regression methods (Tibshirani, 1996; Fan\net al., 2004; Huang et al., 2008; Simon et al., 2013; Piironen et al., 2017; Ida et al., 2019, among others) are\nwidely used to estimate and perform variable selection on \u03b20. However, when the true covariate matrix X is\nnot observed, replacing X with W leads to a naive estimator that is inconsistent in both estimation and vari-\nable selection of \u03b20 (S\u00f8rensen et al., 2015). To address this challenge, several corrections for measurement\nerror in high dimensional linear EIV models have been proposed. For example, Rosenbaum et al. (2010)\nand Rosenbaum et al. (2013) proposed the matrix uncertainty (MU) selector and its improved version, re-\nspectively, while Belloni et al. (2017) proved its near-optimal minimax properties and developed a conic\nprogramming estimator that can achieve the minimax bound. Both the MU selector and conic estimator\nrequire appropriate choice of multiple tuning parameters, which is typically very challenging in practice,\nespecially in high dimensional settings. Another approach for handling measurement error is to modify the\nloss function or the conditional score functions commonly seen in the error-free penalized regressions; ex-\namples include the corrected lasso method of Loh and Wainwright (2012) and S\u00f8rensen et al. (2015), and the\nconvex conditioned lasso of Datta et al. (2017). More recently, Romeo and Thoresen (2019) presented a sim-\nulation study to compare the performance of the MU, corrected lasso, and convex conditioned lasso against\nthe naive estimator in both estimation and variable selection; they concluded that the relative performance of\nthose estimators depend on the structure of \u03a3u. Brown et al. (2019) introduced a boosting algorithm based\non the estimating equation of the corrected lasso, but the theoretical properties of the \ufb01nal estimates were\n2\nnot examined. In another line of research, Nghiem and Potgieter (2019) proposed a SIMSELEX estimator\nthat \ufb01rst uses simulation to evaluate the effect of measurement error on estimated coef\ufb01cients, then selects\nimportant covariates based on these simulated effects, and \ufb01nally extrapolates the simulation to the scenario\nwith no measurement error present. Furthermore, Byrd and McGee (2019) presented an EM-type correction\nmethod, where they iteratively sample the true covariate from the conditional distribution of X given y and\nW, and \ufb01tted penalized regressions of y on these sampled covariates. A major disadvantage of all these\nmethods is that they are not computationally ef\ufb01cient when the number of covariates p is very large. Specif-\nically, while the corrected lasso is de\ufb01ned to be a global minimum of a non-convex optimization problem,\nthe convex conditioned lasso requires computation of the nearest semi-positive de\ufb01nite matrix measured in\nelement-wise max norm in high dimensions; we will elaborate these two estimators in Section 3.2. In addi-\ntion, the SIMSELEX procedure requires running the lasso on a large number of simulated datasets, while the\nmethod of Byrd and McGee (2019) requires sampling from a large p-dimensional multivariate distribution,\nwhich is slow in high dimensional settings.\nIn this paper, motivated by the Bone Mineral Density data, we address the large p problem for linear\nEIV models by proposing two ef\ufb01cient corrected marginal screening methods, namely corrected penalized\nmarginal screening and corrected sure independence screening, respectively. These screening methods\naim to quickly identify a screening index set \u02c6Q \u2282{1, . . . , p} from the observed data, such that \u02c6Q has\nmuch smaller cardinality than p but still retains all the important covariates, a property known as screening\nconsistency. When no measurement error is present, screening methods in high dimensions are usually\ncarried out based on marginal regressions, with the \ufb01rst two proposed methods being the penalized marginal\nbridge regression of Huang et al. (2008) and sure independence screening of Fan and Lv (2008) for the linear\nmodel, followed by a vast literature that improved these two methods and applied them to more complex\nmodels, see Fan and Lv (2008); Fan et al. (2010); Li et al. (2012); Barut et al. (2016); Wen et al. (2018,\namong others). Compared to penalized methods when no measurement error is present, screening methods\nhave received far less attention for EIV models. A screening method for linear EIV models was brie\ufb02y\nmentioned in Kaul et al. (2016). As detailed in Section 2, this method is a special case of our proposed\ncorrected sure independence screening method when all the covariates have the same measurement error\nvariance. In our new methods, after screening, we only compute penalized estimators using the variables\nindexed by \u02c6Q; since the cardinality of the set \u02c6Q is much smaller than p, the total estimation times of\ntwo-stage estimators are much reduced compared to those of one-stage estimators that do not employ a\nscreening. We demonstrate that the bene\ufb01t of the proposed screening procedures is so substantial that it can\nmake many estimators (such as the convex conditioned lasso) computationally feasible in high dimensional\nsettings. Theoretically, we show that under mild conditions, even when the number of covariates p grows at\nan exponential rate with the sample size n, our proposed screening methods achieve screening consistency\nwhile reducing the number of variables to below the sample size. Moreover, under a partial orthogonality\ncondition for the true covariates, we demonstrate that the corrected penalized marginal screening approach\ncan achieve full variable selection consistency. Our simulation studies and an analysis of the motivating\nBone Mineral Density data both verify our theoretical results, and demonstrate that our proposed screening\nprocedures lead to remarkable gains in both computational cost and \ufb01nite sample performance.\n3\nThe remainder of this paper is organized as follows. Section 2 introduces the proposed screening proce-\ndures and establishes their theoretical properties. In Section 3, we present simulation studies to demonstrate\nthe strong empirical performance of screening procedures and several two-stage estimators. Section 4 ap-\nplies the methodologies to analyze the motivating Bone Mineral Density data, and Section 5 offers some\nconcluding remarks.\nThe following notation is used throughout the paper. For a generic matrix A, let Aij denote the (i, j)\nelement of A and let \u2225A\u22252, \u2225A\u2225F , and \u2225A\u2225max denote the \u21132 norm, Frobenius norm, and element-wise max\nnorm, respectively. For a square matrix A, let \u03bbmax(A) denote its maximum eigenvalue. For any vector v,\nlet vj denote its jth component, \u2225v\u22252 and \u2225v\u22251 denote its \u21132 norm and \u21131 norm, respectively. For any set S,\nlet Sc denote its complement, and |S| denote its cardinality. Finally, for any sequence an and bn, we write\nan \u223cbn if there exist positive constants c1 and c2 such that c1an \u2264bn \u2264c2an.\n2\nCorrected marginal screening procedures\nConsider the linear EIV model (1), with the observed data consisting of the outcome vector y \u2208Rn and\nthe surrogate matrix W \u2208Rn\u00d7p. Let \u03c32\n1, . . . , \u03c32\np denote the diagonal elements of \u03a3u; if any true covari-\nate Xij is measured without error, then the corresponding element \u03c32\nj is set to zero. We assume only s\ncomponents of \u03b20 are non-zero, where s \u226amin(n, p), and without loss of generality, let S = {1, . . . , s}\nand Sc = {s + 1, . . . , p} denote the set of indices corresponding to non-zero and zero components of \u03b20,\nrespectively. For the remainder of this paper, unless otherwise stated, we assume the covariance matrix \u03a3u\nis known; in practice, \u03a3u is usually estimated from replicate data (Carroll et al., 2006). For example, in our\nmicroarray data analysis in Section 4, replications are available in the form of multiple probes for each gene\nexpression, and we follow a common procedure to estimate \u03a3u from these data (further details are presented\nin Appendix B).\nWe develop screening procedures that can reduce the number of covariates for model (1) but still main-\ntain all the important variables. Moreover, these procedures are designed to be computationally scalable to\nhigh dimensional settings with p \u226bn. Speci\ufb01cally, we propose to screen variables by minimizing\nL(\u03b2) =\np\nX\nj=1\nLj(\u03b2j) = 1\nn\np\nX\nj=1\n( n\nX\ni=1\n(yi \u2212Wij\u03b2j)2 \u2212\u03c32\nj \u03b22\nj + \u03bbnpn(|\u03b2j|)\n)\n,\n(2)\nwith \u03bbn being a non-negative tuning parameter and pn(|\u03b2j|) a penalty function on |\u03b2j|. Let \u02c6\u03b2 = argmin\u03b2 L(\u03b2).\nThe function L(\u03b2) to be minimized in (2) consists of two parts. The \ufb01rst part, Pp\nj=1 n\u22121 Pn\ni=1 (yi \u2212Wij\u03b2j)2\u2212\nPp\nj=1 \u03c32\nj \u03b22\nj , is the sum of all the \u21132 losses for the regression of the outcome on each surrogate predic-\ntor (i.e column of W) separately, with measurement error accounted for via a (negative) \u21132 penalty term,\n\u2212Pp\nj=1 \u03c32\nj \u03b22\nj . When p is \ufb01xed and n \u2192\u221e, this loss part converges to n\u22121 Pp\nj=1\nPn\ni=1 (yi \u2212Xij\u03b2j)2,\nwhich is the loss that was used by both Huang et al. (2008) and Fan et al. (2010) for their corresponding\nmarginal screening procedures when no measurement error is present. In the theoretical analysis below, we\nallow p to diverge to \u221e; in this case, this loss part still enables us to achieve screening consistency. For\nthe second part of (2), the penalty function \u03bbn\nPp\nj=1 pn(|\u03b2j|) regularizes the estimates, i.e with appropriate\n4\nchoices for \u03bbn, many components of the estimated \u02c6\u03b2 are set to zero. Common choices of the penalty func-\ntion pn(|\u03b2j|) include the lasso |\u03b2j|, the bridge penalty |\u03b2j|\u03b1 with 0 < \u03b1 < 1 (Frank and Friedman, 1993),\nand the SCAD penalty (Fan et al., 2004), among others. In this paper, we use the bridge penalty, which\nmakes the solution of (2) relatively fast to compute and easy to analyze theoretically. However, we note that\nother penalties can be used in practice, although pursuing this is beyond the scope of this paper. Computa-\ntionally, for any value of \u03bbn, each component \u02c6\u03b2j can be obtained by minimizing Lj(\u03b2j) separately. When\nn is large enough, the coef\ufb01cient n\u22121 Pn\ni=1 W 2\nij \u2212\u03c32\nj associated with \u03b22\nj is positive, so Lj(\u03b2j) is a convex\nfunction of \u03b2j and can be minimized quickly using univariate convex optimization routines. Therefore, the\nminimizer of (2) can be computed ef\ufb01ciently even with very large p, making it and subsequent screening\nmethods scalable in high dimensions.\nWe consider two screening approaches arising from (2). For the \ufb01rst approach, which will be referred\nto as corrected penalized marginal screening (PMSc), we consider the case when the tuning parameter \u03bbn\nis strictly positive. In this case, although the minimizer of (2) does not generally have a closed form, some\ncomponents of \u02c6\u03b2 can be set exactly to zero, so the corresponding screening set is de\ufb01ned to be \u02c6QPMSc =\n{j : \u02c6\u03b2j \u0338= 0}. For the second approach, we consider the case when \u03bbn = 0, so the minimizer of (2) has\ncomponents\n\u02dc\u03b2j =\nPn\ni=1 Wijyi\nPn\ni=1 W 2\nij \u2212n\u03c32\nj\n,\nj = 1, . . . , p,\n(3)\neach of which is non-zero with probability one. Note that each component \u02dc\u03b2j is a consistent estimator\nof the slope \u03b3j in the marginal univariate model between the outcome and the true (unobserved) covariate\nyi = \u03b3jXij + \u03b5i, i = 1, . . . , n, which is the quantity that is used by Fan and Lv (2008) and Fan et al.\n(2010) in sure independence screening (SIS) when no measurement error exists. In order to reduce the\nnumber of dimensions in this unpenalized approach, similar to SIS, we keep the d components with largest\nmagnitude |\u02dc\u03b2j|, j = 1, . . . , p, and refer to this approach as corrected sure independence screening (SISc).\nThe corresponding screening set is then de\ufb01ned to be\n\u02c6QSISc = {1 \u2264j \u2264p : |\u02dc\u03b2j| is among the \ufb01rst d largest of all components |\u02dc\u03b21|, . . . , |\u02dc\u03b2p|}.\n(4)\nWe remark that the SISc approach in our paper is more general than the corrected screening method intro-\nduced by Kaul et al. (2016) in the \ufb01rst step of their two-step estimation procedure; speci\ufb01cally, Kaul et al.\n(2016) screened the variables based on \u03b6j = Pn\ni=1 Wijyi, j = 1, . . . , p, which is the numerator of (3). The\nrank of |\u03b6j| is asymptotically the same as the rank of |\u02dc\u03b2j| if all the covariates have the same measurement\nerror variance and are measured on the same scale. Therefore, the method of Kaul et al. (2016) may be\nconsidered as a special case of SISc when all the covariates have the same measurement error variances.\nIn the next two subsections, we study the theoretical properties of the two screening procedures PMSc\nand SISc separately; all the proofs are provided in Appendix A. Throughout the development below, we note\nthat the true covariate matrix X is assumed to be deterministic; this assumption could be relaxed to allow X\nto be random, although we do not explore this in the paper.\n5\n2.1\nCorrected penalized marginal bridge screening\nFirst, we study the properties of the PMSc approach with the penalty function pn(|\u03b2j|) = |\u03b2j|\u03b1 for 0 < \u03b1 <\n1. Since the screening step aims only to reduce the dimension of the feature space, we are interested only in\nvariable selection properties, noting that \u02c6\u03b2 is generally not estimation consistent for the true vector \u03b20. For\nthe theoretical development below, we de\ufb01ne\n\u02dc\u03benj = 1\nnE\n n\nX\ni=1\nyiXij\n!\n= 1\nn\nn\nX\ni=1\n s\nX\nk=1\nXik\u03b20k\n!\nXij,\n\u03benj = 1\nnE\n\" n\nX\ni=1\nyiWij\n! \f\f\f\f Uij\n#\n= 1\nn\nn\nX\ni=1\n s\nX\nk=1\nXik\u03b20k\n!\nWij = \u02dc\u03benj + 1\nn\nn\nX\ni=1\n s\nX\nk=1\nXik\u03b20k\n!\nUij.\nHere, \u02dc\u03benj can be considered as the marginal covariance between the outcome and the jth true covariate. As\nin the no measurement error case, this quantity \u02dc\u03benj with j \u2208S will play an essential role in determining\nwhether the component \u02c6\u03b2j is asymptotically non-zero. However, because the Xij\u2019s are not observed, the\nquantities \u02dc\u03benj are not directly computable, so the random variables \u03benj are used as surrogates for \u02dc\u03benj, j =\n1, . . . , p. Next, we assume the following conditions.\n(C1) The model error terms \u03b51, \u03b52, . . . \u03b5n are iid sub-Gaussian random variables with mean zero and \ufb01nite\nvariance \u03c32,\n(C2) The measurement errors Uij, i = 1, . . . , n are independent sub-Gaussian random variables with \ufb01nite\nvariance \u03c32\nj , j = 1, . . . , p.\n(C3) There exist constants b0 and b1 such that 0 < b0 \u2264min\nk\u2208S |\u03b20k| \u2264max\nk\u2208S |\u03b20k| \u2264b1 < \u221e.\n(C4) There exist constants C1 and C2 such that 0 < C1 \u2264min\nj\nn\u22121\nn\nX\ni=1\nX2\nij < max\nj\nn\u22121\nn\nX\ni=1\nX2\nij \u2264C2 <\n\u221e.\n(C5) There exists a constant \u03be0 such that mink\u2208S |\u02dc\u03benj| \u2265\u03be0 > 0 for all n.\nConditions (C1)-(C2) are standard in the high dimensional measurement error literature; e.g these con-\nditions were used in Rosenbaum et al. (2013) and Belloni et al. (2017). Condition (C3) requires that all\nthe true non-zero components of \u03b20 are bounded away from zero and in\ufb01nity. Condition (C4) assumes that\neach true covariate is well-controlled; in particular, the lower bound implies that the \u201csignal\u201d part on each\ncovariate is not too small (for example, the true covariate matrix X should not be too sparse). With the\nabove conditions, the PMSc procedure can achieve screening consistency, as made precise in Theorem 1.\nTheorem 1. Consider the linear EIV model (1) under conditions (C1)-(C5), and the screening set \u02c6QPMSc =\n{j : \u02c6\u03b2j \u0338= 0}, where \u02c6\u03b2 minimizes (2) with the tuning parameter \u03bbn \u21920 and the number of important\nvariables s = o(n). Then, we have P( \u02c6QPMSc \u2287S) \u21921, meaning all the important variables are included\nin the screening set with probability tending to one as n \u2192\u221e.\n6\nThe conditions for screening consistency as stated in Theorem 1 are relatively weak. Indeed, it is easy to\nsee that setting the tuning parameter \u03bbn = 0, such as in the case of SISc, always guarantees screening con-\nsistency, although no dimension reduction is achieved in this case. If the true important and non-important\ncovariates are weakly correlated, as formalized in the condition (C6) below, then the PMSc procedure can\nachieve full selection consistency, meaning that the screening set contains important variables only. Specif-\nically, we assume the following additional conditions.\n(C6) There exists constants d0 > 0 and 0 \u2264\u03b8 \u22641/2 such that\n\f\f\f\f\fn\u22121\nn\nX\ni=1\nXijXik\n\f\f\f\f\f \u2264d0n\u2212\u03b8, j \u2208Sc, k \u2208\nS.\n(C7) Assume the tuning parameter \u03bbn satis\ufb01es:\n(a) \u03bbn \u21920 and \u03bbnn\u03b8(2\u2212\u03b1)s\u03b1\u22122 \u2192\u221e,\n(b) log (2m) = o(1) \u00d7\n\b\n\u03bbnn(2\u2212\u03b1)/2\t1/(2\u2212\u03b1) , where m = p \u2212s.\nCondition (C6) is similar to the partial orthogonality condition of Huang et al. (2008), which is necessary to\nestablish the full variable selection consistency of the penalized marginal bridge estimator when no measure-\nment error is present. Such a condition is also necessary when measurement errors are present. Condition\n(C7a) implies that the number of important variables s is of order o(n\u03b8) (hence is of order o(n1/2) at most),\nwhile condition (C7b) implies the number of zero coef\ufb01cients m = p \u2212s can be of order o(exp(n1/2)). As\na result, the number of variables p is allowed to grow at an exponential rate with n1/2, while the number of\nimportant variables s grows at a comparably slower rate. These two conditions also control the behavior of\nthe tuning parameter \u03bbn that should be used for PMSc to achieve full variable selection consistency.\nTheorem 2. Consider the linear EIV model (1) under conditions (C1)-(C7), and the screening set \u02c6QPMSc =\n{j : \u02c6\u03b2j \u0338= 0}. Then we have P( \u02c6QPMSc = S) \u21921, meaning the screening set \u02c6QPMSc contains all and only\nthe important variables as n \u2192\u221e.\nNote that in both Theorem 1 and Theorem 2, we do not make any assumption on the structure of the\ncovariance matrix \u03a3u, except that the measurement error variance on each covariate is bounded for all j =\n1, . . . , p, as formalized in condition (C2). In Appendix C, we demonstrate empirically that the conclusion\nfrom Theorem 2 holds even when the measurement error among the covariates is highly correlated.\nIn practice, since the true covariate matrix X is not observed, condition (C6) is hard to verify, and even\nwhen it is assumed to hold, we stress that the theoretical choice of \u03bbn in condition (C7) depends on unknown\nquantities. Therefore, in practice, for the proposed PMSc procedure, we use cross-validation to select the\ntuning parameter \u03bbn, where the loss on test data is computed based on the loss part of (2). This procedure\nwill be referred to as PMSc cross-validation (PMScCV) and is further elaborated upon in Subsection 3.2.\nAlternatively, we can start with a suf\ufb01ciently large value for \u03bbn such that all the coef\ufb01cients are set to zero,\nthen incrementally decrease \u03bbn until a certain number of covariates, M, is included. Note that with this\nversion of PMSc, rather than having to choose the tuning parameter \u03bbn, we instead choose M, the desired\nnumber of covariates to keep from the screening procedure. This strategy will be referred to as PMSc\n7\nforward stepwise (PMScFS). As demonstrated in Appendix C, when the true model is sparse as is assumed\nin our high dimensional settings, in order to retain all the important variables, M can be substantially smaller\nthan the number of covariates d to keep in the SISc approach, and hence also be substantially smaller than\nthe sample size n.\n2.2\nCorrected sure independence screening\nBecause SISc corresponds to the minimizer of (2) with \u03bbn = 0, it screens out important variables by keeping\nonly the d components with largest magnitude among {|\u02dc\u03b21|, . . . , |\u02dc\u03b2p|} as formalized by \u02c6QSISc in (4). In this\nsection, we prove that under certain conditions, the scalar d can be chosen such that we can reduce the\nnumber of dimensions to below the sample size n and still retain all the important variables. To achieve this,\nwe follow the theoretical development of SIS in the linear model with no measurement error by Fan and Lv\n(2008), and aim to theoretically set d = \u230a\u03b3n\u230b, where \u03b3 can be of order n\u2212\u02dc\u03b8 for some 0 < \u02dc\u03b8 < 1. In addition\nto the conditions (C1)-(C5) for screening consistency as in Theorem 1, we impose the following additional\nassumptions.\n(C8) p > n, log(p)/n \u2192C1 with C1 being a constant as both n and p diverge to \u221e.\n(C9) Let Z = U \u03a3\u22121/2\nu\n. Then,\n(a) For some constants c1, c2 > 1 and D > 0, the matrix Z follows a spherical distribution satisfying\nP\nn\n\u03bbmax\n\u0010\n\u02dcp\u22121\u02dcZeZ\u22a4\u0011\n> c2 and \u03bbmin\n\u0010\nep\u22121\u02dcZ\u02dcZ\u22a4\u0011\n< 1/c2\no\n\u2264e\u2212Dn, for any n \u00d7 \u02dcp submatrix \u02dcZ\nof Z with c1n \u2264\u02dcp \u2264p.\n(b) There exists positive constants \u03c41 > 0 and c3 > 0, such that \u03bbmax(\u03a3u) \u2264c3n\u03c41.\n(C10) There exists positive constants c4 > 0 and \u03c42 > 0, such that \u03bbmax\n\u0000n\u22121 X\u22a4X\n\u0001\n\u2264c4n\u03c42.\n(C11) \u03c41 + \u03c42 + logn(s) < 1, where logn(s) denotes the logarithm base n of s.\nCondition (C8) implies that we can allow the number of covariates p to grow exponentially with the sample\nsize n. Condition (C9a) is referred to as the Concentration Property by Fan and Lv (2008), meaning that\nwith large probability, the n nonzero singular values of the n \u00d7 \u02dcp submatrix \u02dcZ of Z are of the same order;\nFan and Lv (2008) suggested that a suf\ufb01cient condition for (C9a) is that each row Ui follows a p-variate\nGaussian distribution. Conditions (C9b) and (C10) indicate that the maximum eigenvalues of the covariance\nmeasurement error matrix \u03a3u and of the scaled Gram matrix n\u22121 X\u22a4X can only grow polynominally with\nthe sample size n. Furthermore, condition (C11) restricts the degree of that polynomial growth to be less\nthan one, which rules out the case when either true covariates or measurement errors are highly correlated.\nCondition (C11) also implies that the number of important covariates s is smaller than the sample size n,\nso that reducing the dimension to below n is reasonable. Compared with the theoretical conditions used to\nestablish screening and full variable selection consistency of PMSc (Theorems 1 and 2), conditions (C8)-\n(C11) impose stricter conditions on the distribution and covariance matrix of measurement error, but allow\nthe number of covariates p to grow at a faster rate.\n8\nWith the conditions above, we establish the following theorem regarding screening consistency of the\nproposed SISc procedure.\nTheorem 3. Consider the linear EIV model (1) under conditions (C1)-(C5) and (C8)-(C11), and the screen-\ning set \u02c6QSISc de\ufb01ned in (4) with d = \u230a\u03b3n\u230band \u02dc\u03b2j given by (3). Then there exists some \u02dc\u03b8 < 1 \u2212\u03c41 \u2212\u03c42 \u2212\nlogn(s) such that when \u03b3 \u223ccn\u2212\u02dc\u03b8 with c > 0, we have P\n\u0010\n\u02c6QSISc \u2287S\n\u0011\n= 1 \u2212O (p exp (\u2212Cn)) for some\npositive constant C.\nTheorem 3 implies that the SISc procedure can reduce exponentially high dimension to a relatively small\ndimension d = O(n1\u2212\u02dc\u03b8) < n while retaining all the important covariates. In practice, as suggested by Fan\nand Lv (2008), common choices for d include d = \u230an/ log(n)\u230bor d = n \u22121. Finally, unlike PMSc, a\nscreening procedure based on ranking components such as SISc can only achieve full selection consistency\nif we know the true number of important covariates s in advance. Since that is rarely the case in practice,\nthen we do not study the theoretical conditions under which we can choose d = s.\n3\nSimulation studies\n3.1\nSimulation setup\nWe conducted simulation studies to demonstrate the bene\ufb01t of the proposed screening procedures by com-\nparing the performance of two-stage estimators, which screen variables in the \ufb01rst stage and compute a\ncorrected penalized estimator on the retained variables in the second stage, against one-stage estimators,\nwhich do not employ any screening method. We simulated data from model (1), where each row of the true\ncovariate matrix Xi was generated from a p-variate Gaussian distribution with a zero mean vector and two\nchoices for the covariance matrix \u03a3x. In the \ufb01rst scenario, \u03a3x had an autoregressive AR(1) structure with\nelements \u03c3ij = \u03c1|i\u2212j|\nx\n. In the second scenario, \u03a3x had an homogeneous (exchangeable) structure where\nall diagonal and off-diagonal elements were set to 1 and \u03c1x correspondingly. For both scenarios, we var-\nied \u03c1x \u2208{0.3, 0.5}. The true p-dimensional vector \u03b20 was constructed such that the \ufb01rst s = 5 non-zero\ncomponents were generated from the uniform distribution U(1, 1.5). Turning to the measurement errors,\nwe generated Ui independently of Xi from another p-variate Gaussian distribution with a zero mean vector\nand two choices of the covariance matrix \u03a3u. For the \ufb01rst choice, \u03a3u was a diagonal matrix with elements\nrandomly generated from the uniform U(0.1, 0.5) distribution; hence, the measurement errors on each co-\nvariate were independent of those of the other covariates with the noise-to-signal ratio ranging from 10%\nto 50%. For the second choice, \u03a3u had a block diagonal structure, where the p covariates were divided\ninto p/4 non-overlapping groups of size 4 with the correlation between any pair in the same group equal\nto 0.2, and the diagonal elements of \u03a3u equal to 0.4; hence, the measurement errors on one covariate were\npositively correlated with those on three other covariates with the noise-to-signal ratio being 40%. We set\nthe sample size to n = 500, and varied the number of covariates p \u2208{1000, 2000}. Finally, the elements\nof the model error term \u03b5 were independently generated from a Gaussian distribution with mean zero and\nvariance \u03c32 = 0.25.\n9\n3.2\nEstimators and performance metrics\nWe computed several one-stage and two-stage estimators on each simulated dataset. For one-stage estima-\ntors, we implemented the corrected lasso estimator of Loh and Wainwright (2012) and S\u00f8rensen et al. (2015),\nand the convex conditioned (CoCo) lasso of Datta et al. (2017). Without using any screening procedure, the\none-stage corrected lasso is given by\n\u02c6\u03b2CL = argmin\n\u03b2\n(\n1\nn\nn\nX\ni=1\n(yi \u2212W\u22a4\ni \u03b2)2 \u2212\u03b2\u22a4\u03a3u\u03b2 + \u00b5\u2225\u03b2\u22251\n)\n, subject to \u2225\u03b2\u22251 \u2264R,\n(5)\nwith \u00b5 and R being two positive tuning parameters. As noted in Loh and Wainwright (2012), the problem\n(5) is generally non-convex when p > n, because the matrix n\u22121 Pn\ni=1 W\u22a4\ni Wi \u2212\u03a3u has a large number\nof negative eigenvalues and is not positive de\ufb01nite. Therefore, classic gradient descent algorithms, as out-\nlined in Loh and Wainwright (2012) and S\u00f8rensen et al. (2015), are only guaranteed to converge to a local\nminimum under a careful choice of the tuning parameters \u00b5 and R. S\u00f8rensen et al. (2015) suggested that\nthe tuning parameter \u00b5 be chosen as the minimum of the ten-fold cross-validation curve of the naive lasso\n\u02c6\u03b2naive = argmin\n\u03b2\n(\n1\nn\nn\nX\ni=1\n(yi \u2212W\u22a4\ni \u03b2)2 + \u00b5\u2225\u03b2\u22251\n)\n,\nand R be chosen by another ten-fold cross-validation from a grid of equally spaced values between 10\u22123\u03ba\nand \u03ba, with \u03ba = 2\u2225\u02c6\u03b2naive\u22251. The one-stage CoCo lasso estimator is given by\n\u02c6\u03b2CoCo = argmin\n\u03b2\n\u001a1\n2 \u03b2\u2032 e\u03a3 \u03b2 \u2212\u02dc\u03c1\u22a4\u03b2 +\u02dc\u00b5\u2225\u03b2 \u22251\n\u001b\n,\nwhere \u02dc\u03c1 = W\u22a4y and e\u03a3 = argmin\n\u03a3\u2208Rp\u00d7p\n+\n\u2225\u03a3 \u2212b\u03a3\u2225max, with b\u03a3 = n\u22121W\u22a4W \u2212\u03a3u and Rp\u00d7p\n+\nthe set of p \u00d7 p\npositive semi-de\ufb01nite matrices. In other words, e\u03a3 is the nearest positive semi-de\ufb01nite matrix to \u02c6\u03a3 measured\nby the element-wise max norm, which was computed by an alternating direction method of multipliers\n(ADMM) algorithm. We point out that this element-wise max norm makes computation of e\u03a3 so expensive\nwhen p is large that in our simulation, we only computed the CoCo lasso estimator if the number of variables\nwas smaller than 1000. The non-negative tuning parameter \u02dc\u00b5 for the CoCo lasso estimator was selected via\na corrected cross-validation procedure; see Datta et al. (2017) and Datta and Zou (2020) for more details.\nFor two-stage estimators, we \ufb01rst implemented either PMScCV, PMScFS, or SISc, and then applied\neither the corrected lasso or CoCo lasso to the covariates selected from the \ufb01rst stage. For the two versions\nof PMSc, we set \u03b1 = 0.5 as is commonly done when the bridge penalty is used in practice (Huang et al.,\n2008, 2009; Polson et al., 2014). For PMScCV, we selected the tuning parameter \u03bbn via the following\n\ufb01ve-fold cross-validation procedure: on each simulated dataset, the data were randomly split into 5 folds\nF1, . . . , F5 of equal sizes; for the kth iteration, the fold Fk was left out to be the test set and PMSc was\napplied on the remaining 4 folds on a grid consisting of 40 equally spaced values of the tuning parameter\n\u03bbn. Let \u02c6\u03b2\n(\u2212k)\n\u03bb\n= (\u02c6\u03b2(\u2212k)\n1,\u03bb , . . . , \u02c6\u03b2(\u2212k)\np,\u03bb )\u22a4denote the solution of (2) when Fk was left out. Then the \ufb01nal \u03bbn\n10\nwas selected to minimize\n5\nX\nk=1\n\uf8f1\n\uf8f2\n\uf8f3\nX\ni\u2208Fk\np\nX\nj=1\n\u0010\nyi \u2212\u02c6\u03b2(\u2212k)\nj,\u03bb\n\u02dcWij\n\u00112\n\u2212|Fk|\np\nX\nj=1\n\u0010\n\u02c6\u03b2(\u2212k)\nj,\u03bb\n\u00112\n\u03c32\nj\n\uf8fc\n\uf8fd\n\uf8fe,\nwhere |Fk| denotes the cardinality of the fold Fk. For the PMScFS and SISc, the number of covariates to be\nincluded and kept was chosen to be M = d = \u230an/ log(n)\u230b, respectively. This choice of d and M follows\na common practice for screening procedures in high dimensional settings (Fan and Lv, 2008; Zhu et al.,\n2011; Cui et al., 2015). The two-stage estimator when PMScCV and corrected lasso are used in the \ufb01rst and\nthe second stage respectively is referred to as the PMScCV-Corrected estimator. Similar de\ufb01nitions hold for\nother estimators resulting from other combinations of the methods used in the \ufb01rst and second stage. Also,\nwe refer to all the estimators that used SISc in the \ufb01rst stage as SISc-based estimators; similar de\ufb01nitions\nare applied to the estimators that used another screening procedure in the \ufb01rst stage.\nWe report false positive rate (FPR) and false negative rate (FNR) of the screening procedures, and\ncompare all the one-stage and two-stage estimators based on FPR, FNR, and \u21132 estimation error (i.e the\n\u21132 norm of the difference between an estimate and the true coef\ufb01cient vector). All metrics were averaged\nacross 500 simulations. We also report the mean total computation time for each estimator. All the reported\ntimes are based on implementation on Artemis, a high performance computing cluster at the University of\nSydney where every sample was computed on a single core (Intel(R) Xeon(R) CPU E5-2697A v4 2.60GHz\nand 4GB RAM).\n3.3\nSimulation results\nTables 1 and 2 present the summary results for the simulations with \u03c1x = 0.5. The results for other settings\nshow similar trend; complete results, including the standard errors and separation of total computation time\ninto the \ufb01rst and second stages for all the settings, can be found in Appendix D.\nWhen only a few true covariates are correlated with one another (i.e \u03a3x had an AR(1) structure), Table 1\nshows that all the screening procedures (\ufb01rst stage) were able to keep all the important variables and reduce\nthe number of dimensions effectively. The PMScCV method had FPR and FNR closest to zero; as a result,\nafter the second stage, the PMScCV-based estimators had the lowest estimation error. The PMScFS and SISc\nscreening procedures had zero FNR but some false positives, which were expected from further investigation\nof Figure 1 in Appendix C, where we demonstrate that the choice of d and M in this simulation was greater\nthan the minimum number of variables that PMScFS and SISc needed to include in order to retain all the\nimportant variables. However, after the second stage, the FPRs of the two-stage PMScFS and SISc-based\nestimators were reduced to close to zero, and notably lower than the FPRs of the corresponding one-stage\nestimators; the gain was more pronounced when the CoCo lasso was used in the second stage. In turn,\nthe estimation errors of the two-stage estimators were also lower than those of the one-stage estimators,\nwith the improvement increasing when p increased from 1000 to 2000. When \u03a3u was diagonal, there was\nlittle difference in the \u21132 error of the two-stage estimators; however, when \u03a3u was block-diagonal, the\nSISc-based estimators tended to perform worse than the PMScFS and PMScCV-based estimators. Regarding\n11\ncomputation time, among the three screening methods, SISc was the fastest to compute, followed by PMScFS\nand PMScCV. As a whole, the total computation times of the two-stage estimators were considerably lower\nthan those of the one-stage estimators; further investigation of Table C.1 in Appendix C shows that the\nscreening stage substantially decreased the time needed for the second stage estimation. Finally, when p\nincreased from p = 1000 to p = 2000, there was a remarkable increase in the computation time of the one-\nstage estimators; however, the corresponding increases for the two-stage estimators were small, re\ufb02ecting\ntheir computational scalability.\nWhen all the true covariates were highly correlated with each other (i.e \u03a3x had a homogeneous struc-\nture), Table 2 shows that PMScCV was not helpful in either improving performance or reducing computation\ntime. For the \ufb01rst step, PMScCV kept all the variables, meaning no dimension reduction was achieved. As a\nresult, the two-stage PMScCV-based estimators had essentially the same performance as the corresponding\none-stage estimators. Additionally, PMScFS and SISc had similar and much lower FPRs than PMScCV, with\nthe former tending to have lower FNR than the latter. As further demonstrated in Web Figure 1 of Appendix\nC, these FNRs were expected because the minimum number of variables SISc had to keep in order to retain\nall the important variables was much higher than that of PMScFS. After the second stage, the FPRs of all\nthe PMSc-based (both versions) and SISc-based estimators were much reduced, while the corresponding\nFNRs were unchanged. Regarding estimation error, the two-stage PMScFS and SISc-based estimators had\nnoticeably lower estimation errors than the corresponding one-stage estimators, with the improvement being\ngreater when p increased from 1000 to 2000. When \u03a3u was diagonal, the improvement was considerable\nfor both the corrected and CoCo lasso. In contrast, when \u03a3u was block-diagonal, the most remarkable\nimprovement was made for the CoCo lasso only. In all the settings, the PMScFS-CoCo estimators had the\nsmallest \u21132 estimation error. Finally, regarding computation time, it was not surprising that the SISc was\nstill computationally the fastest screening method. The two-stage PMScFS-based estimators were also fast\nto compute, and when p increased to 2000, the increase in computation time for PMSc and SISc was small\ncompared to that of the one-stage estimators.\nIn summary, the simulation study both con\ufb01rms the theoretical results for the screening procedures\nwhen the true covariates are not highly correlated, and demonstrates the superior \ufb01nite sample performance,\ncomputational ef\ufb01ciency and scalability of the proposed two-stage estimators compared to the one-stage\nestimators. Among the three screening methods implemented in the simulations, the PMScFS was the most\nreliable and ef\ufb01cient screening method, taking account of both performance metrics and computation time\nwhen the true covariates were either moderately or highly correlated.\n12\nTable 1: Performance of the one-stage and two-stage estimators in the simulation study based on mean false\npositive rate (FPR, in percentage), false negative rate (FNR, in percentage), \u21132 error and computation time\n(in seconds) when \u03a3x has an AR(1) structure with autocorrelation \u03c1x = 0.5. The CoCo estimator was only\ncomputed when the number of variables was no more than 1000 (either in 1st or 2nd step).\n\u03a3u\np\nEstimator\n1st step\n2nd step\nTime\nFPR\nFNR\nFPR\nFNR\n\u21132\nDiagonal\n1000\nOne-stage Corrected\n-\n-\n2.2\n0.0\n0.35\n49.7\nPMScCV-Corrected\n0.1\n0.0\n0.1\n0.0\n0.26\n23.8\nPMScFS-Corrected\n7.5\n0.0\n0.0\n0.0\n0.28\n7.0\nSISc-Corrected\n7.5\n0.0\n1.2\n0.0\n0.33\n3.5\nOne-stage CoCo\n-\n-\n7.6\n0.0\n0.47\n516.8\nPMScCV-CoCo\n0.1\n0.0\n0.1\n0.0\n0.26\n23.4\nPMScFS-CoCo\n7.5\n0.0\n0.0\n0.0\n0.27\n6.1\nSISc-CoCo\n7.5\n0.0\n0.1\n0.0\n0.31\n2.2\n2000\nOne-stage Corrected\n-\n-\n1.3\n0.0\n0.36\n136.5\nPMScCV-Corrected\n0.0\n0.0\n0.0\n0.0\n0.26\n38.2\nPMScFS-Corrected\n3.8\n0.0\n0.6\n0.0\n0.29\n20.0\nSISc-Corrected\n3.8\n0.0\n0.6\n0.0\n0.33\n4.5\nPMScCV-CoCo\n0.0\n0.0\n0.0\n0.0\n0.26\n38.0\nPMScFS-CoCo\n3.8\n0.0\n0.1\n0.0\n0.27\n18.0\nSISc-CoCo\n3.8\n0.0\n0.1\n0.0\n0.31\n3.5\nBlock\ndiagonal\n1000\nOne-stage Corrected\n-\n-\n1.8\n0.0\n0.62\n27.8\nPMScCV-Corrected\n0.0\n0.0\n0.0\n0.0\n0.43\n23.8\nPMScFS-Corrected\n7.5\n0.0\n0.0\n0.0\n0.48\n7.2\nSISc-Corrected\n7.5\n0.0\n1.0\n0.0\n0.57\n4.1\nOne-stage CoCo\n-\n-\n7.4\n0.0\n0.74\n417.4\nPMScCV-CoCo\n0.0\n0.0\n0.0\n0.0\n0.43\n23.4\nPMScFS-CoCo\n7.5\n0.0\n0.0\n0.0\n0.46\n6.2\nSISc-CoCo\n7.5\n0.0\n0.1\n0.0\n0.54\n3.2\n2000\nOne-stage Corrected\n-\n-\n1.0\n0.0\n0.64\n68.0\nPMScCV-Corrected\n0.0\n0.0\n0.0\n0.0\n0.44\n38.2\nPMScFS-Corrected\n3.8\n0.0\n0.5\n0.1\n0.46\n19.5\nSISc-Corrected\n3.8\n0.0\n0.5\n0.0\n0.57\n5.1\nPMScCV-CoCo\n0.0\n0.0\n0.0\n0.0\n0.45\n37.9\nPMScFS-CoCo\n3.8\n0.0\n0.0\n0.0\n0.46\n18.4\nSISc-CoCo\n3.8\n0.0\n0.1\n0.0\n0.54\n4.2\n13\nTable 2: Performance of the one-stage and two-stage estimators in the simulation study based on false\npositive rate (FPR, in percentage), false negative rate (FNR, in percentage) \u21132 error, and computation time\n(in seconds) when \u03a3x has an homogeneous structure with \u03c1x = 0.5. The CoCo estimator was only computed\nwhen the number of variables was no more than 1000 (either in 1st or 2nd step).\n\u03a3u\np\nEstimator\n1st step\n2nd step\nTime\nFPR\nFNR\nFPR\nFNR\n\u21132\nDiagonal\n1000\nOne-stage Corrected\n-\n-\n20.0\n0.1\n0.96\n18.9\nPMScCV-Corrected\n100.0\n0.0\n19.7\n0.2\n0.95\n31.4\nPMScFS-Corrected\n7.6\n3.2\n1.6\n3.2\n0.56\n6.8\nSISc-Corrected\n7.6\n6.7\n1.0\n6.7\n0.54\n2.2\nOne-stage CoCo\n-\n-\n11.5\n0.0\n0.77\n481.5\nPMScCV-CoCo\n100.0\n0.0\n11.5\n0.0\n0.77\n489.4\nPMScFS-CoCo\n7.6\n3.2\n1.0\n3.2\n0.45\n7.2\nSISc-CoCo\n7.6\n6.7\n1.0\n6.7\n0.52\n2.9\n2000\nOne-stage Corrected\n-\n-\n29.1\n1.1\n1.57\n49.9\nPMScCV-Corrected\n100.0\n0.0\n28.3\n1.0\n1.53\n74.5\nPMScFS-Corrected\n3.8\n4.0\n0.7\n4.0\n0.62\n9.6\nSISc-Corrected\n3.8\n11.0\n0.5\n11.0\n0.61\n3.6\nPMScCV-CoCo\n100.0\n0.0\n-\n-\n-\n-\nPMScFS-CoCo\n3.8\n4.0\n0.5\n4.0\n0.45\n10.2\nSISc-CoCo\n3.8\n11.0\n0.5\n11.0\n0.60\n4.6\nBlock\ndiagonal\n1000\nOne-stage Corrected\n-\n-\n23.5\n1.1\n1.25\n22.7\nPMScCV-Corrected\n100.0\n0.0\n25.6\n0.8\n1.30\n37.0\nPMScFS-Corrected\n7.6\n3.4\n3.0\n3.6\n1.15\n6.7\nSISc-Corrected\n7.6\n14.2\n1.5\n14.2\n0.97\n2.4\nOne-stage CoCo\n-\n-\n14.3\n0.4\n1.23\n417.2\nPMScCV-CoCo\n100.0\n0.0\n14.3\n0.5\n1.23\n427.9\nPMScFS-CoCo\n7.6\n3.4\n0.9\n3.6\n0.68\n8.2\nSISc-CoCo\n7.6\n14.2\n0.9\n14.2\n0.84\n3.6\n2000\nOne-stage Corrected\n-\n-\n32.4\n3.0\n1.86\n52.5\nPMScCV-Corrected\n100.0\n0.0\n31.8\n3.3\n1.84\n79.8\nPMScFS-Corrected\n3.8\n6.6\n1.6\n6.6\n1.21\n9.4\nSISc-Corrected\n3.8\n21.4\n0.8\n21.4\n1.09\n3.8\nPMScCV-CoCo\n100.0\n0.0\n-\n-\n-\n-\nPMScFS-CoCo\n3.8\n6.6\n0.5\n6.7\n0.73\n10.8\nSISc-CoCo\n3.8\n21.4\n0.5\n21.4\n0.97\n5.3\n14\n4\nAnalysis of microarray data\nWe applied the proposed methodology to analyze the motivating Bone Mineral Density Data consisting of\ngene expression measurements of 54, 675 probe sets and bone mineral density (BMD) for n = 84 Norwegian\nwomen. The dataset is publicly available at the European Bioinformatics Institute ArrayExpress repository\nunder the access number E-MEXP-1618. Microarray measurements are known to be noisy (Rocke and\nDurbin, 2001); although biological variation in the data is usually of primary interest to investigators, it can\nbe obscured by measurement errors coming from many sources (Zakharkin et al., 2005). Furthermore, a\ndistinctive feature of the Affymetrix microarray dataset is that multiple probes are used to measure each\ngene expression; hence, these replicated measurements can be used to estimate the covariance matrix of\nmeasurement errors \u03a3u.\nWe followed the procedures described in S\u00f8rensen et al. (2015), Nghiem and Potgieter (2019) and\nRomeo and Thoresen (2019) to process the raw data using the BGX package of Hein et al. (2005), and\nassumed the measurement error on each gene was mutually independent from that on the other. As a result,\nthe measurement error covariance matrix \u03a3u was set to be diagonal. Similar to S\u00f8rensen et al. (2015), we\nalso kept p = 993 genes that had between-patient variability greater than measurement error variance, i.e\nthe noise-to-signal ratio was smaller than 1. After processing, we obtained the surrogate matrix W along\nwith \u03a3u. More details on the raw data processing and the estimated covariance matrix of measurement error\ncan be found in Appendix B. The response variable was chosen to be the (centered) total hip T-score.\nWe computed the eight estimators as in the simulation study. For the screening step, the PMScCV\nprocedure selected 610 genes, so it did not reduce the number of dimensions by any great amount. This\nwas likely due to the high correlation among the gene expressions, similar to the case when \u03a3x had a\nhomogeneous structure in the simulation. Table 3 shows the number of genes selected by one-stage and\ntwo-stage estimators, as well as the \u21132 norm of the estimated coef\ufb01cients. It can be seen that while the\none-stage corrected lasso and PMScCV-Corrected estimators select many more genes than all the other\nestimators, the \u21132 norms of the corresponding estimated coef\ufb01cients were smaller or equivalent to that of the\nother estimators; hence, the one-stage corrected lasso and PMScCV-Corrected estimators likely contained\nmany false positives. Among the p = 993 genes, there were 15 genes selected by one estimator, 113 genes\nselected by two estimators, 1 gene selected by 3 estimators, 1 gene selected by 4 estimators, and notably 3\ngenes selected by 6 estimators. Table 4 demonstrates that the estimated coef\ufb01cients associated with these\nthree genes had larger magnitude than those of other selected genes. Furthermore, compared to the one-\nstage CoCo lasso and PMScCV-CoCo estimators, the two-stage PMScFS-CoCo and SISc-CoCo estimators\nmagni\ufb01ed the effects of these three genes. These estimators might contain some false negatives, but they\nhelp practitioners obtain stronger signals of the important variables that are present, as has been noted in\nthe literature of measurement error correction in high dimensional settings; see S\u00f8rensen et al. (2015) and\nNghiem and Potgieter (2019). Finally, regarding computation time, the two-stage estimators were generally\nfaster to compute, with the exception of the PMScCV-based estimators; the gain in computation time was\nthe largest when the CoCo estimator was used in the second stage.\n15\nTable 3: The number of selected genes and the corresponding \u21132 norm of the estimated coef\ufb01cients obtained\nfrom each estimator in the microarray data analysis. Computation time is based on implementation on a\nlaptop with one Dual-Core Intel Core i5 2.7GHz processor and 8GB RAM\nEstimator\n# of selected genes\n\u21132 norm\nTime (in seconds)\n1st step\n2nd step\n1st step\n2nd step\nOne-stage Corrected\n-\n108\n0.33\n0.00\n85.97\nPMScCV-Corrected\n798\n104\n0.34\n74.02\n43.48\nPMScFS-Corrected\n19\n4\n2.10\n6.54\n2.49\nSISc-Corrected\n19\n4\n2.00\n0.73\n2.49\nOne-stage CoCo\n-\n18\n0.76\n0.00\n2702.81\nPMScCV-CoCo\n798\n17\n0.78\n74.02\n1648.86\nPMScFS-CoCo\n19\n6\n1.05\n6.54\n0.21\nSISc-CoCo\n19\n5\n1.12\n0.73\n0.30\nTable 4: The estimated coef\ufb01cients and their corresponding rank in terms of magnitude for the three genes\nmost frequently selected by most estimators.\nEstimator\nEstimate\nRank (1 = largest)\nGene 1\nGene 2\nGene 3\nGene 1\nGene 2\nGene 3\nPMScFS-Corrected\n0.96\n-1.54\n0.62\n2\n1\n4\nSISc-Corrected\n0.86\n-1.43\n0.63\n2\n1\n4\nOne-stage CoCo\n0.61\n-0.25\n0.27\n1\n3\n2\nPMScCV-CoCo\n0.62\n-0.28\n0.29\n1\n3\n2\nPMScFS-CoCo\n0.78\n-0.47\n0.29\n1\n2\n4\nSISc-CoCo\n0.49\n-0.94\n0.18\n2\n1\n4\n5\nConclusion\nThis paper proposes two screening procedures for linear errors-in-variables models in high dimensional\nsettings, namely the corrected penalized marginal regression and the corrected sure independence screening\nprocedures. Both procedures are based on \ufb01tting corrected marginal regressions of the outcome on each\ncontaminated covariate, which could be computed ef\ufb01ciently in high dimensions. Under mild technical\nconditions, these procedures are shown to achieve screening consistency, meaning all the important variables\nare fully retained. Under a stronger condition of partial orthogonality among the true covariates, we further\nillustrate that the corrected penalized marginal screening approach (using the bridge penalty) can achieve full\nselection consistency. We demonstrated the advantages of these screening procedures in practice through a\nsimulation study and an analysis of a microarray data concerning gene expressions of bone mineral density\nfor Norweigian women, both of which showed that the proposed screening procedures reduced computation\ntime considerably and/or improved performance metrics for both estimation and variable selection.\nFuture research could aim at reducing the false negative rates for screening procedures when the true\ncovariates are highly collinear; for such a setting, an iterative corrected marginal screening procedure sim-\n16\nilar to the iterative sure independent screening of Fan and Lv (2008) may be considered. Furthermore,\nthe screening procedures presented in this paper can be extended to more complicated errors-in-variables\nmodels, such as generalized linear models and non-parametric regression settings, although it may be more\nchallenging to correct for measurement errors in marginal regressions of the outcome on each contaminated\ncovariate in these models.\nReferences\nBarut, E., Fan, J., and Verhasselt, A. (2016). Conditional sure independence screening. Journal of the\nAmerican Statistical Association, 111(515):1266\u20131277.\nBelloni, A., Rosenbaum, M., and Tsybakov, A. B. (2017). Linear and conic programming estimators in high\ndimensional errors-in-variables models. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology), 79(3):939\u2013956.\nBrown, B., Weaver, T., and Wolfson, J. (2019). Meboost: Variable selection in the presence of measurement\nerror. Statistics in Medicine, 38(15):2705\u20132718.\nByrd, M. and McGee, M. (2019). A simple correction procedure for high-dimensional general linear models\nwith measurement error. arXiv preprint arXiv:1912.11740.\nCarroll, R. J., Ruppert, D., Stefanski, L. A., and Crainiceanu, C. M. (2006). Measurement Error in Nonlinear\nModels: A Modern Perspective. CRC press.\nCui, H., Li, R., and Zhong, W. (2015). Model-free feature screening for ultrahigh dimensional discriminant\nanalysis. Journal of the American Statistical Association, 110(510):630\u2013641.\nDatta, A. and Zou, H. (2020). A note on cross-validation for lasso under measurement errors. Technometrics,\n62(4):549\u2013556.\nDatta, A., Zou, H., et al. (2017). Cocolasso for high-dimensional error-in-variables regression. The Annals\nof Statistics, 45(6):2400\u20132426.\nDo, C. B., Tung, J. Y., Dorfman, E., Kiefer, A. K., Drabant, E. M., Francke, U., Mountain, J. L., Goldman,\nS. M., Tanner, C. M., Langston, J. W., et al. (2011). Web-based genome-wide association study identi\ufb01es\ntwo novel loci and a substantial genetic component for parkinson\u2019s disease. PLoS Genet, 7(6):e1002141.\nFan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space. Journal of\nthe Royal Statistical Society: Series B (Statistical Methodology), 70(5):849\u2013911.\nFan, J., Peng, H., et al. (2004). Nonconcave penalized likelihood with a diverging number of parameters.\nThe Annals of Statistics, 32(3):928\u2013961.\nFan, J., Song, R., et al. (2010).\nSure independence screening in generalized linear models with np-\ndimensionality. The Annals of Statistics, 38(6):3567\u20133604.\n17\nFrank, L. E. and Friedman, J. H. (1993). A statistical view of some chemometrics regression tools. Techno-\nmetrics, 35(2):109\u2013135.\nHein, A.-M. K., Richardson, S., Causton, H. C., Ambler, G. K., and Green, P. J. (2005). Bgx: A fully\nBayesian integrated approach to the analysis of Affymetrix Genechip data. Biostatistics, 6(3):349\u2013373.\nHuang, J., Horowitz, J. L., Ma, S., et al. (2008). Asymptotic properties of bridge estimators in sparse\nhigh-dimensional regression models. The Annals of Statistics, 36(2):587\u2013613.\nHuang, J., Ma, S., Xie, H., and Zhang, C.-H. (2009). A group bridge approach for variable selection.\nBiometrika, 96(2):339\u2013355.\nIda, Y., Fujiwara, Y., and Kashima, H. (2019). Fast sparse group lasso. In Wallach, H., Larochelle, H.,\nBeygelzimer, A., d'Alch\u00b4e-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information\nProcessing Systems, volume 32, pages 1702\u20131710.\nKaul, A., Koul, H. L., Chawla, A., and Lahiri, S. N. (2016). Two stage non-penalized corrected least\nsquares for high dimensional linear models with measurement error or missing covariates. arXiv preprint\narXiv:1605.03154.\nLi, G., Peng, H., Zhang, J., Zhu, L., et al. (2012). Robust rank correlation based screening. The Annals of\nStatistics, 40(3):1846\u20131877.\nLoh, P.-L. and Wainwright, M. J. (2012). High-dimensional regression with noisy and missing data: Prov-\nable guarantees with nonconvexity. The Annals of Statistics, 40(3):1637\u20131664.\nNghiem, L. and Potgieter, C. (2019). Simulation-selection-extrapolation: Estimation in high-dimensional\nerrors-in-variables models. Biometrics, 75(0):1133\u20131144.\nPiironen, J., Vehtari, A., et al. (2017). Sparsity information and regularization in the horseshoe and other\nshrinkage priors. Electronic Journal of Statistics, 11(2):5018\u20135051.\nPolson, N. G., Scott, J. G., and Windle, J. (2014). The Bayesian bridge. Journal of the Royal Statistical\nSociety: Series B (Statistical Methodology), 76:713\u2013733.\nRocke, D. M. and Durbin, B. (2001). A model for measurement error for gene expression arrays. Journal\nof Computational Biology, 8(6):557\u2013569.\nRomeo, G. and Thoresen, M. (2019). Model selection in high-dimensional noisy data: a simulation study.\nJournal of Statistical Computation and Simulation, 89(11):2031\u20132050.\nRosenbaum, M., Tsybakov, A. B., et al. (2010). Sparse recovery under matrix uncertainty. The Annals of\nStatistics, 38(5):2620\u20132651.\nRosenbaum, M., Tsybakov, A. B., et al. (2013). Improved matrix uncertainty selector. In From Probability to\nStatistics and Back: High-Dimensional Models and Processes\u2013A Festschrift in Honor of Jon A. Wellner,\npages 276\u2013290. Institute of Mathematical Statistics.\n18\nSimon, N., Friedman, J., Hastie, T., and Tibshirani, R. (2013). A sparse-group lasso. Journal of Computa-\ntional and Graphical Statistics, 22(2):231\u2013245.\nS\u00f8rensen, \u00d8., Frigessi, A., and Thoresen, M. (2015). Measurement error in Lasso: Impact and likelihood\nbias correction. Statistica Sinica, 25:809\u2013829.\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical\nSociety: Series B (Statistical Methodology), 58(1):267\u2013288.\nWainwright, M. J. (2019). High-dimensional Statistics: A Non-Asymptotic Viewpoint, volume 48. Cam-\nbridge University Press.\nWen, C., Pan, W., Huang, M., and Wang, X. (2018). Sure independence screening adjusted for confounding\ncovariates with ultrahigh dimensional data. Statistica Sinica, 28:293\u2013317.\nZakharkin, S. O., Kim, K., Mehta, T., Chen, L., Barnes, S., Scheirer, K. E., Parrish, R. S., Allison, D. B.,\nand Page, G. P. (2005). Sources of variation in Affymetrix microarray experiments. BMC bioinformatics,\n6(1):1\u201311.\nZhou, T., Thung, K.-H., Liu, M., and Shen, D. (2018). Brain-wide genome-wide association study for\nalzheimer\u2019s disease via joint projection learning and sparse regression model. IEEE Transactions on\nBiomedical Engineering, 66(1):165\u2013175.\nZhu, L.-P., Li, L., Li, R., and Zhu, L.-X. (2011). Model-free feature screening for ultrahigh-dimensional\ndata. Journal of the American Statistical Association, 106(496):1464\u20131475.\n19\nAppendix\nA\nTechnical proofs\nWe \ufb01rst recall the technical conditions that are given in the main paper.\n(C1) The model error terms \u03b51, \u03b52, . . . \u03b5n are iid sub-Gaussian random variables with mean zero and \ufb01nite\nvariance \u03c32, i.e there exists a \ufb01nite constant \u03c3\u2217> 0 such that for all t \u2208R,\nE {exp (t\u03b5i)} \u2264exp\n\u0012\u03c32\n\u2217t2\n2\n\u0013\n, i = 1, . . . , n,\nwhere the constant \u03c32\n\u2217is referred to as the \u201cvariance proxy\u201c for \u03b5i.\n(C2) The measurement errors Uij, i = 1, . . . , n are independent sub-Gaussian random variables with\nvariance proxy \u03c32\n\u2217, j = 1, . . . , p.\n(C3) There exist constants b0 and b1 such that 0 < b0 \u2264min\nk\u2208S |\u03b20k| \u2264max\nk\u2208S |\u03b20k| \u2264b1 \u2264\u221e.\n(C4) There exist constants C1 and C2 such that 0 < C1 \u2264min\nj\nn\u22121\nn\nX\ni=1\nX2\nij < max\nj\nn\u22121\nn\nX\ni=1\nX2\nij \u2264C2 <\n\u221e.\n(C5) There exists a constant \u03be0 such that min\nk\u2208S |\u02dc\u03benj| > \u03be0 > 0 for all n.\n(C6) There exists constants d0 > 0 and 0 \u2264\u03b8 \u22641/2 such that\n\f\f\f\f\fn\u22121\nn\nX\ni=1\nXijXik\n\f\f\f\f\f \u2264d0n\u2212\u03b8,\nj \u2208Sc, k \u2208S,\n(C7) Assume the tuning parameter \u03bbn for PMSc satis\ufb01es:\n(a) \u03bbn \u21920 and \u03bbnn\u03b8(2\u2212\u03b1)s\u03b1\u22122 \u2192\u221e,\n(b) log (2m) = o(1) \u00d7\n\b\n\u03bbnn(2\u2212\u03b1)/2\t1/(2\u2212\u03b1) , where m = p \u2212s.\n(C8) p > n, log(p)/n \u2192C1 with C1 being a constant as both n and p diverge to \u221e.\n(C9) Let Z = U \u03a3\u22121/2\nu\n. Then\n(a) For some constants c1, c2 > 1 and D > 0, the matrix Z follows a spherical distribution satisfying\nP\n\u0010\n\u03bbmax\n\u0010\n\u02dcp\u22121\u02dcZeZ\u22a4\u0011\n> c2 and \u03bbmin\n\u0010\nep\u22121\u02dcZ\u02dcZ\u22a4\u0011\n< 1/c2\n\u0011\n\u2264e\u2212Dn,\n(6)\nfor any n \u00d7 \u02dcp submatrix \u02dcZ of Z with c1n \u2264\u02dcp \u2264p.\n20\n(b) There exist some constants \u03c41 > 0 and c3 > 0, such that \u03bbmax(\u03a3u) \u2264c3n\u03c41.\n(C10) There exists positive constants c4 > 0 and \u03c42 > 0, such that \u03bbmax\n\u0000n\u22121 X\u22a4X\n\u0001\n\u2264c4n\u03c42.\n(C11) \u03c41 + \u03c42 + logn(s) < 1, where logn(s) denotes the logarithm base n of s.\nA.1\nProof of Theorem 1\nFirst, we state the following lemma from Huang et al. (2008) that will be used in the proof.\nLemma 1. Let g(u) = u2 \u22122au + \u03bb|u|\u03b1 where a \u0338= 0, \u03bb \u22650 and 0 < \u03b1 < 1. De\ufb01ne\nc\u03b1 =\n\u0012\n2\n2 \u2212\u03b1\n\u0013 \u001a2(1 \u2212\u03b1)\n2 \u2212\u03b1\n\u001b1\u2212\u03b1\n.\nThen arg min(g) = 0 if and only if \u03bb > c\u03b1|a|2\u2212\u03b1.\nAdditionally, we state the following properties of sub-Gaussian random variables from Wainwright\n(2019):\n(B1) Let S1, . . . , Sn be zero-mean independent sub-Gaussian random variables with variance proxy \u03c32\n0.\nDenote v = (v1, . . . , vn) \u2208Rn. Then Pn\ni=1 viSi is sub-Gaussian with variance bounded by\n\u0000Pn\ni=1 v2\ni\n\u0001\n\u03c32\n0.\n(B2) Let S1, . . . , Sn be zero-mean sub-Gaussian random variables (not necessarily independent) with com-\nmon variance proxy \u03c32\n0. Then\nE\n\u0012\nmax\ni=1,...,n |Si|\n\u0013\n\u2264\u03c30\np\n2 log(2n).\n(B3) If S1 and S2 are zero-mean independent sub-Gaussian random variables with \ufb01nite variance proxies\n\u03c32\n01 and \u03c32\n02 respectively, then V = S1S2 is sub-exponential with (\ufb01nite) parameter (\u03bd0, t0), meaning\nthat E(exp(sV )) \u2264exp( 1\n2\u03bd2\n0s2) for all |s| > t0.\n(B4) Let S1, . . . , Sn be sub-exponential random variables (not necessarily independent) with common pa-\nrameter (\u03bd0, t0). Then\nE\n\u0012\nmax\ni=1,...,n |Si|\n\u0013\n\u2264\u03bd0\np\n2 log(2n) + t0 log(2n).\nProof. Recall that,\n\u02dc\u03benj = n\u22121\nn\nX\ni=1\n s\nX\nk=1\nXik\u03b20k\n!\nXij,\n\u03benj = \u02dc\u03benj + n\u22121\nn\nX\ni=1\n s\nX\nk=1\nXik\u03b20k\n!\nUij,\n21\nso we have\nL(\u03b2) = 1\nn\np\nX\nj=1\nn\nX\ni=1\n(yi \u2212Wij\u03b2j)2 \u2212\np\nX\nj=1\n\u03c32\nj \u03b22\nj + \u03bbn\np\nX\nj=1\n|\u03b2j|\u03b1\n=\np\nX\nj=1\nn\nX\ni=1\n1\nn\n s\nX\nk=1\nXik\u03b20k + \u03b5i \u2212Wij\u03b2j\n!2\n\u2212\np\nX\nj=1\n\u03c32\nj \u03b22\nj + \u03bbn\np\nX\nj=1\n|\u03b2j|\u03b1\n=\np\nX\nj=1\n\uf8ee\n\uf8f01\nn\nn\nX\ni=1\n\u03b52\ni +\n \n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj\n!\n\u03b22\nj \u22122\nn\n\u0010\n\u03b5\u22a4aj + n\u03benj\n\u0011\n\u03b2j + 1\nn\nn\nX\ni=1\n s\nX\nk=1\nXik\u03b20k\n!2\n+ \u03bbn |\u03b2j|\u03b1\n\uf8f9\n\uf8fb\nwhere aj = (W1j, . . . , Wnj)\u22a4= (X1j, . . . , Xnj)\u22a4+ (U1j, . . . , Unj)\u22a4= Xj + Uj. We can ignore the\nterms that do not contain \u03b2, so minimizing L(\u03b2) is equivalent to minimizing Pp\nj=1 hj(\u03b2j), where\nhj(\u03b2j) =\n \n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj\n!\n\u03b22\nj \u22122\nn\n\u0010\n\u03b5\u22a4aj + n\u03benj\n\u0011\n\u03b2j + \u03bbn |\u03b2j|\u03b1 ,\nj = 1, . . . , p.\nTo simplify the notation, let Vj = n\u22121 Pn\ni=1 W 2\nij \u2212\u03c32\nj . By Lemma 1, \u02c6\u03b2j = 0 is the only solution of\narg min\u03b2j hj(\u03b2j) if and only if\nV \u22121\nj\n\u03bbn > c\u03b1\nn\n(nVj)\u22121 \f\f\f\u03b5\u22a4aj + n\u03benj\n\f\f\f\no2\u2212\u03b1\n.\nBy some algebra, letting \u03c6n = c\u22121/(2\u2212\u03b1)\n\u03b1\n\u03bb1/(2\u2212\u03b1)\nn\nn1/2, the above inequality is equivalent to\n\u03c6n > n\u22121/2V (\u22121+\u03b1)/(2\u2212\u03b1)\nj\n\f\f\f\u03b5\u22a4aj + n\u03benj\n\f\f\f .\nHence, Theorem 1 will follow if we can prove that\nlim\nn\u2192\u221eP\n\u001a\n\u03c6n > n\u22121/2 min\nj\u2208S V (\u22121+\u03b1)/(2\u2212\u03b1)\nj\n\f\f\f\u03b5\u22a4aj + n\u03benj\n\f\f\f\n\u001b\n\u21920.\n(7)\nIn order to prove (7), note that as n \u2192\u221e, for every j = 1, . . . , p, we have\n\f\f\f\f\f\n \n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj\n!\n\u22121\nn\nn\nX\ni=1\nX2\nij\n\f\f\f\f\f =\n\f\f\f\f\f\n1\nn\nn\nX\ni=1\n(Xij + Uij)2 \u2212\u03c32\nj \u22121\nn\nn\nX\ni=1\nX2\nij\n\f\f\f\f\f\n\u2264\n\f\f\f\f\f\f\n1\nn\n\uf8eb\n\uf8ed\nn\nX\nj=1\nU 2\nij\n\uf8f6\n\uf8f8\u2212\u03c32\nj\n\f\f\f\f\f\f\n+\n\f\f\f\f\f\n2\nn\nn\nX\ni=1\nXijUij\n\f\f\f\f\f = op(1),\nby the law of large numbers and that E(Uij) = 0, E(U 2\nij) = \u03c32\nj and E(U 4\nij) < \u221e. Let \u03c4j = (n\u22121 Pn\ni=1 X2\nij)(\u22121+\u03b1)/(2\u2212\u03b1).\nBy condition (C4) and because 0 < \u03b1 < 1, there exist constants C3 and C4 such that C3 \u2264minj \u03c4j \u2264\n22\nmaxj \u03c4j \u2264C4 < \u221e. It suf\ufb01ces to show that\nP\n\u001a\n\u03c6n > n\u22121/2 min\nj\u2208S \u03c4j\n\f\f\f\u03b5\u22a4aj + n\u03benj\n\f\f\f\n\u001b\n\u21920.\n(8)\nNow, to prove (8), we have\nP\n\u0012\n\u03c6n > min\nj\u2208S \u03c4j\n\f\f\fn\u22121/2\u03b5\u22a4aj + n1/2\u03benj\n\f\f\f\n\u0013\n= P\n\uf8eb\n\uf8ed[\nj\u2208S\nn\n\u03c4j\n\f\f\fn\u22121/2\u03b5\u22a4aj + n1/2\u03benj\n\f\f\f < \u03c6n\no\n\uf8f6\n\uf8f8\n\u2264\nX\nj\u2208S\nP\n\u0010\n\u03c4j\n\f\f\fn\u22121/2\u03b5\u22a4aj + n1/2\u03benj\n\f\f\f < \u03c6n\n\u0011\n\u2264\nX\nj\u2208S\nP\n\u0010\f\f\fn\u22121/2\u03b5\u22a4aj + n1/2\u03benj\n\f\f\f < C\u22121\n3 \u03c6n\n\u0011\n(9)\nwhere the last inequality follows from \u03c4j \u2265C3. Then noting that for all j \u2208S, we have\nP\n\u0010\f\f\fn\u22121/2\u03b5\u22a4aj + n1/2\u03benj\n\f\f\f < C\u22121\n3 \u03c6n\n\u0011\n= 1 \u2212P\n\u0010\f\f\fn\u22121/2\u03b5\u22a4aj + n1/2\u03benj\n\f\f\f \u2265C\u22121\n3 \u03c6n\n\u0011\n,\n(10)\nand hence\nP\n\u0010\f\f\fn\u22121/2\u03b5\u22a4aj + n1/2\u03benj\n\f\f\f \u2265C\u22121\n3 \u03c6n\n\u0011\n\u2265P\n\u0010\nn1/2 |\u03benj| \u2212n\u22121/2 \f\f\f\u03b5\u22a4aj\n\f\f\f \u2265C\u22121\n3 \u03c6n\n\u0011\n= 1 \u2212P\n\u0010\nn\u22121/2 \f\f\f\u03b5\u22a4aj\n\f\f\f > n1/2 |\u03benj| \u2212C\u22121\n3 \u03c6n\n\u0011\n\u22651 \u2212P\n \nn\u22121/2 \f\f\f\u03b5\u22a4aj\n\f\f\f > n1/2|\u02dc\u03benj| \u2212n\u22121/2\n\f\f\f\f\f\nn\nX\ni=1\n s\nX\nk=1\nXik\u03b20kUij\n!\f\f\f\f\f \u2212C\u22121\n3 \u03c6n\n!\n\u22651 \u2212P\n \nn\u22121/2 \f\f\f\u03b5\u22a4Xj\n\f\f\f + n\u22121/2 \f\f\f\u03b5\u22a4Uj\n\f\f\f + n\u22121/2\n\f\f\f\f\f\nn\nX\ni=1\n s\nX\nk=1\nXik\u03b20kUij\n!\f\f\f\f\f > n1/2|\u03be0| \u2212C\u22121\n3 \u03c6n\n!\n\u22651 \u2212P\n \f\f\f\f\fn\u22121/2\u03b5\u22a4Xj + n\u22121/2\u03b5\u22a4Uj + n\u22121/2\nn\nX\ni=1\n s\nX\nk=1\nXik\u03b20kUij\n!\f\f\f\f\f > n1/2|\u03be0| \u2212C\u22121\n3 \u03c6n\n!\n\u22651 \u2212E\n\b\nn\u22121/2\u03b5\u22a4Xj + n\u22121/2\u03b5\u22a4Uj + n\u22121/2b1\nPn\ni=1 (Ps\nk=1 XikUij)\n\t2\n\b\nn1/2\u03be0 \u2212C\u22121\n3 \u03c6n\n\t2\n\u22651 \u2212n\u22121Var\n\u0002\n\u03b5\u22a4Xj + \u03b5\u22a4Uj + b1\nPn\ni=1 (Ps\nk=1 XikUij)\n\u0003\n\b\nn1/2\u03be0 \u2212C\u22121\n3 \u03c6n\n\t2\n.\nNow, consider the numerator of the last expression. Note that,\nCov(\u03b5\u22a4Xj, \u03b5\u22a4Uj) =\nn\nX\ni=1\nXijCov (\u03f5i, \u03f5iUji) =\nn\nX\ni=1\nXij\n\u0002\nE(\u03f52\ni )E(Uij) \u2212E(\u03f5i)E(\u03f5iUij)\n\u0003\n= 0\nand also\nCov(XikUij, \u03b5\u22a4Uj) = Cov(XikUij, \u03b5iUij) = XikE(\u03b5i)E(U 2\nij) = 0.\n23\nHence,\nVar\n\"\n\u03b5\u22a4Xj + \u03b5\u22a4Uj + b1\nn\nX\ni=1\n s\nX\nk=1\nXikUij\n!#\n= Var(\u03b5\u22a4Xj) + Var(\u03b5\u22a4Uj) + Var\n\"\nb1\nn\nX\ni=1\n s\nX\nk=1\nXikUij\n!#\n= \u03c32\nn\nX\ni=1\nX2\nij +\nn\nX\ni=1\nE(\u03f52\ni )E(U 2\nij) + b2\n1\nn\nX\ni=1\n s\nX\nk=1\nXik\n!2\nE(U 2\nij)\n\u2264nC2\u03c32 + n\u03c32\u03c32\n\u2217+ nb2\n1C2s\u03c32\n\u2217= nO(1) + nsO(1).\nIt follows that\nP\n\u0010\f\f\fn\u22121/2\u03b5\u22a4aj + n1/2\u03benj\n\f\f\f < C\u22121\n3 \u03c6n\n\u0011\n\u2264\nO(1) + sO(1)\n\b\nn1/2\u03be0 \u2212C\u22121\n3 \u03c6n\n\t2 .\nSince \u03bbn = o(1), we then have\n\u03c6n\nn1/2 = O(1)\u03bb1/(2\u2212\u03b1)\nn\n= o(1),\nand n\u22121s = o(1). Therefore, we have\nP\n\u0012\n\u03c6n > min\nj\u2208S\n\f\f\fn\u22121/2\u03b5\u22a4aj + n1/2\u03benj\n\f\f\f\n\u0013\n= O(1)n\u22121s2 \u21920\nand (8) follows.\nA.2\nProof of Theorem 2\nUsing the same notation as in the proof of Theorem 1, it suf\ufb01ces to show that\nP\n\u001a\n\u03c6n > n\u22121/2 max\nj\u2208Sc \u03c4j\n\f\f\f\u03b5\u22a4aj + n\u03benj\n\f\f\f\n\u001b\n\u21921.\n(11)\nTo prove (11), by condition (C6), for all j \u2208Sc, we obtain\nn1/2 \f\f\f\u02dc\u03benj\n\f\f\f = n\u22121/2\n\f\f\f\f\f\ns\nX\nk=1\nn\nX\ni=1\nXikXij\u03b20k\n\f\f\f\f\f \u2264n\u22121/2b1\n s\nX\nl=1\n\f\f\f\f\f\nn\nX\ni=1\nXikXij\n\f\f\f\f\f\n!\n\u2264b1d0n1/2\u2212\u03b8s = c1n1/2\u2212\u03b8s\n24\nwith c1 = b1d0. Next,\nP\n\u0012\n\u03c6n > n\u22121/2 max\nj\u2208Sc \u03c4j\n\f\f\f\u03b5\u22a4aj + n\u03benj\n\f\f\f\n\u0013\n\u2265P\n\u0012\nC\u22121\n4 \u03c6n > n\u22121/2 max\nj\u2208Sc\n\f\f\f\u03b5\u22a4aj\n\f\f\f + n1/2 max\nj\u2208Sc |\u03benj|\n\u0013\n\u2265P\n\u0012\nC\u22121\n4 \u03c6n > n\u22121/2 max\nj\u2208Sc\n\f\f\f\u03b5\u22a4Xj\n\f\f\f + n\u22121/2 max\nj\u2208Sc\n\f\f\f\u03b5\u22a4Uj\n\f\f\f + n1/2 max\nj\u2208Sc |\u02dc\u03benj|\n+ max\nj\u2208Sc\n\f\f\f\f\fn\u22121/2\nn\nX\ni=1\n s\nX\nk=1\nXik\u03b20k\n!\nUij\n\f\f\f\f\f\n!\n\u2265P\n \nC\u22121\n4 \u03c6n > n\u22121/2 max\nj\u2208Sc\n\f\f\f\u03b5\u22a4Xj\n\f\f\f + n\u22121/2 max\nj\u2208Sc\n\f\f\f\u03b5\u22a4Uj\n\f\f\f + c1n1/2\u2212\u03b8s + b1 max\nj\u2208Sc\n\f\f\f\f\fn\u22121/2\nn\nX\ni=1\n s\nX\nk=1\nXik\n!\nUij\n\f\f\f\f\f\n!\n= 1\u2212\nP\n \nn\u22121/2 max\nj\u2208Sc\n\f\f\f\u03b5\u22a4Xj\n\f\f\f + n\u22121/2 max\nj\u2208Sc\n\f\f\f\u03b5\u22a4Uj\n\f\f\f + b1 max\nj\u2208Sc\n\f\f\f\f\fn\u22121/2\nn\nX\ni=1\n s\nX\nk=1\nXik\n!\nUij\n\f\f\f\f\f \u2265C\u22121\n4 \u03c6n \u2212c1n1/2\u2212\u03b8s\n!\n\u22651 \u2212E\n\u0000n\u22121/2 maxj\u2208Sc \f\f\u03b5\u22a4Xj\n\f\f + n\u22121/2 maxj\u2208Sc \f\f\u03b5\u22a4Uj\n\f\f + b1 maxj\u2208Sc \f\fn\u22121/2 Pn\ni=1 (Ps\nk=1 Xik) Uij\n\f\f\u0001\nC\u22121\n4 \u03c6n \u2212c1n1/2\u2212\u03b8s\n.\nConsider each term in the numerator. For the \ufb01rst term, by property (B1), because \u03b5i is sub-Gaussian with\ncommon variance \u03c32, i = 1, . . . , n, then \u03b5\u22a4Xj is sub-Gaussian with variance \u03c32 Pn\ni=1 X2\nij and variance\nproxy C2n\u03c32. Therefore, by property (B3), we have\nE\n\u001a\nn\u22121/2 max\nj\u2208Sc\n\f\f\f\u03b5\u22a4Xj\n\f\f\f\n\u001b\n\u2264C1/2\n2\n\u03c3\np\nlog(2m) = O(1) {log(2m)}1/2 .\nFor the second term, by property (B3), each variable \u03b5iUij is sub-exponential with parameter (\u03bd0, t0), so\n\u03b5\u22a4Uj = Pn\ni=1 \u03f5iUij is sub-exponential with parameter (n\u03bd0, t0). Hence, by property (B4), we have\nE\n\u001a\nn\u22121/2 max\nj\u2208Sc\n\f\f\f\u03b5\u22a4Uj\n\f\f\f\n\u001b\n\u2264\u03bd1/2\n0\np\nlog(2m) + n\u22121/2t0 log(2m).\nFor the third term, by a similar argument, each variable Pn\ni=1 (Ps\nk=1 Xik) Uij is sub-Gaussian with variance\nproxy C2\u03c32\n\u2217ns. Hence,\nE\n(\nmax\nj\u2208Sc\n\f\f\f\f\fn\u22121/2\nn\nX\ni=1\n s\nX\nk=1\nXik\n!\nUij\n\f\f\f\f\f\n)\n\u2264C1/2\n2\n\u03c3\u2217\np\ns log(2m) = O(1)\np\ns log(2m).\nPutting the results together, we have\nP\n\u0012\n\u03c6n > n\u22121/2 max\nj\u2208Sc \u03c4j\n\f\f\f\u03b5\u22a4aj + n\u03benj\n\f\f\f\n\u0013\n\u22651 \u2212\nlog(2m)1/2 n\nO(1) + \u221as + n\u22121/2p\nlog(2m)\no\nC\u22121\n4 \u03c6n \u2212c1n1/2\u2212\u03b8s\n.\n(12)\n25\nNext, by condition (C7a),\nn1/2\u2212\u03b8s\nC\u22121\n4 \u03c6n\n= O(1)\n \ns(2\u2212\u03b1)\n\u03bbnn\u03b8(2\u2212\u03b1)\n!1/(2\u2212\u03b1)\n= o(1),\nand by condition (C7b),\nlog(2m)\nC\u22121\n4 \u03c6n\n= O(1)\n \nlog(2m)(2\u2212\u03b1)\n\u03bbnn(2\u2212\u03b1)/2\n!1/(2\u2212\u03b1)\n= o(1).\nTherefore,\np\nlog(2m)/\n\u0000C\u22121\n4 \u03c6n\n\u0001\n= o(1) and also\np\ns log(2m)/\n\u0000C\u22121\n4 \u03c6n\n\u0001\n= o(1). Finally, we have\nP\n\u0012\n\u03c6n > n\u22121/2 max\nj\u2208Sc \u03c4j\n\f\f\f\u03b5\u22a4aj + n\u03benj\n\f\f\f\n\u0013\n\u21921,\nas required.\nA.3\nProof of Theorem 3\nRecall that the screening set for SISc is de\ufb01ned as\n\u02c6QSISc =\nn\n1 \u2264i \u2264p : |\u02dc\u03b2i| is among the \ufb01rst d = \u230a\u03b3n\u230blargest of all\no\n.\nIn this proof, we will call this set \u02c6Q\u03b3 to emphasize the dependence of the screening set on \u03b3, while the\nsubscript SISc is omitted to ease the notation. Following Fan & Lv (2007), the proof consists of two main\nsteps. In the \ufb01rst step, we de\ufb01ne the following set\nf\nM1\n\u03b4 =\nn\n1 \u2264i \u2264p : |\u02dc\u03b2i| is among the \ufb01rst [\u03b4p] largest of all\no\n,\nand prove that for some positive constant C, we have\nP\n\u0010\nf\nM1\n\u03b4 \u2287S\n\u0011\n= O(p exp(\u2212Cn)).\n(13)\nIn the second step, we will apply the dimensional reduction procedure above successively until the number\nof covariates to be kept is smaller than n.\nStep 1.\nWe will prove (13) by bounding \u2225\u02dc\u03b2\u22252\n2 from above and |\u02dc\u03b2j|, j \u2208S from below. Note that\n\u02dc\u03b2j =\nn\u22121 Pn\ni=1 Wijyi\nn\u22121 Pn\ni=1 W 2\nij \u2212\u03c32\nj\n,\nj = 1, . . . , p,\n(14)\n26\nso for the denominator of (14), we have\nE\n \n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj\n!\n= 1\nn\nn\nX\ni=1\nX2\nij,\nand\n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj \u22121\nn\nn\nX\ni=1\nX2\nij = 2\nn\nn\nX\ni=1\nXijUij + 1\nn\nn\nX\ni=1\nU 2\nij \u2212\u03c32\nj .\nBy condition (C2) and properties of sub-Gaussian random variables, the variable Pn\ni=1 XijUij is zero-mean\nsub-Gaussian with variance proxy (Pn\ni=1 X2\nij)\u03c32\n\u2217, which is of order O(n) by condition (C4). Therefore, by\nthe Hoeffding inequality, there exists positive constants d1 and K1 such that\nP\n \f\f\f\f\f\n1\nn\nn\nX\ni=1\nXijUij\n\f\f\f\f\f \u2265d1\n!\n= O (exp (\u2212K1n)) .\nSimilarly, by condition (C2), the variables U 2\nij\u2019s are sub-exponential with (\ufb01nite) parameter (\u03bd0, \u03b10), hence\nPn\ni=1 U 2\nij is also sub-exponential with parameter (\u03bd0\n\u221an, \u03b10). Because E(U 2\nij) = \u03c32\nj , there exist constants\nd2 \u2208[0, \u03bd2\n0/\u03b10] and K2 such that\nP\n \f\f\f\f\f\n1\nn\nn\nX\ni=1\nU 2\nij \u2212\u03c32\nj\n\f\f\f\f\f \u2265d2\n!\n= O (exp (\u2212K2n)) .\nCombining these results, for some positive constants d3 and K3, we have\nP\n \f\f\f\f\f\n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj \u22121\nn\nn\nX\ni=1\nX2\nij\n\f\f\f\f\f \u2265d3\n!\n= O(exp(\u2212K3n)).\n(15)\nBy condition (C4), the term n\u22121 Pn\ni=1 X2\nij = O(1), so inequality (15) implies that n\u22121 Pn\ni=1 W 2\nij \u2212\u03c32\nj is\nboth bounded above and bounded away from zero with probability tending to one as n \u2192\u221e. Now for the\nnumerator of (14), we have\nE\n \n1\nn\nn\nX\ni=1\nWijyi\n!\n= E\n \n1\nn\nn\nX\ni=1\nXijyi\n!\n= \u03benj,\nand\n1\nn\nn\nX\ni=1\nWijyi \u2212\u03benj = 1\nn\nn\nX\ni=1\nXij\u03b5i + 1\nn\nn\nX\nj=1\nUij\u03b5i.\nBy conditions (C1) and (C2), using similar arguments, we obtain\nP\n \f\f\f\f\f\n1\nn\nn\nX\ni=1\nWijyi \u2212\u03benj\n\f\f\f\f\f \u2265d4\n!\n= O(exp(\u2212K4n)),\n27\nfor some positive constant d4 and K4. Hence if j \u2208S, along with the condition (C5), the above inequality\nimplies that |n\u22121 Pn\ni=1 Wijyi| is bounded away from zero with probability at least 1 \u2212O(exp(\u2212Cn)).\nUsing the union bound, there exist positive constants d7 = d6/d5 with d5 = n\u22121 Pn\ni=1 X2\nij + d3 and\nd6 = \u03benj \u2212d4 such that for j \u2208S,\nP(| \u02dc\u03b2j| \u2265d7) \u2265P\n \f\f\f\f\f\n1\nn\nn\nX\ni=1\nWijyi\n\f\f\f\f\f \u2265d6 and\n\f\f\f\f\f\n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj\n\f\f\f\f\f \u2264d5\n!\n= 1 \u2212P\n \f\f\f\f\f\n1\nn\nn\nX\ni=1\nWijyi\n\f\f\f\f\f \u2264d6 or\n\f\f\f\f\f\n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj\n\f\f\f\f\f \u2265d5\n!\n\u22651 \u2212\n(\nP\n \f\f\f\f\f\n1\nn\nn\nX\ni=1\nWijyi\n\f\f\f\f\f \u2264d6\n!\n+ P\n \f\f\f\f\f\n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj\n\f\f\f\f\f \u2265d5\n!)\n= 1 \u2212(O exp(\u2212K4n) + O(exp(\u2212K3n))) = 1 \u2212O(exp(\u2212K5n)),\n(16)\nfor some constant C. The \ufb01nal inequality states that for all j \u2208S we have |\u02c6\u03b2j| is bounded away from 0 with\nprobability tending to one as n \u2192\u221e.\nNext, we will bound the norm \u2225\u02dc\u03b2\u22252\n2 from above. Let \u039b denote the p \u00d7 p diagonal matrix with elements\nn\u22121 Pn\ni=1 W 2\nij \u2212\u03c32\nj j = 1, . . . , p, then we can write \u02dc\u03b2 as\n\u02dc\u03b2 = \u039b\u22121\n\u0012 1\nn W\u22a4y\n\u0013\n= \u039b\u22121\n\u0012 1\nn X\u22a4X \u03b20 + 1\nn U\u22a4X \u03b20 + 1\nn X\u22a4\u03b5 + 1\nn U\u22a4\u03b5\n\u0013\n.\nHence, by the submultiplicity of matrix norm and the triangle inequality, we have\n\u2225\u02dc\u03b2\u22252 \u2264\u2225\u039b\u22121\u22252\n\u0012\r\r\r\r\n1\nn X\u22a4X \u03b20\n\r\r\r\r\n2\n+\n\r\r\r\r\n1\nn U\u22a4X \u03b20\n\r\r\r\r\n2\n+\n\r\r\r\r\n1\nn X\u22a4\u03b5\n\r\r\r\r\n2\n+\n\r\r\r\r\n1\nn U\u22a4\u03b5\n\r\r\r\r\n2\n\u0013\n(17)\nwhere\n\r\r\u039b\u22121\r\r\n2 = maxj=1,...,p\nn\n1/(|n\u22121 Pn\ni=1 W 2\nij \u2212\u03c32\nj |)\no\n=\nn\nminj=1,...,p |n\u22121 Pn\ni=1 W 2\nij \u2212\u03c32\nj |\no\u22121\nis\nthe magnitude of the largest eigenvalue of \u039b\u22121 . From equation (15) and condition (C4), there exist suf\ufb01-\nciently large constants d8 and K8 such that\nP\n\u0000\r\r\u039b\u22121\r\r\n2 \u2264d8\n\u0001\n= P\n \nmin\nj=1,...,p\n(\f\f\f\f\f\n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj\n\f\f\f\f\f\n)\n\u22651\nd8\n!\n= P\n\uf8eb\n\uf8ed\n\\\nj=1,...,p\n\f\f\f\f\f\n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj\n\f\f\f\f\f \u22651\nd8\n\uf8f6\n\uf8f8\n= 1 \u2212P\n\uf8eb\n\uf8ed\n[\nj=1,...,p\n\f\f\f\f\f\n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj\n\f\f\f\f\f \u22641\nd8\n\uf8f6\n\uf8f8\n\u22651 \u2212\np\nX\nj=1\nP\n \f\f\f\f\f\n1\nn\nn\nX\ni=1\nW 2\nij \u2212\u03c32\nj\n\f\f\f\f\f \u22641\nd8\n!\n= 1 \u2212O(p exp(\u2212K8n)).\n(18)\n28\nSince log(p)/n \u2192C1 by condition (C8), as long as K8 > C1, the operator norm\n\r\r\u039b\u22121\r\r\n2 is bounded above\nby a constant with probability tending to one n \u2192\u221e.\nNext, we will bound each term in the bracket of (17). First,\n\r\r\r\r\n1\nn X\u22a4X \u03b20\n\r\r\r\r\n2\n2\n= \u03b2\u22a4\n0\n\u0012 1\nn X\u22a4X\n\u00132\n\u03b20 \u2264\u03bb2\nmax\n\u0012 1\nn X\u22a4X\n\u0013\n\u2225\u03b20 \u22252\n2 = O(sn2\u03c42)\n(19)\nby conditions (C3) and (C4). For the second term,\n\r\r\r\r\n1\nn U\u22a4X \u03b20\n\r\r\r\r\n2\n2\n= 1\nn2 \u03b2\u22a4\n0 X\u22a4U U\u22a4X \u03b20 = 1\nn2 \u03b2\u22a4\n0 X\u22a4Z \u03a3u Z\u22a4X \u03b20\n\u22641\nn2 \u03bbmax(\u03a3u)\u03b2\u22a4\n0 X\u22a4Z Z\u22a4X \u03b20\n\u22641\nn2 p\u03bbmax(\u03a3u)\u03bbmax(p\u22121 Z Z\u22a4) \u03b2\u22a4\n0 X\u22a4X \u03b20\n\u2264p\u03bbmax(\u03a3u)\u03bbmax(p\u22121 Z Z\u22a4)\u03bbmax\n\u0012 1\nn X\u22a4X\n\u0013\n\u2225\u03b20 \u22252\n2\n(i)\n\u2264pn\u03c41+\u03c42\u22121\u03bbmax(p\u22121 Z Z\u22a4)\u2225\u03b20 \u22252\n2,\nwhere step (i) follows conditions (C9b) and (C10). Therefore, by conditions (C9a) and (C3) there exist\npositive constants d9 and K9 such that\nP\n \r\r\r\r\n1\nn U\u22a4X \u03b20\n\r\r\r\r\n2\n2\n\u2265d9psn\u03c41+\u03c42\u22121)\n!\n\u2264O(exp(\u2212K9n)).\nFor the third term, we have\n\r\r\r\r\n1\nn X\u22a4\u03b5\n\r\r\r\r\n2\n2\n= 1\nn\u03b5\u22a4\n\u0012 1\nn X\u22a4X\n\u0013\n\u03b5 \u2264\u03bbmax\n\u0012 1\nn X\u22a4X\n\u0013 1\nn\u2225\u03b5\u22252\n2.\nBy condition (C1), each term \u03b52\ni is sub-exponential with \ufb01nite parameters; furthermore, since \u03b51, . . . , \u03b5n are\nmutually independent, there exist positive constants d10 and K10 such that\nP\n\u0012 1\nn\u2225\u03b5\u22252\n2 \u2265d10\n\u0013\n= P\n \n1\nn\nn\nX\ni=1\n\u03b52\ni \u2265d10\n!\n= O(exp(\u2212K10n)).\n(20)\nAs a result, we obtain\nP\n \r\r\r\r\n1\nn X\u22a4\u03b5\n\r\r\r\r\n2\n2\n\u2265d11n\u03c42\n!\n= O(exp(\u2212K11n))\n29\nfor some positive constants d11 and K11. Finally, for the last term in the bracket of (17), we have\n\r\r\r\r\n1\nn U\u22a4\u03b5\n\r\r\r\r\n2\n2\n= 1\nn2 \u03b5\u22a4U U\u22a4\u03b5 = 1\nn2 \u03b5\u22a4Z \u03a3u Z\u22a4\u03b5\n\u22641\nn2 \u03bbmax(\u03a3u)\u03b5\u22a4Z Z\u22a4\u03b5\n\u22641\nnp\u03bbmax(\u03a3u)\u03bbmax(p\u22121 Z Z\u22a4)\n\u0012 1\nn\u2225\u03b5\u22252\n2\n\u0013\n\u2264c3pn\u03c41\u22121\u03bbmax(p\u22121 Z Z\u22a4)\n\u0012 1\nn\u2225\u03b5\u22252\n2\n\u0013\n.\n(21)\nLet d12 = c2c3d10, then by condition (C9a) and (20), we obtain\nP\n \r\r\r\r\n1\nn U\u22a4\u03b5\n\r\r\r\r\n2\n2\n\u2265d12pn\u03c41\u22121\n!\n\u2264P\n\u001a\n\u03bbmax(p\u22121 Z Z\u22a4)\n\u0012 1\nn\u2225\u03b5\u22252\n2\n\u0013\n\u2265c2d10\n\u001b\n= 1 \u2212P\n\u001a\n\u03bbmax(p\u22121 Z Z\u22a4)\n\u0012 1\nn\u2225\u03b5\u22252\n2\n\u0013\n\u2264c2d10\n\u001b\n\u22641 \u2212P\n\u001a\n\u03bbmax(p\u22121 Z Z\u22a4) \u2264c2 and 1\nn\u2225\u03b5\u22252\n2 \u2264d10\n\u001b\n= P\n\u001a\n\u03bbmax(p\u22121 Z Z\u22a4) \u2265c2 or 1\nn\u2225\u03b5\u22252\n2 \u2265d10\n\u001b\n\u2264P\n\u0010\n\u03bbmax(p\u22121 Z Z\u22a4) \u2265c2\n\u0011\n+ P\n\u0012 1\nn\u2225\u03b5\u22252\n2 \u2265d10\n\u0013\n= O(exp(\u2212K12n))\n(22)\nfor some positive constant K12. Substituting (18)-(22) to (17), for some suf\ufb01ciently large constants d13 and\nK13 > C1 and under the condition that p \u2265n\u03c42\u2212\u03c41+1, we have\nP\n\u0010\n\u2225\u02dc\u03b2\u22252\n2 \u2265d13psn\u03c41+\u03c42+\u22121)\n\u0011\n\u2264O(p exp(\u2212K13n))\n(23)\nFinally, by Bonferonni\u2019s inequality, it follows from (16) and (23) that\nP\n\u0012\nmin\ni\u2208S |\u02dc\u03b2i| < d7 or \u2225\u02dc\u03b2\u22252\n2 > d13psn\u03c41+\u03c42\u22121\n\u0013\n\u2264O (p exp (\u2212K14n)) ,\n(24)\nfor some constants K14 > C1. Hence, for suf\ufb01ciently large constants C > C1, with probability 1 \u2212\nO (p exp (\u2212Cn)), the magnitudes of \u02dc\u03b2i, i \u2208S are bounded away from zero and for some d > 0,\ncard\n\u001a\n1 \u2264k \u2264p : |\u02dc\u03b2k| \u2265min\ni\u2208S |\u02dc\u03b2i|\n\u001b\n\u2264dpn\u03c41+\u03c42+logn(s)\u22121.\nTherefore, if \u03b4 \u21920 and satis\ufb01es \u03b4n1\u2212\u03c41\u2212\u03c42\u2212logn(s) \u2192\u221eas n \u2192\u221e, then (13) holds with a constant C > 0\nlarger than K14 in (24).\n30\nStep 2.\nThis step follows Step 2 in the proof of Theorem 1 of Fan & Lv (2007). In this step, we will use C\nto denote generic constants that are larger than C1 in condition (C8). Fix an arbitrary r \u2208(0, 1) and choose\na shrinking factor \u03b4 of the form \u03b4 = ( n\np )1/(k\u2212r) for some integer k \u22651. We successively perform dimension\nreduction until the number of remaining variables drops to below the sample size n. In other words we\nobtain a sequence of nested sets\nf\nMk\n\u03b4 \u2282f\nMk\u22121\n\u03b4\n\u2282. . . \u2282f\nM1\n\u03b4,\nwhere each set f\nM\u03b4 = f\nMj\n\u03b4 has cardinality \u2308\u03b4jp\u2309and d = \u2308\u03b4kp\u2309= \u2308\u03b4rn\u2309< n but \u2308\u03b4k\u22121p\u2309= \u2308\u03b4r\u22121n\u2309> n.\nHence we see that f\nM\u03b4 = \u02c6Q\u03b3, with \u03b3 = \u03b4r < 1.\nNext, \ufb01x an arbitrary \u03b81 \u2208(0, 1 \u2212\u03c41 \u2212\u03c42 \u2212logn(s)) and pick some r < 1 very close to 1 such that\n\u03b80 = \u03b81/r < 1 \u2212\u03c41 \u2212\u03c42 \u2212logn(s), and choose a sequence of integers k \u22651 in a way such that when\nn \u2192\u221e,\n\u03b4n1\u2212\u03c41\u2212\u03c42\u2212logn(s) \u2192\u221eand \u03b4n\u03b80 \u21920,\n(25)\nwhere \u03b4 = ( n\np )1/(k\u2212r). Therefore, with the above dimension reduction process, we can raise both sides of\n(25) to the rth power, and hence the set f\nM\u03b4 = \u02c6Q\u03b3 with \u03b3 = \u03b4r satis\ufb01es\n\u03b3nr(1\u2212\u03c41\u2212\u03c42\u2212logn(s)) \u2192\u221eand \u03b3n\u03b81 \u21920.\nSince for any principal submatrix \u03a30\nu of \u03a3u, we have \u03bbmax(\u03a30\nu) < \u03bbmax(\u03a3u) \u2264c2n\u03c41, and that property\n(6) in condition (C9a) holds for any n \u00d7 \u02dcp submatrix eZ of Z with c1n < \u02dcp \u2264p, for some constant C > 0,\nin each step 1 \u2264i \u2264k in the above dimension reduction framework, we have\nP\n\u0010\nf\nMi\n\u03b4 \u2287S | f\nMi\u22121\n\u03b4\n\u2287S\n\u0011\n= 1 \u2212O (p exp (\u2212Cn)) ,\nand hence by Bonferroni\u2019s inequality we have\nP\n\u0010\n\u02c6Q\u03b3 \u2287S\n\u0011\n= 1 \u2212O (kp exp (\u2212Cn)) .\n(26)\nIt follows from (25) that we require \u03b4 \u21920 for all r when both n \u2192\u221eand p\n\u2192\u221e(i.e log(n)/k \u2212\nlog(p)/k \u2192\u2212\u221e). Since p > n implies k = O(log(p)/ log(n)), a suitable increase of the constant C > 0\nin (26) gives\nP\n\u0010\n\u02c6Q\u03b3 \u2287S\n\u0011\n= 1 \u2212O (p exp (\u2212Cn)) .\nThis probability bound holds for any \u03b3 \u223ccn\u2212\u02dc\u03b8, with \u02dc\u03b8 < 1 \u2212\u03c41 \u2212\u03c42 \u2212logn(s) and c > 0, completing the\nproof.\nB\nPreprocessing for the microarray data\nIn this subsection, we provide more detail on the steps that we used to preprocess the microarray data in\nSection 4 of the main paper.\nAs noted in Hein et al. (2005), on Affymetrix \u201cGeneChips\u201d oligonucleotide arrays, each gene is repre-\n31\nsented by a probe set, consisting of a number of probe pairs. A probe pair further contains a perfect match\n(PM) probe and a mismatch probe (MM). On the one hand, the intensity observed for the PM measure-\nment for probe r at gene j was assumed due partly to the binding of the cRNA that perfectly matches the\nsequence on the array, denoted as Sjr (true signal), and partly to hybridization of the cRNA that did not\nperfectly match the sequence, denoted as Hjr (nonspeci\ufb01c hybridization). On the other hand, the intensity\nobserved for the MM measurement for probe r at gene j was due partly to binding of a fraction \u03c6 \u2208(0, 1)\nof the true signal and partly due to nonspeci\ufb01c hybridization. Furthermore, it was assumed that\nPMjr \u223cN(Sjr + Hjr, \u03c4 2),\nMMjr \u223cN(\u03c6Sjr + Hjr, \u03c4 2),\nwhere N(\u00b5, \u03c32) denotes the normal distribution with mean \u00b5 and variance \u03c32. Next, Hein et al. (2005)\nmodeled the true signal Sgj and nonspeci\ufb01c hybridization Hgj on the log scale. Since they allowed them to\nbe zero, they assumed that\nlog(Sjr + 1) \u223cTN(Xj, \u03c32\nj ),\nlog(Hjr + 1) \u223cTN(\u03bb, \u03b72),\nwhere TN(\u00b5, \u03c32) denotes the normal distribution with mean \u00b5 and variance \u03c32 left truncated at 0. Further-\nmore, to account for outlying probes and stabilize the gene-speci\ufb01c variance parameters, all the variances\n\u03c32\ng were assumed to be exchangeable,\nlog(\u03c32\nj ) iid\n\u223cN(a, b2).\nFinally, the Bayesian model was fully speci\ufb01ed by assuming non-informative priors on Xj, \u03c6, \u03bb, \u03c4 \u22122 and\n\u03b7\u22122, while a and b were \ufb01xed at values obtained by an empirical procedure. The primary parameter of\ninterest was Xj, the mean gene expression level for gene j on the log scale.\nWe \ufb01tted the above Bayesian model for each patient i = 1, . . . , n separately by the BGX package of\nHein et al. (2005) in R and obtained the posterior distribution for Xij. Let c\nWij and var( b\nXij) denote the\nposterior mean and variance of Xij respectively; so we considered c\nWij as a surrogate for the true Xij con-\ntaminated by some measurement error Uij with variance Var( b\nXij). Next, for each gene j, we standardized\nthe posterior mean \u02dcWij to obtain Wij = (c\nWij \u2212W j)/sj, i = 1, . . . , n, where W j = n\u22121 Pn\ni=1 c\nWij and\ns2\nj = n\u22121 Pn\ni=1(c\nWij \u2212W j)2. To simplify the analysis, we assumed that measurement error was inde-\npendent of the patient\u2019s true gene expression levels and that the associated variance was constant across\npatients for a given gene; hence, the matrix \u03a3u was set to be diagonal. We averaged the posterior variance,\nb\u03c32\nu,j = n\u22121 Pn\ni=1 Var( b\nXij), and the diagonal elements of the measurement error covariance matrix \u03a3u were\nestimated as (b\u03a3u)j,j = b\u03c32\nuj/s2\nj, j = 1, . . . , p. As noted by S\u00f8rensen et al. (2015), if the measurement error\nvariance is too large compared to the between-sample variance of the between-patient variability, then little\ncan be done. Hence, only the p = 993 genes with b\u03c32\nu,j < 0.5s2\nj, i.e. estimated noise-to-signal ratio less than\n1, were retained for further analysis.\n32\nC\nEf\ufb01ciency of the screening procedures\nIn this section, we present additional simulation results to investigate the ef\ufb01ciency of the proposed screening\nprocedures. We generated data in the same way as in the subsection 3.1 of the main paper, with the exception\nthat we considered one more scenario for \u03a3u, the covariance matrix of the measurement error and set \u03c1x =\n0.5. In this new scenario, \u03a3u was set to have a homogeneous structure, with all off-diagonal elements equal\nto 0.2 and diagonal elements equal to 0.4; hence the measurement errors on all covariates were correlated\nwith one another.\nWe report the mean false positive rate (FPR) and mean false negative rate (FNR) across 500 samples\nfor PMScCV (Table C.1), and the empirical cumulative distribution of the minimum number of variables that\nhad to be included for PMSc forward stepwise and SISc in order to retain all the important variables (Figure\n1). Note that the latter is always between s and p. On the one hand, a low minimum number of variables to\nbe included indicates a high screening ef\ufb01ciency; on the other hand, if the minimum number of variables to\nbe included is close to p, then dimension reduction is not achievable via screening.\nTable C.1: Average false positive rates (FPR, in percentage) and false negative rates (FNR, in percentage)\nof PMScCV with tuning parameters selected via \ufb01ve fold cross-validation. Standard errors are included in\nparentheses.\n\u03a3x\np\n\u03a3u diagonal\n\u03a3u block diagonal\n\u03a3u homogeneous\nFPR\nFNR\nFPR\nFNR\nFPR\nFNR\nAR(1)\n1000\n0.1 (0.1)\n0.0 (0.0)\n0.0 (0.1)\n0.0 (0.0)\n0.1 (0.1)\n0.0 (0.0)\n2000\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\nHomogen\n1000\n100.0 (0.0)\n0.0 (0.0)\n100.0 (0.0)\n0.0 (0.0)\n100.0 (0.0)\n0.0 (0.0)\n2000\n100.0 (0.0)\n0.0 (0.0)\n100.0 (0.0)\n0.0 (0.0)\n100.0 (0.0)\n0.0 (0.0)\nTable C.1 and Figure 1 indicate that when \u03a3x had an AR(1) structure, all the screening methods had\nhigh ef\ufb01ciency in variable selection. Speci\ufb01cally, regardless of the structure of measurement error variance\n\u03a3u, the PMScCV method had both false positive and false negative rates very close to zero. For the SISc\nand PMScFS, the number of variables each had to keep was also close to 5, the true number of non-zero\ncoef\ufb01cients in \u03b20. These results con\ufb01rm both the variable selection consistency of PMSc and the ability of\nSISc to reduce the number of dimensions dramatically when all the true covariates are not highly collinear.\nBy contrast, when \u03a3x had a homogeneous structure, the ef\ufb01ciency of screening methods decreased\nconsiderably. The PMScCV method was essentially unable to reduce the number of variables in that case,\nas evidenced by the false positive rates close to 100%. Similarly, to ensure all the important variables were\nretained, the SISc method had to keep 75-80% of the variables, which was usually more than the sample size\nn. Among the three screening methods, the PMScFS was the most effective; in all the considered settings, it\ncould effectively reduce the dimension to below the sample size n = 500. Looking at the distribution of the\nminimum number of variables that must be kept for PMScFS more closely, it is interesting that the ef\ufb01ciency\nof PMScFS seemed to increase when measurement errors were more correlated (i.e \u03a3u is homogeneous).\n33\n(a) \u03a3x has an homogeneous structure.\nGG\nG\nG\nGG\nG\nG\nGG\nGG\nGGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGGGGG\nGG\nG\nGGGGGGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nGGG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\n\u03a3u  diagonal\n\u03a3u  block diagonal\n\u03a3u  homogeneous\np = 1000\np = 2000\nCSIS\nCPMSFS\nCSIS\nCPMSFS\nCSIS\nCPMSFS\n0\n500\n1000\n1500\n2000\n0\n500\n1000\n1500\n2000\nMethod\nMinimum number of variables\n(b) \u03a3x has an AR(1) structure.\nG\nG\n\u03a3u  diagonal\n\u03a3u  block diagonal\n\u03a3u  homogeneous\np = 1000\np = 2000\nCSIS\nCPMSFS\nCSIS\nCPMSFS\nCSIS\nCPMSFS\n3\n4\n5\n6\n7\n3\n4\n5\n6\n7\nMethod\nMinimum number of variables\nFigure 1: Minimum number of variables that has to be kept for the SISc and PMScFS to retain all the\nimportant variables in the simulation study.\n34\nD\nDetailed simulation results for one-stage and two-stage estimators\nTable D.1: Performance of the one-stage and two-stage estimators in the simulation study based on false\npositive rate (FPR, in percentage), false negative rate (FNR, in percentage), \u21132 estimation error, and com-\nputation time (in seconds) when \u03a3x has an AR(1) structure with autocorrelation \u03c1x = 0.5. Standard error\nare included in parentheses. The CoCo estimator was only computed when the number of variables was no\nmore than 1000 (either in the 1st or 2nd step).\n\u03a3u\np\nEstimator\n1st step\n2nd steps\nTime\nFPR\nFNR\nFPR\nFNR\n\u21132\n1st step\n2nd step\nDiagonal\n1000\nOne-stage Corrected\n-\n-\n2.2 (1.1)\n0.0 (0.0)\n0.35 (0.10)\n0.0 (0.0)\n49.7 (19.5)\nPMScCV-Corrected\n0.1 (0.1)\n0.0 (0.0)\n0.1 (0.1)\n0.0 (0.0)\n0.26 (0.11)\n23.4 (0.3)\n0.4 (0.1)\nPMScFS-Corrected\n7.5 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.28 (0.10)\n5.4 (1.1)\n1.6 (0.5)\nSISc-Corrected\n7.5 (0.0)\n0.0 (0.0)\n1.2 (0.6)\n0.0 (0.0)\n0.33 (0.10)\n1.5 (0.0)\n2.0 (0.6)\nOne-stage CoCo\n-\n-\n7.6 (2.0)\n0.0 (0.0)\n0.47 (0.11)\n0.0 (0.0)\n516.8 (32.1)\nPMScCV-CoCo\n0.1 (0.1)\n0.0 (0.0)\n0.1 (0.0)\n0.0 (0.0)\n0.26 (0.11)\n23.4 (0.3)\n0.0 (0.0)\nPMScFS-CoCo\n7.5 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.27 (0.09)\n5.4 (1.1)\n0.7 (0.2)\nSISc-CoCo\n7.5 (0.0)\n0.0 (0.0)\n0.1 (0.1)\n0.0 (0.0)\n0.31 (0.09)\n1.5 (0.0)\n0.8 (0.2)\n2000\nOne-stage Corrected\n-\n-\n1.3 (0.6)\n0.0 (0.0)\n0.36 (0.10)\n0.0 (0.0)\n136.5 (55.7)\nPMScCV-Corrected\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.26 (0.10)\n38.0 (2.0)\n0.3 (0.1)\nPMScFS-Corrected\n3.8 (0.0)\n0.0 (0.0)\n0.6 (0.3)\n0.0 (0.0)\n0.29 (0.10)\n17.2 ( 3.1)\n2.4 ( 0.7)\nSISc-Corrected\n3.8 (0.0)\n0.0 (0.0)\n0.6 (0.3)\n0.0 (0.0)\n0.33 (0.10)\n2.8 (0.1)\n1.8 (0.5)\nPMScCV-CoCo\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.26 (0.11)\n38.0 (2.0)\n0.0 (0.0)\nPMScFS-CoCo\n3.8 (0.0)\n0.0 (0.0)\n0.1 (0.0)\n0.0 (0.0)\n0.27 (0.09)\n17.2 ( 3.1)\n0.8 ( 0.2)\nSISc-CoCo\n3.8 (0.0)\n0.0 (0.0)\n0.1 (0.0)\n0.0 (0.0)\n0.31 (0.09)\n2.8 (0.1)\n0.7 (0.2)\nBlock\ndiagonal\n1000\nOne-stage Corrected\n-\n-\n1.8 (0.9)\n0.0 (0.0)\n0.62 (0.17)\n0.0 (0.0)\n27.8 (14.9)\nPMScCV-Corrected\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.43 (0.18)\n23.4 (0.4)\n0.4 (0.1)\nPMScFS-Corrected\n7.5 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.4)\n0.48 (0.16)\n5.3 (1.0)\n1.9 (0.5)\nSISc-Corrected\n7.5 (0.0)\n0.0 (0.0)\n1.0 (0.5)\n0.0 (0.0)\n0.57 (0.18)\n1.5 (0.0)\n2.6 (0.8)\nOne-stage CoCo\n-\n-\n7.4 (1.9)\n0.0 (0.0)\n0.74 (0.13)\n0.0 (0.0)\n417.4 (22.2)\nPMScCV-CoCo\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.43 (0.18)\n23.4 (0.4)\n0.0 (0.0)\nPMScFS-CoCo\n7.5 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.4)\n0.46 (0.13)\n4.9 (1.6)\n1.3 (0.2)\nSISc-CoCo\n7.5 (0.0)\n0.0 (0.0)\n0.1 (0.0)\n0.0 (0.0)\n0.54 (0.15)\n1.5 (0.0)\n1.7 (0.2)\n2000\nOne-stage Corrected\n-\n-\n1.0 (0.4)\n0.0 (0.9)\n0.64 (0.17)\n0.0 (0.0)\n68.0 (36.1)\nPMScCV-Corrected\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.44 (0.17)\n37.9 (1.6)\n0.3 (0.1)\nPMScFS-Corrected\n3.8 (0.0)\n0.0 (0.0)\n0.5 (0.2)\n0.1 (1.3)\n0.46 (0.14)\n16.7 (3.6)\n2.8 (0.8)\nSISc-Corrected\n3.8 (0.0)\n0.0 (0.0)\n0.5 (0.2)\n0.0 (0.0)\n0.57 (0.17)\n2.8 (0.1)\n2.3 (0.6)\nPMScCV-CoCo\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.45 (0.18)\n37.9 (1.6)\n0.0 (0.0)\nPMScFS-CoCo\n3.8 (0.0)\n0.0 (0.0)\n0.1 (0.0)\n0.0 (0.9)\n0.44 (0.13)\n16.7 (3.6)\n1.7 (0.2)\nSISc-CoCo\n3.8 (0.0)\n0.0 (0.0)\n0.1 (0.0)\n0.0 (0.0)\n0.54 (0.15)\n2.8 (0.1)\n1.4 (0.2)\n35\nTable D.2: Performance of the one-stage and two-stage estimators in the simulation study based on false\npositive rate (FPR, in percentage), false negative rate (FNR, in percentage), \u21132 estimation error, and compu-\ntation time (in seconds) when \u03a3x has an homogeneous structure with \u03c1x = 0.5. Standard error are included\nin parentheses. The CoCo estimator was only computed when the number of variables was no more than\n1000 (either in the 1st or 2nd step).\n\u03a3u\np\nEstimator\n1st step\n2nd step\nTime\nFPR\nFNR\nFPR\nFNR\n\u21132\n1st step\n2nd step\nDiagonal\n1000\nOne-stage Corrected\n-\n-\n20.0 (29.0)\n0.1 (1.5)\n0.96 (0.77)\n0.0 (0.0)\n18.9 (3.3)\nPMScCV-Corrected\n100.0 (0.0)\n0.0 (0.0)\n19.7 (28.8)\n0.2 (2.0)\n0.95 (0.77)\n12.7 (0.7)\n18.7 (3.3)\nPMScFS-Corrected\n7.6 (0.0)\n3.2 (7.8)\n1.2 (1.6)\n3.2 (7.8)\n0.56 (0.41)\n5.4 (0.7)\n1.4 (0.2)\nSISc-Corrected\n7.6 (0.1)\n6.7 (10.5)\n1.0 (0.9)\n6.7 (10.5)\n0.54 (0.30)\n1.3 (0.1)\n1.0 (0.2)\nOne-stage CoCo\n-\n-\n11.5 (2.2)\n0.0 (0.0)\n0.77 (0.13)\n0.0 (0.0)\n481.5 (27.5)\nPMScCV-CoCo\n100.0 (0.0)\n0.0 (0.0)\n11.5 (2.2)\n0.0 (0.0)\n0.77 (0.13)\n12.7 (0.7)\n476.7 (26.8)\nPMScFS-CoCo\n7.6 (0.0)\n3.2 (7.8)\n1.0 (0.4)\n3.2 (7.8)\n0.45 (0.18)\n5.4 (0.7)\n1.8 (0.3)\nSISc-CoCo\n7.6 (0.1)\n6.7 (10.5)\n1.0 (0.4)\n6.7 (10.5)\n0.52 (0.23)\n1.3 (0.1)\n1.6 (0.2)\n2000\nOne-stage Corrected\n-\n-\n29.1 (21.0)\n1.1 (5.1)\n1.57 (0.84)\n0.0 (0.0)\n49.9 (7.0)\nPMScCV-Corrected\n100.0 (0.0)\n0.0 (0.0)\n28.3 (21.3)\n1.0 (5.1)\n1.53 (0.84)\n25.2 (1.4)\n49.3 (6.9)\nPMScFS-Corrected\n3.8 (0.0)\n4.0 (8.5)\n0.7 (0.9)\n4.0 (8.5)\n0.62 (0.51)\n8.5 (1.2)\n1.1 (0.2)\nSISc-Corrected\n3.8 (0.0)\n11.0 (13.1)\n0.5 (0.4)\n11.0 (13.1)\n0.61 (0.31)\n2.7 (0.1)\n0.9 (0.2)\nPMScCV-CoCo\n100.0 (0.0)\n0.0 (0.0)\n-\n-\n-\n25.2 (1.4)\n-\nPMScFS-CoCo\n3.8 (0.0)\n4.0 (8.5)\n0.5 (0.2)\n4.0 (8.5)\n0.45 (0.18)\n8.5 (1.2)\n1.7 (0.2)\nSISc-CoCo\n3.8 (0.0)\n11.0 (13.1)\n0.5 (0.3)\n11.0 (13.1)\n0.60 (0.29)\n2.7 (0.1)\n1.9 (0.2)\nBlock\ndiagonal\n1000\nOne-stage Corrected\n-\n-\n23.5 (29.0)\n1.1 (5.2)\n1.25 (0.73)\n0.0 (0.0)\n22.7 (4.5)\nPMScCV-Corrected\n100.0 (0.0)\n0.0 (0.0)\n25.6 (29.4)\n0.8 (4.2)\n1.30 (0.74)\n14.5 (0.7)\n22.5 (4.6)\nPMScFS-Corrected\n7.6 (0.0)\n3.4 (8.1)\n3.0 (3.2)\n3.6 (8.2)\n1.15 (0.69)\n5.3 (0.7)\n1.4 (0.3)\nSISc-Corrected\n7.6 (0.1)\n14.2 (14.8)\n1.5 (2.1)\n14.2 (14.9)\n0.97 (0.48)\n1.3 (0.1)\n1.1 (0.3)\nOne-stage CoCo\n-\n-\n14.3 (2.3)\n0.4 (3.2)\n1.23 (0.17)\n0.0 (0.0)\n417.2 (28.2)\nPMScCV-CoCo\n100.0 (0.0)\n0.0 (0.0)\n14.3 (2.2)\n0.5 (3.3)\n1.23 (0.17)\n14.5 (0.7)\n413.4 (28.5)\nPMScFS-CoCo\n7.6 (0.0)\n3.4 (8.1)\n0.9 (0.4)\n3.6 (8.2)\n0.68 (0.18)\n5.3 (0.7)\n2.9 (0.3)\nSISc-CoCo\n7.6 (0.1)\n14.2 (14.8)\n0.9 (0.4)\n14.2 (14.8)\n0.84 (0.31)\n1.3 (0.1)\n2.3 (0.2)\n2000\nOne-stage Corrected\n-\n-\n32.4 (17.1)\n3.0 (10.1)\n1.86 (0.65)\n0.0 (0.0)\n52.5 (9.7)\nPMScCV-Corrected\n100.0 (0.0)\n0.0 (0.0)\n31.8 (17.4)\n3.3 (10.1)\n1.84 (0.65)\n27.7 (1.5)\n52.1 (9.6)\nPMScFS-Corrected\n3.8 (0.0)\n6.6 (10.8)\n1.6 (1.6)\n6.6 (10.9)\n1.21 (0.69)\n8.3 (1.1)\n1.1 (0.2)\nSISc-Corrected\n3.8 (0.0)\n21.4 (16.6)\n0.8 (1.0)\n21.4 (16.6)\n1.09 (0.48)\n2.8 (0.1)\n1.0 (0.3)\nPMScCV-CoCo\n100.0 (0.0)\n0.0 (0.0)\n-\n-\n-\n27.7 (1.5)\n-\nPMScFS-CoCo\n3.8 (0.0)\n6.6 (10.8)\n0.5 (0.2)\n6.7 (10.9)\n0.73 (0.22)\n8.3 (1.1)\n2.5 (0.2)\nSISc-CoCo\n3.8 (0.0)\n21.4 (16.6)\n0.5 (0.3)\n21.4 (16.6)\n0.97 (0.36)\n2.8 (0.1)\n2.5 (0.2)\n36\nTable D.3: Performance of the one-stage and two-stage estimators in the simulation study based on false\npositive rate (FPR, in percentage), false negative rate (FNR, in percentage), \u21132 estimation error, and com-\nputation time (in seconds) when \u03a3x has an AR(1) structure with autocorrelation \u03c1x = 0.3. Standard error\nare included in parentheses. The CoCo estimator was only computed when the number of variables was no\nmore than 1000 (either in the 1st or 2nd step).\n\u03a3u\np\nEstimator\n1st step\n2nd step\nTime\nFPR\nFNR\nFPR\nFNR\n\u21132\n1st step\n2nd step\nDiagonal\n1000\nOne-stage Corrected\n-\n-\n2.5 (1.3)\n0.0 (0.0)\n0.35 (0.07)\n0.0 (0.0)\n31.5 (10.9)\nPMScCV-Corrected\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.19 (0.07)\n23.4 (0.3)\n0.3 (0.0)\nPMScFS-Corrected\n7.5 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.25 (0.07)\n5.3 (1.0)\n1.3 (0.3)\nSISc-Corrected\n7.5 (0.0)\n0.0 (0.0)\n1.2 (0.6)\n0.0 (0.0)\n0.31 (0.07)\n1.5 (0.0)\n1.8 (0.3)\nOne-stage CoCo\n-\n-\n8.6 (1.9)\n0.0 (0.0)\n0.49 (0.08)\n0.0 (0.0)\n519.1 (33.7)\nPMScCV-CoCo\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.19 (0.07)\n23.4 (0.3)\n0.0 (0.0)\nPMScFS-CoCo\n7.5 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.26 (0.07)\n5.3 (1.0)\n0.6 (0.1)\nSISc-CoCo\n7.5 (0.0)\n0.0 (0.0)\n0.2 (0.1)\n0.0 (0.0)\n0.32 (0.07)\n1.5 (0.0)\n0.6 (0.1)\n2000\nOne-stage Corrected\n-\n-\n1.5 (0.7)\n0.0 (0.0)\n0.36 (0.08)\n0.0 (0.0)\n80.6 (30.8)\nPMScCV-Corrected\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.19 (0.07)\n38.2 (1.6)\n0.2 (0.1)\nPMScFS-Corrected\n3.8 (0.0)\n0.0 (0.0)\n0.6 (0.3)\n0.0 (0.0)\n0.26 (0.07)\n16.9 (2.9)\n2.0 (0.4)\nSISc-Corrected\n3.8 (0.0)\n0.0 (0.0)\n0.6 (0.3)\n0.0 (0.0)\n0.31 (0.08)\n2.8 (0.1)\n1.5 (0.3)\nPMScCV-CoCo\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.19 (0.07)\n38.2 (1.6)\n0.0 (0.0)\nPMScFS-CoCo\n3.8 (0.0)\n0.0 (0.0)\n0.1 (0.1)\n0.0 (0.0)\n0.27 (0.07)\n16.9 (2.9)\n0.6 (0.1)\nSISc-CoCo\n3.8 (0.0)\n0.0 (0.0)\n0.1 (0.1)\n0.0 (0.0)\n0.31 (0.07)\n2.8 (0.1)\n0.6 (0.1)\nBlock\ndiagonal\n1000\nOne-stage Corrected\n-\n-\n2.0 (0.8)\n0.0 (0.0)\n0.59 (0.12)\n0.0 (0.0)\n18.9 (7.4)\nPMScCV-Corrected\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.36 (0.13)\n23.4 (0.4)\n0.3 (0.1)\nPMScFS-Corrected\n7.5 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.2)\n0.45 (0.13)\n5.4 (1.1)\n1.6 (0.3)\nSISc-Corrected\n7.5 (0.0)\n0.0 (0.0)\n1.0 (0.5)\n0.0 (0.0)\n0.53 (0.14)\n1.5 (0.0)\n2.2 (0.4)\nPMScCV-CoCo\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.36 (0.13)\n23.4 (0.4)\n0.0 (0.0)\nPMScFS-CoCo\n7.5 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.3)\n0.46 (0.11)\n5.4 (1.1)\n1.3 (0.1)\nSISc-CoCo\n7.5 (0.0)\n0.0 (0.0)\n0.1 (0.0)\n0.0 (0.0)\n0.55 (0.12)\n1.5 (0.0)\n1.6 (0.2)\n2000\nOne-stage Corrected\n-\n-\n1.2 (0.5)\n0.0 (0.0)\n0.61 (0.13)\n0.0 (0.0)\n45.7 (23.4)\nPMScCV-Corrected\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.37 (0.13)\n37.6 (1.6)\n0.3 (0.1)\nPMScFS-Corrected\n3.8 (0.0)\n0.0 (0.0)\n0.5 (0.3)\n0.0 (0.0)\n0.45 (0.12)\n17.2 (3.0)\n2.4 (0.4)\nSISc-Corrected\n3.8 (0.0)\n0.0 (0.0)\n0.5 (0.3)\n0.0 (0.0)\n0.55 (0.13)\n2.8 (0.1)\n1.9 (0.3)\nPMScCV-CoCo\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.0)\n0.0 (0.9)\n0.37 (0.17)\n37.6 (1.6)\n0.0 (0.0)\nPMScFS-CoCo\n3.8 (0.0)\n0.0 (0.0)\n0.1 (0.0)\n0.1 (1.3)\n0.46 (0.12)\n17.2 (3.0)\n1.7 (0.2)\nSISc-CoCo\n3.8 (0.0)\n0.0 (0.0)\n0.1 (0.0)\n0.0 (0.0)\n0.56 (0.13)\n2.8 (0.1)\n1.4 (0.1)\n37\nTable D.4: Performance of the one-stage and two-stage estimators in the simulation study based on false\npositive rate (FPR, in percentage), false negative rate (FNR, in percentage), \u21132 estimation error, and compu-\ntation time (in seconds) when \u03a3x has an homogeneous structure with \u03c1x = 0.3. Standard error are included\nin parentheses. The CoCo estimator was only computed when the number of variables was no more than\n1000 (either in the 1st or 2nd step).\n\u03a3u\np\nEstimator\n1st step\n2nd step\nTime\nFPR\nFNR\nFPR\nFNR\n\u21132\n1st step\n2nd step\nDiagonal\n1000\nOne-stage Corrected\n-\n-\n20.0 (29.0)\n0.1 (1.5)\n0.96 (0.77)\n0.0 (0.0)\n18.9 (3.3)\nPMScCV-Corrected\n100.0 (0.0)\n0.0 (0.0)\n19.7 (28.8)\n0.2 (2.0)\n0.95 (0.77)\n12.7 (0.7)\n18.7 (3.3)\nPMScFS-Corrected\n7.5 (0.0)\n1.2 (4.8)\n3.8 (3.4)\n1.2 (4.8)\n1.12 (0.85)\n6.0 (1.1)\n0.9 (0.1)\nSISc-Corrected\n7.6 (0.1)\n6.7 (10.5)\n1.0 (0.9)\n6.7 (10.5)\n0.54 (0.30)\n1.3 (0.1)\n1.0 (0.2)\nOne-stage CoCo\n-\n-\n11.5 (2.2)\n0.0 (0.0)\n0.77 (0.13)\n0.0 (0.0)\n481.5 (27.5)\nPMScCV-CoCo\n100.0 (0.0)\n0.0 (0.0)\n11.5 (2.2)\n0.0 (0.0)\n0.77 (0.13)\n12.7 (0.7)\n476.7 (26.8)\nPMScFS-CoCo\n7.5 (0.0)\n1.2 (4.8)\n0.8 (0.3)\n1.2 (4.8)\n0.36 (0.12)\n6.0 (1.1)\n0.9 (0.2)\nSISc-CoCo\n7.6 (0.1)\n6.7 (10.5)\n1.0 (0.4)\n6.7 (10.5)\n0.52 (0.23)\n1.3 (0.1)\n1.6 (0.2)\n2000\nOne-stage Corrected\n-\n-\n29.1 (21.0)\n1.1 (5.1)\n1.57 (0.84)\n0.0 (0.0)\n49.9 (7.0)\nPMScCV-Corrected\n100.0 (0.0)\n0.0 (0.0)\n28.3 (21.3)\n1.0 (5.1)\n1.53 (0.84)\n25.2 (1.4)\n49.3 (6.9)\nPMScFS-Corrected\n3.8 (0.0)\n1.1 (4.5)\n1.8 (1.7)\n1.1 (4.5)\n1.09 (0.86)\n10.1 (1.7)\n0.7 (0.1)\nSISc-Corrected\n3.8 (0.0)\n11.0 (13.1)\n0.5 (0.4)\n11.0 (13.1)\n0.61 (0.31)\n2.7 (0.1)\n0.9 (0.2)\nPMScCV-CoCo\n100.0 (0.0)\n0.0 (0.0)\n-\n-\n-\n25.2 (1.4)\n-\nPMScFS-CoCo\n3.8 (0.0)\n1.1 (4.5)\n0.4 (0.2)\n1.1 (4.5)\n0.33 (0.11)\n10.1 (1.7)\n0.9 (0.2)\nSISc-CoCo\n3.8 (0.0)\n11.0 (13.1)\n0.5 (0.3)\n11.0 (13.1)\n0.60 (0.29)\n2.7 (0.1)\n1.9 (0.2)\nBlock\ndiagonal\n1000\nOne-stage Corrected\n-\n-\n23.5 (29.0)\n1.1 (5.2)\n1.25 (0.73)\n0.0 (0.0)\n22.7 (4.5)\nPMScCV-Corrected\n100.0 (0.0)\n0.0 (0.0)\n25.6 (29.4)\n0.8 (4.2)\n1.30 (0.74)\n14.5 (0.7)\n22.5 (4.6)\nPMScFS-Corrected\n7.5 (0.0)\n0.9 (4.4)\n5.3 (3.2)\n0.9 (4.4)\n1.56 (0.74)\n5.9 (1.1)\n1.3 (0.4)\nSISc-Corrected\n7.6 (0.1)\n14.2 (14.8)\n1.5 (2.1)\n14.2 (14.9)\n0.97 (0.48)\n1.3 (0.1)\n1.1 (0.3)\nOne-stage CoCo\n-\n-\n14.3 (2.3)\n0.4 (3.2)\n1.23 (0.17)\n0.0 (0.0)\n417.2 (28.2)\nPMScCV-CoCo\n100.0 (0.0)\n0.0 (0.0)\n14.3 (2.2)\n0.5 (3.3)\n1.23 (0.17)\n14.5 (0.7)\n413.4 (28.5)\nPMScFS-CoCo\n7.5 (0.0)\n0.9 (4.4)\n0.7 (0.4)\n0.9 (4.4)\n0.57 (0.15)\n5.9 (1.1)\n2.2 (0.2)\nSISc-CoCo\n7.6 (0.1)\n14.2 (14.8)\n0.9 (0.4)\n14.2 (14.8)\n0.84 (0.31)\n1.3 (0.1)\n2.3 (0.2)\n2000\nOne-stage Corrected\n-\n-\n32.4 (17.1)\n3.0 (10.1)\n1.86 (0.65)\n0.0 (0.0)\n52.5 (9.7)\nPMScCV-Corrected\n100.0 (0.0)\n0.0 (0.0)\n31.8 (17.4)\n3.3 (10.1)\n1.84 (0.65)\n27.7 (1.5)\n52.1 (9.6)\nPMScFS-Corrected\n3.8 (0.0)\n2.0 (6.3)\n2.8 (1.5)\n2.0 (6.3)\n1.63 (0.71)\n10.1 (1.7)\n0.9 (0.3)\nSISc-Corrected\n3.8 (0.0)\n21.4 (16.6)\n0.8 (1.0)\n21.4 (16.6)\n1.09 (0.48)\n2.8 (0.1)\n1.0 (0.3)\nOne-stage CoCo\n-\n-\n-\n-\n-\n0.0 (0.0)\n-\nPMScCV-CoCo\n100.0 (0.0)\n0.0 (0.0)\n-\n-\n-\n27.7 (1.5)\n-\nPMScFS-CoCo\n3.8 (0.0)\n2.0 (6.3)\n0.4 (0.2)\n2.0 (6.3)\n0.59 (0.16)\n10.1 (1.7)\n1.9 (0.2)\nSISc-CoCo\n3.8 (0.0)\n21.4 (16.6)\n0.5 (0.3)\n21.4 (16.6)\n0.97 (0.36)\n2.8 (0.1)\n2.5 (0.2)\n38\n"}