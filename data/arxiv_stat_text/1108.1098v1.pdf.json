{"text": "arXiv:1108.1098v1  [math.ST]  4 Aug 2011\nAdjusted likelihood inference in an elliptical multivariate\nerrors-in-variables model\nTatiane F. N. Melo\nInstituto de Matem\u00b4atica e Estat\u00b4\u0131stica, Universidade Federal de Goi\u00b4as, Brazil\nSilvia L. P. Ferrari\u2217\nDepartamento de Estat\u00b4\u0131stica, Universidade de S\u02dcao Paulo, Brazil\nAbstract:\nIn this paper we obtain an adjusted version of the likelihood ratio test for errors-in-variables multivariate linear regression\nmodels. The error terms are allowed to follow a multivariate distribution in the class of the elliptical distributions, which\nhas the multivariate normal distribution as a special case. We derive a modi\ufb01ed likelihood ratio statistic that follows\na chi-squared distribution with a high degree of accuracy. Our results generalize those in Melo & Ferrari (Advances\nin Statistical Analysis, 2010, 94, 75\u201387) by allowing the parameter of interest to be vector-valued in the multivariate\nerrors-in-variables model. We report a simulation study which shows that the proposed test displays superior \ufb01nite\nsample behavior relative to the standard likelihood ratio test.\nKeywords: Elliptical distribution; Measurement error; Modi\ufb01ed likelihood ratio statistic; Multivariate errors-in-variables\nmodel.\n1\nIntroduction\nStatisticians are often faced with the problem of modeling data measured with error.\nAs an\nexample, we refer to Aoki et al. (2001), who compared the e\ufb00ectiveness of two types of toothbrushes\nin removing dental plaque. One explanatory variable is the dental plaque index before toothbrushing\nand the response variable is the dental plaque index after brushing, the amount of plaque being\nimprecisely measured. The authors proposed a null intercept regression model that assumes that this\nexplanatory variable is measured with an additive random error and the measurement error of the\nresponse variable is assumed to be absorbed by the error term of the model.\nErrors-in-variables models are generalizations of classical regression models.\nThe true (non-\nobservable) explanatory variables are treated either as random variables, in which case the model\nis said to be structural, or as unknown parameters, leading to a functional model. Structural mod-\nels are, in general, non-identi\ufb01able, while functional models induce unlimited likelihood functions.\n\u2217Corresponding author. Email: silviaferrari.usp@gmail.com\n1\nSuch di\ufb03culties disappear if some variances are assumed to be known (e.g. Chan & Mak (1979) and\nWong (1989)) or that the intercept is null (e.g. Aoki et al. (2001)). For details on errors-in-variables\nmodels the reader is referred to, for instance, Fuller (1987) and Buonaccorsi (2010).\nThe most popular errors-in-variables models for continuous outcomes are based on normality\nassumptions. The family of the elliptical distributions provides a useful alternative to the normal\ndistribution when outlying observations are present in the data. It nests the normal distribution,\nheavy-tailed distributions, such as the exponential power and the Student-t distributions, and light-\ntailed distributions. Further information on elliptical distributions can be found in Fang et al. (1990)\nand Fang and Anderson (1990).\nAs shown by Melo & Ferrari (2010), statistical inference in errors-in-variables models based on\n\ufb01rst-order asymptotic approximations can be imprecise for small or moderate sized samples.\nIn\nparticular the type I error of the likelihood ratio test is often larger than the designed level of the\ntest. Skovgaard (2001) proposed a general strategy to adjust the likelihood ratio statistic when interest\nlies in inference on a vector-valued parameter. The adjustment makes the resulting statistic to follow\na chi-squared distribution with a high degree of accuracy. The adjustment is broadly general, but\nrequires either some unusual likelihood quantities or the identi\ufb01cation of a suitable ancillary statistic\nsuch that, when coupled with the maximum likelihood estimator, constitutes a su\ufb03cient statistic for\nthe model. In the present paper, we obtain an appropriate ancilary statistic and derive Skovgaard\u2019s\nadjustment for a structural elliptical multivariate errors-in-variables model.\nThe paper unfolds as follows. Section 2 introduces the model. Section 3 contains our main results,\nnamely the ancillary statistic and an explicit formula for the modi\ufb01ed likelihood ratio test. The \ufb01nite\nsample behavior of the likelihood ratio test and its adjusted version is evaluated and discussed in\nSection 4. Our simulation results clearly show that the likelihood ratio test tends to be oversized\nand its modi\ufb01ed version is much less size-distorted.\nFinally, Section 5 closes the paper with our\nconclusions. Technical details are left for three appendices.\n2\nThe model\nThe (l+1)\u00d71 random vector Z is said to have a (l+1)-variate elliptical distribution with location\nvector \u00b5 ((l + 1) \u00d7 1), dispersion matrix \u03a3 ((l + 1) \u00d7 (l + 1)) and density generating function p0, and\nwe write Z \u223cEl(l+1)(\u00b5, \u03a3; p0), if\nZ d= \u00b5 + AZ\u2217,\nwhere A is (l + 1) \u00d7 k matrix with rank(A) = k, AA\u22a4= \u03a3 and Z\u2217is a (l + 1) \u00d7 1 random vector\nwith density function p0(z\u22a4z), for z \u2208\u211c(l+1). The notation X\nd= Y indicates that X and Y have\nthe same distribution. It is assumed that\nR \u221e\n0 y(l+1)/2\u22121p0(y)dy < \u221e. The density function of Z is\np(z, \u00b5, \u03a3) = |\u03a3|\u22121/2p0\n\u0010\n(z \u2212\u00b5)\u22a4\u03a3\u22121(z \u2212\u00b5)\n\u0011\n.\n(1)\n2\nSome special cases of (1) are the following multivariate distributions: normal, exponential power,\nPearson II, Pearson VII, Student-t, generalized Student-t, logistic I, logistic II and Cauchy.\nThe\nelliptical distributions share many properties with the multivariate normal distribution. In particular,\nmarginal distributions are elliptical. For a full account of the properties of the elliptical distributions,\nsee Fang et al. (1990, Sect. 2.5).\nWe consider the following model, which consists of p independent errors-in-variables structural\nmodels:\nY jk = \u03b1k + \u03b2kxjk + ejk,\nXjk = xjk + ujk,\n(2)\nfor j = 1, 2, . . . , nk and k = 1, 2, . . . , p, where Y jk = (Y1jk, Y2jk, . . . , Yljk), \u03b1k = (\u03b11k, \u03b12k, . . . , \u03b1lk),\n\u03b2k = (\u03b21k, \u03b22k, . . . , \u03b2lk) and ejk = (e1jk, e2jk, . . . , eljk)\u22a4. Here, xjk is not observed directly. Instead,\nwe observe Xjk, which is viewed as xjk plus a measurement error, ujk. We assume that E(ejk) = 0,\nVar(ejk) = \u03a3ek = diag\n\b\n\u03c32\ne1k, \u03c32\ne2k, . . . , \u03c32\nelk\n\t\nand Cov(ej\u2032k\u2032, ejk) = 0; E(ujk) = 0, Var(ujk) = \u03c32\nuk\nand Cov(uj\u2032k\u2032, ujk) = 0; E(xjk) = \u00b5xk, Var(xjk) = \u03c32\nxk and Cov(xj\u2032k\u2032, xjk) = 0; Cov(xj\u2032k\u2032, ujk) = 0;\nCov(ej\u2032k\u2032, ujk) = Cov(ej\u2032k\u2032, xjk) = 0, with (j\u2032, k\u2032) \u0338= (j, k).\nModel (2) can be written as\nZjk = \u03b4k + \u2206kbjk,\n(3)\nfor j = 1, 2, . . . , nk and k = 1, 2, . . . , p, where\nZjk =\n\u0012 Y jk\nXjk\n\u0013\n,\n\u03b4k =\n\u0012 \u03b1k\n0\n\u0013\n,\n\u2206k =\n\u0012 \u03b2k\nIl\n0\n1\n0\n1\n\u0013\nand bjk =\n\uf8eb\n\uf8ed\nxjk\nejk\nujk\n\uf8f6\n\uf8f8,\nwhere Il is the identity matrix of dimension l. We assume that, for each k = 1, 2, . . . , p, the errors\nb1k, b2k, . . . , bnkk are independent and bjk \u223cEl(l+2)(\u03b7k, \u2126k; p0), with\n\u03b7k =\n\uf8eb\n\uf8ed\n\u00b5xk\n0\n0\n\uf8f6\n\uf8f8\nand \u2126k =\n\uf8eb\n\uf8ed\n\u03c32\nxk\n0\n0\n0\n\u03a3ek\n0\n0\n0\n\u03c32\nuk\n\uf8f6\n\uf8f8.\nTherefore, for each k = 1, 2, . . . , p, the random vectors Z1k, Z2k, . . . , Znkk are independent and Zjk \u223c\nEll+1(\u00b5k, \u03a3k; p0), with \u00b5k = \u03b4k + \u2206k\u03b7k and \u03a3k = \u2206k\u2126k\u2206\u22a4\nk (Fang et al. 1990, Sect. 2.5). We can\nwrite \u00b5k and \u03a3k as\n\u00b5k =\n\u0012 \u03b1k + \u03b2k\u00b5xk\n\u00b5xk\n\u0013\nand \u03a3k =\n\u0012 \u03b2k\u03c32\nxk\u03b2\u22a4\nk + \u03a3ek\n\u03b2k\u03c32\nxk\n\u03c32\nxk\u03b2\u22a4\nk\n\u03c32\nxk + \u03c32\nuk\n\u0013\n.\nRegression model (2) generalizes the normal structural models proposed by Cox (1976) (l = 1) and\nRusso et al. (2009) (l = 2), and the elliptical structural model considered in Melo & Ferrari (2010)\n(l = p = 1); a closely related model is presented by Garcia-Alfaro & Bolfarine (2001). As expected,\n3\nmodel (2) is not identi\ufb01able because the relation between the parameters of the distribution of Zjk\nand \u03b8(k) =\n\u0000\u03b2\u22a4\nk , \u03b1k, \u00b5xk, \u03c32\nxk, \u03c32\nuk, \u03c32\u22a4\nek\n\u0001\u22a4is not unique.\nAssumptions on \u03c32\nxk and \u03c32\nek are usually\nimposed to overcome identi\ufb01ability problems. It is common to assume that the \u03bbxk = \u03c32\nxk/\u03c32\nuk or\n\u03bbeik = \u03c32\neik/\u03c32\nuk, for i = 1, 2, . . . , l, is known. An alternative assumption is that the intercept \u03b1k is\nknown (see Aoki et al., 2001). Under each of these identi\ufb01ability assumptions we have:\n(i) if \u03bbxk is known,\n\u03b8(k) =\n\u0010\n\u03b2\u22a4\nk , \u03b1k, \u00b5xk, \u03c32\nuk, \u03c32\u22a4\nek\n\u0011\u22a4\nand \u03a3k =\n\u0012 \u03b2k\u03b2\u22a4\nk \u03bbxk\u03c32\nuk + \u03a3ek\n\u03b2k\u03bbxk\u03c32\nuk\n\u03bbxk\u03c32\nuk\u03b2\u22a4\nk\n(\u03bbxk + 1)\u03c32\nuk\n\u0013\n;\n(ii) if \u03bbek is known,\n\u03b8(k) =\n\u0010\n\u03b2\u22a4\nk , \u03b1k, \u00b5xk, \u03c32\nxk, \u03c32\nuk\n\u0011\u22a4\nand \u03a3k =\n\u0012 \u03b2k\u03c32\nxk\u03b2\u22a4\nk + \u03bbek\u03c32\nuk\n\u03b2k\u03c32\nxk\n\u03c32\nxk\u03b2\u22a4\nk\n\u03c32\nxk + \u03c32\nuk\n\u0013\n,\nwith \u03bbek = diag {\u03bbe1k, \u03bbe2k, . . . , \u03bbelk};\n(iii) if \u03b1 is known,\n\u03b8(k) =\n\u0010\n\u03b2\u22a4\nk , \u00b5xk, \u03c32\nxk, \u03c32\nuk, \u03c32\u22a4\nek\n\u0011\u22a4\nand \u03a3k =\n\u0012 \u03b2k\u03c32\nxk\u03b2\u22a4\nk + \u03a3ek\n\u03b2k\u03c32\nxk\n\u03c32\nxk\u03b2\u22a4\nk\n\u03c32\nxk + \u03c32\nuk\n\u0013\n.\nThe independent structural elliptical model can be de\ufb01ned in terms of the density function of\nZ =\n\u0010\nZ\u22a4\n(1), Z\u22a4\n(2), . . . , Z\u22a4\n(p)\n\u0011\u22a4\n, with Z(k) =\n\u0000Z\u22a4\n1k, Z\u22a4\n2k, . . . , Z\u22a4\nnkk\n\u0001\u22a4, for k = 1, 2, . . . , p, which is given\nby\npZ(z, \u03b8) =\np\nY\nk=1\nnk\nY\nj=1\n|\u03a3k|\u22121/2p0(d\u22a4\njk\u03a3\u22121\nk djk),\nwhere djk = djk(\u03b8(k)) = zjk \u2212\u00b5k, for j = 1, 2, . . . , nk, k = 1, 2, . . . , p, and \u03b8 =\n\u0010\n\u03b8\u22a4\n(1), \u03b8\u22a4\n(2), . . . , \u03b8\u22a4\n(p)\n\u0011\u22a4\n.\nThe log-likelihood function for the k-th group, k = 1, 2, . . . , p, is given by\n\u2113k(\u03b8, z) = \u2212nk\n2 log |\u03a3k| +\nnk\nX\nj=1\nlog p0(d\u22a4\njk\u03a3\u22121\nk djk).\nFor a sample of size n = Pp\nk=1 nk and p populations, the log-likelihood function is\n\u2113(\u03b8, z) =\np\nX\nk=1\n\u2113k(\u03b8, z).\n(4)\nMaximum likelihood estimation of the parameters can be carried out by numerically maximizing the\nlog-likelihood function (4) through an iterative algorithm such as the Newton\u2013Raphson, the Fisher\nscoring, EM or BFGS. Our numerical results were obtained using the library function MaxBFGS in the\nOx matrix programming language (Doornik 2006).\n4\n3\nAncillary statistic and modi\ufb01ed likelihood ratio test\nThe parameter vector \u03b8 is partitioned as \u03b8 = (\u03c8\u22a4, \u03c9\u22a4)\u22a4, with \u03c8 representing the parameter of\ninterest and \u03c9 the nuisance parameter. Our interest lies in testing H0 : \u03c8 = \u03c8(0) versus H1 : \u03c8 \u0338= \u03c8(0),\nwhere \u03c8(0) is a q-dimensional parameter of known constants. The maximum likelihood estimator of \u03b8\nis denoted by b\u03b8 = ( b\n\u03c8\n\u22a4, b\u03c9\u22a4)\u22a4and the corresponding estimator obtained under the null hypothesis is\ne\u03b8 = ( e\n\u03c8\n\u22a4, e\u03c9\u22a4)\u22a4, where e\u03c8 = \u03c8(0). We use hat and tilde to indicate evaluation at b\u03b8 and e\u03b8, respectively.\nThe likelihood ratio statistic for testing H0 is given by\nLR = 2\nn\n\u2113( b\n\u03c8) \u2212\u2113( e\n\u03c8)\no\n.\nUnder H0, LR converges to a chi-square distribution with q degrees of freedom, where q in the\nnumber of restrictions imposed by H0. This approximation can be improved if one applies a suitable\nadjustment to the test statistic. Skovgaard (2001) proposed two adjusted likelihood ratio statistics\nthat are asymptotically equivalent for testing H0. We shall denote them by LR\u2217and LR\u2217\u2217. The\nadjustment terms depend on a suitable ancillary statistic and involves derivatives with respect to\nthe sample space. A statistic a is said to be an ancillary statistic if it is distribution constant and,\nwhen coupled with the maximum likelihood estimator b\u03b8, is a minimal su\ufb03cient statistic for the model\n(Barndor\ufb00\u2013Nielsen (1986)). If (b\u03b8, a) is su\ufb03cient, but not minimal su\ufb03cient, Skovgaard\u2019s results still\nhold; see Severini (2000, Sect. 6.5). In this case, the log-likelihood function depends on the data only\nthrough (b\u03b8, a) and we write \u2113(\u03b8; b\u03b8, a). The sample space derivatives involved are \u2113\u2032 = \u2202\u2113(\u03b8; b\u03b8, a)/\u2202b\u03b8\nand U \u2032 = \u22022\u2113(\u03b8; b\u03b8, a)/\u2202b\u03b8\u2202\u03b8\u22a4. The adjusted statistics are given by\nLR\u2217= LR\n\u0012\n1 \u2212\n1\nLR log \u03c1\n\u00132\n(5)\nand\nLR\u2217\u2217= LR \u22122 log \u03c1,\n(6)\nwith\n\u03c1 = | bJ |1/2|eU \u2032|\u22121| eJ\u03c9\u03c9|1/2|eeJ\u03c9\u03c9|\u22121/2|eeJ |1/2\n{eU \u22a4eeJ\n\u22121 eU}p/2\nLRq/2\u22121(b\u2113\u2032 \u2212e\u2113\u2032)\u22a4(eU \u2032)\u22121 eU\n.\n(7)\nHere eeJ equals \u22022\u2113(\u03b8; b\u03b8, a)/\u2202b\u03b8\u2202\u03b8\u22a4evaluated at b\u03b8 = e\u03b8 and \u03b8 = e\u03b8. Also,\neeJ\u03c9\u03c9 is the lower right\nsubmatrix of eeJ that corresponds to the nuisance parameter \u03c9. Both statistics have an approximate\nX 2\nq distribution with high degree of accuracy under the null hypothesis (Skovgaard, 2001, p. 7).\nLet a = a(z) =\n\u0010\na\u22a4\n(1)(z), . . . , a\u22a4\n(p)(z)\n\u0011\u22a4\n, where a\u22a4\n(k)(z) =\n\u0010\na\u22a4\n1k(z), . . . , a\u22a4\nnkk(z)\n\u0011\u22a4\n, with ajk(z) =\nbP \u22121\nk (z) (zjk \u2212b\u00b5k(z)) ,\nj = 1, 2, . . . , nk,\nk = 1, 2, . . . , p, where Pk is a lower triangular matrix such\n5\nthat PkP \u22a4\nk = \u03a3k is the Cholesky decomposition. Following Melo & Ferrari (2010) it can be shown\nthat a is an ancillary statistic. With this ancillary statistic we can obtain the sample space derivatives\nwhich are required for the computation of the adjustment term \u03c1.\nIn the following we present some matrices and vectors that form the score U (Appendix A), the\nobserved information matrix J (Appendix A) and the sample space derivatives \u2113\u2032, U \u2032 and eeJ (Appendix\nB). In matrix notation, we have U = (U \u22a4\n(1), . . . , U \u22a4\n(p))\u22a4, J = diag{J(1), . . . , J(p)}, \u2113\u2032 = (\u2113\u2032\u22a4\n(1), . . . , \u2113\u2032\u22a4\n(p))\u22a4,\nU \u2032 = diag{U \u2032\n(1), . . . , U \u2032\n(p)} and eeJ = diag{ eeJ(1), . . . , eeJ(p)}, with U(k) = \u2212nk\n2 n\u2217\n(k) + R\u22a4\n(k)h(k),\nJ(k) =\nnk\n2 T(k)\u2212R\u22a4\n(k)M(k)\u2212V \u22a4\n(k)Q(k), \u2113\u2032(k) = 2R\u22a4\n(k)w(k), U \u2032(k) = 2(R\u22a4\n(k)B(k)+V \u22a4\n(k)C(k)) and eeJ(k) = 2( bR\u22a4\n(k)F(k)+\nbV \u22a4\n(k)G(k)). The i-th element of n\u2217\n(k) is tr(\u03a3\u22121\nk \u03a3(k)i), for i = 1, 2, . . . , s and k = 1, 2, . . . , p. Here, s is\nthe total number of parameters in \u03b8(k). When the ratio \u03bbxk or the intercept \u03b1k is known, we have\ns = 2l + 3, and when the ratio \u03bbek is known, s = l + 4. The (i, i\u2032)-th element of T(k) is\nt(k)ii\u2032 = tr(\u03a3(k)i\u03a3(k)i\u2032) + tr(\u03a3k\u22121\u03a3(k)ii\u2032),\nwhere \u03a3(k)i = \u2202\u03a3k/\u2202\u03b8(k)i, \u03a3(k)ii\u2032 = \u2202\u03a3(k)i/\u2202\u03b8(k)i\u2032 and \u03a3(k)i = \u2202\u03a3\u22121\nk /\u2202\u03b8(k)i = \u2212\u03a3\u22121\nk \u03a3(k)i\u03a3\u22121\nk , for i, i\u2032 =\n1, 2, . . . , s. Here, \u03b8(k)i is the i-th element of \u03b8k; see Appendix C. Also, R(k) and V(k) are block-diagonal\nmatrices given by R(k) = diag(r(k), r(k), . . . , r(k)) and V(k) = diag(v(k), v(k), . . . , v(k)), with dimension\nsnk \u00d7 s, and j-th element of the vectors r(k) and v(k) given by r(k)j = Wp0(d\u22a4\njk\u03a3\u22121\nk djk) and v(k)j =\nW \u2032\np0(d\u22a4\njk\u03a3\u22121\nk djk), respectively. Additionally, we de\ufb01ne the column vectors h(k) =\n\u0010\nh(1)\u22a4\n(k) , . . . , h(s)\u22a4\n(k)\n\u0011\u22a4\nand w(k) =\n\u0010\nw(1)\u22a4\n(k) , . . . , w(s)\u22a4\n(k)\n\u0011\u22a4\n, with dimension snk, and j-th element of the vectors h(i)\n(k) and w(i)\n(k),\nfor i = 1, 2, . . . , s, given, respectively, by\nh(i)\n(k)j = d\u22a4\njk\u03a3(k)idjk \u22122\u00b5\u22a4\n(k)i\u03a3\u22121\nk djk\nand\nw(i)\n(k)j =\n\u0010\nbP(k)iajk + b\u00b5(k)i\n\u0011\u22a4\n\u03a3\u22121\nk\n\u0010\nbPkajk + b\u00b5k \u2212\u00b5k\n\u0011\n,\nwhere bP(k)i = \u2202bPk/\u2202b\u03b8(k)i and b\u00b5(k)i = \u2202b\u00b5k/\u2202b\u03b8(k)i.\nThe derivative bP(k)i is obtained through the\nalgorithm proposed by Smith (1995) and the derivative b\u00b5(k)i is presented in Appendix C. The block\nmatrices B(k), C(k), F(k), G(k), M(k) and Q(k), with dimension snk \u00d7 s, have the (i, i\u2032)-th block given,\nrespectively, by the vectors bii\u2032\n(k), cii\u2032\n(k), fii\u2032\n(k), gii\u2032\n(k), mii\u2032\n(k) and qii\u2032\n(k). The j-th elements of these vectors\nare, respectively,\nbii\u2032\n(k)j = ( bP(k)i\u2032 ajk + b\u00b5(k)i\u2032)\u22a4\u03a3(k)i( bPkajk + b\u00b5k \u2212\u00b5k) \u2212\u00b5\u22a4\n(k)i\u03a3\u22121\nk ( bP(k)i\u2032 ajk + b\u00b5(k)i\u2032),\ncii\u2032\n(k)j = ( bP(k)i\u2032 ajk + b\u00b5(k)i\u2032)\u22a4\u03a3\u22121\nk ( bPkajk + b\u00b5k \u2212\u00b5k)\nh\n( bPkajk + b\u00b5k \u2212\u00b5k)\u22a4\u03a3(k)i( bPkajk + b\u00b5k \u2212\u00b5k)\n\u22122\u00b5\u22a4\n(k)i\u03a3k\u22121( bPkajk + b\u00b5k \u2212\u00b5k)\ni\n,\nf ii\u2032\n(k)j = ( eP(k)i\u2032 ajk + e\u00b5(k)i\u2032)\u22a4e\u03a3(k)i ePkajk \u2212e\u00b5\u22a4\n(k)ie\u03a3\u22121\nk ( eP(k)i\u2032 ajk + e\u00b5(k)i\u2032),\n6\ngii\u2032\n(k)j = ( eP(k)i\u2032 ajk + e\u00b5(k)i\u2032)\u22a4e\u03a3\u22121\nk\nePkajk\n\u0010\na\u22a4\njk eP \u22a4\nk e\u03a3(k)i ePkajk \u22122e\u00b5\u22a4\n(k)ie\u03a3\u22121\nk\nePkajk\n\u0011\n,\nmii\u2032\n(k)j = d\u22a4\njk\u03a3(k)ii\u2032djk \u22122\u00b5\u22a4\n(k)i\u03a3(k)i\u2032djk \u22122\u00b5\u22a4\n(k)i\u2032\u03a3(k)idjk \u22122\u00b5\u22a4\n(k)ii\u2032\u03a3k\u22121djk + 2\u00b5\u22a4\n(k)i\u03a3k\u22121\u00b5(k)i\u2032,\nq(jk)\ni\n=\n\u0010\nd\u22a4\njk\u03a3(k)idjk \u22122\u00b5\u22a4\n(k)i\u03a3k\u22121djk\n\u0011 \u0010\nd\u22a4\njk\u03a3(k)i\u2032djk \u22122\u00b5\u22a4\n(k)i\u2032\u03a3k\u22121djk\n\u0011\n,\nwhere \u00b5(k)ii\u2032 = \u2202\u00b5(k)i/\u2202\u03b8(k)i\u2032 and \u03a3(k)ii\u2032 = \u2202\u03a3(k)i/\u2202\u03b8(k)i\u2032 = \u22122\u03a3(k)i\u03a3(k)i\u2032\u03a3\u22121\nk\n\u2212\u03a3\u22121\nk \u03a3(k)ii\u2032\u03a3\u22121\nk ; see\nAppendix C.\nBy replacing bJ, eJ\u03c9\u03c9, eeJ\u03c9\u03c9, eeJ, eU, eU \u2032, b\u2113\u2032 \u2212e\u2113\u2032 and the likelihood ratio statistic LR in (7) we obtain\n\u03c1, the quantity that is required for computing the adjusted statistic LR\u2217in (5) and its equivalent\nversion LR\u2217\u2217given in (6). Note that \u03c1 depends on Zjk, \u00b5k, \u03a3k, \u03a3\u22121\nk , Pk and their \ufb01rst and second\nderivatives with respect to the parameters. It is worth mentioning that the distribution of Zjk is only\nrequired for obtaining the matrices R(k) and V(k).\nAs a \ufb01nal remark, we mention the connection between our results and those obtained by Melo &\nFerrari (2010). In their paper, the model under study is the special case of model (2) when l = p = 1.\nThe authors obtained the Barndor\ufb00-Nielsen (1986) adjustment to the signed likelihood ratio statistic\nfor testing hypotheses on a scalar parameter. The adjustment term, given in eq. (6) of their paper,\ncan be calculated using the quantities obtained in the present paper for the case in which \u03c8 is scalar\n(q = 1). Therefore, our results enables us to calculate Barndor\ufb00-Nielsen\u2019s (1986) adjusted signed\nlikelihood ratio statistic in model (2). Hence, our results generalize those in Melo & Ferrari (2010).\n4\nSimulation study\nIn this section we present a Monte Carlo simulation study to evaluate the e\ufb03cacy of the adjust-\nments derived in the previous section. The performances of the tests that use the likelihood ratio\nstatistic (LR), and the adjusted statistics (LR\u2217and LR\u2217\u2217) will be compared with respect to the type\nI error probability.\nThe simulations use model (3) with l = 1 and p = 5. Two di\ufb00erent distributions for Zjk are\nconsidered, namely a bivariate normal distribution and a bivariate Student-t with 3 degrees of freedom\n(\u03bd = 3). The number of Monte Carlo replications was 10,000, the nominal levels of the tests are\n\u03b3 = 1%, 5% and 10% and the sample sizes are n1 = . . . = np = 10, 20, 30 and 40. All simulations were\nperformed using the Ox matrix programming language; Doornik (2006).\nWe consider tests of H0 : \u03c8 = \u03c8(0)\nversus\nH1 : \u03c8 \u0338= \u03c8(0), where \u03c8 = (\u03b21, \u03b22, . . . , \u03b2q)\u22a4,\nfor q = 2, 3, 4, 5. Also, we consider \u03c8(0) = 0 when \u03bbxk or \u03bbek is known, and \u03c8(0) = 1 when the\n7\nintercept is null.\nThe true parameter values are \u03b11 = \u00b7 \u00b7 \u00b7 = \u03b15 = 0.5, \u03c32\nx1 = \u00b7 \u00b7 \u00b7 = \u03c32\nx5 = 1.5,\n\u03c32\nu1 = \u00b7 \u00b7 \u00b7 = \u03c32\nu5 = 0.5, \u03c32\ne1 = \u00b7 \u00b7 \u00b7 = \u03c32\ne5 = 2.0. For \u03bbxk or \u03bbek known, we set \u00b5x1 = \u00b7 \u00b7 \u00b7 = \u00b5x5 = 0.5,\nand when the intercept is null we set \u00b5x1 = \u00b7 \u00b7 \u00b7 = \u00b5x5 = 5.0.\nTables 1 and 2 present rejection rates (in percentage) of the three tests for all the scenarios\ndescribed above. We notice that the likelihood ratio test (LR) is liberal when the sample size is small\nin all the cases considered here. For instance, when Zjk is normally distributed, q = 3, \u03bbek is known\nand nk = 10, the rejection rates of the test that uses LR are 11.4% (\u03b3 = 5%) and 19.4% (\u03b3 = 10%);\nsee Table 1. Under the same scenario, except that Zjk now follows a Student-t distribution, the\nrejection rates are 10.9% (\u03b3 = 5%) and 18.6% (\u03b3 = 10%). The adjusted tests (LR\u2217and LR\u2217\u2217), on\nthe other hand, display much better behavior in all cases: they are much less size distorted than the\nlikelihood ratio test. For example, in the normal case with \u03bbxk known, nk = 10 and \u03b3 = 10%, the\nrejection rates are 16.9% (LR), 10.2% (LR\u2217) and 9.8% (LR\u2217\u2217) for q = 2, and 18.6% (LR), 10.2%\n(LR\u2217) and 9.5% (LR\u2217\u2217) for q = 3. As a second example, we mention the case in which the underlying\ndistribution is normal, the intercept is null, q = 5, nk = 10 and \u03b3 = 5%. The rejection rates are\n9.3% (LR), 5.2% (LR\u2217) and 5.0% (LR\u2217\u2217). Also, for the normal case with \u03bbek known, nk = 20 and\n\u03b3 = 1% the rejection rates are 1.9% (LR), 1.1% (LR\u2217) and 1.0% (LR\u2217\u2217). It can be noticed that,\nas the number of parameters under test (q) grows, the likelihood ratio test deteriorates while the\nbehavior of the adjusted tests remains unaltered. See, for example, the \ufb01gures in Table 1 relative to\nthe Student-t case with \u03bbek known, nk = 10 and \u03b3 = 10%; the rejection rates are 18.4% (q = 2),\n18.6% (q = 3), 19.9% (q = 4) and 21.3% (q = 5) for LR, 11.4% (q = 2), 10.4% (q = 3), 10.8% (q = 4)\nand 11.0% (q = 5) for LR\u2217, and 10.9% (q = 2), 10.1% (q = 3), 10.2% (q = 4) and 10.1% (q = 5) for\nLR\u2217\u2217.\n[Tables 1 and 2 here]\nOur numerical results con\ufb01rm that the adjusted tests are much better behaved than the original\nlikelihood ratio test in small samples. For almost all the cases, the test that uses the LR\u2217\u2217displays\nslightly better performance than its asymptotically equivalent version, LR\u2217.\n5\nConcluding remarks\nIn this paper we dealt with the issue of performing hypothesis testing in an elliptical multivariate\nerrors-in-variables model when the sample size is small. We derived modi\ufb01ed likelihood ratio statistics\nthat follow very closely a chi-squared distribution under the null hypothesis. Our approach is based\non Skovgaard\u2019s (2001) proposal, which requires the identi\ufb01cation of a suitable ancillary statistic. We\nobtained the required ancillary and all the needed quantities to explicitly write the correction term.\nOur simulation results clearly suggested that the adjustment we derived is able to correct the liberal\nbehavior of the likelihood ratio test in small samples.\n8\nAcknowledgements\nWe gratefully acknowledge \ufb01nancial support from FAPESP and CNPq.\nAppendix A. The observed information matrix\nThe \ufb01rst derivative of the log-likelihood function for the k-th group, k = 1, 2, . . . , p, with respect to the parameters\nis\n\u2202\u2113k(\u03b8)\n\u2202\u03b8(k)i\n= \u2212nk\n2 tr\n\u0000\u03a3k\n\u22121\u03a3(k)i\n\u0001\n+\nnk\nX\ni=1\nWp0(d\u22a4\njk\u03a3\u22121\nk djk)\n\u0010\nd\u22a4\njk\u03a3(k)idjk \u22122\u00b5(k)i\u03a3k\n\u22121djk\n\u0011\n,\nfor i = 1, 2, . . . , s, where Wp0(u) = \u2202log p0(u)/\u2202u, \u00b5(k)i = \u2202\u00b5k/\u2202\u03b8(k)i, \u03a3(k)i = \u2202\u03a3k/\u2202\u03b8(k)i and \u03a3(k)i = \u2202\u03a3k\n\u22121/\u2202\u03b8(k)i =\n\u2212\u03a3k\n\u22121\u03a3(k)i\u03a3k\n\u22121.\nThe (i, i\u2032)-th element of the observed information matrix for the k-th group, J(k), is given by\nJ(k)ii\u2032 = \u2212\u22022\u2113k(\u03b8)/\u2202\u03b8(k)i\u2202\u03b8(k)i\u2032, i.e.\nJ(k)ii\u2032 = nk\n2 tr\n\u0010\n\u03a3(k)i\u03a3(k)i\u2032\n\u0011\n+ nk\n2 tr\n\u0000\u03a3k\n\u22121\u03a3(k)ii\u2032\u0001\n\u2212\nnk\nX\ni=1\n(\nW \u2032\np0\n\u0010\nd\u22a4\njk\u03a3k\n\u22121djk\n\u0011 \u0010\nd\u22a4\njk\u03a3(k)idjk\n\u22122\u00b5\u22a4\n(k)i\u03a3k\n\u22121djk\n\u0011\u0010\nd\u22a4\njk\u03a3(k)i\u2032djk \u22122\u00b5\u22a4\n(k)i\u2032\u03a3k\n\u22121djk\n\u0011\n+ Wp0\n\u0010\nd\u22a4\njk\u03a3k\n\u22121djk\n\u0011 \u0010\nd\u22a4\njk\u03a3(k)ii\u2032djk\n\u22122\u00b5\u22a4\n(k)i\u03a3(k)i\u2032djk \u22122\u00b5\u22a4\n(k)i\u2032\u03a3(k)idjk \u22122\u00b5\u22a4\n(k)ii\u2032\u03a3k\n\u22121djk + 2\u00b5\u22a4\n(k)i\u03a3k\n\u22121\u00b5(k)i\u2032\n\u0011)\n,\nfor i, i\u2032 = 1, 2, . . . , s and k = 1, 2, . . . , p, where W \u2032\np0(u) = \u2202Wp0(u)/\u2202u, \u00b5(k)ii\u2032 = \u2202\u00b5(k)i/\u2202\u03b8(k)i\u2032, \u03a3(k)ii\u2032 = \u2202\u03a3(k)i/\u2202\u03b8(k)i\u2032\nand \u03a3(k)ii\u2032 = \u2202\u03a3(k)i/\u2202\u03b8(k)i\u2032 = \u22122\u03a3(k)i\u2032\u03a3(k)i\u03a3k\n\u22121 \u2212\u03a3k\n\u22121\u03a3(k)ii\u2032\u03a3k\n\u22121.\nAppendix B. Sample space derivatives (\u2113\u2032, U \u2032 and eeJ)\nLet a be the ancillary statistic de\ufb01ned in Section 3 and let us write zjk = bPkajk + b\u00b5k.\nInserting zjk in the\nlog-likelihood function we have\n\u2113k(\u03b8; b\u03b8, a) = \u2212nk\n2 log |\u03a3k| +\nnk\nX\ni=1\nlog p0\n\u0010\n( bPkajk + b\u00b5k \u2212\u00b5k)\u22a4\u03a3k\n\u22121( bPkajk + b\u00b5k \u2212\u00b5k)\n\u0011\n.\nHence, \u2113\u2032 = \u2202\u2113(\u03b8; b\u03b8, a)/\u2202b\u03b8 =\n\u0010\n\u2113\u2032\u22a4\n(1), \u2113\u2032\u22a4\n(2), . . . , \u2113\u2032\u22a4\n(p)\n\u0011\u22a4\n, where the i-th element of the vector \u2113\u2032\n(k) is\n\u2113\u2032\n(k)i = 2\nnk\nX\ni=1\nWp0\n\u0010\n( bPkajk + b\u00b5k \u2212\u00b5k)\u22a4\u03a3k\n\u22121( bPkajk + b\u00b5k \u2212\u00b5k)\n\u0011\n( bP(k)iajk + b\u00b5(k)i)\u22a4\u03a3k\n\u22121( bPkajk + b\u00b5k \u2212\u00b5k),\n9\nfor i\n=\n1, 2, . . . , s and k\n=\n1, 2, . . . , p.\nAlso, we have that U \u2032\n=\n\u22022\u2113(\u03b8; b\u03b8, a)/\u2202b\u03b8\u2202\u03b8\u22a4\n=\ndiag\nn\nU \u2032\n(1), U \u2032\n(2),\n. . . , U \u2032\n(p)\no\n, where the (i, i\u2032)-th element of the matrix U \u2032\n(k) is given by\nU \u2032\n(k)ii\u2032 = 2\nnk\nX\ni=1\n(\nWp0\n\u0010\nbPkajk + b\u00b5k \u2212\u00b5k)\u22a4\u03a3k\n\u22121( bPkajk + b\u00b5k \u2212\u00b5k)\n\u0011 \u0010\n( bP(k)i\u2032ajk + b\u00b5(k)i\u2032)\u22a4\u03a3(k)i( bPkajk + b\u00b5k\n\u2212\u00b5k) \u2212\u00b5\u22a4\n(k)i\u03a3k\n\u22121( bP(k)i\u2032ajk + b\u00b5(k)i\u2032)\n\u0011\n+ W \u2032\np0\n\u0010\n( bPkajk + b\u00b5k \u2212\u00b5k)\u22a4\u03a3k\n\u22121( bPkajk + b\u00b5k \u2212\u00b5k)\n\u0011\n( bP(k)i\u2032ajk + b\u00b5(k)i\u2032)\u22a4\u03a3k\n\u22121( bPkajk + b\u00b5k \u2212\u00b5k)\n\u0010\n( bPkajk + b\u00b5k \u2212\u00b5k)\u22a4\u03a3(k)i( bPkajk + b\u00b5k \u2212\u00b5k) \u22122\u00b5\u22a4\n(k)i\n\u03a3k\n\u22121( bPkajk + b\u00b5k \u2212\u00b5k)\n\u0011)\n,\nwhere bP(k)i = \u2202bPk/b\u03b8(k)i, for i = 1, 2, . . . , s and k = 1, 2, . . . , p. We also have that\neeJ = \u22022\u2113(\u03b8; b\u03b8, a)\n\u2202b\u03b8\u2202\u03b8\u22a4\n\f\f\f\f\fb\u03b8=e\u03b8, \u03b8=e\u03b8\n= diag\nn eeJ (1), eeJ (2), . . . , eeJ (p)\no\n,\nwhere the (i, i\u2032)-th element of eeJ(k) is given by\neeJ (k)ii\u2032 = 2\nnk\nX\ni=1\n(\nWp0\n\u0010\nbd\n\u22a4\njk b\u03a3\u22121\nk\nbdjk\n\u0011 \u0010\n( eP(k)i\u2032ajk + e\u00b5(k)i\u2032)\u22a4e\u03a3(k)i ePkajk \u2212e\u00b5\u22a4\n(k)ie\u03a3\u22121\nk ( eP(k)i\u2032ajk + e\u00b5(k)i\u2032)\n\u0011\n+ W \u2032\np0\n\u0010\nbd\n\u22a4\njk b\u03a3\u22121\nk\nbdjk\n\u0011\n( eP(k)i\u2032ajk + e\u00b5(k)i\u2032)\u22a4e\u03a3\u22121\nk\nePkajk\n\u0010\na\u22a4\njk eP \u22a4\nk e\u03a3(k)i ePkajk \u22122e\u00b5\u22a4\n(k)ie\u03a3\u22121\nk\nePkajk\n\u0011 )\n.\nIn matrix notation we have\n\u2113\u2032\n(k) = 2R\u22a4\n(k)w(k),\nU \u2032\n(k) = 2\n\u0010\nR\u22a4\n(k)B(k) + V \u22a4\n(k)C(k)\n\u0011\nand\neeJ(k) = 2\n\u0010\nbR\u22a4\n(k)F(k) + bV \u22a4\n(k)G(k)\n\u0011\n;\nthe elements of the matrices B(k), C(k), F(k), G(k) and of the vector w(k) are de\ufb01ned in Section 3 for k = 1, 2, . . . , p.\nAppendix C. Derivatives of the vector \u00b5k and of the matrix \u03a3k with\nrespect to the parameters\nWhen the ratio \u03bbxk = \u03c32\nxk/\u03c32\nuk is known the \ufb01rst derivative of \u00b5k has elements\n\u00b5(k)i =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u00000, . . . , 0,\n\u00b5xk\n|{z}\nposition i\n, 0, . . . , 0)\u22a4,\nif i = 1, 2, . . . , l\n\u00001\u22a4, 0\n\u0001\u22a4,\nif i = l + 1\n\u0000\u03b2\u22a4\nk , 1\n\u0001\u22a4,\nif i = l + 2\n0,\nif i = l + 3, l + 4, . . . , s.\n(8)\nThe \ufb01rst derivative of \u03a3k with respect to the parameter vector \u03b8(k) is now given for i = 1, 2, . . . , s:\n\u2022 if i = l + 1 or i = l + 2, then \u03a3(k)i is null;\n\u2022 if i = l + 3, we have \u03a3(k)i =\n\u0012 \u03b2k\u03b2\u22a4\nk \u03bbxk\n\u03b2k\u03bbxk\n\u03bbxk\u03b2\u22a4\nk\n\u03bbxk + 1\n\u0013\n;\n10\n\u2022 is i = l + 4, l + 5, . . . , s, the elements of \u03a3(k)i are null except for the (i \u2212l \u22123, i \u2212l \u22123)-th element, which is given\nby 1.\n\u2022 if i = 1, . . . , l, we have\n\u03a3(k)i =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n2\u03b21k\u03bbxk\u03c32\nuk\n\u03b22k\u03bbxk\u03c32\nuk\n. . .\n\u03b2lk\u03bbxk\u03c32\nuk\n\u03bbxk\u03c32\nuk\n\u03b22k\u03bbxk\u03c32\nuk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n...\n...\n...\n...\n...\n\u03b2lk\u03bbxk\u03c32\nuk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\u03bbxk\u03c32\nuk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nif i = 1\n...\n...\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n0\n0\n. . .\n\u03b21k\u03bbxk\u03c32\nuk\n0\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03b22k\u03bbxk\u03c32\nuk\n0\n...\n...\n...\n...\n...\n\u03b21k\u03bbxk\u03c32\nuk\n\u03b22k\u03bbxk\u03c32\nuk\n\u00b7 \u00b7 \u00b7\n2\u03b2lk\u03bbxk\u03c32\nuk\n\u03bbxk\u03c32\nuk\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03bbxk\u03c32\nuk\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nif i = l.\nThe second order derivative of \u00b5k is\n\u00b5(k)ii\u2032 =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u00000, . . . , 0,\n1\n|{z}\nposition i\n, 0, . . . , 0)\u22a4,\nif i = 1, 2, . . . , l and i\u2032 = l + 2\n\u00000, . . . , 0,\n1\n|{z}\nposition i\u2032\n, 0, . . . , 0)\u22a4,\nif i = l + 2 and i\u2032 = 1, 2, . . . , l\n0,\notherwise.\n(9)\nFor i, i\u2032 = 1, 2, . . . , s, the elements of the matrix \u03a3(k)ii\u2032 are null except for the following cases:\n\u2022 if i = 1, 2, . . . , l and i\u2032 = 1, 2, . . . , l, we have that, i = i\u2032, the elements of \u03a3(k)ii are null except the (i, i)-th element,\nwhich is equal to 2\u03bbxk\u03c32\nuk. When i \u0338= i\u2032, the elements of the matrix \u03a3(k)ii\u2032 are null, except those in positions\n(i, i\u2032) and (i\u2032, i), which are equal to \u03bbxk\u03c32\nuk;\n\u2022 if i = 1, . . . , l and i\u2032 = l + 3 we have\n\u03a3(k)ii\u2032 = \u03a3(k)i\u2032i =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n2\u03b21k\u03bbxk\n\u03b22k\u03bbxk\n. . .\n\u03b2lk\u03bbxk\n\u03bbxk\n\u03b22k\u03bbxk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n...\n...\n...\n...\n...\n\u03b2lk\u03bbxk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\u03bbxk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nif i = 1\n...\n...\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n0\n0\n. . .\n\u03b21k\u03bbxk\n0\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03b22k\u03bbxk\n0\n...\n...\n...\n...\n...\n\u03b21k\u03bbxk\n\u03b22k\u03bbxk\n\u00b7 \u00b7 \u00b7\n2\u03b2lk\u03bbxk\n\u03bbxk\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03bbxk\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nif i = l.\nWhen the ratio \u03bbek is known the \ufb01rst and second order derivatives of \u00b5k are given, respectively, in (8) and (9),\nwith s = l + 4. The derivative of \u03a3k with respect to the parameter vector \u03b8(k) is\n11\n\u2022 if i = 1, . . . , l we have\n\u03a3(k)i =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n2\u03b21k\u03c32\nxk\n\u03b22k\u03c32\nxk\n. . .\n\u03b2lk\u03c32\nxk\n\u03c32\nxk\n\u03b22k\u03c32\nxk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n...\n...\n...\n...\n...\n\u03b2lk\u03c32\nxk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\u03c32\nxk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nif i = 1\n...\n...\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n0\n0\n. . .\n\u03b21k\u03c32\nxk\n0\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03b22k\u03c32\nxk\n0\n...\n...\n...\n...\n...\n\u03b21k\u03c32\nxk\n\u03b22k\u03c32\nxk\n\u00b7 \u00b7 \u00b7\n2\u03b2lk\u03c32\nxk\n\u03c32\nxk\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03c32\nxk\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nif i = l;\n\u2022 if i = l + 1 or i = l + 2, the matrix \u03a3(k)i is null;\n\u2022 if i = l + 3, \u03a3(k)i =\n\u0012 \u03b2k\u03b2\u22a4\nk\n\u03b2k\n\u03b2\u22a4\nk\n1\n\u0013\n;\n\u2022 if i = l + 4 = s, \u03a3(k)i =\n\u0012 \u03bbek\n0\n0\n1\n\u0013\n.\nFor i, i\u2032 = 1, 2, . . . , s, we have that the elements of \u03a3(k)ii\u2032 are null except for the cases:\n\u2022 if i = 1, . . . , l and i\u2032 = l + 3, we have\n\u03a3(k)ii\u2032 = \u03a3(k)i\u2032i =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n2\u03b21k\n\u03b22k\n. . .\n\u03b2lk\n1\n\u03b22k\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n...\n...\n...\n...\n...\n\u03b2lk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n1\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nif i = 1\n...\n...\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n0\n0\n. . .\n\u03b21k\n0\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03b22k\n0\n...\n...\n...\n...\n...\n\u03b21k\n\u03b22k\n\u00b7 \u00b7 \u00b7\n2\u03b2lk\n1\n0\n0\n\u00b7 \u00b7 \u00b7\n1\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nif i = l;\n\u2022 if i = 1, 2, . . . , l and i\u2032 = 1, 2, . . . , l, we have that, for i = i\u2032, the elements of the matrix \u03a3(k)ii are null except\nthe (i, i)-th elements, which is equal to 2\u03c32\nxk. When i \u0338= i\u2032, the elements of the matrix \u03a3(k)ii\u2032 are null except the\n(i, i\u2032)-th and the (i\u2032, i)-th elements, which are equal to \u03c32\nxk.\nWhen the intercept \u03b1k in known the \ufb01rst order derivative of \u00b5k is\n\u00b5(k)i =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u00000, . . . , 0,\n\u00b5xk\n|{z}\nposition i\n, 0, . . . , 0)\u22a4,\nif i = 1, 2, . . . , l\n\u0000\u03b2\u22a4\nk , 1\n\u0001\u22a4,\nif i = l + 1\n0,\nif i = l + 2, l + 3, . . . , s.\nThe derivative of \u03a3k with respect to \u03b8(k) is now presented for i = 1, 2, . . . , s. We have\n12\n\u2022 if i = l + 1, the matrix \u03a3(k)i is null;\n\u2022 if i = l + 2 we have\n\u03a3(k)i =\n\u0012 \u03b2k\u03b2\u22a4\nk\n\u03b2k\n\u03b2\u22a4\nk\n1\n\u0013\n;\n\u2022 if i = l + 3 we have \u03a3(k)i =\n\u0012 0\n0\n0\n1\n\u0013\n;\n\u2022 if i = l + 4, l + 5, . . . , s, the elements of the matrix \u03a3(k)i is null except for the (i \u2212l \u22123, i \u2212l \u22123)-th element,\nwhich equals 1;\n\u2022 if i = 1, . . . , l, we have\n\u03a3(k)i =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n2\u03b21k\u03c32\nxk\n\u03b22k\u03c32\nxk\n. . .\n\u03b2lk\u03c32\nxk\n\u03c32\nxk\n\u03b22k\u03c32\nxk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n...\n...\n...\n...\n...\n\u03b2lk\u03c32\nxk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\u03c32\nxk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nif i = 1\n...\n...\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n0\n0\n. . .\n\u03b21k\u03c32\nxk\n0\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03b22k\u03c32\nxk\n0\n...\n...\n...\n...\n...\n\u03b21k\u03c32\nxk\n\u03b22k\u03c32\nxk\n\u00b7 \u00b7 \u00b7\n2\u03b2lk\u03c32\nxk\n\u03c32\nxk\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03c32\nxk\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nif i = l.\nThe second order derivative of \u00b5k is\n\u00b5(k)ii\u2032 =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u00000, . . . , 0,\n1\n|{z}\nposition i\n, 0, . . . , 0)\u22a4,\nif i = 1, 2, . . . , l and i\u2032 = l + 1\n\u00000, . . . , 0,\n1\n|{z}\nposition i\u2032\n, 0, . . . , 0)\u22a4,\nif i = l + 1 and i\u2032 = 1, 2, . . . , l\n0,\notherwise.\nFor i, i\u2032 = 1, 2, . . . , s, we have that the elements of \u03a3(k)ii\u2032 are null except for the cases:\n\u2022 if i = 1, 2, . . . , l and i\u2032 = 1, 2, . . . , l, we have that, for i = i\u2032, the elements of \u03a3(k)ii are null except for the (i, i)-th\nelement, which is equal to 2 \u03c32\nxk. When i \u0338= i\u2032, the elements of \u03a3(k)ii\u2032 are null except for those in positions (i, i\u2032)\nand (i\u2032, i), which are equal to \u03c32\nxk;\n\u2022 if i = 1, . . . , l and i\u2032 = l + 2, we have\n\u03a3(k)ii\u2032 = \u03a3(k)i\u2032i =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n2\u03b21k\n\u03b22k\n. . .\n\u03b2lk\n1\n\u03b22k\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n...\n...\n...\n...\n...\n\u03b2lk\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n1\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nif i = 1\n...\n...\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n0\n0\n. . .\n\u03b21k\n0\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03b22k\n0\n...\n...\n...\n...\n...\n\u03b21k\n\u03b22k\n\u00b7 \u00b7 \u00b7\n2\u03b2lk\n1\n0\n0\n\u00b7 \u00b7 \u00b7\n1\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nif i = l.\n13\nReferences\n[1] Aoki, R., Bolfarine, H. & Singer, J.M., Null intercept measurement error regression models, Test,\n10, 441-457 (2001).\n[2] Barndor\ufb00\u2013Nielsen, O.E., Inference on full or partial parameters, based on the standardized signed\nlog likelihood ratio, Biometrika, 73, 307-322 (1986).\n[3] Buonaccorsi, J. P., Measurement Error:\nModels, Methods and Applications. Chapman and\nHall/CRC, Boca Raton (2010).\n[4] Chan, L.K. & Mak, T.K., On the maximum likelihood estimation of a linear structural relashion-\nship when the intercept is known, Journal of Multivariate Analysis, 9, 304-313 (1979).\n[5] Cox, N.R., The linear structural relation for several groups of data, Biometrika, 63, 231-237\n(1976).\n[6] Doornik, J.A., Ox: An Object-Oriented Matrix Language. Timberlake Consultants Press, London\n(2006).\n[7] Fang, K.T., Kotz, S. & Ng, K.W., Symmetric Multivariate and Related Distributions. Chapman\nand Hall, London (1990).\n[8] Fang, K.T. & Anderson, T.W., Statistical Inference in Elliptically Contoured and Related Distri-\nbutions. Allerton Press Inc, New York (1990).\n[9] Fuller, S., Measurement Error Models. Wiley, New York (1987).\n[10] Garcia-Alfaro, K. & Bolfarine, H., Comparative calibration with subgroups, Communications in\nStatistics - Theory and Methods, 30, 2057-2078 (2001).\n[11] Melo, T.F.N. & Ferrari, S.L.P., A modi\ufb01ed signed likelihood ratio test in elliptical structural\nmodels, Advances in Statistical Analysis, 94, 75-87 (2010).\n[12] Russo, C.M., Aoki, R. & Le\u02dcao-Pinto, D., Hypotheses testing on a multivariate null intercept\nerrors-in-variables model, Communications in Statistics - Simulation and Computation, 38, 1447-\n1469 (2009).\n[13] Smith, S.P. Di\ufb00erentiation of the Choleski algorithm. Journal of Computational and Graphical\nStatistics, 4, 134-147 (1995).\n[14] Severini, T.A., Likelihood Methods in Statistics. Oxford University Press (2000).\n[15] Skovgaard, I.M. Likelihood asymptotics. Scandinavian Journal of Statistics, 28, 3-32 (2001).\n[16] Wong, M.Y., Likelihood estimation of a simple linear regression model when both variables have\nerror, Biometrika, 76, 141-148 (1989).\n14\nTable 1: Null rejection rates; nk = 10.\n\u03bbxk known\nNormal distribution\nStudent-t distribution (\u03bd = 3)\n\u03b3 = 5%\n\u03b3 = 10%\n\u03b3 = 5%\n\u03b3 = 10%\nq\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\n2\n10.1\n5.1\n4.9\n16.9\n10.2\n9.8\n9.9\n5.2\n5.0\n16.7\n10.6\n10.2\n3\n11.6\n5.3\n5.0\n18.6\n10.2\n9.5\n10.7\n5.2\n4.9\n18.4\n10.3\n9.8\n4\n12.5\n5.3\n4.8\n20.4\n10.2\n9.6\n12.0\n5.4\n5.1\n19.9\n10.6\n10.1\n5\n13.6\n5.2\n4.8\n21.5\n10.3\n9.6\n12.7\n5.5\n5.1\n20.8\n10.4\n9.8\n\u03bbek known\nNormal distribution\nStudent-t distribution (\u03bd = 3)\n\u03b3 = 5%\n\u03b3 = 10%\n\u03b3 = 5%\n\u03b3 = 10%\nq\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\n2\n10.1\n5.1\n4.9\n17.3\n10.1\n9.6\n10.7\n5.8\n5.5\n18.4\n11.4\n10.9\n3\n11.4\n5.3\n4.9\n19.4\n10.1\n9.6\n10.9\n5.3\n5.0\n18.6\n10.4\n10.1\n4\n13.0\n5.2\n4.7\n20.8\n10.7\n9.8\n11.9\n5.5\n5.2\n19.9\n10.8\n10.2\n5\n13.7\n5.2\n4.7\n22.4\n10.3\n9.6\n13.2\n5.5\n5.2\n21.3\n11.0\n10.1\nnull intercept\nNormal distribution\nStudent-t distribution (\u03bd = 3)\n\u03b3 = 5%\n\u03b3 = 10%\n\u03b3 = 5%\n\u03b3 = 10%\nq\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\n2\n7.8\n5.1\n4.9\n13.9\n10.2\n10.0\n7.3\n5.5\n5.4\n13.3\n10.6\n10.5\n3\n8.6\n5.0\n4.9\n15.2\n10.2\n10.0\n7.4\n5.2\n5.2\n13.6\n10.4\n10.2\n4\n8.9\n5.1\n5.0\n15.7\n10.2\n10.0\n7.3\n4.8\n4.7\n13.9\n10.2\n10.0\n5\n9.3\n5.2\n5.0\n16.4\n10.1\n9.8\n7.9\n5.1\n5.0\n14.6\n10.3\n10.2\n15\nTable 2: Null rejection rates; q = 3. Normal distribution.\n\u03bbxk known\n\u03b3 = 1%\n\u03b3 = 5%\n\u03b3 = 10%\nnk\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\n10\n3.6\n1.1\n1.0\n11.6\n5.3\n5.0\n18.6\n10.2\n9.5\n20\n1.9\n0.9\n0.9\n7.8\n5.1\n5.1\n13.8\n10.0\n9.9\n30\n1.4\n1.0\n0.9\n6.3\n4.7\n4.7\n12.3\n9.7\n9.6\n40\n1.3\n0.9\n0.9\n6.0\n4.8\n4.8\n11.7\n9.9\n9.9\n\u03bbek known\n\u03b3 = 1%\n\u03b3 = 5%\n\u03b3 = 10%\nnk\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\n10\n3.5\n1.2\n1.0\n11.4\n5.3\n4.9\n19.4\n10.1\n9.6\n20\n1.9\n1.1\n1.0\n8.0\n5.2\n5.1\n14.6\n10.4\n10.3\n30\n1.4\n0.9\n0.9\n6.9\n5.2\n5.2\n12.4\n10.0\n10.0\n40\n1.3\n0.9\n0.9\n6.0\n4.8\n4.8\n11.7\n9.9\n9.9\nnull intercept\n\u03b3 = 1%\n\u03b3 = 5%\n\u03b3 = 10%\nnk\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\nLR\nLR\u2217\nLR\u2217\u2217\n10\n2.1\n1.1\n1.0\n8.6\n5.0\n4.9\n15.2\n10.2\n10.0\n20\n1.5\n0.9\n0.9\n6.0\n4.5\n4.5\n11.9\n9.8\n9.7\n30\n1.3\n1.1\n1.1\n6.0\n5.1\n5.1\n11.6\n10.0\n9.9\n40\n1.4\n1.1\n1.1\n5.6\n4.9\n4.9\n11.1\n10.1\n10.1\n16\n"}