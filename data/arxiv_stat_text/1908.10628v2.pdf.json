{"text": "Changepoint in Linear Relations\nMichal Pe\u02c7sta\nCharles University, Prague, Czech Republic.\nE-mail: Michal.Pesta@m\ufb00.cuni.cz\nAbstract. Linear relations, containing measurement errors in input and output data, are con-\nsidered. Parameters of these so-called errors-in-variables models can change at some unknown\nmoment. The aim is to test whether such an unknown change has occurred or not. For in-\nstance, detecting a change in trend for a randomly spaced time series is a special case of the\ninvestigated framework. The designed changepoint tests are shown to be consistent and in-\nvolve neither nuisance parameters nor tuning constants, which makes the testing procedures\ne\ufb00ortlessly applicable. A changepoint estimator is also introduced and its consistency is proved.\nA boundary issue is avoided, meaning that the changepoint can be detected when being close\nto the extremities of the observation regime. As a theoretical basis for the developed meth-\nods, a weak invariance principle for the smallest singular value of the data matrix is provided,\nassuming weakly dependent and non-stationary errors. The results are presented in a simu-\nlation study, which demonstrates computational e\ufb03ciency of the techniques. The completely\ndata-driven tests are illustrated through a calibration problem, however, the methodology can\nbe applied to other areas such as clinical measurements, dietary assessment, computational\npsychometrics, or environmental toxicology as manifested in the paper.\nKeywords: changepoint, errors-in-variables, hypothesis testing, non-stationarity, nuisance-\nparameter-free, singular value, weak invariance principle\n1.\nIntroduction and main aims\nIf measured input and output data are supposed to be in some linear relations, then it is of\nparticular interest to detect whether impact of the input characteristics has changed over\ntime on the output observables. Moreover, only error-prone surrogates of the unobserv-\nable input-output characteristics are in hand instead of a precise measurement. Despite\nthe fact that the relations and, consequently, suitable underlying stochastic models are\nlinearly de\ufb01ned, the possible estimates and the corresponding inference may be highly\nnon-linear (Gleser, 1981). It becomes even more challenging to handle measurement er-\nrors in input and output data simultaneously, when the linear relations are subject to\nchange at some unknown time point\u2014changepoint.\nThere is a vast literature aimed at linear relations modeled through so-called measure-\nment error models or errors-in-variables models (for an overview, see Fuller (1987), Van\nHu\ufb00el and Vandewalle (1991), Carroll et al. (2006), Buonaccorsi (2010), or Yi (2017)), but\nvery little has been explored in the changepoint analysis for these models yet. A change\nin regression has been explored thoroughly, cf. Horv\u00b4ath (1995) or Aue et al. (2008). How-\never, such a framework does not cover the case of measurement error models. Maximum\nlikelihood approach (Chang and Huang, 1997; Staudenmayer and Spiegelman, 2002) and\narXiv:1908.10628v2  [math.ST]  17 Jan 2020\n2\nM. Pe\u02c7sta\nBayesian approach (Carroll et al., 1999; G\u00a8ossl and K\u00a8uchenho\ufb00, 2001) to the changepoint\nestimation in the measurement error models were applied, both requiring parametric dis-\ntributional assumptions on the errors. Kukush et al. (2007) estimated the changepoint in\nthe input data only. A change in the variance parameter of the normally distributed errors\nwithin the measurement error models was investigated by Dong et al. (2016). All of these\nmentioned contributions dealt with the changepoint estimation solely. Our main goal is\nto test for a possible change in the parameters relating the input and output data, both\nencumbered by some errors. Consequently, if a change is detected, we aim to estimate\nit. By our best knowledge, we are not aware of any similar results even for the indepen-\ndent and identically distributed errors. Additionally to that, our changepoint tests are\nsupposed to be nuisance-parameter-free, distributional-free, and to allow for very general\nerror structures.\n1.1.\nOutline\nThe paper is organized as follows: In the next section, our data model for the changepoint\nin errors-in-variables is introduced and several practical motivations for such a model are\ngiven. Section 3 contains a spectral weak invariance principle for weakly dependent and\nnon-stationary random variables. It serves as the main theoretical tool for the conse-\nquent inference. The technical assumptions are discussed as well. Two test statistics for\nthe changepoint detection are proposed in Section 4. Consequently, their asymptotic be-\nhavior is derived under the null as well as under the alternative hypothesis. Moreover,\na consistent changepoint estimator is introduced. Section 5 contains a simulation study\nthat compares \ufb01nite sample performance of the investigated tests. It numerically em-\nphasizes the advantages of the proposed detection procedures. A practical application\nof the developed approach to a calibration problem is presented in Subsection 6.1. On\nthe other hand, a theoretical application to randomly spaced time series is performed in\nSubsection 6.2. Afterwards, our conclusion follows. Proofs are given in the Appendix A.\n2.\nChangepoint in errors-in-variables\nErrors-in-variables (EIV) or also called measurement error model\nX = Z + \u0398\n(M)\nand\nY = Z\u03b2 + \u03b5\n(H0)\nis considered, where \u03b2 \u2208Rp is a vector of unknown regression parameters possibly subject\nto change, X \u2208Rn\u00d7p and Y \u2208Rn\u00d71 consist of observable random variables (X are\ncovariates and Y is a response), Z \u2208Rn\u00d7p consists of unknown constants and has full\nrank, \u03b5 \u2208Rn\u00d71 and \u0398 \u2208Rn\u00d7p are random errors.\nThis setup can be extended to\na multivariate case, where \u03b2 \u2208Rp\u00d7q, Y \u2208Rn\u00d7q, and \u03b5 \u2208Rn\u00d7q, q \u22651, see Subsection 3.3.\nThe EIV model (M)\u2013(H0) with non-random unknown constants Z is sometimes called\nfunctional EIV model (Fuller, 1987; Booth and Hall, 1993). On the other hand, a dif-\nferent approach may handle Z as random covariates, which is called structural EIV\nChangepoint in Linear Relations\n3\nmodel (Chang and Huang, 1997). Stefanski (2000) stated: \u2018However, functional mod-\nels played an important role in the study of measurement error models and in statistics\nmore generally.\u2019 And here, we will concentrate on the functional EIV model not because\nof this matter-of-fact quote, but because we wish to demonstrate a distributional-free ap-\nproach, where \u2018no, or only minimal, assumptions are made about the distribution of the\nXs\u2019 (Carroll et al., 2006), as challenged in the introduction. Nevertheless with respect\nto derivation of the forthcoming theory for the functional EIV model, changing some\ntechnical assumptions would allow to prove suitable results for the structural case as well.\nTo estimate the unknown parameter \u03b2, one usually minimizes the Frobenius matrix\nnorm of the errors [\u0398, \u03b5], see Golub and Van Loan (1980). This approach leads to a to-\ntal least squares (TLS) estimate \u02c6\u03b2 = (X\u22a4X \u2212\u03bbmin([X, Y ]\u22a4[X, Y ])Ip)\u22121X\u22a4Y , where\n\u03bbmin(M) is the smallest eigenvalue of the matrix M and Ip is a (p \u00d7 p) identity matrix.\nGeometrically speaking, the Frobenius norm tries to minimize the orthogonal distance be-\ntween the observations and the \ufb01tted hyperplane. Therefore, the TLS are usually known\nas orthogonal regression. One can generalize this method by replacing the Frobenius norm\nby any unitary invariance matrix norm, which surprisingly yields the same TLS estimate,\nhaving interesting invariance and equivariance properties (Pe\u02c7sta, 2016). The TLS esti-\nmate is shown to be strongly and weakly consistent (Gleser, 1981; Gallo, 1982a; Pe\u02c7sta,\n2011) as well as to be asymptotically normal (Gallo, 1982b; Pe\u02c7sta, 2013b, 2017) under\nvarious conditions.\nWe aim to detect a possible change in the linear relation parameter \u03b2. The interest lies\nin testing the null hypothesis (H0) of all observations Yi\u2019s being random variables having\nexpectations Zi,\u2022\u03b2\u2019s. Our goal is to test against the alternative of the \ufb01rst \u03c4 outcome\nobservations have expectations Zi,\u2022\u03b2\u2019s and the remaining n \u2212\u03c4 observations come from\ndistributions with expectations Zi,\u2022(\u03b2 + \u03b4)\u2019s, where \u03b4 \u0338= 0. A \u2018row-column\u2019 notation for\na matrix M is used in this manner: Mi,\u2022 denotes the ith row of M and M\u2022,j corresponds\nto the jth column of M. Furthermore, if i \u2208N0, then Mi stays for the \ufb01rst i rows of M\nand M\u2212i represents the remaining n \u2212i rows of M, when the \ufb01rst i rows are deleted.\nNow more precisely, our alternative hypothesis is\nY\u03c4 = Z\u03c4\u03b2 + \u03b5\u03c4\nand\nY\u2212\u03c4 = Z\u2212\u03c4(\u03b2 + \u03b4) + \u03b5\u2212\u03c4.\n(HA)\nHere, \u03b4 \u2261\u03b4(n) \u0338= 0 is an unknown vector parameter representing the size of change and\nis possibly depending on n. The changepoint \u03c4 \u2261\u03c4(n) < n is also an unknown scalar\nparameter, which depends on n as well. Although, \u03b2 is considered to be independent\nof n. One may also think of the changepoint in errors-in-variables framework as segmented\nregression with measurement errors, cf. Staudenmayer and Spiegelman (2002).\n2.1.\nIntercept and \ufb01xed regressors\nNote that the EIV model (M)\u2013(H0) has no intercept and all the covariates are encumbered\nby some errors. To overcome such a restriction, one can think of an extended regression\nmodel, where some explanatory variables are subject to error and some are measured\nprecisely. I.e., Y = W \u03b3 + Z\u03b2 + \u03b5, where W are observable true and Z are unobservable\ntrue constants, both having full rank. Regression parameters \u03b3 and \u03b2 remain unknown.\nThen, the non-random (\ufb01xed) intercept can be incorporated into the regression model by\n4\nM. Pe\u02c7sta\nsetting one column of the matrix W equal to [1, . . . , 1]\u22a4. Consequently, we may project\nout exact observations using projection matrix R := In \u2212W (W \u22a4W )\u22121W \u22a4. Notice that\nR is symmetric and idempotent. Finally, one may work with RY = RZ\u03b2 + R\u03b5 instead\nof (H0).\n2.2.\nMotivations\nThe proposed class of models\u2014errors-in-variables with changepoint\u2014is very rich and gen-\neral. Our approach and results are motivated in the context of several applications taken\nfrom chemistry, biological sciences, medicine, and epidemiological studies.\nCase 1: Assessing agreement in clinical measurement.\nDirect measurement of cardiac\nstroke volume or blood pressure without adverse e\ufb00ects is di\ufb03cult or even impossible. The\ntrue values remain unknown. Indirect methods are, therefore, used instead. When a new\nmeasurement technique is developed, it has to be evaluated by comparison with an estab-\nlished technique rather than with the true quantity (Bland and Altman, 1986). Clinicians\nneed to test whether both measurement techniques agree su\ufb03ciently. Thereafter, the old\ntechnique may be replaced by the new one.\nCase 2: Nutritional epidemiology.\nStaudenmayer and Spiegelman (2002) analyzed data\nfrom a nutritional study that investigates the relation between dietary folate intake (calo-\nries adjusted \u00b5g/day) on plasma homocysteine concentration (\u00b5mol/liter of blood). There\nexists a suspicion that serum homocysteine is signi\ufb01cantly elevated when ingested folate\nis below a certain changepoint. Moreover, the analysis used estimates of folate that were\ndeveloped with a food frequency questionnaire, which is recognized to be imperfect.\nCase 3: Psychometric testing.\nLet us think of two psychometric instruments: unspeeded\n15-item vocabulary tests and highly speeded 75-item vocabulary tests, cf. Lord (1973).\nThe results of both tests are error-prone. Within a group of people, there is a speculation\nthat individuals with an unspeeded test\u2019s result exceeding some unknown level should\nperform dramatically better in the highly speeded test.\nCase 4: Environmental toxicology.\nA threshold limiting value in toxicology is the dose\nof a toxin or a substance under which there is harmless or insigni\ufb01cant in\ufb02uence on some\nresponse. In a dose-response relationship, both of them are measured with errors. And\nthe goal is to set the threshold limiting value. Such a problem was dealt by G\u00a8ossl and\nK\u00a8uchenho\ufb00(2001) using fully Bayesian approach. Moreover, a similar task regarding the\nNO2 concentration is discussed by Stefanski (2000).\nCase 5: Device calibration.\nLater on in Subsection 6.1, we concentrate in more details on\nthe calibration task and exemplify the proposed methodology through analysis of data from\na calibrated device and a casual device (needs to be calibrated) in order to demonstrate\npractical e\ufb03ciency of our detection method.\nBesides that, there are many other applications of the changepoint within the linear\nrelations framework in, for instance, glaciology (Gleser and Watson, 1973), empirical\neconomics (Chang and Huang, 1997), dietary assessment (Carroll et al., 1999), or image\nforensics (Ryu and Lee, 2014).\nChangepoint in Linear Relations\n5\n3.\nSpectral weak invariance principle\nA theoretical device is going to be developed in order to construct the changepoint\ntests. The smallest eigenvalue of \u03a3\u22121[X, Y ]\u22a4[X, Y ]\u2014the squared smallest singular value\nof [X, Y ]\u03a3\u22121/2, i.e., the data matrix [X, Y ] multiplied by the inverse of a matrix square\nroot from the error variance structure (cf. subsequent Assumption E)\u2014plays a key role.\nWe proceed to the assumptions that are needed for deriving forthcoming asymptotic re-\nsults. Henceforth,\nP\u2212\u2192denotes convergence in probability,\nD\u2212\u2192convergence in distribution,\nD[0,1]\n\u2212\u2212\u2212\u2192\nn\u2192\u221eweak convergence in the Skorokhod topology D[0, 1] of c`adl`ag functions on [0, 1],\nand [x] denotes the integer part of the real number x.\n3.1.\nAssumptions\nFirstly, a design assumption on the unobservable regressors is needed.\nAssumption D. \u2206t := limn\u2192\u221en\u22121Z\u22a4\n[nt]Z[nt], \u2206\u2212t := limn\u2192\u221en\u22121Z\u22a4\n\u2212[nt]Z\u2212[nt] for every\nt \u2208(0, 1), and \u2206:= limn\u2192\u221en\u22121Z\u22a4Z are positive de\ufb01nite.\nIt basically says that the error-free design points do not concentrate to close to each\nother (i.e., strict positive de\ufb01niteness) and, simultaneously, they do not spread-out too\nfar (i.e., existence of limits). For example in one-dimensional case (i.e., p = 1), a simple\nequidistant design, where Zi,1 = i/(n + 1), provides \u2206t = t3/3 and \u2206= 1/3.\nPrior to postulating an errors\u2019 assumption, we summarize the notion of strong mix-\ning (\u03b1-mixing) dependence in more detail, which will be imposed on the model\u2019s er-\nrors.\nSuppose that {\u03ben}\u221e\nn=1 is a sequence of random elements on a probability space\n(\u2126, F, P). For sub-\u03c3-\ufb01elds A, B \u2286F, let \u03b1(A|B) := supA\u2208A,B\u2208B |P(A \u2229B) \u2212P(A)P(B)|.\nIntuitively, \u03b1(\u00b7|\u00b7) measures the dependence of the events in B on those in A.\nThere\nare many ways in which one can describe weak dependence or, in other words, asymp-\ntotic independence of random variables, see Bradley (2005).\nConsidering a \ufb01ltration\nFn\nm = \u03c3{\u03bei \u2208F, m \u2264i \u2264n}, sequence {\u03ben}\u221e\nn=1 of random variables is said to be\nstrong mixing (\u03b1-mixing) if \u03b1(\u03be\u25e6, n) = supk\u2208N \u03b1(Fk\n1 |F\u221e\nk+n) \u21920 as n \u2192\u221e. Anderson\n(1958) comprehensively analyzed a class of m-dependent processes. They are \u03b1-mixing,\nsince they are \ufb01nite order ARMA processes with innovations satisfying Doeblin\u2019s condition\n(Billingsley, 1968, p. 168). Finite order processes, which do not satisfy Doeblin\u2019s condi-\ntion, can be shown to be \u03b1-mixing (Ibragimov and Linnik, 1971, pp. 312\u2013313). Rosenblatt\n(1971) provides general conditions under which stationary Markov processes are \u03b1-mixing.\nSince functions of mixing processes are themselves mixing (Bradley, 2005), time-varying\nfunctions of any of the processes just mentioned are mixing as well. This means that the\nclass of the \u03b1-mixing processes is su\ufb03ciently large for the further practical applications\nand that is why we chose such a mixing condition.\nAssumption E. {[\u0398n,\u2022, \u03b5n]}\u221e\nn=1 is a sequence of \u03b1-mixing absolutely continuous random\nvectors having zero mean and a variance matrix \u03c32\u03a3 with an unknown \u03c32 > 0 and\na known positive de\ufb01nite \u03a3 =\n\"\n\u03a3\u0398\n\u03a3\u0398,\u03b5\n\u03a3\u22a4\n\u0398,\u03b5\n1\n#\nsuch that \u03b1([\u0398\u25e6,\u2022, \u03b5\u25e6], n) = O(n\u22121\u2212\u03d6) as\nn \u2192\u221efor some \u03d6 > 0, supn\u2208N Z2\nn,j < \u221e, supn\u2208N E |\u0398n,j|4+\u03c9 < \u221e, j \u2208{1, . . . , p}, and\nsupn\u2208N E |\u03b5n|4+\u03c9 < \u221efor some \u03c9 > 0 such that \u03c9\u03d6 > 2.\n6\nM. Pe\u02c7sta\nLet us emphasize that the sequence of the errors do not have to be stationary. The\nassumption of an unknown \u03c32 and a known \u03a3 implies that we know the ratio of any\npair of covariances in advance.\nIn the simplest situation, a homoscedastic covariance\nstructure of the within-individual errors [\u0398n,\u2022, \u03b5n] can be assumed (i.e., \u03a3 = Ip+1), if\nprior experience or essence of the analyzed problem allow for that. On the other hand,\nif the covariance matrix \u03a3 is unknown, it can be estimated when possessing replicate\nmeasurements or validation data as commented by Stefanski (2000). There are various\napproaches proposed to serve this purpose. In order ot mention at least some of them, we\nrefer to Cheng and Riu (2006), Guo and Little (2011), Pe\u02c7sta (2013b), or Li et al. (2019).\nOn the top of that, we have to bear in mind that \u03a3 cannot be completely unspeci\ufb01ed.\nNussbaum (1977) showed that if \u03a3 is unrestricted, no strongly consistent estimator for \u03b2\ncan exist even under normally distributed errors.\nFurthermore, a variance assumption for the mis\ufb01t disturbances is stated. It can be\nconsidered as a typical assumption for the long-run variance of residuals. Let us denote\n\u03a3\u22121/2 =\n\" \u00af\u03a3\u0398\n\u00af\u03a3\u0398,\u03b5\n\u00af\u03a3\u22a4\n\u0398,\u03b5\n\u00af\u03a3\u03b5\n#\na symmetric square root of \u03a3\u22121, where \u00af\u03a3\u03b5 \u2208R is a scalar.\nAssumption V. There exist \u03c6 := \u00af\u03a3\u03b5 \u2212\u00af\u03a3\u22a4\n\u0398,\u03b5( \u00af\u03a3\u0398 + \u03b2 \u00af\u03a3\u22a4\n\u0398,\u03b5)\u22121( \u00af\u03a3\u0398,\u03b5 + \u03b2\u00af\u03a3\u03b5) \u0338= 0 and\n\u03c5 := limn\u2192\u221en\u22121 Var \u2225Y \u2212X\u03b2\u22252\n2 > 0.\nLet us remark that \u00af\u03a3\u0398,\u03b5 = 0 for the uncorrelated error structure and, then, \u03c6 = \u00af\u03a3\u03b5.\n3.2.\nSWIP\nFinally, the spectral weak invariance principle for the smallest eigenvalues is provided.\nLet us denote \u03bbi := \u03bbmin(\u03a3\u22121[Xi, Yi]\u22a4[Xi, Yi]) for 2 \u2264i \u2264n, \u03bb0 := \u03bb1 := 0 and\ne\u03bbi := \u03bbmin(\u03a3\u22121[X\u2212i, Y\u2212i]\u22a4[X\u2212i, Y\u2212i]) for 0 \u2264i \u2264n \u22122, e\u03bbn := e\u03bbn\u22121 := 0. Note that\n\u03bbn \u2261e\u03bb0.\nProposition 3.1 (SWIP). Let M and H0 hold. Under Assumptions D, E, and V,\n\u001a 1\n\u221an\n\u0010\n\u03bb[nt] \u2212[nt]\u03c32\u0011\u001b\nt\u2208[0,1]\nD[0,1]\n\u2212\u2212\u2212\u2192\nn\u2192\u221e\n(\n\u03c62\u03c5\n1 + \u2225\u03b1\u22252\n2\nW(t)\n)\nt\u2208[0,1]\nand\n\u001a 1\n\u221an\n\u0010\ne\u03bb[n(1\u2212t)] \u2212[n(1 \u2212t)]\u03c32\u0011\u001b\nt\u2208[0,1]\nD[0,1]\n\u2212\u2212\u2212\u2192\nn\u2192\u221e\n(\n\u03c62\u03c5\n1 + \u2225\u03b1\u22252\n2\nf\nW(t)\n)\nt\u2208[0,1]\n,\nwhere {W(t)}t\u2208[0,1] is a standard Wiener process , f\nW(t) = W(1) \u2212W(t), and \u03b1 =\n( \u00af\u03a3\u0398 + \u03b2 \u00af\u03a3\u22a4\n\u0398,\u03b5)\u22121( \u00af\u03a3\u0398,\u03b5 + \u03b2\u00af\u03a3\u03b5).\n3.3.\nExtension to multivariate case\nSuppose that \u03b2 \u2208Rp\u00d7q, Y \u2208Rn\u00d7q, and \u03b5 \u2208Rn\u00d7q, q \u22651.\nLet the singular value\ndecomposition (SVD) of the partial transformed data be\n[X[nt], Y[nt]]\u03a3\u22121/2 = U(t)\u0393(t)V \u22a4(t) =\np+q\nX\ni=1\n\u03c2(t)(i)u(t)(i)v(t)(i)\u22a4,\nChangepoint in Linear Relations\n7\nwhere u(t)(i)\u2019s are the left-singular vectors, v(t)(i)\u2019s are the right-singular vectors, and\n\u03c2(t)(i)\u2019s are the singular values in the non-increasing order. One may replace \u03bb[nt] by\n\u039b[nt] :=\nq\nX\nj=1\n\u0010\n\u03c2(t)(p+j)\u00112\nin Proposition 3.1 (and analogously for e\u03bb[n(1\u2212t)]). Then, the SWIP can be derived again\n(see the proof of Proposition 3.1), provided adequately extended assumptions on the errors\n{\u03b5n,1}\u221e\nn=1, . . . , {\u03b5n,q}\u221e\nn=1 instead of the original ones {\u03b5n}\u221e\nn=1. However, the consequent\nproofs would become more technical.\n4.\nNuisance-parameter-free detection\nConsistent estimation of \u03b2 can be performed via the generalized TLS approach (Gallo,\n1982a; Van Hu\ufb00el and Vandewalle, 1989). The optimizing problem\n[b, \u02c6\u0398, \u02c6\u03b5] :=\narg min\n[\u0398,\u03b5]\u2208Rn\u00d7(p+1),\u03b2\u2208Rp\n\r\r\r[\u0398, \u03b5] \u03a3\u22121/2\r\r\r\nF\ns.t.\nY \u2212\u03b5 = (X \u2212\u0398)\u03b2,\nwhere \u2225\u00b7\u2225F stands for the Frobenius matrix norm, has a solution consisting of the estimator\nb = (X\u22a4X \u2212\u03bbn\u03a3\u0398)\u22121(X\u22a4Y \u2212\u03bbn\u03a3\u0398,\u03b5)\n(4.1)\nand the \ufb01tted errors [ \u02c6\u0398, \u02c6\u03b5] such that\n\r\r[ \u02c6\u0398, \u02c6\u03b5]\u03a3\u22121/2\r\r2\nF = \u03bbn.\n(4.2)\nWe construct the changepoint test statistics based on property (4.2).\n4.1.\nChangepoint test statistics\nLet us think of two TLS estimates of \u03b2: The \ufb01rst one based on the \ufb01rst i data lines [Xi, Yi]\nand the second one based on the \ufb01rst k data lines [Xk, Yk] such that 1 \u2264i \u2264k \u2264n. Under\nthe null H0, these two TLS estimates should be close to each other. On the other hand,\nunder the alternative HA such that \u03c4 \u2208{i, . . . , k}, they should be somehow di\ufb00erent.\nA similar conclusion can be made for the goodness-of-\ufb01t statistics coming from (4.2). It\nmeans that\n\u03bbi \u2212i\nk\u03bbk\nshould be reasonably small under the null H0. Under the alternative HA such that \u03c4 \u2208\n{i, . . . , k}, it should be relatively large. For the multivariate case described in previous\nSubsection 3.3, one has to replace \u03bbk by \u039bk = Pq\nj=1\n\u0000\u03c2(k/n)(p+j)\u00012.\nWe rely on self-normalized test statistics introduced by Shao and Zhang (2010), be-\ncause the unknown quantity \u03c62\u03c5/(1 + \u2225\u03b1\u22252\n2) from Proposition 3.1 cancels out in the test\nstatistics. Our supremum-type self-normalized test statistic based on the goodness-of-\ufb01t is\nde\ufb01ned as\nSn := max\n1\u2264k<n\n\f\f\u03bbk \u2212k\nn\u03bbn\n\f\f\nmax1\u2264i<k\n\f\f\u03bbi \u2212i\nk\u03bbk\n\f\f + maxk<i\u2264n\n\f\fe\u03bbi \u2212n\u2212i\nn\u2212k e\u03bbk\n\f\f\n(4.3)\n8\nM. Pe\u02c7sta\nand the integral-type self-normalized test statistic is de\ufb01ned as\nTn :=\nn\u22121\nX\nk=1\n\u0000\u03bbk \u2212k\nn\u03bbn\n\u00012\nPk\u22121\ni=1\n\u0000\u03bbi \u2212i\nk\u03bbk\n\u00012 + Pn\ni=k+1\n\u0000e\u03bbi \u2212n\u2212i\nn\u2212k e\u03bbk\n\u00012 .\n(4.4)\nLet us note that evaluations of the above de\ufb01ned test statistics require just several\nsingular value decompositions, which is reasonably quick. Our new test statistics involve\nneither nuisance parameters nor tuning constants and will work for non-stationary and\nweakly dependent data. On the top of that, no boundary issue is present meaning that\nthe tests can detect the change close to the beginning or to the end of the studied regime.\nUnder the null hypothesis and the technical assumptions from Subsection 3.1, the test\nstatistics de\ufb01ned in (4.3) and (4.4) converge to non-degenerate limit distributions (their\nquantiles can be found in Subsection 4.2).\nTheorem 4.1 (Under the null). Let M and H0 hold. Under Assumptions D, E, and V,\nSn\nD\n\u2212\u2212\u2212\u2192\nn\u2192\u221e\nsup\nt\u2208[0,1]\n\f\fW(t) \u2212tW(1)\n\f\f\nsups\u2208[0,t]\n\f\fW(s) \u2212s\ntW(t)\n\f\f + sups\u2208[t,1]\n\f\f f\nW(s) \u22121\u2212s\n1\u2212t f\nW(t)\n\f\f\n(4.5)\nand\nTn\nD\n\u2212\u2212\u2212\u2192\nn\u2192\u221e\nZ 1\n0\n\bW(t) \u2212tW(1)\n\t2\nR t\n0\n\bW(s) \u2212s\ntW(t)\n\t2ds +\nR 1\nt\n\b f\nW(s) \u22121\u2212s\n1\u2212t f\nW(t)\n\t2ds\ndt,\n(4.6)\nwhere {W(t)}t\u2208[0,1] is a standard Wiener process and f\nW(t) = W(1) \u2212W(t).\nThe null hypothesis is rejected at signi\ufb01cance level \u03b1 for large values of Sn and Tn.\nThe critical values can be obtained as the (1\u2212\u03b1)-quantiles of the asymptotic distributions\nfrom (4.5) and (4.6). In order to describe limit behavior of the test statistics under the\nalternative, an additional changepoint assumption is required.\nAssumption C. For some \u03b6 \u2208(0, 1), as n \u2192\u221e,\n\u2225\u03b4\u22252 \u21920\nand\n(\u03b7\u03ba \u2212\u03d5\u22a4\u03d5)\u221an \u2192\u221e,\n(4.7)\nwhere \u03ba := ( \u00af\u03a3\u22a4\n\u0398,\u03b5+ \u00af\u03a3\u03b5\u03b2\u22a4)\u2206\u03b6( \u00af\u03a3\u0398,\u03b5+\u03b2\u00af\u03a3\u03b5)+( \u00af\u03a3\u22a4\n\u0398,\u03b5+ \u00af\u03a3\u03b5(\u03b2+\u03b4)\u22a4)\u2206\u2212\u03b6( \u00af\u03a3\u0398,\u03b5+(\u03b2+\u03b4)\u00af\u03a3\u03b5),\n\u03d5 := ( \u00af\u03a3\u0398 + \u00af\u03a3\u0398,\u03b5\u03b2\u22a4)\u2206\u03b6( \u00af\u03a3\u0398,\u03b5 + \u03b2\u00af\u03a3\u03b5) + ( \u00af\u03a3\u0398 + \u00af\u03a3\u0398,\u03b5(\u03b2 + \u03b4)\u22a4)\u2206\u2212\u03b6( \u00af\u03a3\u0398,\u03b5 + (\u03b2 + \u03b4)\u00af\u03a3\u03b5),\nand \u03b7 := \u03bbmin(( \u00af\u03a3\u0398 + \u00af\u03a3\u0398,\u03b5\u03b2\u22a4)\u2206( \u00af\u03a3\u0398 + \u03b2 \u00af\u03a3\u22a4\n\u0398,\u03b5) + \u03c32Ip) \u2212\u03c32.\nThis assumption may be considered as a changepoint detectability requirement for\nlocal alternatives, because it manages the relationship between the size of the change,\nthe location of the change, and the noisiness of the data in order to be able to detect the\nchangepoint. In case of uncorrelated error structure, the previous formulae become simpler\ndue to \u00af\u03a3\u0398,\u03b5 = 0. Assumption C is automatically ful\ufb01lled, for instance, for an arbitrary\n\u03b4 \u21920 and the one-dimensional equidistant design points Zi\u2019s on (0, 1) with homoscedastic\nerror structure, because then \u03b7\u03ba \u2212\u03d5\u22a4\u03d5 = \u03b22{\u03b63 + (1 \u2212\u03b6)3}{1 \u2212\u03b63 \u2212(1 \u2212\u03b6)3}/9 + O(\u03b4)\nas \u03b4 \u21920.\nFurthermore, let us remark that \u03d1 := \u00af\u03a3\u0398 + \u03b2 \u00af\u03a3\u22a4\n\u0398,\u03b5 has full rank under\nAssumption V.\nNow, the tests based on Sn and Tn are shown to be consistent, as the test statistics\nconverge to in\ufb01nity under some local alternatives, provided that the size of the change\ndoes not convergence to zero too fast, cf. Assumption C where \u03ba and \u03d5 depend on \u03b4.\nChangepoint in Linear Relations\n9\nTable 1. Simulated asymptotic critical values for Sn and Tn\n100(1 \u2212\u03b1)%\n90%\n95%\n97.5%\n99%\n99.5%\nS -based\n1.209008\n1.393566\n1.571462\n1.782524\n1.966223\nT -based\n5.700222\n7.165705\n8.807070\n10.597625\n11.755233\nTheorem 4.2 (Under local alternatives). Let M and HA hold such that \u03c4 = [n\u03b6] for\nsome \u03b6 \u2208(0, 1). Under Assumptions C, D, E, and V,\nSn\nP\n\u2212\u2212\u2212\u2192\nn\u2192\u221e\u221e\nP\n\u2190\u2212\u2212\u2212\nn\u2192\u221eTn.\n(4.8)\nAssumption C can be sharpened as remarked below with the corresponding proof in\nthe Appendix A.\nRemark 4.3. The second part of relation (4.7) can be replaced by\n{\u03ba+\u03b7\u2212\nq\n(\u03ba + 2\u03c32 + \u03b7)2 \u22124(\u03ba + \u03c32 \u2212\u03d5\u22a4(\u03d1\u22a4\u2206\u03d1 + \u03c32Ip)\u22121\u03d5)(\u03c32 + \u03b7)}\u221an \u2192\u221e(4.9)\nand the assertion of Theorem 4.2 still holds.\nBasically, Theorem 4.2 discloses that in presence of the structural change in linear\nrelations, the test statistics explode above all bounds. Hence, the asymptotic distributions\nfrom Theorem 4.1 can be used to construct the tests. Although, explicit forms of those\ndistributions stated in (4.5) and (4.6) are unknown.\n4.2.\nAsymptotic critical values\nThe critical values may be determined by simulations from the limit distributions Sn and\nTn from Theorem 4.1. Theorem 4.2 ensures that we reject the null hypothesis for large\nvalues of the test statistics. We have simulated the asymptotic distributions (4.5) and (4.6)\nby discretizing the standard Wiener process and using the relationship of a random walk to\nthe standard Wiener process. We considered 1000 as the number of discretization points\nwithin [0, 1] interval and the number of simulation runs equals to 100000. In Table 1, we\npresent several critical values for the test statistics Sn and Tn.\n4.3.\nChangepoint estimator\nIf a change is detected, it is of interest to estimate the changepoint. It is sensible to use\n\u02c6\u03c4n := argmax\n1\u2264k\u2264n\u22121\n\f\f\u03bbk \u2212k\nn\u03bbn\n\f\f +\n\f\fe\u03bbk \u2212n\u2212k\nn e\u03bb0\n\f\f\nmax1\u2264i<k\n\f\f\u03bbi \u2212i\nk\u03bbk\n\f\f + maxk<i\u2264n\n\f\fe\u03bbi \u2212n\u2212i\nn\u2212k e\u03bbk\n\f\f\nas a changepoint estimator.\nOur next theorem shows that under the alternative, the\nchangepoint \u03c4 is consistently estimated by the estimator \u02c6\u03c4n.\nCorollary 4.4 (Consistency). Let the assumptions of Theorem 4.2 hold. If\n\u2200t \u2208(\u03b6, 1) : {\u03b7(t)\u03ba(t) \u2212\u03d5(t)\u22a4\u03d5(t)}\u221an n\u2192\u221e\n\u2212\u2212\u2212\u2192\u221e;\n(4.10)\n10\nM. Pe\u02c7sta\n\u2200t \u2208(0, \u03b6) : {\u02dc\u03b7(t)\u02dc\u03ba(t) \u2212\u02dc\u03d5(t)\u22a4\u02dc\u03d5(t)}\u221an n\u2192\u221e\n\u2212\u2212\u2212\u2192\u221e,\n(4.11)\nwhere \u03ba(t) := ( \u00af\u03a3\u22a4\n\u0398,\u03b5+ \u00af\u03a3\u03b5\u03b2\u22a4)\u2206\u03b6( \u00af\u03a3\u0398,\u03b5+\u03b2\u00af\u03a3\u03b5)+( \u00af\u03a3\u22a4\n\u0398,\u03b5+ \u00af\u03a3\u03b5(\u03b2+\u03b4)\u22a4)(\u2206t\u2212\u2206\u03b6)( \u00af\u03a3\u0398,\u03b5+(\u03b2+\n\u03b4)\u00af\u03a3\u03b5), \u03d5(t) := ( \u00af\u03a3\u0398 + \u00af\u03a3\u0398,\u03b5\u03b2\u22a4)\u2206\u03b6( \u00af\u03a3\u0398,\u03b5 +\u03b2\u00af\u03a3\u03b5)+( \u00af\u03a3\u0398 + \u00af\u03a3\u0398,\u03b5(\u03b2+\u03b4)\u22a4)(\u2206t \u2212\u2206\u03b6)( \u00af\u03a3\u0398,\u03b5 +\n(\u03b2 + \u03b4)\u00af\u03a3\u03b5), \u02dc\u03ba(t) := ( \u00af\u03a3\u22a4\n\u0398,\u03b5 + \u00af\u03a3\u03b5\u03b2\u22a4)\u2206\u2212\u03b6( \u00af\u03a3\u0398,\u03b5 + \u03b2\u00af\u03a3\u03b5) + ( \u00af\u03a3\u22a4\n\u0398,\u03b5 + \u00af\u03a3\u03b5(\u03b2 + \u03b4)\u22a4)(\u2206\u2212t \u2212\n\u2206\u2212\u03b6)( \u00af\u03a3\u0398,\u03b5 + (\u03b2 + \u03b4)\u00af\u03a3\u03b5), \u02dc\u03d5(t) := ( \u00af\u03a3\u0398 + \u00af\u03a3\u0398,\u03b5\u03b2\u22a4)\u2206\u2212\u03b6( \u00af\u03a3\u0398,\u03b5 + \u03b2\u00af\u03a3\u03b5) + ( \u00af\u03a3\u0398 + \u00af\u03a3\u0398,\u03b5(\u03b2 +\n\u03b4)\u22a4)(\u2206\u2212t \u2212\u2206\u2212\u03b6)( \u00af\u03a3\u0398,\u03b5 + (\u03b2 + \u03b4)\u00af\u03a3\u03b5), \u03b7(t) := \u03bbmin(( \u00af\u03a3\u0398 + \u00af\u03a3\u0398,\u03b5\u03b2\u22a4)\u2206t( \u00af\u03a3\u0398 + \u03b2 \u00af\u03a3\u22a4\n\u0398,\u03b5) +\nt\u03c32Ip)\u2212t\u03c32, and \u02dc\u03b7(t) := \u03bbmin(( \u00af\u03a3\u0398+ \u00af\u03a3\u0398,\u03b5\u03b2\u22a4)\u2206\u2212t( \u00af\u03a3\u0398+\u03b2 \u00af\u03a3\u22a4\n\u0398,\u03b5)+(1\u2212t)\u03c32Ip)\u2212(1\u2212t)\u03c32,\nthen\n\u02c6\u03c4n\nn\nP\n\u2212\u2212\u2212\u2192\nn\u2192\u221e\u03b6.\nConditions (4.10) and (4.11) serve as a uniform intermediary between the size of the\nchange, the location of the change, the sample size, and the heteroscedasticity of the\ndisturbances for assuring changepoint estimator\u2019s consistency.\nThese assumptions are\nagain automatically ful\ufb01lled for the case discussed below Assumption C.\nIn order to estimate more than one changepoint, it is possible to use an arbitrary\n\u2018divide-and-estimate\u2019 multiple changepoints method relying on our changepoint estimator,\nfor instance, wild binary segmentation by Fryzlewicz (2014).\n5.\nSimulation study\nWe are interested in the performance of the tests based on the self-normalized test statistics\nSn and Tn that are completely nuisance-parameter-free. We focused on the comparison\nof the accuracy of critical values obtained by the simulation from the limit distributions.\nIn Figures 1\u20134, one may see size-power plots considering the test statistics Sn and\nTn under the null hypothesis and under the alternative. Figures 1 and 2 correspond to\none input covariate (i.e., p = 1) with choices of \u03b2 = 1 and Zi,1 = 100i/(n + 1). A case\nwith two error-prone regressors (i.e., p = 2) is illustrated in Figures 3 and 4 for choices\nof \u03b2 = [1, 1]\u22a4and Zi,\u2022 = 100 \u00d7 [i/(n + 1), (i/(n + 1))3/2]. Next, n \u2208{200, 1000} and \u03c4 \u2208\n{n/4, n/2}. The size of change is \u03b4 \u2208{0.1, 0.5} for p = 1 and \u03b4 \u2208{[0.1, 0.1]\u22a4, [0.5, 0.5]\u22a4}\nfor p = 2. Especially smaller values of the break should represent the situations under\nthe local alternatives. In Figures 1 and 3, the empirical rejection frequency under the\nnull hypothesis (actual \u03b1-errors) is plotted against the theoretical size (theoretical \u03b1-\nerrors with \u03b1 \u2208{1%, 5%, 10%}), illustrating the size of the tests. The ideal situation\nunder the null hypothesis is depicted by the straight diagonal dotted line. The empirical\nrejection frequencies (1\u2212errors of the second type) under the alternative (with di\ufb00erent\nchangepoints and values of the change) are shown in Figures 2 and 4, illustrating the power\nof the tests. Under the alternative, the desired situation would be a steep function with\nvalues close to 1. For more details on the size-power plots we may refer, e.g., to Kirch\n(2006). The standard deviation of the random disturbances was set to \u03c3 \u2208{0.5, 1.0}\nand the random error terms {\u0398n,1}\u221e\nn=1, . . . , {\u0398n,p}\u221e\nn=1, and {\u03b5n}\u221e\nn=1 were independently\nsimulated as three time series:\n\u2022 IID . . . independent and identically distributed random variables;\n\u2022 AR(1) . . . autoregressive (AR) process of order one having a coe\ufb03cient of autore-\ngression equal 0.5;\nChangepoint in Linear Relations\n11\n\u2022 ARCH(1) . . . autoregressive conditional heteroscedasticity (ARCH) process with the\nsecond coe\ufb03cient equal 0.5.\nThe standard normal distribution and the Student t-distribution with 3 degrees of\nfreedom are used for generating the innovations of the models\u2019 errors. All of the time series\nare standardized such that they have variance equal \u03c32. Let us remark that the setup of\nStudent t3-distribution does not satisfy Assumption E. However, it can be considered as\na misspeci\ufb01ed model and one would like to inspect performance of our procedures on such\na model that violates our assumptions. In the simulations of the rejection rates, we used\n10000 repetitions.\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n\u03c3=0.5\nN(0, 1)\n\u03c3=0.5\nt3\n\u03c3=1.0\nN(0, 1)\n\u03c3=1.0\nt3\nn=200\nn=1000\n0.01\n0.05\n0.10\n0.01\n0.05\n0.10\n0.01\n0.05\n0.10\n0.01\n0.05\n0.10\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\nSignificance level \u03b1\nRejection rate\nErrors\nIID\nAR(1)\nARCH(1)\nStatistics\nS (sup\u2212type)\nT (int\u2212type)\nH0\nFigure 1. Size-power plots for Sn and Tn under H0 (p = 1)\nIn all of the sub\ufb01gures of Figures 1 and 3 depicting a situation under the null hypothesis,\nwe may see that comparing the accuracy of \u03b1-levels (sizes) for di\ufb00erent self-normalized test\nstatistics, the integral-type (T -based) method seems to keep the theoretical signi\ufb01cance\nlevel more \ufb01rmly than the supremum-type (S -based) method. Comparing the case of\nN(0, 1) innovations with the case of t3 innovations, the rejection rates under the null tend\nto be slightly higher for the t3-distribution. In spite of the fact that the t3-distributed\nerrors violate Assumption E, the performance of our tests is still surprisingly satisfactory\nin such case. As expected, the accuracy of the critical values tends to be better for larger n.\nThe more complicated dependence structure of errors is assumed, the worse performance\nof the tests is obtained. Furthermore, the less volatile errors are set, the better tests\u2019 sizes\nare attained.\nThe T -method performs better under the null. However under the alternative, the S -\nmethod has a tendency to have slightly higher power than the T -method (see Figures 2\nand 4). We may also conclude that under HA with less volatile errors, the power of the\ntest increases. The power decreases when the changepoint is closer to the beginning or the\nend of the input-output data. The heavier tails (t3 against N(0, 1)) give worse results in\ngeneral for both test statistics. Moreover, \u2018more dependent\u2019 scenarios reveal worsening of\nthe test statistics\u2019 performance. Furthermore, the smaller size of the change is considered,\n12\nM. Pe\u02c7sta\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n\u03b4=0.1\nN(0, 1)\n\u03b4=0.1\nt3\n\u03b4=0.5\nN(0, 1)\n\u03b4=0.5\nt3\n\u03c4=n/4\n\u03c3=0.5\nn=200\n\u03c4=n/2\n\u03c3=0.5\nn=200\n\u03c4=n/4\n\u03c3=1.0\nn=200\n\u03c4=n/2\n\u03c3=1.0\nn=200\n\u03c4=n/4\n\u03c3=0.5\nn=1000\n\u03c4=n/2\n\u03c3=0.5\nn=1000\n\u03c4=n/4\n\u03c3=1.0\nn=1000\n\u03c4=n/2\n\u03c3=1.0\nn=1000\n0.01\n0.05\n0.10\n0.01\n0.05\n0.10\n0.01\n0.05\n0.10\n0.01\n0.05\n0.10\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nSignificance level \u03b1\nRejection rate\nErrors\nIID\nAR(1)\nARCH(1)\nStatistics\nS (sup\u2212type)\nT (int\u2212type)\nHA\nFigure 2. Size-power plots for Sn and Tn under HA (p = 1)\nthe lower power of the test is achieved. And again, the power gets higher for larger n.\nAfterwards, a simulation experiment is performed to study the \ufb01nite sample proper-\nties of the changepoint estimator for a change in the linear relations\u2019 parameter.\nWe\nnumerically present only the case of p = 1. In particular, the interest lies in the empir-\nical distributions of the proposed estimator visualized via boxplots, see Figure 5. The\nChangepoint in Linear Relations\n13\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n\u03c3=0.5\nN(0, 1)\n\u03c3=0.5\nt3\n\u03c3=1.0\nN(0, 1)\n\u03c3=1.0\nt3\nn=200\nn=1000\n0.01\n0.05\n0.10\n0.01\n0.05\n0.10\n0.01\n0.05\n0.10\n0.01\n0.05\n0.10\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\nSignificance level \u03b1\nRejection rate\nErrors\nIID\nAR(1)\nARCH(1)\nStatistics\nS (sup\u2212type)\nT (int\u2212type)\nH0\nFigure 3. Size-power plots for Sn and Tn under H0 (p = 2)\nsimulation setup is kept the same as described above.\nIt can be concluded that the precision of our changepoint estimate is satisfactory\neven for relatively small sample sizes regardless of the errors\u2019 structure.\nLess volatile\nmodel errors provide more precise changepoint estimate. The less complicated dependence\nstructure is assumed, the higher accuracy of the estimator is obtained. Furthermore, the\ndisturbances with heavier tails yield less precise estimates than innovations with light\ntails. One may notice that higher precision is obtained when the changepoint is closer to\nthe middle of the data. It is also clear that the precision of \u02c6\u03c4n improves markedly as the\nsize of change increases.\n6.\nApplications\n6.1.\nPractical application: Calibration\nA company has two industrial devices, where the \ufb01rst one is calibrated according to some\ninstitute of standards and the second one is just a casual device. We want to test whether\nthe second device is calibrated according to the \ufb01rst one. In this calibration problem, it\nmeans to know whether the second device has approximately the same performance up\nto some unknown multiplication constant as the \ufb01rst one. Consequently, other devices of\nthe same type are needed to be calibrated as well. For some reasons, e.g., economic or\nlogistic, it is only possible to calibrate one device by the o\ufb03cial authorities.\nOur data set, provided by a Czech steelmaker, contains 100 couples of speed values of\ntwo hammer rams (see Figure 6), where the \ufb01rst forging hammer is calibrated. We set the\nsame power level on both hammers and measure the speed of each hammer ram repeatedly\nchanging only the power level. Our measurements of the speed are encumbered with errors\nof the same variability in both cases, because we use the same device for measuring the\nspeed and both forging hammers are of the same type.\nSince the power set for the\nforging hammer is directly proportional to the speed of the hammer ram, our goal is to\n14\nM. Pe\u02c7sta\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n\u03b4=0.1\nN(0, 1)\n\u03b4=0.1\nt3\n\u03b4=0.5\nN(0, 1)\n\u03b4=0.5\nt3\n\u03c4=n/4\n\u03c3=0.5\nn=200\n\u03c4=n/2\n\u03c3=0.5\nn=200\n\u03c4=n/4\n\u03c3=1.0\nn=200\n\u03c4=n/2\n\u03c3=1.0\nn=200\n\u03c4=n/4\n\u03c3=0.5\nn=1000\n\u03c4=n/2\n\u03c3=0.5\nn=1000\n\u03c4=n/4\n\u03c3=1.0\nn=1000\n\u03c4=n/2\n\u03c3=1.0\nn=1000\n0.01\n0.05\n0.10\n0.01\n0.05\n0.10\n0.01\n0.05\n0.10\n0.01\n0.05\n0.10\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nSignificance level \u03b1\nRejection rate\nErrors\nIID\nAR(1)\nARCH(1)\nStatistics\nS (sup\u2212type)\nT (int\u2212type)\nHA\nFigure 4. Size-power plots for Sn and Tn under HA (p = 2)\ntest whether the ratio of two hammer rams\u2019 speeds is kept constant over changing the\npower level or not. Therefore, our changepoint in the EIV model is very suitable for this\nsetup\u2014a linear dependence and errors in both measured speeds (with the same variance).\nBoth our changepoint tests\u2014Sn = 83.2 and Tn = 861.4\u2014reject the null hypothesis of\na constant linear coe\ufb03cient between two hammer rams\u2019 speed values at the signi\ufb01cance\nChangepoint in Linear Relations\n15\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGG\nGGGG\nGGGG\nGG\nG\nG\nG\nGGG\nGGGGGGGG\nGGGG\nG\nG\nGGG\nG\nG\nG\nG\nGGG\nG\nG\nGGG\nG\nG\nG\nGGG\nG\nG\nG\nGGG\nG\nG\nGG\nG\nGGGG\nG\nGGG\nG\nG\nGGGGGGG\nG\nGGG\nGG\nG\nGGG\nG\nGG\nG\nGGGGG\nG\nG\nGG\nG\nG\nGGGGG\nG\nG\nGG\nGGGGGG\nG\nGG\nG\nG\nGG\nG\nG\nG\nGG\nGGGG\nG\nG\nG\nG\nGGGGG\nGGGG\nG\nG\nG\nGGG\nG\nGG\nGG\nGGGGGGGGG\nGG\nGGG\nG\nG\nGGG\nG\nG\nGG\nGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGG\nG\nGGGGGGGGG\nG\nGGGGG\nG\nGGGGGGGGGGGGGGGGGGGGGG\nG\nGG\nG\nGGGGGGGG\nG\nGGGGGGGG\nG\nGGGGGGGGGGGGGGGGGGGGGGG\nG\nGGGGGGGGGGGGGGG\nG\nGGGGGG\nG\nGGGGGGGGG\nG\nGGGGGGGGGGG\nG\nG\nG\nG\nGGGGG\nGG\nG\nG\nG\nGGGGGGG\nG\nGGGG\nG\nGGGGGGGG\nG\nGGG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nGG\nGG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nGGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGG\nG\nGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGG\nGGGGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGGG\nG\nG\nG\nGG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nGG\nG\nG\nGG\nGGGG\nGGGGGGG\nG\nGG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nGG\nG\nG\nGG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGGGG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nGGG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGGG\nG\nG\nGG\nGGGGGGGGGGGGGGGG\nG\nGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nGGG\nG\nGGGGGG\nGG\nG\nG\nGGGGG\nG\nGG\nG\nGGGGG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGG\nG\nGGGGGGGG\nG\nGGGG\nGGGGGGGGGG\nG\nG\nGGGGGGGGGGGG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGG\nG\nGGGGGGGGGG\nG\nG\nGGG\nG\nGG\nGGG\nG\nG\nGG\nG\nG\nGG\nG\nGGG\nG\nGGG\nG\nGGGGGG\nG\nG\nG\nGGG\nGG\nG\nGGG\nGGGG\nG\nGGG\nGG\nG\nGG\nGGGG\nG\nG\nG\nGGGG\nG\nGG\nGGGGGG\nG\nGGG\nG\nGGGGGG\nG\nGGG\nG\nG\nGGGGG\nG\nGG\nG\nGG\nG\nGG\nG\nG\nG\nG\nGGG\nGG\nG\nG\nG\nGG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGGGGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGG\nG\nGGG\nG\nG\nGG\nG\nG\nG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nG\nGGGGG\nG\nG\nG\nGGGG\nG\nG\nG\nGGGG\nG\nGGGG\nG\nG\nG\nGGGGGGGGG\nG\nG\nGG\nGGG\nG\nG\nG\nGGG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nGG\nGGG\nG\nGGGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nGGG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGG\nG\nGGGGG\nG\nGGGGGGGGGGGGGG\nG\nG\nGGGG\nG\nGGGG\nGGGGGGGGGGGGGGGGG\nG\nGGGGG\nG\nGG\nG\nG\nGGGGGG\nG\nG\nG\nGGGGGGGGGGGGG\nGGG\nG\nGGGGGG\nG\nGGGGGGG\nG\nGGGGGGGG\nG\nGGGGGGGGGGGGGGG\nGGGG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nGGGGGGGGGG\nG\nGGGG\nGGGGGGGG\nG\nG\nG\nGGG\nG\nGG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nGGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGGG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nGG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nGGG\nG\nG\nG\nGG\nG\nG\nG\nGG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nGG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nGG\nGG\nGG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nGG\nG\nGG\nG\nGGGGGGGG\nG\nG\nG\nGGGGGGGGGGGGGGGGGG\nG\nGGGG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nGGGG\nG\nG\nG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nG\nGGG\nG\nGGG\nG\nGGG\nGGGGGG\nG\nGGGGGGGGGGGGGGG\nG\nGGGGGGGGG\nGGGGGG\nG\nGGGGGG\nG\nG\nGGGGGGGG\nGGG\nG\nG\nG\nGGGGGGGG\nG\nG\nG\nGGG\nG\nG\nG\nGGGG\nG\nGGGG\nG\nGGG\nG\nG\nG\nGG\nG\nGG\nG\nGG\nGG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nGG\nGGGG\nGG\nG\nGG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nGGG\nG\nGG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGG\nGGGGGGG\nG\nG\nGGGG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGGGGG\nG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGGGGGGGGG\nG\nGGGG\nGGGGG\nG\nGG\nG\nG\nG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGG\nGGGGGGG\nGGGG\nGG\nG\nGGGGGGGG\nGGGGGG\nG\nG\nGG\nGGGGGGGG\nG\nGGGGGGGGGGGGG\nGGGGGGGGGGGGGG\nG\nG\nGGGGG\nG\nG\nG\nG\nG\nGGG\nG\nGGG\nG\nG\nG\nG\nG\nGG\nG\nGGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nGG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGG\nGGGG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGGG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nGG\nG\nGGG\nG\nG\nGGG\nG\nG\nG\nG\nGGGGG\nG\nG\nGG\nG\nGGG\nG\nG\nG\nG\nGG\nG\nG\nG\nGGG\nG\nG\nG\nGG\nGG\nG\nGG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nGG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGGG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGGGGG\nG\nG\nG\nG\nG\nGGGGGG\nG\nG\nGGGGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGGGG\nG\nGG\nG\nG\nG\nGG\nG\nG\nGGG\nG\nGGG\nG\nG\nG\nGGGGGGG\nG\nGGG\nG\nGGGGG\nG\nGGG\nG\nGGGG\nG\nGGGGG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nGGGGGGGGGGGGGG\nG\nGGG\nGGGGGGGGGGGGGGGG\nG\nGG\nG\nGGGGGGGGGGGGGGGGGGGGG\nG\nGGGGGGGG\nGGG\nGGGG\nGGGGGGGGGGGGG\nGG\nG\nG\nGGGGG\nG\nG\nG\nG\nG\nG\nG\nGGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGG\nG\nGGGGG\nG\nGG\nG\nGG\nG\nGG\nG\nGG\nG\nGGGGGG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGGGGG\nG\nG\nG\nGGGGG\nG\nGG\nG\nGG\nG\nG\nGG\nG\nGG\nG\nG\nG\nG\nGGG\nG\nGGG\nG\nGGG\nG\nG\nG\nG\nGG\nGG\nG\nG\nGG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGGGGGG\nGG\nGGG\nG\nG\nGG\nGGGG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nGGG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nGGG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGGG\nGG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nGGGGG\nG\nGGGGGGG\nG\nG\nG\nG\nGG\nG\nGG\nGGG\nG\nGG\nGG\nGGG\nG\nGG\nG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGGGG\nG\nGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGG\nG\nGGGGG\nG\nG\nG\nGGGGGGGGGGGGGG\nG\nG\nG\nG\nGG\nG\nGGG\nGG\nG\nGGGGGG\nG\nGGG\nG\nGGGG\nG\nG\nG\nGGGGGG\nG\nG\nG\nGG\nG\nG\nGG\nG\nGGGGGG\nG\nG\nG\nG\nG\nGGG\nG\nGGGGGG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nGG\nGG\nG\nGGGG\nG\nG\nG\nG\nGGGG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nGG\nG\nGG\nGG\nGG\nGG\nGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGG\nG\nG\nGGGGGGG\nG\nGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGG\nG\nGGGGGGG\nG\nGGGGGGGGGGG\nG\nGGGGGGG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nGG\nG\nG\nG\nGGGGGGGGGGG\nG\nGGGGGGGGGGGGG\n\u03b4=0.1\nN(0, 1)\n\u03b4=0.1\nt3\n\u03b4=0.5\nN(0, 1)\n\u03b4=0.5\nt3\n\u03c3=1.0\nn=200\n\u03c3=0.5\nn=200\n\u03c3=1.0\nn=1000\n\u03c3=0.5\nn=1000\nn/4\nn/2\nn/4\nn/2\nn/4\nn/2\nn/4\nn/2\n0\n50\n100\n150\n200\n0\n50\n100\n150\n200\n0\n250\n500\n750\n1000\n250\n500\n750\n1000\n\u03c4\nErrors\nIID\nAR(1)\nARCH(1)\nChangepoint estimate\nFigure 5. Boxplots of the estimated changepoint \u02c6\u03c4n (p = 1)\nlevel of \u03b1 = 0.5% (cf. Table 1; the signi\ufb01cance level for technical \ufb01elds is usually smaller\nthan the standard 5%), indicating a changed performance of the second non-calibrated\nhammer ram.\nAs an estimate for our change, we obtain \u02c6\u03c4n = 60 (depicted by a vertical line in Fig-\nure 6), which corresponds to the 60th measurement of pair of speeds. After this particular\n16\nM. Pe\u02c7sta\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n0\n2000\n4000\n6000\n0\n2000\n4000\n6000\nTwo Hammer Rams\nFigure 6. Speeds of two hammer rams, where the \ufb01rst one displayed on the x-axis is calibrated. The\nchangepoint estimate corresponding to the technical issues of the second hammer ram after the 60th\nmeasurement is depicted by the vertical line\nmeasurement, we have background information that a technical issue appeared to the sec-\nond hammer ram\u2014one of its oil tubes started to leak. Our procedure is indeed capable to\ndetect and, consequently, to estimate the changepoint in the ratio of the hammer rams\u2019\nspeeds. And this is done fully automatically without expert knowledge about the oil tube\nissue and also without setting tuning parameters. Moreover, the estimated ratio via the\nTLS approach before the change is 1.000891 (the slope of the green line in Figure 6), which\nbasically says that the hammer rams work approximately in the same way. However, the\nestimated ratio via the TLS approach after the change is 0.9892154 (the slope of the red\nline in Figure 6), which is signi\ufb01cantly di\ufb00erent from constant 1 (see a formal statistical\ntest by Pe\u02c7sta (2013b)).\nOther calibration examples, where our methodology is applicable, can be found in,\ne.g., Cheng and Riu (2006) or Guo and Little (2011).\n6.2.\nTheoretical application: Randomly spaced time series\nA motivation for the changepoint problem in randomly spaced time series comes from\nthe changepoint in the polynomial trending regression (Aue et al., 2009). Let us think\nof a single regressor measured precisely such that Xi,1 \u2261Zi,1 = i/(n + 1). This indeed\ncorresponds to a situation of a one-dimensional equally (regularly) spaced time series,\nwhere the original time points {i}n\ni=1 are \u2018squeezed\u2019 into the interval [0, 1] by dividing\nof n + 1.\nNow, let us assume that our outcome observations Yi\u2019s are supposed to be measured at\nsome unknown time points Zi,1\u2019s. However, due to some measurement imprecision or some\nouter random in\ufb02uence, the actual observation Yi, which should correspond to Zi,1, is not\nrecorded at time point Zi,1, but at time point Xi,1. One can imagine a long-distance time\ntrial against the chronometer (e.g., an individual competition in cross-country skiing).\nThere are n intermediate spots on the track, where the athlete\u2019s time is recorded. If we\nChangepoint in Linear Relations\n17\nthink of one particular athlete, we measure at the intermediate spot i her/his error-prone\ncompetition time Xi,1, which was encumbered by some randomness, instead of the true\nunobservable time is Zi,1.\nThis is because each race is speci\ufb01c and every athlete has\na unique performance during that particular race. We also observe a time lag Yi between\nher/him and the current leader at that spot. Now, one is interested whether there is\na change in linear trend. This would help to analyze whether the particular athlete tried\nto improve or not during the time trial. One can argue that a distance of the intermediate\nspot should be taken into account instead of the athlete\u2019s intermediate time. However,\nthe intermediate distance is also measured with error, for instance, a rounded value of the\ntrue unobserved distance is provided. Another example of randomly spaced time series is\na case when the observation times are driven by the series itself. For instance, cumulative\ncounts of occurrences of a disease in a given area (Wright, 1986).\nThe unobservable sequence {Zi,1}n\ni=1 can be regularly or irregularly spaced. The key\nissue is to have satis\ufb01able Assumption D. Since the developed detection procedures rely\non the orthogonal regression, it is su\ufb03cient to transform the original randomly spaced time\nseries {Xi,1, Yi}n\ni=1 into, e.g., {Xi,1/(maxi{|Xi,1|} + \u03f5), Yi/(maxi{|Xi,1|} + \u03f5)}n\ni=1, where\na constant \u03f5 is reasonably large. Afterwards, the proposed tests remain valid when applied\nto the transformed randomly spaced time series, because \u03b2 stays unchanged after such\na transformation. Hence, one can test whether the linear trend has or has not changed\nover time.\n7.\nConclusions\nOur changepoint problem in linear relations is linearly de\ufb01ned, but comes with a highly\nnon-linear solution and inference. We have proposed two tests for changepoints with de-\nsirable theoretical properties: The asymptotic size of the tests is guaranteed by a limit\ntheorem even under non-stationarity and weak dependency, the tests and the related\nchangepoint estimator are consistent. We are not aware of any similar results even for\nindependent and identically distributed errors. By combining self-normalization and the\nproposed spectral weak invariance principle, there are neither tuning constants nor nui-\nsance parameters involved in the whole testing procedure. Therefore, the detection meth-\nods are completely data-driven, which makes this framework e\ufb00ortlessly applicable as\ndemonstrated. In our simulations, the tests show reliable performance.\nA.\nProofs\nProof of Proposition 3.1. Let the singular value decomposition of the transformed \u2018partial\u2019\ndata matrix be\n[X[nt], Y[nt]]\u03a3\u22121/2 = U(t)\u0393(t)V (t)\u22a4=\np+1\nX\ni=1\n\u03c2(t)(i)u(t)(i)v(t)(i)\u22a4\nfor some t \u2208(0, 1]. Note that we are in a situation of no change in the parameter \u03b2.\nBearing in mind Assumptions D and E, Gleser (1981, Lemma 2.1) and Pe\u02c7sta (2011,\nTheorem 3.1) provide that 0 \u0338= vp+1(t)(p+1) (i.e., the last element of the last right-singular\n18\nM. Pe\u02c7sta\nvector v(t)(p+1) corresponding to the smallest singular value) with probability tending to\none as n increases. According to Gleser (1981, proof of Lemma 4.2), one gets\n1\n\u221an\n\u0010\n\u03bb[nt] \u2212[nt]\u03c32\u0011\n=\n\u0010\nvp+1(t)(p+1)\u00112\n[a\u22a4\nt , \u22121]\n\u001a 1\n\u221an (Dt \u2212E Dt)\n\u001b \u0014 at\n\u22121\n\u0015\n(A.1)\n+\n\u0010\nvp+1(t)(p+1)\u00112\u221an[a\u22a4\nt , \u22121]\u03a3\u22121/2\n\u0014 Ip\n\u03b2\u22a4\n\u0015 1\nnZ\u22a4\n[nt]Z[nt][Ip, \u03b2]\u03a3\u22121/2\n\u0014 at\n\u22121\n\u0015\n,\n(A.2)\nwhere at := ( \u02dc\nX\u22a4\n[nt] \u02dc\nX[nt] \u2212\u03bb[nt]Ip)\u22121 \u02dc\nX\u22a4\n[nt] \u02dcY[nt] is the TLS estimator for the transformed\ndata [ \u02dc\nX[nt], \u02dcY[nt]] := [X[nt], Y[nt]]\u03a3\u22121/2 and Dt := \u03a3\u22121/2[X[nt], Y[nt]]\u22a4[X[nt], Y[nt]]\u03a3\u22121/2.\nWith respect to Pe\u02c7sta (2011), we have\n\u0010\nvp+1(t)(p+1)\u00112\n= 1 \u2212\n\r\r\r[v1(t)(p+1), . . . , vp(t)(p+1)]\u22a4\r\r\r\n2\n2 \u2192\n1\n1 + \u2225\u03b1\u22252\n2\nalmost surely as n \u2192\u221e. Moreover, \u221an(at \u2212\u03b1) = OP(1) as n \u2192\u221eby Pe\u02c7sta (2013a).\nThe strong law of large numbers for \u03b1-mixing by Chen and Wu (1989) together with\nTheorem 3.1 by Pe\u02c7sta (2011) lead to at \u2212\u03b1 = o(1) almost surely. Since Assumption D\nholds, the expression in (A.2) is oP(1). Furthermore, the expression on the right hand\nside of (A.1) is o(1) away from\n1\n1 + \u2225\u03b1\u22252\n2\n[\u03b1\u22a4, \u22121]\n\u001a 1\n\u221an (Dt \u2212E Dt)\n\u001b \u0014 \u03b1\n\u22121\n\u0015\n(A.3)\nas n \u2192\u221e. Hence, the process from the left hand side of (A.1) in D[0, 1] has approximately\nthe same distribution as the process (A.3).\nNote that\n[\u03b1\u22a4, \u22121]Dt\n\u0014 \u03b1\n\u22121\n\u0015\n= (\u00af\u03a3\u03b5 \u2212\u00af\u03a3\u22a4\n\u0398,\u03b5\u03b1)2\r\rY[nt] \u2212X[nt]\u03b2\n\r\r2\n2.\nUsing the functional central limit theorem for \u03b1-mixing by Herrndorf (1983) or Lin and\nLu (1997, Corollary 3.2.1) in an analogous fashion as in the proof of Theorem 2.3 by Pe\u02c7sta\n(2013a), one gets\n\u001a\n[\u03b1\u22a4, \u22121]\n\u001a 1\n\u221an (Dt \u2212E Dt)\n\u001b \u0014 \u03b1\n\u22121\n\u0015\u001b\nt\u2208[0,1]\nD[0,1]\n\u2212\u2212\u2212\u2192\nn\u2192\u221e{\u03c62\u03c5W(t)}t\u2208[0,1]\ndue to Assumption V.\nSimilarly for\nn\n1\n\u221an\n\u0010\ne\u03bb[n(1\u2212t)] \u2212[n(1 \u2212t)]\u03c32\u0011o\nt\u2208[0,1] and { f\nW(t)}t\u2208[0,1].\nProof of Theorem 4.1. The spectral weak invariance principle from Proposition 3.1 and\nLemma 1 by Pe\u02c7sta and Wendler (2019) in combination with the continuous mapping\ndevice complete the proof.\nProof of Theorem 4.2. Under HA, let us \ufb01nd a lower bound for the smallest eigenvalue of\nthe positive semi-de\ufb01nite matrix\n1\nn\u03a3\u22121/2[X, Y ]\u22a4[X, Y ]\u03a3\u22121/2 = 1\nn\n\" \u02dc\nX\u22a4\u02dc\nX\n\u02dc\nX\u22a4\u02dcY\n\u02dcY \u22a4\u02dc\nX\n\u02dcY \u22a4\u02dcY\n#\n=:\n\u0014A\nc\nc\u22a4\nd\n\u0015\n,\n(A.4)\nChangepoint in Linear Relations\n19\nwhere [ \u02dc\nX, \u02dcY ] := [X, Y ]\u03a3\u22121/2. With respect to Dembo (1988, Theorem 1), we get\n\u03bbmin\n\u0012\u0014A\nc\nc\u22a4\nd\n\u0015\u0013\n\u2265d + \u2113\n2\n\u2212\ns\n(d \u2212\u2113)2\n4\n+ c\u22a4c,\n(A.5)\nwhere \u2113is any lower bound on the smallest eigenvalue of the matrix A.\nRecall that\nAssumption E and the proof of Theorem 3.1 by Pe\u02c7sta (2011) provide\n1\nn \u02dc\u03b5\u22a4\u02dc\u03b5 \u2192\u03c32, 1\nn\n\u02dc\u0398\u22a4\u02dc\u03b5 \u21920, 1\nn\n\u02dc\u0398\u22a4\u02dc\u0398 \u2192\u03c32Ip, 1\nn\n\u02dcZ\u22a4\u02dc\u03b5 \u21920, 1\nn\n\u02dcZ\u22a4\u02dc\u0398 \u21920\n(A.6)\nalmost surely as n \u2192\u221e, where [ \u02dc\u0398, \u02dc\u03b5] := [\u0398, \u03b5]\u03a3\u22121/2 and \u02dcZ := Z[Ip, \u03b2]\n\" \u00af\u03a3\u0398\n\u00af\u03a3\u22a4\n\u0398,\u03b5\n#\n. By\nAssumptions C and D, one can obtain\n\u03bb(A)min = \u03bbmin\n\u0012 1\nn( \u02dcZ + \u02dc\u0398)\u22a4( \u02dcZ + \u02dc\u0398)\n\u0013\n\u2192\u03bbmin\n \n[ \u00af\u03a3\u0398, \u00af\u03a3\u0398,\u03b5]\n\u0014 Ip\n\u03b2\u22a4\n\u0015\n\u2206[Ip, \u03b2]\n\" \u00af\u03a3\u0398\n\u00af\u03a3\u22a4\n\u0398,\u03b5\n#\n+ \u03c32Ip\n!\n= \u03c32 + \u03b7\n(A.7)\nalmost surely as n \u2192\u221e. Relation (A.7) immediately provides a limit of a candidate for \u2113.\nNow, (A.4) and (A.5) lead to\nlim inf\nn\u2192\u221e\u03bbmin\n\u0012 1\nn\u03a3\u22121/2[X, Y ]\u22a4[X, Y ]\u03a3\u22121/2\n\u0013\n\u2265\nlim\nn\u2192\u221e\n1\nn\n\u02dcY \u22a4\u02dcY + \u03c32 + \u03b7\n2\n\u2212\nv\nu\nu\nu\nt\n\u0010\nlim\nn\u2192\u221e\n1\nn\n\u02dcY \u22a4\u02dcY \u2212\u03c32 \u2212\u03b7\n\u00112\n4\n+ lim\nn\u2192\u221e\n\r\r\r\r\n1\nn\n\u02dc\nX\u22a4\u02dcY\n\r\r\r\r\n2\n2\n.\n(A.8)\nAssumptions C, D, and relations (A.6) yield\n1\nn\n\u02dcY \u22a4\u02dcY = 1\nn\n\u02dcY \u22a4\n\u03c4 \u02dcY\u03c4 + 1\nn\n\u02dcY \u22a4\n\u2212\u03c4 \u02dcY\u2212\u03c4 = ( \u00af\u03a3\u22a4\n\u0398,\u03b5 + \u00af\u03a3\u03b5\u03b2\u22a4)\u2206\u03b6( \u00af\u03a3\u0398,\u03b5 + \u03b2\u00af\u03a3\u03b5)\n+ \u03c32 + ( \u00af\u03a3\u22a4\n\u0398,\u03b5 + \u00af\u03a3\u03b5(\u03b2 + \u03b4)\u22a4)\u2206\u2212\u03b6( \u00af\u03a3\u0398,\u03b5 + (\u03b2 + \u03b4)\u00af\u03a3\u03b5) + o(1) = \u03ba + \u03c32 + o(1)\nand\n1\nn\n\u02dc\nX\u22a4\u02dcY = 1\nn\n\u02dc\nX\u22a4\n\u03c4 \u02dcY\u03c4 + 1\nn\n\u02dc\nX\u22a4\n\u2212\u03c4 \u02dcY\u2212\u03c4 = ( \u00af\u03a3\u0398 + \u00af\u03a3\u0398,\u03b5\u03b2\u22a4)\u2206\u03b6( \u00af\u03a3\u0398,\u03b5 + \u03b2\u00af\u03a3\u03b5)\n+ ( \u00af\u03a3\u0398 + \u00af\u03a3\u0398,\u03b5(\u03b2 + \u03b4)\u22a4)\u2206\u2212\u03b6( \u00af\u03a3\u0398,\u03b5 + (\u03b2 + \u03b4)\u00af\u03a3\u03b5) + o(1) = \u03d5 + o(1)\nalmost surely as n \u2192\u221e. Thus,\n1\nn \u02dcY \u22a4\u02dcY + \u03c32 + \u03b7\n2\n\u2212\nv\nu\nu\nt\n\u0000 1\nn \u02dcY \u22a4\u02dcY \u2212\u03c32 \u2212\u03b7\n\u00012\n4\n+\n\r\r\r\r\n1\nn\n\u02dc\nX\u22a4\u02dcY\n\r\r\r\r\n2\n2\n=\n\u03ba + \u03b7 \u2212\nq\n(\u03ba \u2212\u03b7)2 + 4\u03d5\u22a4\n2 \u03d52\n2\n+ \u03c32 + o(1)\n(A.9)\n20\nM. Pe\u02c7sta\nalmost surely as n \u2192\u221e. Hence, combining (A.8) and (A.9) ends up with\nlim inf\nn\u2192\u221e\u03bbmin\n\u0012 1\nn[ \u02dc\nX, \u02dcY ]\u22a4[ \u02dc\nX, \u02dcY ]\n\u0013\n\u2212\u03c32 \u2265lim\nn\u2192\u221e\n2{\u03b7\u03ba \u2212\u03d5\u22a4\u03d5}\n\u03ba + \u03b7 +\nq\n(\u03ba \u2212\u03b7)2 + 4\u03d5\u22a4\u03d5\n.\nThen,\n1\n\u221an|\u03bbn \u2212n\u03c32| a.s.\n\u2212\u2212\u2212\u2192\nn\u2192\u221e\u221e\n(A.10)\nby Assumption C.\nWith respect to Assumptions D, E, V and according to the underlying proof of The-\norem 4.1,\n1\n\u221an max1\u2264i<\u03c4\n\f\f\u03bbi \u2212i\n\u03c4 \u03bb\u03c4\n\f\f and\n1\n\u221an max\u03c4<i\u2264n\n\f\fe\u03bbi \u2212n\u2212i\nn\u2212\u03c4 e\u03bb\u03c4\n\f\f are OP(1) as n \u2192\u221e.\nMoreover,\n1\n\u221an\n\f\f\u03bb\u03c4 \u2212\u03c4\u03c32\f\f = OP(1) as n \u2192\u221edue to Proposition 3.1.\nNote that there are no changes in the linear parameter corresponding to the \ufb01rst \u03c4\nobservations as well as to the last (remaining) n \u2212\u03c4 observations. Let k = \u03c4. Thus,\nunder HA,\nSn \u2265\n\f\f\u03bb\u03c4 \u2212\u03c4\nn\u03bbn\n\f\f\nmax1\u2264i<\u03c4\n\f\f\u03bbi \u2212i\n\u03c4 \u03bb\u03c4\n\f\f + max\u03c4<i\u2264n\n\f\fe\u03bbi \u2212n\u2212i\nn\u2212\u03c4 e\u03bb\u03c4\n\f\f\n\u2265\n1\n\u221an\n\f\f\f\n\f\f\u03bb\u03c4 \u2212\u03c4\u03c32\f\f \u2212\u03c4\nn\n\f\fn\u03c32 \u2212\u03bbn\n\f\f\n\f\f\f\n1\n\u221an max1\u2264i<\u03c4\n\f\f\u03bbi \u2212i\n\u03c4 \u03bb\u03c4\n\f\f +\n1\n\u221an max\u03c4<i\u2264n\n\f\fe\u03bbi \u2212n\u2212i\nn\u2212\u03c4 e\u03bb\u03c4\n\f\f\nP\n\u2212\u2212\u2212\u2192\nn\u2192\u221e\u221e,\nbecause of (A.10).\nFurthermore, again under HA,\nTn \u2265\n\u0000\u03bb\u03c4 \u2212\u03c4\nn\u03bbn\n\u00012\nP\u03c4\u22121\ni=1\n\u0000\u03bbi \u2212i\n\u03c4 \u03bb\u03c4\n\u00012 + Pn\ni=\u03c4+1\n\u0000e\u03bbi \u2212n\u2212i\nn\u2212\u03c4 e\u03bb\u03c4\n\u00012\n\u2265\n1\nn\n\u0010\f\f\u03bb\u03c4 \u2212\u03c4\u03c32\f\f \u2212\u03c4\nn\n\f\fn\u03c32 \u2212\u03bbn\n\f\f\n\u00112\n1\nn\nP\u03c4\u22121\ni=1\n\u0000\u03bbi \u2212i\n\u03c4 \u03bb\u03c4\n\u00012 + 1\nn\nPn\ni=\u03c4+1\n\u0000e\u03bbi \u2212n\u2212i\nn\u2212\u03c4 e\u03bb\u03c4\n\u00012\nP\n\u2212\u2212\u2212\u2192\nn\u2192\u221e\u221e,\nbecause of similar arguments as in the case of Sn.\nProof of Remark 4.3. It is su\ufb03cient to replace Theorem 1 by Dembo (1988) with Theo-\nrem 3.1 by Ma and Zarowski (1995) in the proof of Theorem 4.2.\nProof of Corollary 4.4. The estimator can be rewritten as\n\u02c6\u03c4n = argmax\n1\u2264k\u2264n\u22121\n1\nn\n\f\f\u03bbk \u2212k\nn\u03bbn\n\f\f + 1\nn\n\f\fe\u03bbk \u2212n\u2212k\nn e\u03bb0\n\f\f\nmax1\u2264i<k\n1\n\u221an\n\f\f\u03bbi \u2212i\nk\u03bbk\n\f\f + maxk<i\u2264n\n1\n\u221an\n\f\fe\u03bbi \u2212n\u2212i\nn\u2212k e\u03bbk\n\f\f.\n(A.11)\nWe will treat the numerator Nn(k) and the denominator Dn(k) of the above stated ratio\nseparately. Let us use notations from the previous proofs and let us recall Assumption C,\nD, and relations (A.6). If [nt] \u2264\u03c4, then\n1\nn\n\u02dcY \u22a4\n[nt] \u02dcY[nt] = ( \u00af\u03a3\u22a4\n\u0398,\u03b5 + \u00af\u03a3\u03b5\u03b2\u22a4)\u2206t( \u00af\u03a3\u0398,\u03b5 + \u03b2\u00af\u03a3\u03b5) + t\u03c32 + o(1)\nChangepoint in Linear Relations\n21\nalmost surely as n \u2192\u221e. Otherwise, if [nt] > \u03c4, then\n1\nn\n\u02dcY \u22a4\n[nt] \u02dcY[nt] = ( \u00af\u03a3\u22a4\n\u0398,\u03b5 + \u00af\u03a3\u03b5\u03b2\u22a4)\u2206\u03b6( \u00af\u03a3\u0398,\u03b5 + \u03b2\u00af\u03a3\u03b5) + t\u03c32\n+ ( \u00af\u03a3\u22a4\n\u0398,\u03b5 + \u00af\u03a3\u03b5(\u03b2 + \u03b4)\u22a4)(\u2206t \u2212\u2206\u03b6)( \u00af\u03a3\u0398,\u03b5 + (\u03b2 + \u03b4)\u00af\u03a3\u03b5) + o(1)\nalmost surely as n \u2192\u221e. In both cases, we have\n1\nn[ \u02dc\nX[nt], \u02dcY[nt]]\u22a4[ \u02dc\nX[nt], \u02dcY[nt]]\na.s.\n\u2212\u2212\u2212\u2192\nn\u2192\u221e\n\"\n\u03d1\u22a4\u2206t\u03d1 + t\u03c32Ip\n( \u00af\u03a3\u0398 + \u00af\u03a3\u0398,\u03b5\u03b2\u22a4)\u2206t( \u00af\u03a3\u0398,\u03b5 + \u03b2\u00af\u03a3\u03b5)\n( \u00af\u03a3\u22a4\n\u0398,\u03b5 + \u00af\u03a3\u03b5\u03b2\u22a4)\u2206t( \u00af\u03a3\u0398 + \u03b2\u00af\u03a3\u22a4\n\u0398,\u03b5)\n( \u00af\u03a3\u22a4\n\u0398,\u03b5 + \u00af\u03a3\u03b5\u03b2\u22a4)\u2206t( \u00af\u03a3\u0398,\u03b5 + \u03b2\u00af\u03a3\u03b5) + t\u03c32\n#\n= t\u03c32Ip+1 + \u03a3\u22121/2\n\u0014 Ip\n\u03b2\u22a4\n\u0015\n\u2206t[Ip, \u03b2]\u03a3\u22121/2.\nTherefore, for the Frobenius matrix norm \u2225\u00b7 \u2225F,\nlim\nn\u2192\u221e\n\f\f\f\f\f\u03bbmin\n\u0012 1\nn[ \u02dc\nX[nt], \u02dcY[nt]]\u22a4[ \u02dc\nX[nt], \u02dcY[nt]]\n\u0013\n\u2212[nt]\nn \u03bbmin\n\u0012 1\nn[ \u02dc\nX, \u02dcY ]\u22a4[ \u02dc\nX, \u02dcY ]\n\u0013 \f\f\f\f\f\n=: \u03bbdif(t) \u2264\n\r\r\r\r\u03a3\u22121/2\n\u0014 Ip\n\u03b2\u22a4\n\u0015\n\u2206\u2212t[Ip, \u03b2]\u03a3\u22121/2\n\r\r\r\r\nF\nuniformly in t almost surely, because |\u03bbmin(A) \u2212\u03bbmin(B)| \u2264\u2225A \u2212B\u2225F due to Gallo\n(1982b, proof of Lemma 2.3).\nFor k = \u03c4, Proposition 3.1 together with the continuous mapping theorem yield that\nthe denominator from (A.11)\nDn(\u03c4)\nD\n\u2212\u2212\u2212\u2192\nn\u2192\u221e\n\u03c62\u03c5\n1 + \u2225\u03b1\u22252\n2\n\u001a\nsup\n0\u2264t\u2264\u03b6\n\f\f\f\fW(t) \u2212t\n\u03b6 W(\u03b6)\n\f\f\f\f + sup\n\u03b6<t\u22641\n\f\f\f\f f\nW(t) \u22121 \u2212t\n1 \u2212\u03b6\nf\nW(\u03b6)\n\f\f\f\f\n\u001b\n=: W,\nwhere the limit W is strictly positive almost surely. We conclude that |Nn(\u03c4)/Dn(\u03c4)|\nconverge in distribution to the random variable \u03bbdif(\u03b6) + e\u03bbdif(\u03b6)/W such that e\u03bbdif(t) :=\nlimn\u2192\u221e|e\u03bb[nt] \u2212n\u2212[nt]\nn\ne\u03bb0|. For k = [nt] with t > \u03b6, we obtain\nmax\n1\u2264i<[nt]\n1\n\u221an\n\f\f\f\f\u03bbi \u2212\ni\n[nt]\u03bb[nt]\n\f\f\f\f +\nmax\n[nt]<i\u2264n\n1\n\u221an\n\f\f\f\fe\u03bbi \u2212\nn \u2212i\nn \u2212[nt]\ne\u03bb[nt]\n\f\f\f\f\n\u2265\n1\n\u221an\n\f\f\f\f\u03bb[n\u03b6] \u2212[n\u03b6]\n[nt] \u03bb[nt]\n\f\f\f\f \u2265\n1\n\u221an\n\f\f\f\f\f\n\f\f\f\f\u03bb[n\u03b6] \u2212[n\u03b6]\u03c32\n\f\f\f\f \u2212[n\u03b6]\n[nt]\n\f\f\f\f\u03bb[nt] \u2212[nt]\u03c32\n\f\f\f\f\n\f\f\f\f\f\n\u2248\n\f\f\f\f\fOP(1) \u2212\u221an\u03b6\nt\n\f\f\f\f\n\u03bb[nt]\nn\n\u2212t\u03c32\n\f\f\f\f\n\f\f\f\f\f\n\u2248\n\f\f\f\f\fOP(1) \u22122\u221an\u03b6\nt\n\f\f\u03b7(t)\u03ba(t) \u2212\u03d5(t)\u22a4\u03d5(t)\n\f\f\n\u03ba(t) + \u03b7(t) +\nq\n(\u03ba(t) \u2212\u03b7(t))2 + 4\u03d5(t)\u22a4\u03d5(t)\n\f\f\f\f\f\nP\n\u2212\u2212\u2212\u2192\nn\u2192\u221e\u221e\n22\nM. Pe\u02c7sta\naccording to the proof of Theorem 4.2 and assumption (4.10). Similar arguments can\nbe applied in the case t < \u03b6 and the convergence holds uniformly for all t outside any\n\u03f5-neighborhood of \u03b6. It follows that for an arbitrary \u03f5 > 0,\nmax\nk:|k\u2212\u03c4|\u2265n\u03f5\n|Nn(k)|\nDn(k) = OP\n\u0012\n1\n|\u03b7(t)\u03ba(t) \u2212\u03d5(t)\u22a4\u03d5(t)|\u221an\n\u0013\n.\nNow, let us chose a sequence dn \u21920 with dn|\u03b7(t)\u03ba(t) \u2212\u03d5(t)\u22a4\u03d5(t)|\u221an \u2192\u221e. Then, for\nany \u03f5 > 0,\nP[|\u02c6\u03c4/n \u2212\u03b6| > \u03f5] \u2264P[|Nn(\u03c4)/Dn(\u03c4)| < dn] + P\n\u0014\nmax\nk:|k\u2212\u03c4|\u2265n\u03f5 |Nn(k)/Dn(k)| > dn\n\u0015\nn\u2192\u221e\n\u2212\u2212\u2212\u21920.\nAcknowledgements\nThe research of Michal Pe\u02c7sta was supported by the Czech Science Foundation project\nGA\u02c7CR No. 18-01781Y.\nReferences\nAnderson, T. W. (1958). An Introduction to Multivariate Statistical Analysis. New York,\nNY: John Wiley & Sons.\nAue, A., L. Horv\u00b4ath, M. Hu\u02c7skov\u00b4a, and P. Kokoszka (2008). Testing for changes in poly-\nnomial regression. Bernoulli 14(3), 637\u2013660.\nAue, A., L. Horv\u00b4ath, and M. Hu\u02c7skov\u00b4a (2009). Extreme value theory for stochastic integrals\nof Legendre polynomials. J. Multivariate Anal. 100(5), 1029\u20131043.\nBillingsley, P. (1968). Convergence of Probability Measures (1st ed.). New York, NY: John\nWiley & Sons.\nBland, J. M. and D. G. Altman (1986).\nStatistical methods for assessing agreement\nbetween two methods of clinical measurement. Lancet 1(8476), 307\u2013310.\nBooth, J. G. and P. Hall (1993). Bootstrap con\ufb01dence regions for functional relationships\nin errors-in-variables models. Ann. Stat. 21(4), 1780\u20131791.\nBradley, R. C. (2005). Basic properties of strong mixing conditions. A survey and some\nopen questions. Probab. Surveys 2, 107\u2013144.\nBuonaccorsi, J. P. (2010). Measurement Error: Models, Methods, and Applications. Boca\nRaton, FL: Chapman and Hall/CRC.\nCarroll, R. J., K. Roeder, and L. Wasserman (1999). Flexible parametric measurement\nerror models. Biometrics 55(1), 44\u201354.\nCarroll, R. J., D. Ruppert, L. A. Stefanski, and C. M. Crainiceanu (2006). Measurement\nError in Nonlinear Models: A Modern Perspective (2nd ed.). Boca Raton, FL: Chapman\nand Hall/CRC.\nChang, Y.-P. and W.-T. Huang (1997). Inferences for the linear errors-in-variables with\nchangepoint models. J. Am. Stat. Assoc. 92(437), 171\u2013178.\nChangepoint in Linear Relations\n23\nChen, X. and Y. Wu (1989). Strong law for mixing sequence. Acta Math. Appl. Sin. 5(4),\n367\u2013371.\nCheng, C.-L. and J. Riu (2006). On estimating linear relationships when both variables\nare subject to heteroscedastic measurement errors. Technometrics 48(4), 511\u2013519.\nDembo, A. (1988). Bounds on the extreme eigenvalues of positive-de\ufb01nite Toeplitz ma-\ntrices. IEEE T. Inform. Theory 34(2), 352\u2013355.\nDong, C., C. Tan, B. Jin, and B. Miao (2016). Inference on the change point estimator\nof variance in measurement error models. Lith. Math. J. 56(4), 474\u2013491.\nFryzlewicz, P. (2014).\nWild binary segmentation for multiple change-point detection.\nAnn. Stat. 42(6), 2243\u20132281.\nFuller, W. A. (1987). Measurement Error Models. New York, NY: Wiley.\nGallo, P. P. (1982a). Consistency of regression estimates when some variables are subject\nto error. Commun. Stat. A-Theor. 11, 973\u2013983.\nGallo, P. P. (1982b). Properties of Estimators in Errors-in-Variables Models. Ph. D.\nthesis, University of North Carolina, Chapel Hill, NC.\nGleser, L. J. (1981). Estimation in a multivariate \u201cerrors in variables\u201d regression model:\nLarge sample results. Ann. Stat. 9, 24\u201344.\nGleser, L. J. and G. S. Watson (1973).\nEstimation of a linear transformation.\nBiometrika 60(3), 525\u2013534.\nGolub, G. H. and C. F. Van Loan (1980). An analysis of the total least squares problem.\nSIAM J. Numer. Anal. 17(6), 883\u2013893.\nG\u00a8ossl, C. and H. K\u00a8uchenho\ufb00(2001).\nBayesian analysis of logistic regression with an\nunknown change point and covariate measurement error. Statist. Med. 20(20), 3109\u2013\n3121.\nGuo, Y. and R. J. Little (2011).\nRegression analysis with covariates that have het-\neroscedastic measurement error. Statist. Med. 30, 2278\u20132294.\nHerrndorf, N. (1983). Stationary strongly mixing sequences not satisfying the central limit\ntheorem. Ann. Probab. 11(3), 809\u2013813.\nHorv\u00b4ath, L. (1995). Detecting changes in linear regressions. J. Am. Stat. Assoc. 26(3),\n189\u2013208.\nIbragimov, I. A. and Y. V. Linnik (1971).\nIndependent and Stationary Sequences of\nRandom Variables. The Netherlands: Wolters-Noordho\ufb00.\nKirch, C. (2006). Resampling Methods for the Change Analysis of Dependent Data. Ph.\nD. thesis, University of Cologne, Germany.\nKukush,\nA.,\nI. Markovsky,\nand S. Van Hu\ufb00el (2007).\nEstimation in a linear\nmultivariate measurement error model with a change point in the data.\nCom-\nput. Stat. Data An. 52(2), 1167\u20131182.\nLi, M., Y. Ma, and R. Li (2019). Semiparametric regression for measurement error model\nwith heteroscedastic error. J. Multivariate Anal. 171(2019), 320\u2013338.\n24\nM. Pe\u02c7sta\nLin, Z. and C. Lu (1997). Limit Theory for Mixing Dependent Random Variables. New\nYork, NY: Springer-Verlag.\nLord, F. M. (1973). Testing if two measuring procedures measure the same dimension.\nPsychol. Bull. 79(1), 71\u201372.\nMa, E. M. and C. J. Zarowski (1995). On lower bounds for the smallest eigenvalue of\na Hermitian positive-de\ufb01nite matrix. IEEE T. Inform. Theory 41(2), 539\u2013540.\nNussbaum, M. (1977). Asymptotic optimality of estimators of a linear functional relation\nif the ratio of the error variances is known. Statistics 8(2), 173\u2013198.\nPe\u02c7sta, M. (2011). Strongly consistent estimation in dependent errors-in-variables. Acta\nUniversitatis Carolinae. Mathematica et Physica 52(1), 69\u201379.\nPe\u02c7sta, M. (2013a).\nAsymptotics for weakly dependent errors-in-variables.\nKyber-\nnetika 49(5), 692\u2013704.\nPe\u02c7sta, M. (2013b). Total least squares and bootstrapping with application in calibration.\nStatistics 47(5), 966\u2013991.\nPe\u02c7sta, M. (2016). Unitarily invariant errors-in-variables estimation. Stat. Pap. 57(4),\n1041\u20131057.\nPe\u02c7sta, M. (2017). Block bootstrap for dependent errors-in-variables. Commun. Stat. A-\nTheor. 46(4), 1871\u20131897.\nPe\u02c7sta, M. and M. Wendler (2019).\nNuisance-parameter-free changepoint detection in\nnon-stationary series. TEST, doi.org/10.1007/s11749\u2013019\u201300659\u20131, Online First.\nRosenblatt, M. (1971). Markov Processes: Structure and Asymptotic Behavior. Berlin:\nSpringer-Verlag.\nRyu, S.-J. and H.-K. Lee (2014). Estimation of linear transformation by analyzing the\nperiodicity of interpolation. Pattern Recogn. Lett. 36(2014), 89\u201399.\nShao, X. and X. Zhang (2010). Testing for change points in time series. J. Am. Stat. As-\nsoc. 105(491), 1228\u20131240.\nStaudenmayer, J. and D. Spiegelman (2002). Segmented regression in the presence of\ncovariate measurement error in main study/validation study designs. Biometrics 58(4),\n871\u2013877.\nStefanski, L. A. (2000). Measurement error models. J. Am. Stat. Assoc. 95(452), 1353\u2013\n1358.\nVan Hu\ufb00el, S. and J. Vandewalle (1989). Analysis and properties of the generalized total\nleast squares problem AX \u2248B when some or all columns in A are subject to error.\nSIAM J. Matrix Anal. Appl. 10(3), 294\u2013315.\nVan Hu\ufb00el, S. and J. Vandewalle (1991). The Total Least Squares Problem: Computational\nAspects and Analysis. Philadelphia, PA: SIAM.\nWright, D. J. (1986).\nForecasting data published at irregular time intervals using an\nextension of holt\u2019s method. Manage. Sci. 32(4), 499\u2013510.\nYi, G. Y. (2017). Statistical Analysis with Measurement Error or Misclassi\ufb01cation. New\nYork, NY: Spriger.\n"}