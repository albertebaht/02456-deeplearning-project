{"text": "arXiv:1407.1372v2  [math.NA]  15 Jun 2015\nA NEW ERROR IN VARIABLES MODEL FOR SOLVING POSITIVE\nDEFINITE LINEAR SYSTEM USING ORTHOGONAL MATRIX\nDECOMPOSITIONS\nNEGIN BAGHERPOUR \u2217AND NEZAM MAHDAVI-AMIRI \u2020\nAbstract. The need to estimate a positive de\ufb01nite solution to an overdetermined linear system\nof equations with multiple right hand side vectors arises in several process control contexts. The\ncoe\ufb03cient and the right hand side matrices are respectively named data and target matrices.\nA\nnumber of optimization methods were proposed for solving such problems, in which the data matrix\nis unrealistically assumed to be error free.\nHere, considering error in measured data and target\nmatrices, we present an approach to solve a positive de\ufb01nite constrained linear system of equations\nbased on the use of a newly de\ufb01ned error function.\nTo minimize the de\ufb01ned error function, we\nderive necessary and su\ufb03cient optimality conditions and outline a direct algorithm to compute the\nsolution. We provide a comparison of our proposed approach and two existing methods, the interior\npoint method and a method based on quadratic programming. Two important characteristics of our\nproposed method as compared to the existing methods are computing the solution directly and con-\nsidering error both in data and target matrices. Moreover, numerical test results show that the new\napproach leads to smaller standard deviations of error entries and smaller e\ufb00ective rank as desired by\ncontrol problems. Furthermore, in a comparative study, using the Dolan-Mor\u00b4e performance pro\ufb01les,\nwe show the approach to be more e\ufb03cient.\nKey words. Error in variables models, positive de\ufb01niteness constraints, overdetermined linear\nsystem of equations, multiple right hand side vectors\nAMS subject classi\ufb01cations. 65F05, 65F20, 49M05\n1. Introduction. Computing a symmetric positive de\ufb01nite solution of an overde-\ntermined linear system of equations arises in a number of physical problems such as\nestimating the mass inertia matrix in the design of controllers for solid structures and\nrobots; see, e.g., [9], [17], [14]. Modeling a deformable structure also leads to such\na mathematical problem; e.g., see [25]. The problem turns into \ufb01nding an optimal\nsolution of the system\nDX \u2243T,\n(1.1)\nwhere D, T \u2208Rm\u00d7n, with m \u2265n, are given and a symmetric positive de\ufb01nite matrix\nX \u2208Rn\u00d7n is to be computed as a solution of (1.1). In some special applications,\nthe data matrix D has a simple structure, which may be taken into consideration\nfor e\ufb03ciently organized computations. Estimation of the covariance matrix and com-\nputation of the correlation matrix in \ufb01nance are two such examples where the data\nmatrices are respectively block diagonal and the identity matrix; e.g., see [31].\nA number of least squares formulations have been proposed for physical problems,\nwhich may be classi\ufb01ed as ordinary and error in variables (EIV) models.\nAlso, single or multiple right hand side least squares may arise. With a single right\nhand side, we have an overdetermined linear system of equations Dx \u2243t, where\nD \u2208Rm\u00d7n, t \u2208Rm\u00d71, with m \u2265n, are known and the vector x \u2208Rn\u00d71 is to be\ncomputed. In an ordinary least squares formulation, the error is only attributed to\n\u2217Faculty of Mathematical Sciences, Sharif University of Technology, Tehran, Iran, (nbagher-\npour@mehr.sharif.ir).\n\u2020Faculty\nof\nMathematical\nSciences,\nSharif\nUniversity\nof\nTechnology,\nTehran,\nIran,\n(nezamm@sharif.ir).\n1\nt. So, to minimize the corresponding error, the following mathematical problem is\ndevised:\nmin \u2225\u2206t\u2225\ns.t. Dx = t + \u2206t.\n(1.2)\nThere are a number of methods for solving (1.2), identi\ufb01ed as direct and iterative\nmethods. A well known direct method is based on using the QR factorization of the\nmatrix D [27]. An iterative method has also been introduced in [7] for solving (1.2)\nusing the GMRES algorithm.\nIn an EIV model, however, errors in both D and t are considered; e.g., see [3]. Total\nleast squares formulation is a well-known EIV model, where the goal is to solve the\nfollowing mathematical problem (e.g., see [6] and [16]):\nmin \u2225[\u2206D, \u2206t]\u2225\ns.t. (D + \u2206D)x = t + \u2206t.\n(1.3)\nWe note that \u2225\u00b7 \u2225in (1.2) and (1.3) respectively denote the vector 2-norm and\nthe matrix Frobenius norm. Both direct [24] and iterative [12] methods have been\npresented for solving (1.3). Moreover, the scaled total least squares formulation has\nbeen considered to unify both ordinary and total leats squares formulation; e.g., see\n[24]. In a scaled toal least squares formulation, the mathematical problem\nmin \u2225[\u2206D, \u2206t]\u2225\ns.t.(D + \u2206D)x = \u03bbt + \u2206t\n(1.4)\nis to be solved for an arbitrary scalar \u03bb. Zhou [19] has studied the e\ufb00ect of perturba-\ntion and gave an error analysis of such a formulation.\nA least squares problem with multiple right hand side vectors can also be for-\nmulated as an overdetermined system of equations DX \u2243T , where D \u2208Rm\u00d7n,\nT \u2208Rm\u00d7k, with m \u2265n, are given and the matrix X \u2208Rn\u00d7k is to be computed. With\nordinary and total least squares formulations, the respective mathematical problems\nare:\nmin \u2225\u2206T \u2225\ns.t. DX = T + \u2206T\nX \u2208Rn\u00d7k\n(1.5)\nand\nmin \u2225[\u2206D, \u2206T ]\u2225\ns.t. (D + \u2206D)X = T + \u2206T\nX \u2208Rn\u00d7k.\n(1.6)\nCommon methods for solving (1.5) are similar to the ones for (1.2); see, e.g., [7],\n[27]. Solving (1.6) is possible by using the method described in [8], based on the SVD\nfactorization of the matrix [D, T ]. Connections between ordinary least squares and\ntotal least squares formulations have been discussed in [11].\n2\nHere, we consider a newly de\ufb01ned EIV model for solving a positive de\ufb01nite linear\nproblem. Our goal is to compute a symmetric positive de\ufb01nite solution X \u2208Rn\u00d7n\nto the overdetermined system of equations DX \u2243T , where both matrices D and\nT may contain errors. We refer to this problem as positive de\ufb01nite linear system of\nequations later. No EIV model, even the well-known total least squares formulation, is\nconsidered for solving the positive de\ufb01nite linear system of equations in the literature.\nSeveral approaches have been proposed for this problem, commonly considering the\nordinary least squares formulation and minimizing the error \u2225\u2206T \u2225F over all n \u00d7 n\nsymmetric positive de\ufb01nite matrices, where \u2225.\u2225F is the Frobenious norm; see e.g.\n[10, 23]. Larson [13] discussed a method for solving a positive de\ufb01nite least squares\nproblem considering the corresponding normal system of equations. He considered\nboth symmetric and positive de\ufb01nite least squares problems. Krislock [25] proposed\nan interior point method for solving a variety of least squares problems with positive\nsemi-de\ufb01nite constraints.\nWoodgate [18] described a new algorithm for solving a\nsimilar problem in which a symmetric positive semi-de\ufb01nite matrix P is computed to\nminimize \u2225F \u2212PG\u2225, with known F and G. Hu [10] presented a quadratic programming\napproach to handle the positive de\ufb01nite constraint. In her method, the upper and\nlower bounds for the entries of the target matrix can be given as extra constraints. In\nreal measurements, however, both the data and target matrices may contain errors;\nhence, the total least squares formulation appears to be appropriate.\nThe rest of our work is organized as follows. In Section 2, we de\ufb01ne a new error\nfunction and discuss some of its characteristics. A method for solving the resulting\noptimization problem with the assumption that D has full column rank is presented\nin Section 3. In Section 4, we generalize the method to the case of data matrix having\nan arbitrary rank.\nIn Section 5, a detailed discussion is made on computational\ncomplexity of both methods. Computational results and comparisons with available\nmethods are given in Section 6. Section 7 gives our concluding remarks.\n2. Problem Formulation. Consider a single equation ax \u2243b, where a, b \u2208Rn\nand x \u2208R+. Errors in the ith entry of b and a are respectively equal to | aix \u2212bi |\nand | ai \u2212bi\nx |; e.g., see [24].\nIn [24], Pn\ni=1Li was considered as a value to represent errors in both a and b. As\nshown in Figure ??, Li is the height of the triangle ABC which turns to be equal to\nLi = |bi\u2212aix|\n\u221a\n1+x2 . Here, to represent the errors in both a and b, we de\ufb01ne the area error\nto be\nXn\ni=1|bi \u2212aix||ai \u2212bi\nx |,\n(2.1)\nwhich is equal to\nXn\ni=1(bi \u2212aix)(ai \u2212bi\nx ),\nfor x \u2208R+.\nConsidering the problem of \ufb01nding a symmetric and positive de\ufb01nite solution to\nthe overdetermined system of linear equations DX \u2243T , in which both D and T\ninclude error, the values DX and T X\u22121 are predicted values for T and D from the\nmodel DX \u2243T ; hence, vectors \u2206T j = (DX \u2212T )j and \u2206Dj = (D \u2212T X\u22121)j are\nthe entries of errors in the jth column of T and D, respectively. Extending the error\n3\nformulation (2.1), the value\nE =\nXn\nj=1(DXj \u2212Tj)T (Dj \u2212(T X\u22121)j\n(2.2)\nseems to be an appropriate measure of error. We also have\nE =\nXn\nj=1\nXm\ni=1(DX \u2212T )ij(D \u2212T X\u22121)ij = tr((DX \u2212T )T (D \u2212T X\u22121)),\n(2.3)\nwith tr(.) standing for trace of a matrix. Therefore, the problem can be formulated\nas\nmin\nX\u227b0 tr((DX \u2212T )T (D \u2212T X\u22121)),\n(2.4)\nwhere X is symmetric and by X \u227b0, we mean X is positive de\ufb01nite. Problem (9)\nposes a newly de\ufb01ned EIV model for solving the positive de\ufb01nite linear system of\nequations.\nIn Lemma 2.2, we represent an equivalent formulation for the error, E. First, consider\nto a well-known property of positive de\ufb01nite matrices. Note A matrix X \u2208Rn\u00d7n is\npositive de\ufb01nite if and only if there exists a nonsingular matrix Y \u2208Rn\u00d7n such that\nX = Y Y T .\nThe following results about the trace operator are also well-known; e.g., see [21].\nLemma 2.1. For an nonsingulartible matrix P \u2208Rn\u00d7n and arbitrary matrices\nY \u2208Rn\u00d7n, A \u2208Rm\u00d7n and B \u2208Rn\u00d7m we have\n(1)\ntr(Y ) = tr(P \u22121Y P).\n(2)\ntr(AB) = tr(BA).\nLemma 2.2. The error E, de\ufb01ned by (2.3), is equal to\nE = \u2225DY \u2212T Y \u2212T \u22252\nF\n(2.5)\nwhere X = Y Y T and \u2225.\u2225F denotes the Frobenius norm of a matrix.\nProof. Substituting X = Y Y T in (2.3) and using Lemma 2.1, we get\nE = tr((DX \u2212T )T(D \u2212T X\u22121)) = tr((DX \u2212T )T (DX \u2212T )X\u22121)\n= tr((DX \u2212T )T(DX \u2212T )Y \u2212T Y \u22121) = tr(Y \u22121(DX \u2212T )T (DX \u2212T )Y \u2212T )\n= tr((DXY \u2212T \u2212T Y \u2212T )\nT (DXY \u2212T \u2212T Y \u2212T ))\n= tr((DY \u2212T Y \u2212T )\nT (DY \u2212T Y \u2212T ))\n= \u2225DY \u2212T Y \u2212T \u22252\nF .\nConsidering this new formulation for E, it can be concluded that by use of our newly\nde\ufb01ned EIV model, computing a symmetric and positive de\ufb01nite solution to the over-\ndetermined system of equations DX \u2243T is equivalent to computing a nonsingular\nmatrix Y \u2208Rn\u00d7n to be the solution of\nmin \u2225DY \u2212T Y \u2212T \u22252\nF ,\n4\nand letting X = Y Y T . A similar result is obtained by considering the over-determined\nsystem DX \u2243T with X = Y Y T and multiplying both sides by Y \u2212T . We have,\nDY Y T \u2243T,\nor equivalently,\nDY \u2243T Y \u2212T .\n(2.6)\nNow, to assign a solution to (2.6), it makes sense to minimize the norm of residual.\nThus, to compute X = Y Y T , it is su\ufb03cient to let Y to be the solution of\nmin \u2225DY \u2212T Y \u2212T \u22252\nF .\nNote An appropriate characteristic of the error formulation proposed by (2.3) is\nthat for a symmetric and positive de\ufb01nite matrix X, the value of E is nonnegative\nand it is equal to zero if and only if DX = T .\n3. Mathematical Solution: Full Rank Data Matrix. Here, we are to de-\nvelop an algorithm for solving (2.4) with the assumption that D has full column rank.\nUsing Lemma 2.1, with X being symmetric, we have\ntr((DX \u2212T )T (D \u2212T X\u22121)) = tr(DT DX + X\u22121T T T ) \u22122 tr(T TD).\nSo, (2.4) can be written as\nmin tr(AX + X\u22121B),\n(3.1)\nwhere A = DT D and B = T T T and the symmetric and positive de\ufb01nite matrix X is\nto be computed.\nCorollary 3.1. For each X\u2217satisfying the \ufb01rst order necessary conditions of\n(3.1), the su\ufb03cient optimality conditions described in Theorem ?? are satis\ufb01ed and\nsince \u03a6(X) = tr(AX + X\u22121B) is convex on the cone of symmetric positive de\ufb01nite\nmatrices, we can con\ufb01rm that the symmetric positive de\ufb01nite matrix satisfying the\nKKT necessary conditions mentioned in Theorem ?? is the unique global solution of\n(3.1).\nComputing the positive de\ufb01nite matrix satisfying KKT conditions. As\nmentioned in Theorem ??, the KKT conditions lead to the nonlinear matrix equation\nXAX = B.\n(3.2)\nNote that (3.2) is an special case of the continuous time Riccati equation (CARE),\n[22]\nAT XE + ET XA \u2212(ET XB + S)R\u22121(BT XE + ST ) + Q = 0,\n(3.3)\nwith R = 0, E = A\n2 and Q = \u2212B. There is a MATLAB routine to solve CARE for\narbitrary values of A, E, B, S, R and Q. To use the routine, it is su\ufb03cient to type\nthe command\n5\nX=care(A,B,Q,R,S,E),\nfor the input arguments as in (3.3). Higham [22] developed an e\ufb00ective method for\ncomputing the positive de\ufb01nite solution to this special CARE when A and B are sym-\nmetric and positive de\ufb01nite using well-known decompositions. Lancaster and Rodman\n([28]) also discussed solving di\ufb00erent types of algebraic Riccati equations. Moreover,\nthey derived a perturbation analysis for these matrix equations.\nNote (QR decomposition) The QR decomposition [27] of a matrix A \u2208Rm\u00d7n\nwith m \u2265n, is a decomposition of the form A = QR, where R is an m \u00d7 n upper\ntriangular matrix and Q satis\ufb01es QQT = QT Q = I. Moreover, if A has full column\nrank, then R also has full column rank.\nNote (Cholesky decomposition) A Cholesky decomposition [27] of a symmetric\npositive de\ufb01nite matrix A \u2208Rn\u00d7n is a decomposition of the form A = RT R, where R,\nknown as the Cholesky factor of A, is an n \u00d7 n nonsingular upper triangular matrix.\nNote (Spectral decomposition) [27] All eigenvalues of a symmetric matrix, A \u2208\nRn\u00d7n, are real and there exists an orthonormal matrix with columns representing the\ncorresponding eigenvectors. Thus, there exist an orthonormal matrix U with columns\nequal to the eigenvectors of A and a diagonal matrix D containing the eigenvalues\nsuch that A = UDU T. Also, if A is positive de\ufb01nite, then all of its eigenvalues are\npositive, and so we can set D = S2. Thus, spectral decomposition for a symmet-\nric positive de\ufb01nite matrix A is a decomposition of the form A = US2U T , where\nU T U = UU T = I and S is a diagonal matrix.\nTheorem 3.2. [22] Assume D, T \u2208Rm\u00d7n with m \u2265n are known and rank(D) =\nrank(T ) = n.\nLet D = QR be the QR factorization of D.\nLet A = DT D and\nB = T T T . De\ufb01ne the matrix \u02dcQ = RBRT and compute its spectral decomposition,\nthat is, \u02dcQ = RBRT = U \u02dcS2U T . Then, (3.1) has a unique solution, given by\nX\u2217= R\u22121U \u02dcSU T R\u2212T .\nProof. Based on Theorem ?? and the afterwards discussion, it is su\ufb03cient to show\nthat X\u2217satis\ufb01es the necessary optimality conditions, X\u2217AX\u2217= B. Note that from\nD = QR, we have\nA = DT D = RT QT QR = RT R.\nSubstituting X\u2217, we have\nX\u2217AX\u2217= R\u22121U \u02dcSU T R\u2212T RT RR\u22121U \u02dcSU T R\u2212T\n= R\u22121U \u02dcS2U T R\u2212T = R\u22121RBRT R\u2212T = B.\nNote To compute R, it is also possible to \ufb01rst compute A = DT D and then calculate\nthe Cholesky decomposition for A. However, because of more stability, in Theorem\n3.2 the QR decomposition of D is used.\n6\nWe are now ready to outline the steps of our proposed algorithm.\nSolving the EIV model for positive de\ufb01nite linear system using QR\ndecomposition.\nCompute the QR decomposition for D and let D = QR.\nLet \u02dcQ = RBRT , where B = T TT and compute the spectral decomposition of \u02dcQ, that\nis, \u02dcQ = U \u02dcS2U T .\nSet X\u2217= R\u22121U \u02dcSU T R\u2212T .\nSet E = tr((DX\u2217\u2212T )T (D \u2212T X\u2217\u22121)).\nNote that Algorithm 1 computes the solution of (2.4) directly.\nThe following theorem shows that by use of spectral decomposition of A a method\nsimilar to the one introduced in [22] is in hand for solving the continuous time Riccati\nequation.\nTheorem 3.3. Let A = DT D and B = T T T with D, T \u2208Rm\u00d7n, m \u2265n and\nrank(D) = n. Let the spectral decomposition of A be A = US2U T . De\ufb01ne the matrix\n\u02dcQ = SU TBUS and compute its spectral decomposition, \u02dcQ = SU T BUS = \u00afU \u00afS2 \u00afU T .\nThen, the unique minimizer of (3.1) is\nX\u2217= US\u22121 \u00afU \u00afS \u00afU T S\u22121U T .\nProof.\nSimilar to the proof of Theorem 3.2, it is su\ufb03cient to show that the\nmentioned X\u2217satis\ufb01es X\u2217AX\u2217= B. Substituting X\u2217, we have\nX\u2217AX\u2217\n= US\u22121 \u00afU \u00afS \u00afU T S\u22121U T US2U T US\u22121 \u00afU \u00afS \u00afU T S\u22121U T\n= US\u22121 \u00afU \u00afS2 \u00afU T S\u22121U T = US\u22121SU T BUSS\u22121U T = B.\nNext, based on Theorem 3.3, we outline an algorithm for solving (2.4).\nSolving the EIV model for positive de\ufb01nite linear system using spectral\ndecomposition.\nLet A = DT D and compute its spectral decomposition: A = US2U T .\nLet \u02dcQ = SU T BUS, where B = T TT and compute the spectral decomposition of \u02dcQ,\nthat is, \u02dcQ = \u02dcU \u02dcS2 \u02dcU T .\nSet X\u2217= US\u22121 \u02dcU \u02dcS \u02dcU T S\u22121U T .\nSet E = tr((DX\u2217\u2212T )T (D \u2212T X\u2217\u22121)).\nIn Section 4 we generalize our proposed method for solving positive de\ufb01nite linear\nsystem of equations when the data matrix is rank de\ufb01cient.\n7\n4. Mathematical Solution: Rank De\ufb01cient Data Matrix. Since the data\nmatrix D is usually produced from experimental measurements, we may have rank(D) <\nn. Here, we are to generalize Algorithm 1 for solving (2.4), assuming that rank(D) =\nr < n. In Section 4.1 we outline two algorithms to compute the general solution of\n(2.4). It will be shown that, in general, (2.4) may not have a unique solution. Hence,\nin section 4.2 we discuss how to \ufb01nd a particular solution of (2.4) having desirable\ncharacteristics for control problems.\n4.1. General solution. Based on theorems ?? and ??, a symmetric positive\nde\ufb01nite matrix X\u2217is a solution of (2.4) if and only if\nX\u2217AX\u2217= B.\n(4.1)\nTherefore, in the following, we discuss how to \ufb01nd a symmetric positive de\ufb01nite matrix\nX\u2217satisfying (4.1).\nFirst we note that in case D and T are rank de\ufb01cient, there might be no solution for\n(4.1), and if there is any, it is not necessarily a unique solution; see, e.g., [22]. Higham\n[22] considered to X = B\n1\n2 (B\n1\n2 AB\n1\n2 )\n\u22121\n2 B\n1\n2 as a solution of (4.1), which is symmetric\nand positive semide\ufb01nite. However, we are interested in \ufb01nding a symmetric positive\nde\ufb01nite solution to (4.1). Hence, in the following, \ufb01rst the necessary and su\ufb03cient\nconditions on A and B to guarantee the existence of positive de\ufb01nite solution to (4.1)\nare discussed. We then outline two algrithms to compute such a solution.\nLet the spectral decomposition of A be A = U\n\u0012\nS2\n0\n0\n0\n\u0013\nU T , where S2 \u2208Rr\u00d7r\nis a diagonal matrix having the positive eigenvalues of A as its diagonal entries. Sub-\nstituting the decomposition in (4.1), we get\nX\u2217U\n\u0012 S2\n0\n0\n0\n\u0013\nU T X\u2217= B.\n(4.2)\nSince U is orthonormal, (4.2) can be written as\nU T X\u2217U\n\u0012 S2\n0\n0\n0\n\u0013\nU T X\u2217U = U T BU.\nThen, letting \u02dcX = U T XU and \u02dcB = U T BU, we have\n\u02dcX\n\u0012 S2\n0\n0\n0\n\u0013\n\u02dcX = \u02dcB.\n(4.3)\nThus, the matrix X = U \u02dcXU T is a solution of (2.4) if and only if \u02dcX is symmetric posi-\ntive de\ufb01nite and satis\ufb01es (4.3). Substituting the block form \u02dcX =\n\u0012\n\u02dcXrr\n\u02dcXr,n\u2212r\n\u02dcXn\u2212r,r\n\u02dcXn\u2212r,n\u2212r\n\u0013\n,\nwhere \u02dcXrr \u2208Rr\u00d7r, \u02dcXr,n\u2212r = \u02dcXT\nn\u2212r,r \u2208Rr\u00d7(n\u2212r) and \u02dcXn\u2212r,n\u2212r \u2208R(n\u2212r)\u00d7(n\u2212r), in\n(4.3) leads to\n\u0012\n\u02dcXrrS2 \u02dcXrr\n\u02dcXrrS2 \u02dcXr,n\u2212r\n\u02dcXn\u2212r,rS2 \u02dcXrr\n\u02dcXn\u2212r,rS2 \u02dcXr,n\u2212r\n\u0013\n= \u02dcB =\n\u0012\n\u02dcBrr\n\u02dcBr,n\u2212r\n\u02dcBn\u2212r,r\n\u02dcBn\u2212r,n\u2212r\n\u0013\n,\n(4.4)\nwhich is satis\ufb01ed if and only if\n\u02dcXrrS2 \u02dcXrr = \u02dcBrr,\n(4.5a)\n8\n\u02dcXrrS2 \u02dcXr,n\u2212r = \u02dcBr,n\u2212r,\n(4.5b)\n\u02dcXn\u2212r,rS2 \u02dcXr,n\u2212r = \u02dcBn\u2212r,n\u2212r.\n(4.5c)\nBefore discussing how to compute \u02dcX, we show that if (4.1) has a symmetric and\npositive de\ufb01nite solution, then \u02dcBrr must be nonsingular. The matrix \u02dcXrr as a main\nminor of the positive de\ufb01nite matrix \u02dcX is nonsingular.\nS is also supposed to be\nnonsingular. Hence, it can be concluded from (4.5a) that \u02dcBrr is nonsingular.\nLet \u00afD = S and suppose \u00afT satis\ufb01es \u00afT T \u00afT = \u02dcBrr. Consider problem (2.4) corre-\nsponding to the data and target matrices \u00afD and \u00afT as follows:\nmin\n\u00af\nX\u227b0 tr(( \u00afD \u00afX \u2212\u00afT)T ( \u00afD \u2212\u00afT \u00afX\u22121)).\n(4.6)\nWe know from theorems\n?? and\n?? that the necessary and su\ufb03cient optimality\nconditions for the unique solution of problem (4.6) implies (4.5a). Thus, \u02dcXrr can be\ncomputed using Algorithm 1 for the input arguments \u00afD and \u00afT. Substituting the\ncomputed \u02dcXrr in (4.5b), the linear system of equations\n\u02dcXrrS2 \u02dcXr,n\u2212r = \u02dcBr,n\u2212r\n(4.7)\narises, where \u02dcXrr, S2 \u2208Rr\u00d7r are known and \u02dcXr,n\u2212r \u2208Rr\u00d7(n\u2212r) is to be computed.\nSince \u02dcXrr is positive de\ufb01nite and S2 is nonsingular, the coe\ufb03cient matrix of the linear\nsystem (4.7) is nonsingular and \u02dcXr,n\u2212r can be uniquely computed.\nIt is clear that since \u02dcX is symmetric, \u02dcXn\u2212r,r is the same as \u02dcXT\nr,n\u2212r. Now, we check\nwhether the computed \u02dcXn\u2212r,r and \u02dcXr,n\u2212r satisfy (4.5c). Inconsistency of (4.6) means\nthat there is no symmetric positive de\ufb01nite matrix satisfying (4.5a)-(4.5c), and if so,\n(2.4) has no solution. Thus, in solving an speci\ufb01c positive de\ufb01nite system with rank\nde\ufb01cient data and target matrices using the presented EIV model, a straightforward\nmethod to investigate the existence of solution is to check whether (4.5c) holds for\nthe given data and target matrices. On the other hand, for numerical results, it is\nnecessary to generate meaningful test problems. Hence, in the following two lemmas,\nwe investigate the necessary and su\ufb03cient conditions for satisfaction of (4.5c).\nLemma 4.1. Let the spectral decomposition of A be determined as\nA = U\n\u0012\nS2\n0\n0\n0\n\u0013\nU T\nwhere S2 \u2208Rr\u00d7r and rank(A) = rank(B) = r. The necessary and su\ufb03cient condition\nfor satisfaction of (4.5c) is\nBUr(Ur\nT BUr)\n\u22121Ur\nT B \u2212B \u2208Null(Un\u2212r\nT ).\nProof.\nFrom (4.5a), we have\n\u02dcX\u22121\nrr S\u22122 \u02dcX\u22121\nrr = \u02dcB\u22121\nrr ,\n(4.8)\nand from (4.5c), we get\n\u02dcXr,n\u2212r = S\u22122 \u02dcX\u22121\nrr \u02dcBr,n\u2212r,\n(4.9)\n9\n\u02dcXn\u2212r,r = \u02dcBn\u2212r,r \u02dcX\u22121\nrr S\u22122.\n(4.10)\nManipulating (4.5c) with (4.8) and (4.9), we get\n\u02dcBn\u2212r,r \u02dcB\u22121\nrr \u02dcBr,n\u2212r = \u02dcBn\u2212r,n\u2212r.\n(4.11)\nConsidering the block form U =\n\u0000Ur\nUn\u2212r\n\u0001\n, where Ur \u2208Rn\u00d7r and Un\u2212r \u2208\nRn\u00d7(n\u2212r), we have\n\u02dcB = U T BU =\n\u0012\nUr\nT\nUn\u2212r\nT\n\u0013\nB\n\u0000 Ur\nUn\u2212r\n\u0001\n=\n\u0012\nUr\nT BUr\nUr\nT BUn\u2212r\nUn\u2212r\nT BUr\nUn\u2212r\nT BUn\u2212r\n\u0013\n.\nRewriting (4.11) results in\nUn\u2212r\nT BUr(Ur\nT BUr)\n\u22121Ur\nT BUn\u2212r = Un\u2212r\nT BUn\u2212r,\n(4.12)\nwhich is equivalent to (e.g., see [20])\nBUr(Ur\nT BUr)\n\u22121Ur\nT B = B + Z,\n(4.13)\nwhere Z \u2208Rn\u00d7n is in the null space of Un\u2212r\nT . Thus, (4.1) has a positive de\ufb01nite\nsolution if and only if\nBUr(Ur\nT BUr)\n\u22121Ur\nT B \u2212B \u2208Null(Un\u2212r\nT ).\n(4.14)\nNote For real problems with arbitrary values of D and T , the necessary and\nsu\ufb03cient condition given in Lemma 4.1 may not be satis\ufb01ed, in general. Hence, we\nare to propose a threshold to determine if\nF = Un\u2212r\nT \u0010\nBUr(Ur\nT BUr)\n\u22121Ur\nT B \u2212B\n\u0011\n(4.15)\nis close enough to zero. In the following, we show that if \u2225F\u2225< \u03b4, for a su\ufb03ciently\nsmall scalar \u03b4, then Xr(n\u2212r) computed from (4.5b) is a proper approximation for the\nsolution of (4.5c). Substituting F in (4.12), we have\n\u02dcBn\u2212r,r \u02dcB\u22121\nrr \u02dcBr,n\u2212r \u2212\u02dcBn\u2212r,n\u2212r = FUn\u2212r,\n(4.16)\nand\n\u02dcXn\u2212r,rS2 \u02dcXr,n\u2212r \u2212\u02dcBn\u2212r,n\u2212r = FUn\u2212r.\n(4.17)\nLet X\u2217satisfy (4.5c), that is,\nX\u2217n\u2212r,rS2X\u2217r,n\u2212r \u2212\u02dcBn\u2212r,n\u2212r = 0.\n(4.18)\nThen, we have\n\u02dcXn\u2212r,rS2 \u02dcXr,n\u2212r \u2212X\u2217n\u2212r,rS2X\u2217r,n\u2212r = FUn\u2212r.\n(4.19)\n10\nLetting \u02dcY = S \u02dcXr,n\u2212r and Y \u2217= SX\u2217r,n\u2212r, (4.19), we get\n\u02dcY T \u02dcY \u2212Y \u2217T Y \u2217= FUn\u2212r\n(4.20)\nand\n\u02dcyT\ni \u02dcyj \u2212y\u2217\ni\nT y\u2217\nj = (FUn\u2212r)ij,\n(4.21)\nwhere \u02dcyi and y\u2217\ni are the ith column of \u02dcY and Y \u2217respectively. Now, since the 2 norm\nof each column of Un\u2212r is equal to one, every entry of Un\u2212r is less than or equal to\none. Moreover, under the assumption \u2225F\u2225< \u03b4, none of the entries of F are greater\nthan \u03b4. Hence, we have\n|(FUn\u2212r)ij| = |f T\ni uj| \u2264|fi1 + \u00b7 \u00b7 \u00b7 + fi(n\u2212r)| < (n \u2212r)\u03b4,\n(4.22)\nwhere f T\ni and uj are the ith row of F and the jth column of Un\u2212r respectively. Now,\n(4.21) together with (4.22) gives\n| \u02dcyi\nT \u02dcyj \u2212y\u2217\ni\nT y\u2217\nj | < (n \u2212r)\u03b4.\n(4.23)\nHence, there is a constant cij such that\n|\u02dcyij \u2212y\u2217\nij| < cij,\n(4.24)\nwhere \u02dcyij and y\u2217\nij are the (i, j)th entry of \u02dcY and Y \u2217respectively.\nLetting S =\ndiag(s1, \u00b7 \u00b7 \u00b7 , sr), from (4.24) we get\n|si||( \u02dcXn\u2212r,r)ij \u2212(X\u2217\nn\u2212r,r)ij| \u2264cij,\n(4.25)\nfor i = 1, \u00b7 \u00b7 \u00b7 , r and j = 1, \u00b7 \u00b7 \u00b7 , n \u2212r and\n\u2225\u02dcXr,n\u2212r \u2212X\u2217r,n\u2212r\u2225\u2264C.\nHence, assuming\n\u02dcXr,r = X\u2217r,r,\nwe have \u2225\u02dcX \u2212X\u2217\u2225< \u03b1 which means that if FUn\u2212r is close enough to zero, then the\ncomputed solution from the approximate satisfaction of (4.5c) would be close enough\nto the exact solution.\nIn the following lemma, we give a su\ufb03cient condition which guarantees the existence\nof a solution for (4.1). We later use this result to generate consistent test problems\nin Section 6.\nLemma 4.2. Let the spectral decomposition of B be B = V\n\u0012 P2\n0\n0\n0\n\u0013\nV T , where\nP2 \u2208Rr\u00d7r and rank(A) = rank(B) = r. A su\ufb03cient condition for satisfaction of\n(4.5c) is that\nV = U\n\u0012\nQ\n0\n0\nP\n\u0013\n,\n(4.26)\nwhere Q \u2208Rr\u00d7r and P \u2208R(n\u2212r)\u00d7(n\u2212r) satisfy QQT = QT Q = I and PP T = P T P =\nI.\n11\nProof. A possible choice for Z in Lemma 4.2 is zero, for which (4.13) is equivalent\nto\nUr(Ur\nT BUr)\n\u22121Ur\nT = B+ + W,\n(4.27)\nwith W \u2208Rn\u00d7n in the null space of B. To obtain a simpli\ufb01ed su\ufb03cient condition\nfor existence of a positive de\ufb01nite solution to (4.1), we let W = 0.\nMultiplying\n(4.27) by Ur\nT and Ur respectively on the left and right, and substituting the spectral\ndecomposition of B, we get\n(Ur\nT Vr\nX2\nVr\nT Ur)\n\u22121\n= Ur\nT B+Ur = Ur\nT Vr\nX\u22122\nVr\nT Ur.\n(4.28)\nLetting M = Ur\nT Vr, we get\n(M\nX2\nM T )\n\u22121\n= M\nX\u22122\nM T .\n(4.29)\nSince M has full rank, we get\nM \u2212TX\u22122M \u22121 = M\nX\u22122M T .\n(4.30)\nNow, since P\u22122 is nonsingular, (4.30) holds if and only if\nM T M = I.\n(4.31)\nThis leads to\n(Ur\nT Vr)\nT Ur\nT Vr = Vr\nT UrUr\nT Vr = I.\n(4.32)\nSince U is orthonormal, we have UU T = UrUr\nT + Un\u2212rUn\u2212r\nT = I. Hence, we get\nUrUr\nT = I \u2212Un\u2212rUn\u2212r\nT .\n(4.33)\nSubstituting (4.33) in (4.32), we get\nVr\nT (I \u2212Un\u2212rUn\u2212r\nT )Vr = I \u2212Vr\nT Un\u2212rUn\u2212r\nT Vr = I,\n(4.34)\nwhich is satis\ufb01ed if and only if Un\u2212r\nT Vr = 0.\nSince the columns of Ur form an\northogonal basis for the null space of Un\u2212r\nT [27], it can be concluded that each\ncolumn of Vr is a linear combination of the columns of Ur. Thus,\nVr = UrQ\n(4.35)\nis a necessary condition for (4.31) to be satis\ufb01ed, and since both Ur and Vr have\northogonal columns, Q \u2208Rr\u00d7r satis\ufb01es QQT = QTQ = I. On the other hand, we\nknow from the de\ufb01nition of the spectral decomposition that V V T = UU T = I. Thus,\nVrVr\nT + Vn\u2212rVn\u2212r\nT = I,\nUrUr\nT + Un\u2212rUn\u2212r\nT = I.\n(4.36)\n12\nManipulating (4.35) with (4.36), we get\nVn\u2212rVn\u2212r\nT = Un\u2212rUn\u2212r\nT ,\n(4.37)\nwhich holds if and only if there exists a matrix P \u2208R(n\u2212r)\u00d7(n\u2212r) such that PP T =\nP T P = I and\nVn\u2212r = PUn\u2212r.\n(4.38)\nIt can be concluded from (4.35) and (4.38) that V = U\n\u0012 Q\n0\n0\nP\n\u0013\n, where QQT =\nQT Q = I and PP T = P T P = I.\nCorollary 4.3.\nThe matrices P and Q de\ufb01ned in Lemma 4.1 can set to be\nrotation matrices [27] to satisfy\nPP T = P T P = I,\nQQT = QT Q = I.\nThus, to compute a target matrix, T , satisfying Lemma 4.1, it is su\ufb03cient to \ufb01rst\ncompute V from (4.26) with Q \u2208Rr\u00d7r and P \u2208R(n\u2212r)\u00d7(n\u2212r) arbitrary rotation\nmatrices and U as de\ufb01ned in Lemma 4.1 and then set T = \u00afU\n\u0012 P\n0\n0\n0\n\u0013\nV T , where\n\u00afU \u2208Rm\u00d7m and P \u2208Rr\u00d7r are arbitrary orthonormal and diagonal matrices.\nThus,\nproblem (2.4) has a solution if and only if the data and target matrices satisfy (4.14).\nIn this case, \u02dcXrr, \u02dcXr,n\u2212r and its transpose, \u02dcXn\u2212r,r, are respectively computed from\n(4.5a) and (4.5b). Hence, the only remaining step is to compute \u02dcXn\u2212r,n\u2212r so that \u02dcX\nis symmetric and positive de\ufb01nite.\nWe know that \u02dcX is symmetric positive de\ufb01nite if and only if there exists a nonsingular\nlower triangular matrix L \u2208Rn\u00d7n so that\n\u02dcX = LLT ,\n(4.39)\nwhere L is lower triangular and nonsingular.\nConsidering the block forms \u02dcX =\n\u0012\n\u02dcXrr\n\u02dcXr,n\u2212r\n\u02dcXn\u2212r,r\n\u02dcXn\u2212r,n\u2212r\n\u0013\nand L =\n\u0012\nLrr\n0\nLn\u2212r,r\nLn\u2212r,n\u2212r\n\u0013\n, where Ln\u2212r,r is an (n \u2212\nr) \u00d7 r matrix and Lrr \u2208Rr\u00d7r and Ln\u2212r,n\u2212r \u2208R(n\u2212r)\u00d7(n\u2212r) are nonsingular lower\ntriangular matrices, we get\n\u0012\n\u02dcXrr\n\u02dcXr,n\u2212r\n\u02dcXn\u2212r,r\n\u02dcXn\u2212r,n\u2212r\n\u0013\n=\n\u0012\nLrr\n0\nLn\u2212r,r\nLn\u2212r,n\u2212r\n\u0013 \u0012\nLrr\nT\nLn\u2212r,r\nT\n0\nLn\u2212r,n\u2212r\nT\n\u0013\n.\n(4.40)\nThus,\n\u02dcXrr = LrrLrr\nT ,\n(4.41a)\n\u02dcXr,n\u2212r = LrrLn\u2212r,r\nT ,\n(4.41b)\n\u02dcXn\u2212r,r = Ln\u2212r,rLrr\nT ,\n(4.41c)\n13\n\u02dcXn\u2212r,n\u2212r = Ln\u2212r,rLn\u2212r,r\nT + Ln\u2212r,n\u2212rLn\u2212r,n\u2212r\nT .\n(4.41d)\nTherefore, to compute a symmetric positive de\ufb01nite \u02dcX, (4.41a)\u2013(4.41d) must be satis-\n\ufb01ed. Let \u02dcXrr = \u02dcL\u02dcLT be the Cholesky decomposition of \u02dcXrr. Lrr = \u02dcL satis\ufb01es (4.41a).\nSubstituting Lrr in (4.41b), Ln\u2212r,r\nT is computed uniquely by solving the resulting\nlinear system. Since (4.41c) is transpose of (4.41b), it does not give any additional\ninformation. Finally, to compute a matrix \u02dcXn\u2212r,n\u2212r to satisfy (4.41d), it is su\ufb03cient\nto choose an arbitrary lower triangular nonsingular matrix Ln\u2212r,n\u2212r and substitute it\nin (4.41d). The resulting \u02dcXn\u2212r,n\u2212r gives a symmetric positive de\ufb01nite \u02dcX as follows:\n\u02dcX =\n\u0012\n\u02dcXrr\n\u02dcXr,n\u2212r\n\u02dcXn\u2212r,r\n\u02dcXn\u2212r,n\u2212r\n\u0013\n.\nNow, based on the above discussion, we outline the steps of our algorithm for solving\n(2.4) in the case rank(D) = r < n.\n14\nSolving the EIV model for positive de\ufb01nite linear system with rank\nde\ufb01cient data and target matrices using spectral decomposition.\n\u03b4 as the upper bounds for absolute error is taken to be close to the machine (or\nuser\u2019s) zero.\nLet A = DT D and compute its spectral decomposition:\nA = U\n\u0012 S2\n0\n0\n0\n\u0013\nU T .\nLet B = T TT and \u02dcB = U T BU.\nCompute rank(D) = r and let\n\u02dcBrr = \u02dcB(1 : r, 1 : r),\n\u02dcBr,n\u2212r = \u02dcB(1 : r, r + 1 : n),\n\u02dcBn\u2212r,n\u2212r = \u02dcB(r + 1 : n, r + 1 : n)\nLet \u00afD = S, assume \u00afT satis\ufb01es \u02dcBrr = \u00afT T \u00afT.\nPerform Algorithm 1 with input parameters D = \u00afD and T = \u00afT, and let \u02dcXrr = X\u2217.\nSolve the linear system (4.5b) to compute \u02dcXr,n\u2212r and let \u02dcXn\u2212r,r = \u02dcXT\nr,n\u2212r.\nCompute the spectral decomposition for B, that is, B = V\n\u0012 D2\n0\n0\n0\n\u0013\nV T . Compute\nM = Ur\nT Vr.\nIf \u2225Un\u2212r\nT (BUr(Ur\nT BUr)\n\u22121Ur\nT B \u2212B)\u2225\u2265\u03b4 stop ((2.4) has no solution)\nElse\nLet the Cholesky decomposition of \u02dcXrr be \u02dcXrr = \u02dcL\u02dcLT and set Lrr = \u02dcL.\nSolve the lower triangular system (4.41b) to compute Ln\u2212r,r.\nLet Ln\u2212r,n\u2212r \u2208R(n\u2212r)\u00d7(n\u2212r) be an arbitrary nonsingular lower triangular matrix and\ncompute \u02dcXn\u2212r,n\u2212r using (4.41d).\nLet \u02dcX =\n\u0012\n\u02dcXrr\n\u02dcXr,n\u2212r\n\u02dcXn\u2212r,r\n\u02dcXn\u2212r,n\u2212r\n\u0013\nand X\u2217= U \u02dcXU T .\nCompute E = tr((DX\u2217\u2212T )(D \u2212T X\u2217\u22121)).\nEndIf.\nNext, we show how to use the complete orthogonal decomposition of the data matrix\nD instead of the spectral decomposition of A.\nNote (Complete Orthogonal Decomposition) [27] Let A \u2208Rm\u00d7n be an arbi-\ntrary matrix with rank(A) = r. There exist R \u2208Rr\u00d7r, U \u2208Rm\u00d7m and V \u2208Rn\u00d7n\nso that R \u2208Rr\u00d7r is upper triangular, UU T = U T U = I, V V T = V T V = I and\nA = U\n\u0012 R\n0\n0\n0\n\u0013\nV T .\nNext, Algorithm 4 is presented using the complete orthogonal decomposition\nof D.\n15\nSolving the EIV model for positive de\ufb01nite linear system with rank de-\n\ufb01cient data and target matrices using complete orthogonal decomposition.\n\u03b4 as the upper bounds for absolute error is taken to be close to the machine (or\nuser\u2019s) zero.\nCompute the complete orthogonal decomposition of D, that is,\nD = U\n\u0012\nR\n0\n0\n0\n\u0013\nV T .\n(4.42)\nLet A = DT D = VrRT RVr\nT , B = T T T and \u02dcB = V T BV , where Vr consists of the\n\ufb01rst r columns of V .\nCompute rank(D) = r and let\n\u02dcBrr = \u02dcB(1 : r, 1 : r),\n\u02dcBr,n\u2212r = \u02dcB(1 : r, r + 1 : n),\n\u02dcBn\u2212r,n\u2212r = \u02dcB(r + 1 : n, r + 1 : n).\nLet \u00afD = R, assume \u00afT satis\ufb01es \u02dcBrr = \u00afT T \u00afT.\nPerform Algorithm 1 with input parameters D = \u00afD and T = \u00afT, and let \u02dcXrr = X\u2217.\nSolve the linear system (4.5b) to compute \u02dcXr,n\u2212r and let \u02dcXn\u2212r,r = \u02dcXT\nr,n\u2212r.\nCompute the spectral decomposition for B, that is, B = V\n\u0012 D2\n0\n0\n0\n\u0013\nV T . Compute\nM = Ur\nT Vr.\nIf \u2225Un\u2212r\nT (BUr(Ur\nT BUr)\n\u22121Ur\nT B \u2212B)\u2225\u2265\u03b4 stop ((2.4) has no solution)\nElse\nLet the Cholesky decomposition of \u02dcXrr be \u02dcXrr = \u02dcL\u02dcLT and set Lrr = \u02dcL.\nSolve the lower triangular system (4.41b) to compute Ln\u2212r,r.\nLet Ln\u2212r,n\u2212r \u2208R(n\u2212r)\u00d7(n\u2212r) be an arbitrary nonsingular lower triangular matrix and\ncompute \u02dcXn\u2212r,n\u2212r using (4.41d).\nLet \u02dcX =\n\u0012\n\u02dcXrr\n\u02dcXr,n\u2212r\n\u02dcXn\u2212r,r\n\u02dcXn\u2212r,n\u2212r\n\u0013\nand X\u2217= U \u02dcXU T .\nCompute E = tr((DX\u2217\u2212T )(D \u2212T X\u2217\u22121)).\nEndIf.\nThus, based on the above study, the computational complexity of PDEIV-QR is lower\nthan that of PDEIV-Spec, for all matrix sizes. But, for the case of rank de\ufb01cient data\nmatrix, depending on the matrix size and rank, one of the algorithms PDEIV-RD-\nSpec and PDEIV-RD-COD may have a lower computational complexity.\nREFERENCES\n[1] Alizadeh F., Pierre J., Heaberly A., Overton M. L.: Primal-dual interior point methods for\nsemide\ufb01nite programming: convergence rates, stability and numerical result, SIAM J. Op-\ntim., 8, 746-768 (1998)\n[2] Aubry A., Maio A. D., Pallotta L., Farina A.: Maximum likelihood estimation of a structured\ncovariance matrix with a condition number constraint, IEEE Trans. On Signal Processing,\n60(6), 3004-3021 (2012)\n[3] Cheng C. L., Kukush A., Mastronardi N., Paige C., Van Hu\ufb00el S.: Total Least Squares and\nErrors-in-variables Modeling, Comput Stat Data An, 52, 1076-1079 (2007)\n16\n[4] Deng Y., Boley D.: On the Optimal Approximation for the Symmetric Procrustes Problems of\nthe Matrix Equation AXB = C, Proceedings of the International Conference on Compu-\ntational and Mathematical Methods in Science and Engineering, Chicago, 159-168 (2007)\n[5] Dolan E. D, Mor\u00b4e J. J.: Benchmarking optimization software with performance pro\ufb01les, Math-\nematical Programming, 91, 201-213 (2012)\n[6] Golub G. H., Van Loan C. F.: An analysis of the total least squares problem, SIAM J. Numer.\nAnal., 17, 883-893 (1980)\n[7] Hayami K., Yin J. F., Ito T.: GMRES method for least squares problems, SIAM. J. Matrix\nAnal. and Appl., 31(5), 2400-2430 (2010)\n[8] Hn\u02c7etynkov\u00b4a I., Ple\u02c7singer M., Sima D. M., Strako\u02c7s Z., Van Hu\ufb00el S.: The total least squares\nproblem in AX \u2248B, A new classi\ufb01cation with the relationship to the classical works,\nSIAM J. Matrix Anal. Appl., 32(3), 748-770 (2011)\n[9] Hu H., Olkin I.: A numerical procedure for \ufb01nding the positive de\ufb01nite matrix closest to a\npatterned matrix, Statistical and Probability Letters, 12, 511-515 (1991)\n[10] Hu H.: Positive de\ufb01nite constrained least-squares estimation of matrices, Linear Algebra and\nits Applications, 229, 167-174 (1995)\n[11] Van Hu\ufb00el S., Vandewalle J.: Algebraic connections between the least squares and total least\nsquares problems, Numer. Math., 55, 431-449 (1989)\n[12] Kang B., Jung S., Park P.: A new iterative method for solving total least squares problem,\nProceeding of the 8th Asian Control Conference (ASCC), Kaohsiung, Taiwan, (2011)\n[13] Larson H. J.: Least squares estimation of the components of a symmetric matrix, Technomet-\nrics, 8(2), 360-362 (1966)\n[14] McInroy J., Hamann J. C.:\nDesign and control of \ufb02exure jointed hexapods, IEEE Trans.\nRobotics and Automation, 16(4), 372-381 (2000)\n[15] Mor\u00b4e J. J., Wild S. M.: Benchmarking derivative-free optimization algorithms, SIAM J. Optim.,\n20, 172-191 (2009)\n[16] Paige C. C., Strako\u02c7s Z.: Scaled total least squares fundamentals, Numer. Math., 91, 117-146\n(2000)\n[17] Poignet P., Gautier M.: Comparison of Weighted Least Squares and Extended Kalman Filtering\nMethods for Dynamic Identi\ufb01cation of Robots, Proceedings of the IEEE Conference on\nRobotics and Automation, San Francisco, CA, USA, 3622-3627 (2000)\n[18] Woodgate K. G.: Least-squares solution of F = P G over positive semide\ufb01nite symmetric P,\nLinear Algebra Appl., 245, 171-190 (1996)\n[19] Zhou L., Lin L., Wei Y., Qiao S.: Perturbation analysis and condition numbers of scaled total\nleast squares problems, Numer. Algorithms, 51, 381-399 (2009)\n[20] Banerjee S., Roy A.: Quadratic Forms, Linear Algebra and Matrix Analysis for Statistics,\nChapman Hall/CRC Texts in Statistical Sciences, 441-442 (2014)\n[21] Gill P. E. , Murray W., Wright M. H.: Numerical Linear Algebra and Optimization, Addison\nWesley, (1991)\n[22] Higham N. J.: Functions of Matrices: Theory and Computation, SIAM, Philadelphia (2008)\n[23] Horn R. A., Johnson C. R.: Topics in Matrix Analysis. Cambridge University Press (1991)\n[24] Van Hu\ufb00el S., Vandewalle J.: The Total Least Squares Problem: Computational Aspects and\nAnalysis. SIAM, Philadelphia (1991)\n[25] Krislock N. G.: Numerical Solution of Semide\ufb01nite Constrained Least Squares Problems, M.\nSc. Thesis, University of British Colombia (2003)\n[26] Demmel J. W.: Applied Numerical Linear Algebra, 3rd edition, SIAM, Philadelphia (1996)\n[27] Golub G. H., Van Loan C. F.: Matrix Computation, 4th edition, JHU Press (2012)\n[28] Lancaster P., Rodman L.: Algebraic Riccati Equations, Clarendon Press (1995)\n[29] Magnus J. R., Neudecker H.: Matrix Di\ufb00erential Calculus with Applications in Statistics and\nEconometrics, 2nd edition, John Wiley Sons (1999)\n[30] Nocedal J., Wright S. J.: Numerical Optimization, Springer, New York, (1999)\n[31] Higham N. J.: Computing the nearest correlation matrix (A problem from ?nance), MIMS\nEPrint: 2006.70, http://eprints.ma.man.ac.uk/, (2006). Accessed 26 June 2012\n[32] Petersen\nK.\nB.,\nPedersen\nM.\nS.:\nThe\nMatrix\nCookbook,\nhttp://orion.uwaterloo.ca/\nhwolkowi/matrixcookbook.pdf, (2008). Accessed 11 January 2013\n[33] Vershynin\nR.:\nIntroduction\nto\nthe\nnon-asymptotic\nanalysis\nof\nrandom\nmatrices,\nhttp://arxiv.org/pdf/1011.3027v7.pdf, (2011). Accessed 01 February 2013\n[34] American\nMathematical\nSociety,\nEigenvalues\nand\nsums\nof\nHermitian\nmatrices,\nhttp://www.ams.org/bookstore/pspdf/gsm132prev.pdf,\n(2009).\nAccessed\n18\nMarch\n2013\n17\n0\n0.5\n1\n1.5\n2\n2.5\n3\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nPDEIV\u2212QR\nPDEIV\u2212Spec\n0\n2\n4\n6\n8\n10\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nPDEIV\u2212QR\nPDEIV\u2212Spec\n0\n2\n4\n6\n8\n10\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nPDEIV\u2212QR\nPDEIV\u2212Spec\nHuM\nIntP\n0\n2\n4\n6\n8\n10\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nPDEIV\u2212QR\nPDEIV\u2212Spec\nHuM\nIntP\n0\n0.5\n1\n1.5\n2\n2.5\n3\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nPDEIV\u2212RD\u2212Spec\nPDEIV\u2212RD\u2212COD\n0\n20\n40\n60\n80\n100\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nPDEIV\u2212RD\u2212Spec\nPDEIV\u2212RD\u2212COD\n0\n10\n20\n30\n40\n50\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nPDEIV\u2212RD\u2212Spec\nPDEIV\u2212RD\u2212COD\nHuM\nIntP\n0\n5\n10\n15\n20\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nPDEIV\u2212RD\u2212Spec\nPDEIV\u2212RD\u2212COD\nHuM\nIntP\n0\n0.5\n1\n1.5\n2\n2.5\n3\nx 10\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nPDEIV\u2212RD\u2212Spec\nHuM\nIntP\n0\n100\n200\n300\n400\n500\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nPDEIV\u2212RD\u2212Spec\nHuM\nIntP\n"}