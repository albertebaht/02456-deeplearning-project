{"text": "arXiv:2209.01679v3  [math.AG]  17 Jun 2024\nOrthogonal and Linear Regressions and Pencils of Confocal\nQuadrics\nVladimir Dragovi\u00b4c and Borislav Gaji\u00b4c\nAbstract. This paper enhances and develops bridges between statistics, me-\nchanics, and geometry.\nFor a given system of points in Rk representing a\nsample of full rank, we construct an explicit pencil of confocal quadrics with\nthe following properties: (i) All the hyperplanes for which the hyperplanar\nmoments of inertia for the given system of points are equal, are tangent to\nthe same quadrics from the pencil of quadrics. As an application, we develop\nregularization procedures for the orthogonal least square method, analogues\nof lasso and ridge methods from linear regression. (ii) For any given point P\namong all the hyperplanes that contain it, the best \ufb01t is the tangent hyper-\nplane to the quadric from the confocal pencil corresponding to the maximal\nJacobi coordinate of the point P ; the worst \ufb01t among the hyperplanes con-\ntaining P is the tangent hyperplane to the ellipsoid from the confocal pencil\nthat contains P . The confocal pencil of quadrics provides a universal tool to\nsolve the restricted principal component analysis restricted at any given point.\nBoth results (i) and (ii) can be seen as generalizations of the classical result of\nPearson on orthogonal regression. They have natural and important applica-\ntions in the statistics of the errors-in-variables models (EIV). For the classical\nlinear regressions we provide a geometric characterization of hyperplanes of\nleast squares in a given direction among all hyperplanes which contain a given\npoint. The obtained results have applications in restricted regressions, both\nordinary and orthogonal ones. For the latter, a new formula for test statis-\ntic is derived. The developed methods and results are illustrated in natural\nstatistics examples.\n1. Introduction\nThe aim of this paper is to further develop and enhance bridges between three\ndisciplines: statistics, mechanics, and geometry. More precisely, we will explore\nand employ links between quadrics, moments of inertia, and regressions, in both\nthe ordinary linear and the orthogonal settings. As it is well known and as it will\nbe discussed later in detail, orthogonal regression plays a crucial role in Errors-\nin-Variables models (EIV), as seen in [8, 9, 27]. The EIV models have various\napplications in di\ufb00erent areas in science, as seen in [3, 6, 7, 13, 34, 35].\nThis cross-fertilization between statistics, mechanics, and geometry appears\nto be bene\ufb01cial for each of them. While individual quadrics have been in use in\nstatistics, as reviewed in Section 2 (see also [25, 37]), the novelty of this paper\nlies in the construction of a new object, a confocal pencil of quadrics, associated\nto a given data set, which we attest to be a powerful and universal tool to study\nthe data. More about confocal pencils of quadrics and associated Jacobi elliptic\nKey words and phrases.\ndata ellipsoid; confocal pencil of quadrics; planar moments of\ninertia; restricted regression; regularization and shrinkage; restricted PCA.\n1\n2\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\ncoordinates in the space of an arbitrary dimension k is given in Supplementary\nmaterial, see [32] and also e.g. [5], [21].\nFor the reader\u2019s sake, we will brie\ufb02y present the basics of the pencil of conics\nand associated Jacobi elliptic coordinates in the plane. This corresponds to the\ncase k = 2 and should provide enough intuition for the case of an arbitrary number\nof dimensions. Let an ellipse E be given in the plane by the equation:\nE : x2\n\u03b1 + y2\n\u03b2 = 1,\n\u03b1 > \u03b2 > 0.\nAll the conics in the plane, sharing the same focal points with E, form a pencil of\nconfocal conics, C\u03bb, de\ufb01ned by the equation:\n(1.1)\nC\u03bb :\nx2\n\u03b1 \u2212\u03bb +\ny2\n\u03b2 \u2212\u03bb = 1,\nsee Figure 1, where F1, F2 are the common focal points.\nThe parameter \u03bb is real, and each \ufb01xed \u03bb de\ufb01nes a conic C\u03bb. The family C\u03bb (1.1)\ncontains two types of nondegenerate conics: ellipses when \u03bb < \u03b2, and hyperbolas\nwhen \u03bb \u2208(\u03b2, \u03b1), see Figure 1. There are also two degenerate conics in the confocal\npencil: the x-axis for \u03bb = \u03b2, and the y-axis for \u03bb = \u03b1.\nEach point in the plane, P(x0, y0), which is not a focus of the confocal pencil\nof conics (i.e. P /\u2208{F1, F2}), lies at exactly two conics C\u03bb1 and C\u03bb2 from (1.1) \u2013 one\nellipse and one hyperbola, which are orthogonal to each other at the intersection\npoint P, see Figure 1.\nF2\nF1\nx\ny\nP(x0, y0)\n\u03bb = \u03bb2\n\u03bb = \u03bb1\nFigure 1. A confocal pencil of conics in plane; the Jacobi coorinates\nFor a given point P(x0, y0), the parameters \u03bb1 and \u03bb2 of the conics from the\nconfocal pencil that contain P are the solutions of the quadratic equation (1.1) in\n\u03bb, where the given Cartesian coordinates x = x0 and y = y0 of the point P are\nknown parameters. By de\ufb01nition, these (\u03bb1, \u03bb2) are the Jacobi elliptic coordinates\nof the point P associated with the given confocal pencil of conics.\nJacobi (1804-1851) introduced Jacobi\u2019s elliptic coordinates in arbitrary dimen-\nsion in the 26-th Lecture of his Course on Dynamics, delivered in K\u00a8oningsberg in\n1842/43. The Jacobi Lectures [32] were published posthumously by Clebsch in\n1865 in German. Jacobi used these coordinates in Lecture 26 to solve one of the\nimportant open problems of the \ufb01rst half of XIX century\u2013 to integrate the equations\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n3\nof geodesics on an ellipsoid in Rk. Jacobi opened the Lecture 26 with the famous\nlines:\n\u201cThe main di\ufb03culty in integrating a given di\ufb00erential equation lies in intro-\nducing convenient variables, which there is no rule for \ufb01nding. Therefore, we must\ntravel the reverse path and after \ufb01nding some notable substitution, look for problems\nto which it can be successfully applied.\u201d (see also e.g. [5], section 47, [33]).\nFollowing this ideology of Jacobi, in the present paper, we search for problems\nin Statistics to which the Jacobi elliptic coordinates can be e\ufb00ectively applied.\nIn this paper, we will deal with data of full rank. We say that a given system\nof points in Rk is of full rank if these points are not contained in a hyperplane.\nLet us immediately formulate some of the main outcomes of this paper.\nFor a\ngiven system of points in Rk of full rank, we construct an explicit pencil of confocal\nquadrics with the following properties:\n(i) All the hyperplanes for which the hyperplanar moments of inertia (de\ufb01ned\nin Section 3.1) for the given system of points are equal, are tangent to the same\nquadrics from the pencil of quadrics.\nSee Section 3 and Theorem 3.1.\nAs an\napplication, we develop regularization procedures for the orthogonal least square\nmethods, analogues of lasso and ridge methods from linear regression, see Section\n3.5. This is based on a dual version of Theorem 3.1, which is given as Theorem\n3.2. Another motivation for this study comes from the gradient descent methods\nin machine learning. An optimization algorithm may not be guaranteed to arrive\nat the minimum in a reasonable amount of time. As pointed out in e.g. [30], it\noften reaches some quite low value of the cost function, equal to some value s0,\nquickly enough to be useful. Here we deal with the hyperplanar moment as the\ncost function, in application to orthogonal least squares. From Theorem 3.1, we\nknow that the hyperplanes which generate the hyperpanar moment equal to s0 are\nall tangent to the given quadric from the confocal pencil of quadrics, where the\npencil parameter is determined through the value s0.\n(ii) For any given point P, among all the hyperplanes that contain it, the best \ufb01t\nis the tangent hyperplane to the quadric from the confocal pencil corresponding to\nthe maximal Jacobi coordinate of the point P. The worst \ufb01t among the hyperplanes\ncontaining P is the tangent hyperplane to the ellipsoid from the confocal pencil that\ncontains P. See Theorem 4.3 from Section 4.\nBoth results (i) and (ii) can be seen as generalizations of the classical result\nof Pearson on orthogonal regression [36], or in other words, on the orthogonal\nleast square method (see e.g. [10]). The original result of Pearson is stated below\nas Theorem 2.1 in Section 2. The original Pearson result also initiated Principal\nComponent Analysis (PCA), see e.g.\n[4].\nSome of our results have a natural\ninterpretation in terms of PCA. The confocal pencil of quadrics provides a universal\ntool to solve Restricted Principal Component Analysis, restricted at any given\npoint, which we formulate and solve in Section 4, see Theorem 4.1, Corollary 4.1,\nand Example 4.1. Our generalizations of the Pearson Theorem have natural and\nimportant applications in the statistics of the measurement error models, for which\northogonal regression is known to provide a natural framework, see [10], [27], [9].\nWe also study classical linear regression from a geometric standpoint, and we\ndo this in a coordinate-free form in Section 5. For linear regressions, we provide\na geometric characterization of hyperplanes of least squares in a given direction\namong all hyperplanes that contain a given point. In the case k = 2, this is done\nin Theorem 5.2, and for an arbitrary k in Theorem 5.3. All the hyperplanes with\nthe same directional hyperplanar moment of inertia (equal \u00b5) form an ellipsoid.\nWhen \u00b5 varies, the ellipsoids change within a homothetic family of ellipsoids, see\nPropositions 5.3 and 5.4.\n4\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nThe results we obtain have applications in restricted regressions, see e.g. [38]\nfor ordinary linear regressions and see e.g. [27] and [9] for orthogonal regressions\nand measurement error models. Restricted regressions arise in situations with prior\nknowledge, which have numerous applications, for example in economics and econo-\nmetrics, see e.g. [23]. Restricted regressions also appear in tests of hypotheses, see\ne.g. [38, 23]. For precise assumptions about distributional conditions under which\ntest statistics are valid see [27]. For example, consider a system of N points in Rk\nwith the centroid C given. For restricted orthogonal regression, we derive a new\nformula for the test statistic for the hypothesis that the hyperplane of the best \ufb01t\ncontains a given point P:\n(1.2)\nN\nN \u2212k + 1(2\u03bbkC \u2212\u03bbkP ),\nwhere \u03bbkP and \u03bbkC are the maximal Jacobi coordinates respectively of the given\npoint P and the centroid C, see Theorem 4.4. Here, it is assumed that the Jacobi\ncoordinates are induced by the confocal pencil of quadrics associated with the given\nsystem of N points. The elegance of the above formula certi\ufb01es the appropriateness\nof the geometric methods and tools developed here for their use in this statistics\nframework. The developed methods and results are further illustrated in natural\nstatistics settings, see e.g. Examples 4.2 and 5.4.\nWe will say that an ellipsoid is rotational if at least two of its principal semiaxes\ncoincide.\nFor the construction of the confocal pencil of quadrics associated to a given\nsystem of points, we develop a method based on the study of points for which the\nhyperplanar ellipsoid of inertia is rotational. As we will see in Section 3.1, the\nhyperplanar ellipsoid of inertia at a point O determines the moment of inertia for\nany hyperplane that contains O, and vice versa. Hence, the hyperplanar ellipsoid\nof inertia depends on a point O. Our construction of the confocal pencil of quadrics\nin the case of an arbitrary number of dimensions k is studied in Section 3, which\neventually leads to our main result in this direction, Theorem 3.1.\nLet us explain the content of Theorem 3.1 in the simplest case of k = 2 here,\nsee Example 3.1, see Fig. 5, and also [17]. We start from a data set in the plane\nwith full rank and the centroid C. We \ufb01nd two points, F1 and F2, symmetric with\nrespect to C, that have a circle as the ellipse of inertia for the given data set. The\nquestion is what the points F1 and F2 tell us about the initial data set. To answer\nthat question, we construct the pencil of confocal conics with foci F1 and F2. We\ndiscover that the pencil of confocal conics has the following property with respect\nto the initial data set: the lines in the plane that have the same moment with respect\nto the given data set are all tangent to the same conic from the confocal pencil of\nconics.\nOur method of constructing the confocal pencil of quadrics in arbitrary dimen-\nsion k associated with a given data set generalizes these planar considerations. It\nsearches for the points where the ellipsoid of inertia is rotational and also takes into\naccount the subtle challenges in generalizing the notion of focal points in the plane\nto spaces of higher dimensions. The obtained confocal pencil of quadrics does not\ncontain either the hyperplanar ellipsoid of inertia or any of gyrational hyperplanar\nellipsoids of inertia. In statistics terminology, the confocal pencil of quadrics does\nnot contain either the data ellipsoid or the ellipsoid of residuals.\nHowever, the\nconfocal pencil does contain a specially normalized axial ellipsoid of gyration. This\nnormalization is not known a priori and comes only as a consequence of applying\nour method described above.\nLet us also note that the idea of considering the\npoints where the ellipsoid of inertia is rotational is more typical for mechanics than\nfor statistics. A dominant practice in statistics is to consider ellipsoids of data or\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n5\nresiduals exclusively centered at the centroid. The cases of given data of lower rank\ncan be treated similarly, considering subspaces of smaller dimension.\n2. Lines and hyperplanes of the best \ufb01t to the data set in Rk\nNowadays, when we are witnessing the explosion of data science it is important\nto recall the heroic days of formation of statistics and its mathematical foundations.\nEllipses, as two-dimensional quadrics, made a great entrance in statistics in 1886\nin the work of Francis Galton [28], which marked the birth of the law of regression.\nGalton studied the hereditary transmissions and in [28] he focused on the\nheight.\nHe collected the data of 930 adult children and 205 of their respective\nparentages. To each pair of parents he assigned a \u201cmid-parent\u201d height, as a weighted\naverage of the heights of the parents. He established the average regression from\nmid-parent to o\ufb00springs and from o\ufb00springs to mid-parent. He formulated the law\nof regression toward mediocrity:\nWhen Mid-Parents are taller than mediocrity, their Children tend to be shorter\nthan they. When Mid-Parents are shorter than mediocrity, their Children tend to\nbe taller than they.\nThis is how the term regression, thanks to Galton, entered into statistics, al-\nthough the method of least squares which is in the background, existed in mathe-\nmatics from the beginning of the XIX century and works of Gauss and Legendre.\nIn the course of his investigation, Galton discovered a remarkable role of ellipses\nin analysis of such data. Galton explained in a very nice manner his line of thoughts\nand actions. We present that in his own words:\n\u201c...I found it hard at \ufb01rst to catch the full signi\ufb01cance of the entries in the\ntable...They came out distinctly when I \u2018smoothed\u2019 the entries by writing at each\nintersection of a horizontal column with a vertical one, the sum of entries of four\nadjacent squares...\nI then noticed that lines drawn through entries of the same\nvalue formed a series of concentric and similar ellipses. Their common center ...\ncorresponded to 68 1\n4 inches. Their axes are similarly inclined. The points where\neach ellipse in succession was touched by a horizontal tangent, lay in a straight\nline inclined to the vertical in the ratio of\n2\n3; those where they were touched by\na vertical tangent lay in a straight line inclined to the horizontal in the ratio of\n1\n3.\nThese ratios con\ufb01rm the values of average regression already obtained by a\ndi\ufb00erent method, of\n2\n3 from mid-parent to o\ufb00spring, and of\n1\n3 from o\ufb00spring to\nmid-parent...These and other relations were evidently a subject for mathematical\nanalysis and veri\ufb01cation...I noted these values and phrased the problem in abstract\nterms such as a competent mathematician could deal with, disentangled from all\nreferences to heredity, and in that shape submitted it to Mr. Hamilton Dixson, of\nSt. Peter\u2019s College, Cambridge...\nI may be permitted to say that I never felt such a glow of loyalty and respect\ntowards the sovereignty and magni\ufb01cent sway of mathematical analysis as when\nhis answer reached me, con\ufb01rming, by purely mathematical reasoning, my various\nand laborious statistical conclusions with far more minuteness than I had dared to\nhope... His calculations corrected my observed value of mid-parental regression from\n1\n3 to\n6\n17.6, the relation between the major and minor axis of the ellipse was changed\n3 per cent. (it should be as\n\u221a\n7\n\u221a\n2)...\u201d\nIn his seminal paper [36], one of the founding fathers of modern statistics,\nKarl Pearson, investigated the question of the hyperplane which minimizes the\nmean square distance from a given set of points in Rk, for any k \u22653. In his own\nwords, Pearson formulated the problem: \u201cIn the case we are about to deal with, we\nsuppose the observed variables\u2013all subject to error\u2013to be plotted in the plane, three-\ndimensioned or higher space, and we endeavour to take a line (or plane) which will\n6\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nFigure 2. From [28].\nbe the \u2018best \ufb01t\u2019 to such a system of points. Of course the term \u2018best \ufb01t\u2019 is really\narbitrary; but a good \ufb01t will clearly be obtained if we make the sum of the squares of\nthe perpendiculars from the system of points upon the line or plane a minimum.\u201d\nFigure 3. From [36].\nFigure 4. From [36].\nThus, the notion of the best \ufb01t is not uniquely determined. In statistics, the\nchoice of the squares of the perpendiculars is natural in the measurement error\nmodels, or in other words in regression with errors in variables (EIV), [10]. This\ncorresponds to Pearson\u2019s above remark \u201cwe suppose the observed variables\u2013all sub-\nject to error\u201d. In such models it is assumed that both predictors and responses\nare known with some error. In contrast to that, in the classical linear regressions,\nmentioned above, it is assumed that only responses are known with some error,\nwhile for the predictors the exact values are known. Thus, the squares of distances\nalong one of the axes is in use in such classical regression models, see also Section\n5.2. We will call that directional regression in a given direction. We will talk more\nabout its geometric aspects in the last Section 5.\nLet us make a more rigorous de\ufb01nitions of the classical simple linear regression\nmodel and regression with error in variables (EIV) models, [10]. For classical simple\nregression model, it is assumed that the values (x(i))N\ni=1 are known, \ufb01xed values, as\nfor example values set up in advance in the experiment. The values (y(i))N\ni=1 are\nobserved values of uncorrelated random variables Yi, i = 1, . . . , N with the same\nvariance \u03c32. There is a linear relationship assumed between the predictors x(i)\nand responses (y(i))N\ni=1: EYi = \u03b1 + \u03b2x(i),\ni = 1, . . . , N. This can be restated as\nYi = \u03b1 + \u03b2x(i) + \u01ebi,\ni = 1, . . . , N, where \u01ebi are called the random errors and they\nare uncorrelated random variables with zero expectation and the same variance \u03c32.\nIn such models the regression is of Y on x, i.e. in the vertical direction, see Example\n5.1. This model was in the background of the Galton study [28], mentioned above.\nThere are also important situations where predictors are known only up to\nsome error and they are described by measurement error models. There the ob-\nserved pairs (x(i), y(i))N\ni=1 are sampled from random variables (Xi, Yi) with means\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n7\nsatisfying the linear relationship EYi = \u03b1 + \u03b2(EXi),\ni = 1, . . . , N. Denoting\nEXi = \u03bei, the errors in variables model can be de\ufb01ned as Yi = \u03b1 + \u03b2\u03bei + \u01ebi,\nXi =\n\u03bei+\u03b4i,\ni = 1, . . . , N, where both Xi and Yi have error terms which belong to mean\nzero normal distributions, such that all \u01ebi, i = 1, . . . , N have the same variance \u03c32\n\u01eb\nand all \u03b4i, i = 1, . . . , N have the same variance \u03c32\n\u03b4. Since in such models there is a\nsymmetry between xi and yi as they are both known with an error, it is more nat-\nural to apply to them the orthogonal regression, or in other words, the orthogonal\nleast square method. This we are going to introduce next under the assumption\nthat \u03b7 = \u03c32\n\u01eb /\u03c32\n\u03b4 = 1 and for an arbitrary numbers of dimensions k. One should\nalso mention that applications of the orthogonal least square method in the models\nwith measurement errors have limitations, see e.g. [9], [8]. These limitations are\nrelated to a potentially unknown value of \u03b7. Here we assume that \u03b7 is known. At\n\ufb01rst we assume that \u03b7 = 1. The assumption that \u03b7 is known is essential, while that\nit is equal to one is not. If \u03b7 is known, but not equal to 1, it can be made equal to\n1 by rescaling either X or Y . See also Remark 4.1.\nFrom a historic perspective, the case \u03b7 = 1 originated from [1, 2]. Then it\nwas Pearson who established orthogonal regression by selecting the squares of the\nperpendiculars, which corresponds to the case \u03b7 = 1. Nowadays, it is also called the\northogonal least square method, see e.g. [10], as mentioned above. The geometric\naspects of the orthogonal regression are the main subject of this study. We also\nadopt the Pearson\u2019s generality assumption, that the given system of points does\nnot belong to a hyper-plane i.e. a system of points is of full rank.\nTo \ufb01x the idea, suppose the system of N points (x(i)\n1 , x(i)\n2 , . . . , x(i)\nk )N\ni=1 is given.\nDe\ufb01ne the centroid, or the mean values of the coordinates \u00afxj and the variances \u03c32\nxj:\n\u00afxj = 1\nN\nN\nX\ni=1\nx(i)\nj ,\n\u03c32\nxj = 1\nN\nN\nX\ni=1\n(x(i)\nj\n\u2212\u00afxj)2,\nj = 1, . . . , k.\nDue to the full rank assumption, all \u03c32\nxj, for j = 1, . . . , k are non-zero. Then, the\ncorrelations rjl and the covariances pjl are\nrjl =\npjl\n\u03c3xj\u03c3xl\n, pjl = 1\nN\nN\nX\ni=1\n(x(i)\nj\n\u2212\u00afxj)(x(i)\nl\n\u2212\u00afxl), j, l = 1, . . . , k, l \u0338= j.\nThe covariance matrix K is a (k\u00d7k) matrix with diagonal elements Kjj = \u03c32\nxj, j =\n1, . . . , k, and o\ufb00-diagonal elements Kjl = pjl, j, l = 1, . . . , k, l \u0338= j. The covariance\nmatrix is always symmetric positive semide\ufb01nite. However, in this case, we have\nmore: K is a positive-de\ufb01nite matrix due to the full rank assumption. In particular,\nit has the inverse K\u22121 and all its eigenvalues are positive.\nAssuming, as it is\ncustomary in statistics, that the origin of the Cartesian coordinate system coincides\nwith the centroid, Pearson de\ufb01ned the ellipsoid of residuals by the equation\n(2.1)\nk\nX\nj,l=1\nKjlxjxl = const.\nAs we will mention at the end of this Section, the Pearson ellipsoid of residuals is\ndual to the data ellipsoid.\nRemark 2.1. Pearson observed that the radius of the ellipsoid of residuals in\nany direction is equal to the inverse of the standard deviation of the orthogonal\nprojection of the points onto a hyperplane that is orthogonal to the line in the\ndirection of the given radius.\nDenote the eigenvalues of K as \u00b51 \u2265\u00b7 \u00b7 \u00b7 \u2265\u00b5k > 0.\n8\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nTheorem 2.1. [Pearson, [36]] The minimal mean square distance from a hy-\nperplane to the given set of N points is equal to the minimal eigenvalue of the\ncovariance matrix K. A best-\ufb01tting hyperplane contains the centroid and it is or-\nthogonal to the corresponding eigenvector of K. Thus, it is the principal coordinate\nhyperplane of the ellipsoid of residuals which is normal to the major axis.\nThen Pearson studied the lines which best \ufb01t to the given set of points and\nproved\nTheorem 2.2. [Pearson, [36]] The line which \ufb01ts best the given system of N\npoints contains the centroid and coincides with the minor axis of the ellipsoid of\nresiduals.\nPearson integrated the visualization of linear regression with orthogonal regres-\nsion in the planar case in [36], as shown in Fig. 4. The ellipse in Fig. 4 is dual\nto the ellipse of residuals, coinciding with the object studied by Galton. The main\ngoals of this paper are to generalize the aforementioned classical results of Pearson\nin the following directions.\nThe \ufb01rst goal: For a given system of N points in Rk, where k \u22652, and\nunder the full rank assumption, we consider all hyperplanes which equally \ufb01t the\ngiven system of points. In other words, for any \ufb01xed value s that is not less than\nthe smallest eigenvalue \u00b5k of the covariance matrix K, we consider all hyperplanes\nfor which the mean sum of square distances to the given set of points is equal to s.\nStarting from the ellipsoid of residuals, we e\ufb00ectively construct a pencil of confocal\nquadrics with the following property: For each s \u2265\u00b5k there exists a quadric from\nthe confocal pencil that serves as the envelope of all the hyperplanes which s-\ufb01t the\ngiven system of points.\nWe stress that neither the ellipsoid of residuals nor its dual, the data ellipsoid,\nbelongs to the confocal family of quadrics. The construction of this confocal pencil\nof quadrics is fully e\ufb00ective, though quite involved. The obtained pencil of confocal\nquadrics is going to have the same center as the ellipsoid of residuals, and moreover,\nthe same principal axes.\nExample 2.1. Let us recall that \u00b5k denotes the smallest eigenvalue of the\ncovariance matrix K. In the case s = \u00b5k, there is only one hyperplane that \ufb01ts\ns to the given set of N points. This is the best-\ufb01tting hyperplane described in\nTheorem 2.1. The envelope of this single hyperplane is the hyperplane itself. This\nhyperplane is going to be a degenerate quadric from our confocal pencil of quadrics.\nThe second goal: For a given system of N points in Rk, where k \u22652, and\nunder the full rank assumption, \ufb01nd the best \ufb01tting hyperplane under the condition\nthat it contains a selected point in Rk. We also provide an answer to the questions\nof the best \ufb01tting line under the condition that it contains a given point.\nA careful look at Galton\u2019s \ufb01gure (see Fig. 2) reveals an intriguing geometric\nfact that the line of linear regression of y on x intersects the ellipse at the points of\nvertical tangency, while the line of linear regression of x on y intersects the ellipse\nat the points of horizontal tangency. Further analysis of this phenomenon leads us\nto our third goal.\nThe third goal: to study linear regression in Rk in a coordinate free, invariant,\nform, see Theorem 5.3 and Corollary 5.3. We address the following question: for\na given direction and a given system of N points under the generality assumption,\nwhat is the best \ufb01tting hyperplane in the given direction among those that contain\na selected point in Rk?\nApparently, the second and the third goal are addressed using the same confocal\npencil of quadrics, which is constructed in relation to the \ufb01rst goal and mentioned\nabove.\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n9\nLet us conclude the introduction by observing that the ellipsoids reciprocal to\nthe Pearson ellipsoid of residuals, known as the data ellipsoids, have been studied\nin statistics, as seen in [14]. Detailed explanation of the data ellipsoid and the\nconcentration ellipsoid will be given in Section 3.3.\nThe notions of moments in statistics came from mechanics, where they were\noriginally introduced in three-dimensional space. We review these basic notions\nfrom a mechanics perspective in the Supplementary material.\n3. The envelopes of hyperplanes which equally \ufb01t the data in\nk-dimensional case\nIn this Section, we construct a pencil of confocal quadrics associated to a system\nof points in Rk. This pencil of confocal quadrics is going to be instrumental in\nachieving of all three of our main goals as listed in Section 2. In Section 3.4, we\npresent two main steps in our method of constructing the confocal pencil of quadrics:\nstarting from a system of points, we consider the central hyperplanar ellipsoid of\ninertia to which we attach the points for which the ellipsoid of inertia is rotational,\nas per De\ufb01nition 3.2.\nThen, using the obtained list of the attached points, we\nassign to them a confocal pencil of quadrics, in accordance with De\ufb01nition 3.1.\nThis eventually leads to the main result in this direction, Theorem 3.1.\nIn Section 3.5, we deal with the \ufb01rst applications of Theorem 3.1 in the regu-\nlarization for orthogonal regression and in the gradient descent method in machine\nlearning. The initial step is a dual formulation of Theorem 3.1, which is given as\nTheorem 3.2. Following that, methods of regularization for orthogonal regression,\nanalogues to the lasso and ridge for linear regression, are presented.\nWe initiate this Section by recalling the basic notions related to hyper-planar\nmoments of inertia and their corresponding operators.\n3.1. Hyperplanar moments of inertia. In rigid body dynamics, axial mo-\nments of inertia play an important role (see e.g. [5]). Let us recall that for a given\nline u \u2282R3{x, y, z}, the axial moment of inertia Iu is de\ufb01ned as Iu =\nN\nP\nj=1\nmjd2\nj,\nwhere dj is the distance from the point Mj to the line u, for j = 1, . . . , N.\nHowever, here we are studying the hyperplanar moments of inertia. The details\nabout the axial moments of inertia, their de\ufb01nition in higher dimensions, as well\nas their connection with the hyperplanar moments of inertia, are provided in the\nSupplement material, see also [17].\nNow we pass to hyperplanar moments of inertia.\nLet the system of points M1, ..., MN with masses m1, ..., mN be given in Rk.\nConsider a hyperplane \u03c0 in the same space Rk.\nThe hyperplanar moment of\ninertia for the system of points for the hyperplane \u03c0 is, by de\ufb01nition:\nJ\u03c0 =\nN\nP\ni=1\nmid2\ni ,where di is the perpendicular distance form the i-th point to the hyper-\nplane. The Huygens-Steiner Theorem (see e.g. [5]) gives a relationship between\nthe axial moments of inertia for two parallel axes, where one of them contains the\ncenter of masses. In the case of hyperplanar moments of inertia, a generalization of\nthe Huygens-Steiner theorem can be formulated as follows: J\u03c0 = J\u03c01 + md2, where\nit is assumed that the hyperplane \u03c01 contains the center of masses, while \u03c0 is a\nhyperplane parallel to \u03c01 at the distance d. Here m is the total mass of the system\nof points. The hyperplanar operator of inertia at the point O is de\ufb01ned here as a\nk-dimensional symmetric operator as follows: \u27e8JOn1, n2\u27e9=\nN\nP\nj=1\nmj\u27e8rj, n1\u27e9\u27e8rj, n2\u27e9,\nwhere rj is the radius vector of the point Mj. We see that J\u03c0 = \u27e8JOn, n\u27e9, where n\n10\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nis the unit vector orthogonal to the hyperplane \u03c0 which contains O. The diagonal\nelements of the hyperplanar inertia operator are the moments of inertia for the\ncoordinate planes: Jii = PN\nj=1 mj(x(j)\ni )2 is the hyperplanar moment of inertia for\nthe plane orthogonal to coordinate axis Oxi, where x(j)\ni\nis the i-th coordinate of\npoint Mj. The nondiagonal elements of the hyperplanar inertia operator are also\ncalled the centrifugal hyperplanar moments of inertia: Jil = PN\nj=1 mjx(j)\ni x(j)\nl .\nIn a principal basis, i.e. one where the hyperplanar operator of inertia has a\ndiagonal form, the diagonal elements are called the principal hyperplanar moments\nof inertia, and they are the hyperplanar moments of inertia for the coordinate\nhyperplanes: J1 =\nN\nP\nj=1\nmj(x(j)\n1 )2, J2 =\nN\nP\nj=1\nmj(x(j)\n2 )2, . . . , Jk =\nN\nP\nj=1\nmj(x(j)\nk )2. (In\nprincipal coordinates the centrifugal moments are zero). The hyperplanar ellipsoid\nof inertia at the point O is the ellipsoid\n\u27e8JOu, u\u27e9= 1,\nu \u2208Rk.\nLet X be a point on the hyperplanar ellipsoid of inertia centered at O. Let \u03c0 be a\nhyperplane that contains O and is orthogonal to OX. From the de\ufb01nition of the\nhyperplanar moment of inertia, it follows that:\n(3.1)\nJ\u03c0 = \u27e8JO\nOX\n||OX||,\nOX\n||OX||\u27e9=\n1\n||OX||2 .\nThus, the reciprocal value of the square of the distance OX is equal to the hyper-\nplanar moment of inertia for the hyperplane that contains O and that is orthogonal\nto OX. This implies that the ellipsoid of inertia at a point O completely determines\nthe moments of inertia at point O. This also means that the hyperplanar ellipsoid\nof inertia depends on O.\nIn principal coordinates, the hyperplanar ellipsoid of inertia at point O has the\nform J1x2\n1 + ... + Jkx2\nk = 1. The ellipsoid given by\n\u27e8J\u22121\nO u, u\u27e9= 1, u \u2208Rk.\nis referred to us as the hyperplanar ellipsoid of gyration at point O.\nWe also introduce the following de\ufb01nition: the ellipsoid given by\n\u27e8J\u22121\nO u, u\u27e9= m\u22121, u \u2208Rk\nis called the mass normalized hyperplanar ellipsoid of gyration at point O, where m\nis the sum of masses of all points from the given dataset.\nAn important special case is when point O coincides with the center of masses\ni.e. when O is the centroid C. Then, the hyperplanar operator of inertia at C is\ncalled the central hyperplanar operator of inertia, the principal hyperplanar mo-\nments of inertia at C are the central principal hyperplanar moments of inertia, and\nthe hyperplanar ellipsoid of inertia at point C is the central hyperplanar ellipsoid\nof inertia. Similarly, the hyperplanar ellipsoid of gyration at point C and the mass\nnormalized hyperplanar ellipsoid of gyration at point C is called the central hyper-\nplanar ellipsoid of gyration and the central mass normalized hyperplanar ellipsoid\nof gyration, respectively.\n3.2. Planar case and axial moments. The planar case (k = 2) is speci\ufb01c\nsince the axial and planar moments of inertia are both de\ufb01ned with respect to lines.\nThe axial moment of inertia and the planar moment of inertia for a line \u2113in the plane\ncoincide. By de\ufb01nitions, they are the sum of the products of the masses of points Mi\nand the square of the distances from the points Mi to \u2113. However, the associated\ninertia operators are not the same: one can check that in the standard basis of\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n11\nCartesian coordinates, the matrix of the planar inertia operator is the adjugate\nmatrix of the matrix of the axial inertia operator. Hence, for example, one has that\nI11 = J22. Thus, the axial moment of inertia for Ox-axis is I11 = PN\ni=1 miy2\ni , and\nthe planar moment of inertia for Ox-axis is J22 = PN\ni=1 miy2\ni .\nIn the k-dimensional case, one can de\ufb01ne the intermediate moments of inertia\nof s-dimensional planes for 1 \u2264s < k as well. They are studied in [17]. Speci\ufb01cally,\nfor s = 1, one gets an axial moment of inertia for the lines in k-dimensional space see\nSupplement material, Section 1. The hyperplanar moments of inertia correspond\nto the case s = k \u22121.\n3.3. A dictionary between mechanical and statistical terminologies.\nIn order to make the exposition of the paper more accessible to both statistically\nand mechanically oriented communities, we will provide a brief dictionary in the\nfollowing, interrelating the notions from Sections 2 and 3.1. To that end, we will\nsuppose here that all mi for 1 \u2264i \u2264N are equal (by setting mi = 1 for 1 \u2264i \u2264N\nor mi = 1/N for 1 \u2264i \u2264N). Then the mechanical terms discussed above have\ntheir statistical counterparts. We should also note though that having mi not all\nbeing equal also carries signi\ufb01cance not only in mechanics but in statistics as well,\nwhere also the concept of weights is sometimes applied.\nThe matrix of inertia of the operator JO coincides with the well-known notion\nin statistics of SSCP (sum of squares and cross-products) of the radius vectors. The\nprincipal moments of inertia are its eigenvalues. If O coincides with the centroid C,\nthen the matrix of the central hyperplanar inertia operator is the covariance matrix\nK, and the central hyperplanar ellipsoid of inertia is the ellipsoid of residuals. The\ndata ellipsoid becomes the hyperplanar ellipsoid of gyration.\nThe hyperplanar\nmoment of inertia Jjj for the coordinate hyperplane orthogonal to the axis Oxj\nis the variance \u03c32\nxj, while the centrifugal hyperplanar moments of inertia Jij are\ncovariances pij. In a central principal coordinate system, one gets that the central\nprincipal moments of inertia J1, ..., Jk coincide with the eigenvalues of K, \u00b5k, ..., \u00b51.\nIf a hyperplane \u03c0 contains C, then the hyperplanar moment of inertia J\u03c0 is the sum\nof the squared distances from the points of the given system to the hyperplane \u03c0. In\nthe statistical terminology, it represents the variance of the orthogonal projection\nonto \u03c0. Thus, one can view Pearson\u2019s note given in Remark 2.1 as a statistical\ninterpretation of the relation (3.1).\nIn [14] the concentration ellipsoid for a given system of points in Rk is de\ufb01ned\nas the ellipsoid with the following property: the uniform distribution inside the\nconcentration ellipsoid has the same \ufb01rst and second order moments as the given\nsystem of points. The concentration ellipsoid has the equation [14]:\nk\nX\nj,l=1\nK\u22121\njl xjxl = k + 2.\nWhen the constant k + 2 is replaced with an arbitrary positive constant c, a fam-\nily of homothetic ellipsoids is obtained. This family coincides with the ellipsoids\nhomothetic to the hyperplanar ellipsoid of gyration de\ufb01ned below (see[14] for the\nplanar case).\nIn the planar case when k = 2 the ellipsoid of concentration be-\ncomes the ellipse of concentration P2\nj,l=1 K\u22121\njl xjxl = 4. As explained in Section\n3.2, the planar ellipse of inertia and axial ellipse of inertia are dual to each other:\nthe matrix of the planar inertia operator is the adjugate matrix of the matrix of the\naxial inertia operator. In the planar case, the family of ellipses homothetic to the\nconcentration ellipse coincides with the family of ellipses homothetic to the axial\nellipses of inertia, as it is pointed out in [14]. In our paper, the ellipsoid from the\n12\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nfamily of homothetic ellipsoids, given by\nk\nX\nj,l=1\nK\u22121\njl xjxl = 1.\nwill be also referred as the data ellipsoid.\nThe above Pearson results can be naturally restated in terms of the data el-\nlipsoid as well. In particular, in the two-dimensional case, the Galton ellipses are\nexamples of data ellipsoids, and they are, as we mentioned above, dual to ellipses of\nresiduals of Pearson. The data ellipsoids in the two-dimensional case are naturally\ncalled the data ellipses; they are presented in Fig. 2 and 4.\n3.4. Construction of the confocal pencil of quadrics associated to the\ndata in Rk. A confocal pencil of conics in plane is fully determined by two points:\nthe common focal points of all the conics from the pencil. It is quite challenging to\npresent any analogous \u201cfocal set\u201d in arbitrary dimension that would geometrically\nde\ufb01ne a confocal pencil of quadrics through some sort of a rope construction (see\n[17] and references therein). For our purposes, we consider the following set ([17]).\nDefinition 3.1. Let a (k \u22121)-dimensional ellipsoid be given by\n(3.2)\nx2\n1\n\u03b11\n+ ... + x2\nk\n\u03b1k\n= 1,\nwhere \u03b11 > ... > \u03b1k > 0.\nWe say that the set of 2k \u22122 collinear generalized\nfocal points F1i(\u221a\u03b11 \u2212\u03b1i, 0, ..., 0), F2i(\u2212\u221a\u03b11 \u2212\u03b1i, 0, ..., 0), i = 2, ..., k, organized\nin (k \u22121) pairs of points symmetric with respect to one given point is assigned to\nthe ellipsoid (3.2).\nAll these assigned points belong to the coordinate axis Ox1, which contains\nthe major semi-axis of the ellipsoid and remains the same for all quadrics from the\nconfocal pencil\n(3.3)\nx2\n1\n\u03b11 \u2212\u03bb + ... +\nx2\nk\n\u03b1k \u2212\u03bb = 1.\nLet a dataset of full rank in Rk be given. Suppose that the central principal\nmoments of inertia satisfy 0 < J1 < J2 < ... < Jk.\nWe de\ufb01ne a2\n2, a2\n3, ..., a2\nk by\nJ1 + ma2\n2 = J2,\nJ1 + ma2\n3 = J3, ..., J1 + ma2\nk = Jk. From the last formulas, we\nobtain\n(3.4)\nJ1 = Jk \u2212ma2\nk, J2 = Jk + ma2\n2 \u2212ma2\nk, . . . ,\nJk\u22121 = Jk + ma2\nk\u22121 \u2212ma2\nk.\nImportant properties of the points with rotational ellipsoid of inertia were re-\ncently studied in [17].\nDefinition 3.2. For the central hyperplanar ellipsoid of inertia\n(3.5)\nJ1x2\n1 + ... + Jkx2\nk = 1\nlet us attach the points F1i(ai, 0, ..., 0) and F2i(\u2212ai, 0, ..., 0), for i = 2, ..., k, where\n\u00b1ai are determined from (3.4).\nAs a consequence of the generalization of the Huygens-Steiner theorem, the\nattached points from De\ufb01nition 3.2 are characterized by the following property:\nthey lie on the line of the major axis of the central ellipsoid of inertia, where two\nprincipal moments of inertia coincide. In other words, these are the points on the\nmajor axis of the central ellipsoid of inertia at which the hyperplanar ellipsoid of\ninertia becomes rotational.\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n13\nNow, let us construct the confocal pencil of quadrics for which these points\nattached to the central hyperplanar ellipsoid of inertia are assigned points in the\nsense of De\ufb01nition 3.1:\n(3.6)\nx2\n1\nJ1\nm \u2212\u03bb +\nx2\n2\nJ1\nm \u2212a2\n2 \u2212\u03bb + ... +\nx2\nk\nJ1\nm \u2212a2\nk \u2212\u03bb = 1.\nThus, we construct the pencil of confocal quadrics (3.6) beginning with the\ncentral hyperplanar ellipsoid of inertia (3.5) in a highly nontrivial manner: \ufb01rst,\nwe attach points with a rotational ellipsoid of inertia, and then we assign to them\nthe confocal pencil of quadrics. The resulting confocal pencil of quadrics possesses\na remarkable property concerning the initial system of points that de\ufb01ned the hy-\nperplanar ellipsoid of inertia.\nTheorem 3.1. Given a system of points in Rk with a total mass m and central\nprincipal hyperplanar moments of inertia 0 < J1 < J2 < \u00b7 \u00b7 \u00b7 < Jk, along with\nthe center of masses C, consider the family of hyperplanes for which the system\nof points exhibits the same hyperplanar moment of inertia.\nThe envelope of all\nhyperplanes within this family, that do not contain the origin, is the quadric from\nthe pencil of confocal quadrics:\n(3.7)\nx2\n1\nA2 +\nx2\n2\nA2 \u2212a2\n2\n+ ... +\nx2\nk\nA2 \u2212a2\nk\n= 1,\nwhere\na2\ns = Js \u2212J1\nm\n,\ns = 2, . . . , k.\nAll hyperplanes within this family, that contain the origin, are asymptotically\ntangent at in\ufb01nity to the same quadric (3.7) from the pencil of confocal quadrics.\nProof. Formula (3.7) is obtained from (3.6) by substitution\n(3.8)\nA2 = J1\nm \u2212\u03bb.\nGiven a hyperplane \u03c01 at the distance d from the center of masses C, let\nn(\u03c01) = (n1, n2, ...nk) be the unit vector orthogonal to the hyperplane \u03c01. The\nhyperplanar moment of inertia is given by\nJ\u03c01 = J1n2\n1 + .... + Jkn2\nk + md2\n= Jk\nk\nX\nj=1\nn2\nj + m(d2 + a2\n2n2\n2 + a2\n3n2\n3 + ... + a2\nk\u22121n2\nk\u22121 \u2212a2\nk\nk\u22121\nX\nj=1\nn2\nj)\n= Jk + m(d2 + a2\n2n2\n2 + a2\n3n2\n3 + ... + a2\nk\u22121n2\nk\u22121 + a2\nkn2\nk \u2212a2\nk).\nFor all the hyperplanes that share the same hyperplanar moment of inertia, the\nvalue\n(3.9)\nSk(\u03c01) = d2 + a2\n2n2\n2 + a2\n3n2\n3 + ... + a2\nk\u22121n2\nk\u22121 + a2\nkn2\nk \u2212a2\nk\nhas to remain constant, regardless of the choice of \u03c01.\nGiven an arbitrary tangent hyperplane \u03c02 at an arbitrary point (x10, x20, ..., xk0)\nof the quadric (3.7), its equation is as follows:\nx1x10\nA2\n+\nx2x20\nA2 \u2212a2\n2\n+ ... +\nxkxk0\nA2 \u2212a2\nk\n\u22121 = 0,\nwhere the unit normal vector is\nn(\u03c02) = 1\n\u2206\n\u0010x10\nA2 ,\nx20\nA2 \u2212a2\n2\n, ...,\nxk0\nA2 \u2212a2\nk\n\u0011\n,\n14\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nand\n\u22062 = x2\n10\nA4 +\nx2\n20\n(A2 \u2212a2\n2)2 + ... +\nx2\nk0\n(A2 \u2212a2\nk)2 .\nHere, \u2206\u22121 also represents the distance from the center of masses C to the hyper-\nplane \u03c02. For Sk(\u03c02) we have:\nSk(\u03c02) = 1\n\u22062\n\u0010\n1 +\na2\n2x2\n20\n(A2 \u2212a2\n2)2 + ... +\na2\nkx2\nk0\n(A2 \u2212a2\nk)2\n\u0011\n\u2212a2\nk\n= 1\n\u22062\n\u0010x2\n10\nA2 +\nx2\n20\nA2 \u2212a2\n2\n+ ... +\nx2\nk0\nA2 \u2212a2\nk\n+\na2\n2x2\n20\n(A2 \u2212a2\n2)2 + ... +\na2\nkx2\nk0\n(A2 \u2212a2\nk)2\n\u0011\n\u2212a2\nk\n= 1\n\u22062 A2\u0010x2\n10\nA4 +\nx2\n20\n(A2 \u2212a2\n2)2 + ... +\nx2\nk0\n(A2 \u2212a2\nk)2\n\u0011\n\u2212a2\nk = A2 \u2212a2\nk.\nIf Sk(\u03c02) > 0, then Sk(\u03c02) is equal to the square of the smallest semi-axis of the\nquadric (3.7). Similarly, if Sk(\u03c02) < 0, then Sk(\u03c02) is equal to the negative square\nof the smallest semi-axis of the quadric (3.7). This implies that Sk = Sk(\u03c0) remains\nthe same for those hyperplanes \u03c0 which are tangent to the quadric (3.7), where the\npencil parameter A is given by:\nA2 = Sk + a2\nk = Sk + Jk \u2212J1\nm\n.\nThis proves the theorem.\n\u25a1\nThe type of the enveloping quadric depends on the value of the parameter A.\nHere, J\u03c0 = Jk + mSk = Jk + m(A2 \u2212a2\nk), or A2 = J\u03c0\u2212J1\nm\n. From the last formula,\nit follows that hyperplanes with a hyperplanar moment of inertia equal to J\u03c0 are\ntangent to the following quadric from the confocal pencil (3.6):\n(3.10)\nx2\n1\nJ\u03c0 \u2212J1\n+\nx2\n2\nJ\u03c0 \u2212J2\n+ ... +\nx2\nk\nJ\u03c0 \u2212Jk\n= 1\nm.\nThus, in terms of the value of the hyperplanar moment of inertia J\u03c0, one gets\nthe following information about the type of the enveloping quadric:\n\u2022 If J\u03c0 > Jk, then the enveloping quadric is a k-dimensional ellipsoid.\n\u2022 If Ji < J\u03c0 < Ji+1, then the enveloping quadric belongs to the i-th type\nquadrics.\n\u2022 In the case J\u03c0 < J1, there is no hyperplane with a hyperplanar moment\nof inertia equal to J\u03c0.\n\u2022 If J\u03c0 = Ji, for any i = 1, . . . , k, then the enveloping quadric is degenerate,\nand the hyperplane \u03c0 coincides with the enveloping quadric as a coordinate\nhyperplane.\nExample 3.1. We can consider the case k = 2 of Theorem 3.1. For a given\nsystem of points in R2, let C denote the center of masses, and F1 and F2 be the\ntwo points with a circle as the ellipse of inertia.\nThen any conic with the foci\nF1 and F2 has the property that for any of its tangents, \u03c01, \u03c02, and \u03c03, it holds:\nJ\u03c01 = J\u03c02 = J\u03c03, see Fig. 5.\nExample 3.2 ([17]). Given a system of points of full rank in three-dimensional\nspace with the central principal planar moments of inertia J1 < J2 < J3, consider\nthe family of planes for which the system of points has the same planar moment of\ninertia, equal J\u03c0. All planes from the family are tangent to the same quadric from\nthe pencil of confocal quadrics, which is:\n(i) if J\u03c0 > J3, then the envelope is an ellipsoid;\n(ii) if J2 < J\u03c0 < J3, then the envelope is a one-sheeted hyperboloid;\n(iii) if J1 < J\u03c0 < J2, then the envelope is a two-sheeted hyperboloid;\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n15\nF2\nF1\nC\n\u03c02\n\u03c03\n\u03c01\nFigure 5. Theorem 3.1, case k = 2: J\u03c01 = J\u03c02 = J\u03c03\n(iv) if J\u03c0 < J1, then there is no plane with the planar moment of inertia equal\nto J\u03c0;\n(v) if J\u03c0 = Ji, then for i = 1, 2, 3, the envelope is the corresponding principal\ncoordinate plane.\nA trace of the statement of Theorem 3.1 in the three-dimensional case can be\nretrieved from [29]. However, the presentation in [29] was very vague and unusual\neven by the standards of that historic period. It was written more like a report or\nan essay rather than as a scienti\ufb01c paper, as it did not provide clear statements,\nformulas, or proofs of the main assertions.\nProposition 3.1. Given a system of points in Rk with a total mass m and\ncentral principal hyperplanar moments of inertia 0 < J1 < J2 < \u00b7 \u00b7 \u00b7 < Jk, along\nwith the center of masses C, consider all hyperplanes \u03c0 that contain the origin\n\u03c0 : \u03b21x1 + \u00b7 \u00b7 \u00b7 + \u03b2kxk = 0.\nAll such hyperplanes with the same hyperplanar moment J\u03c0 = \u00b5 for a given value\n\u00b5 are parameterized as follows:\n(3.11)\n(J1 \u2212\u00b5)\u03b22\n1 + . . . (Jk \u2212\u00b5)\u03b22\nk = 0.\n3.5. Applications in the regularization of the orthogonal least square\nmethod. Using duality between quadrics of points and quadrics of tangential hy-\nperplanes (see, for example, [5, 21]) , we can reformulate the above Theorem 3.1\nas follows:\nTheorem 3.2. Given a system of points in Rk with a total mass of m and\ncentral principal hyperplanar moments of inertia 0 < J1 < J2 < \u00b7 \u00b7 \u00b7 < Jk, consider\nthe family of hyperplanes for which the system of points has the same hyperplanar\nmoment of inertia. All the hyperplanes from this family which do not contain the\norigin form a quadric. By varying the hyperplanar moment of inertia, the resulting\nquadrics form a linear pencil that is dual to the confocal pencil (3.6).\nThis dual formulation, along with Proposition 3.1, can be used to study non-\nlinear constraints on the hyperplanes of regression. Such situations may arise in\nregularization problems for the orthogonal least square method. In the case of the\nregularization of the standard linear regression, there are various shrinkage meth-\nods developed (see e.g. [31]). For example the ridge or the lasso methods depend\non the type of the norm used to bound the coe\ufb03cients of the hyperplanes. Here\nwe develop similar methods for the orthogonal least square. A ridge-type method\nimposes an L2 bound on the coe\ufb03cients \u03b21, . . . , \u03b2k of the hyperplanes: ||\u03b2||2 \u2264s.\nA lasso-type method assumes the use of the L1 norm and the condition on the\ncoe\ufb03cients \u03b21, . . . , \u03b2k of the hyperplanes can be written as: ||\u03b2||1 \u2264t. The best-\ufb01t\nhyperplane under each of these conditions is determined as the point of tangency\n16\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nof a quadric from the linear pencil from Theorem 3.2 and Proposition 3.1 and the\nL2 circle of radius s in the \ufb01rst case and the L1 circle (aka the diamond) of radius\nt in the second case, see Fig. 6.\nFigure 6. Regularization for the orthogonal least square method:\non the left, L2 or ridge-type case; on the right, L1 or lasso-type\ncase. See Remark 3.1.\nThe situation of the lasso and ridge in classical linear regression was presented\nin Figure 3.11 of [31]. In linear regression, the family of quadrics presents a set\nof homothetic quadrics de\ufb01ned by equation (3.51) from [31], see also Propositions\n5.3 and 5.4. On the other hand, in the orthogonal least square situation developed\nhere, a linear pencil of quadrics is used from Theorem 3.2 and Proposition 3.1 We\ndevote separate publications [18, 19] to a detailed study of geometric aspects of\nthe lasso and its analogue in the orthogonal regression.\nRemark 3.1. A linear pencil of quadrics is generated by two quadrics. In the\ncomplex projective plane, a linear pencil of conics is generated by two conics, as\nseen in Fig. 6. A generic linear pencil of conics consists of all conics that share four\ncommon points, see e.g. [21].\nAnother application of the methods developed in this paper is related to gradi-\nent descent in machine learning. The optimization algorithm may not be guaranteed\nto arrive at the minimum in a reasonable amount of time. However, as noted in\n[30], it often reaches some quite low value of the cost function (here the hyperplanar\nmoment) equal to s0 quickly enough to be useful. In application to the orthogonal\nleast square, from Theorem 3.1, we know that the hyperplanes which generate the\nhyperplanar moment equal to s0 are all tangent to the given quadric from the pencil\n(3.6); the pencil parameter is determined through s0.\n4. Hyperplanes containing a given point which best \ufb01t the data in Rk.\nRestricted PCA\nAs a further employment of the confocal pencil of quadrics which we constructed\nin Section 3.4, associated to a given system of points in Rk, see Theorem 3.1, we\nare going to reach our second goal as listed in Section 2. We also formulate and\nsolve the Restricted PCA, restricted at a point, using the same confocal pencil\nof quadrics as a universal tool, see Theorem 4.1. In Example 4.2, we apply the\ndeveloped methods in a natural situation of an errors-in-variables model. We show\nhow it e\ufb00ectively works in testing a hypothesis that the line of the best \ufb01t contains\na given point. We derive a new formula for test statistics in restricted orthogonal\nleast square method in Theorem 4.4.\nLet the system of points M1, ..., MN with masses m1, ..., mN be given in Rk and\nlet C be the center of masses. We consider the following problem: For a given point\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n17\nP(x01, ..., x0k) in Rk \ufb01nd the principal hyperplanes of the hyperplanar ellipsoid of\ninertia at the point P. We also \ufb01nd the hyperplane that minimizes the hyperplanar\nmoment of inertia among the hyperplanes that contain P.\nThe family of confocal quadrics (3.6) which we associated with the given system\nof points, provides the solution to the above problem in the following way. Assume\nthat the principal axes at the point C are chosen as the coordinate axes.\nLet\nthe hyperplane \u03c0 containing the point P be given, with n = (n1, n2, ..., nk), as\nthe unit normal vector to the hyperplane.\nUsing the Huygens-Steiner theorem,\nthe moment of inertia for the hyperplane \u03c0 is J\u03c0 = J\u03c0C + md2, where J\u03c0C is the\nmoment of inertia for the hyperplane \u03c0C, where \u03c0C contains the center of masses\nC and is parallel to the hyperplane \u03c0. Let d be the distance from C to \u03c0. The\nsquare of the distance is given by formula d2 = (x01n1 + ... + x0knk)2. One has\nJ\u03c0 = J1n2\n1 + ... + Jkn2\nk + md2 = J1n2\n1 + ... + Jkn2\nk + m(x01n1 + ... + x0knk)2. The\ndirection of the principal axes at the point P can be obtained as the stationary\npoints of the function J\u03c0(n1, ..., nk). Since n2\n1 + ... + n2\nk = 1, the stationary points\ncan be obtained using the Lagrange multipliers. As usual, consider the function\n(4.1)\nF(n1, ..., nk, \u00b5) = J1n2\n1 + ... + Jkn2\nk + m(x01n1 + ...\n+ x0knk)2 \u2212\u00b5(n2\n1 + ... + n2\nk \u22121),\nwhere \u00b5 is the Lagrange multiplier. We get the stationary points from the condition\nthat the partial derivatives of F are equal to zero. One gets the system:\n(4.2)\n(JP \u2212\u00b5E)n = 0,\nn2\n1 + ... + n2\nk \u22121 = 0,\nwhere JP is the planar inertia operator at the point P, given by\n(4.3)\nJP =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nJ1 + mx2\n01\nmx01x02\n...\nmx01x0k\nmx02x01\nJ2 + mx2\n02\n...\nmx02x0k\n.\n.\n.\nmx0kx01\nmx0kx02\n...\nJk + mx2\n0k\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n.\nThe \ufb01rst part in (4.2) is a homogeneous system of k equations in n1, ..., nk with\n\u00b5 as a parameter.\nThe nontrivial solutions exist under the condition that the\ndeterminant of the system vanishes. This determinant is equal to the characteristic\npolynomial of the symmetric matrix JP , with roots \u00b51, ..., \u00b5k being the eigenvalues\nof the matrix JP . They are the principal planar moments of inertia at the point P.\nEach eigenvalue \u00b5j has a corresponding eigenvector nj, which is the unit normal\nvector of a principal hyperplane. Since JP is symmetric, all eigenvalues are real,\nand corresponding eigenvectors n(1), ..., n(k) are mutually orthogonal.\nLet us look at the system (4.2) in another way.\nFirst k equations can be\nrewritten as:\nmx01\n\u00b5 \u2212J1\n=\nn1\nn1x01 + ... + nkx0k\n,\n. . . ,\nmx0k\n\u00b5 \u2212Jk\n=\nnk\nn1x01 + ... + nkx0k\n.\nBy multiplying j-th equation by x0j and adding all of them, we come to the confocal\npencil of quadrics:\n(4.4)\nx2\n01\n\u00b5 \u2212J1\n+\nx2\n02\n\u00b5 \u2212J2\n+ ... +\nx2\n0k\n\u00b5 \u2212Jk\n= 1\nm.\nEach eigenvalue \u00b5j of the matrix JP corresponds to one quadric from (4.4) which\npasses through the point P. Denote by XP the k \u00d7 N matrix of the data related\n18\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nto the point P\nXP =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\u221am1(x(1)\n1\n\u2212x01)\n\u221am2(x(2)\n1\n\u2212x01)\n...\n\u221amN(x(N)\n1\n\u2212x01)\n\u221am1(x(1)\n2\n\u2212x02)\n\u221am2(x(2)\n2\n\u2212x02)\n...\n\u221amN(x(N)\n2\n\u2212x02)\n.\n.\n.\n\u221am1(x(1)\nk\n\u2212x0k)\n\u221am2(x(2)\nk\n\u2212x0k)\n...\n\u221amN(x(N)\nk\n\u2212x0k)\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n.\nHere (x(j)\n1 , . . . , x(j)\nk ) denotes the coordinates of the point Mj, for j = 1, . . . , N. We\nhave JP = XP XT\nP . We can formulate and solve the restricted principal component\nanalysis (RPCA) restricted at the point P as follows: If n is a norm 1 vector in\nRk, the variance of nT XP is nT JP n. We search for the maximal variance among\nunit vectors, noncorrelated with the previous ones.\nThe determination of the maximal variance of nT XP among all unit vectors\nleads to the conditional extremum problem exactly as stated above, see (4.1) and\n(4.2). Thus, the above construction and the confocal pencil of quadrics provide a\nuniversal tool to solve Restricted PCA restricted at a given point P, for any point\nP. Finally, we have the following\nTheorem 4.1. Let the system of points M1, ..., MN with masses m1, ..., mN be\ngiven in Rk. For any point P(x01, ..., x0k) there are k mutually orthogonal confocal\nquadrics from the confocal pencil (4.4) that contain the point P. The k tangent\nhyperplanes to these quadrics are the principal hyperplanes of inertia at the point\nP.\nThe obtained principal coordinate axes are the principal components solving\nRestricted PCA, restricted at the point P, i.e. providing the maximum variance\namong the normalized combinations nT XP , uncorrelated with previous ones.\nIn the three-dimensional case, a similar result for the axial moments of inertia\ncan be found in the classical book by Suslov [39]. Let us observe that the pencils\nof quadrics (4.4) and (3.6) coincide. The correspondence of the pencils is realized\nthrough the formula:\n(4.5)\n\u00b5 = 2J1 \u2212m\u03bb.\nWe can reformulate Theorem 4.1 in the following way:\nTheorem 4.2. Let the system of points M1, ..., MN with masses m1, ..., mN be\ngiven in Rk with the associated pencil of confocal quadrics (3.6). For any point\nP(x01, ..., x0k) denote its Jacobi coordinates of the pencil of confocal quadrics (3.6)\nas (\u03bb1 < \u00b7 \u00b7 \u00b7 < \u03bbk). The k tangent hyperplanes to the quadrics with parameters\n(\u03bb1 < \u00b7 \u00b7 \u00b7 < \u03bbk) are the principal hyperplanes of inertia at the point P.\nAs a consequence, we get the following generalization of the Pearson Theorem\n2.1.\nTheorem 4.3. Let the system of points M1, ..., MN with masses m1, ..., mN be\ngiven in Rk with the associated pencil of confocal quadrics (3.6). For any point\nP(x01, ..., x0k) denote its Jacobi coordinates of the pencil of confocal quadrics (3.6)\nby (\u03bb1 < \u00b7 \u00b7 \u00b7 < \u03bbk).\nThe pencil parameters \u03bb1, ..., \u03bbk determine the k orthogo-\nnal quadrics from the pencil containing P. The tangent planes at the point P to\nthese quadrics are the critical points of the hyperplanar moment with respect the\nhyperplanes which contain P.\nThe hyperplane which is the best \ufb01t to the given system of points among the\nhyperplanes that contain the point P is the tangent hyperplane to the quadric from\nconfocal pencil (3.6) with the pencil parameter \u03bbk. Similarly, the hyperplane which\nis the worst \ufb01t to the given system of points among the hyperplanes that contain\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n19\nthe point P is the tangent hyperplane to the quadric from (3.6) with the pencil\nparameter \u03bb1.\nFigure 7. With Corollary 4.1 for k = 2: The ellipse and hyper-\nbola from the confocal pencil passing though P. The tangent t1 to\nthe ellipse at P is the worst \ufb01t among all the lines containing P,\nwhile t2, the tangent to the hyperbola at P is the best \ufb01t among all\nsuch lines. The tangents t1, t2 solve RPCA restricted at the point\nP.\nProof. The proof follows from Theorem 4.2. The principal hyperplanes of\ninertia at the point P are tangent hyperplanes to the quadrics from the confocal\npencil corresponding to the Jacobi coordinates of the point P. The Jacobi coordi-\nnates are related to the eigenvalues \u00b5 according to the formula (4.5). The largest\nJacobi coordinate \u03bbk corresponds to the minimal eigenvalue \u00b51 and the smallest\nJacobi coordinate \u03bb1 corresponds to the largest eigenvalue \u00b5k. Comparing formulas\n(4.4) and (3.10), we see that the smallest eigenvalue \u00b51 corresponds to the minimal\nhyperplanar moment J\u03c01. Similarly, the largest eigenvalue \u00b5k corresponds to the\nmaximal hyperplanar moment J\u03c0k. Thus the smallest Jacobi coordinate \u03bb1 corre-\nsponds to the maximal hyperplanar moment J\u03c0k and the largest Jacobi coordinate\n\u03bbk corresponds to the minimal hyperplanar moment J\u03c01. Thus, \u03c01 is the tangent\nhyperplane at P to the quadric with the parameter \u03bbk, which is the hyperplane of\nthe best \ufb01t among all the hyperplanes containing P. Similarly, \u03c0k is the tangent\nhyperplane at P to the quadric with the parameter \u03bb1. This quadric is an ellipsoid\nand its tangent hyperplane at P is the hyperplane of the worst \ufb01t among all the\nhyperplanes containing P.\n\u25a1\nWe now formulate low-dimensional specializations of the last Theorem 4.3.\nCorollary 4.1. Let the system of points M1, ..., MN with masses m1, ..., mN\nbe given in Rk with the associated pencil of confocal quadrics (3.6). Given a point\nP.\n\u2022 For k = 2 let the point P have the Jacobi coordinates (\u03bb1 < \u03bb2). The line\nwhich is the best \ufb01t for the given system of points among the lines which\ncontain P is the tangent line to the hyperbola with the parameter \u03bb = \u03bb2.\nThe line which is the worst \ufb01t for the given system of points among the\nlines which contain P is the tangent line to the ellipse with the parameter\n\u03bb = \u03bb1. The tangents t1, t2 solve RPCA restricted at the point P. (See\nFig. 7.)\n\u2022 For k = 3 let the point P have the Jacobi coordinates (\u03bb1 < \u03bb2 < \u03bb3).\nThe plane which is the best \ufb01t for the given system of points among the\nplanes which contain P is the tangent plane to the two-sheeted hyperboloid\n20\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nwith the parameter \u03bb = \u03bb3. The plane which is the worst \ufb01t for the given\nsystem of points among the planes which contain P is the tangent plane\nto the ellipsoid with the parameter \u03bb = \u03bb1. The lines of intersection of\neach two of the three tangent planes solve RPCA restricted at the point\nP.\nProof. Let us provide the proof in the case k = 2. The matrix of the planar\ninertia operator at the point P in given coordinates has the form\nJP =\n\u0012\nJx\nJxy\nJxy\nJy\n\u0013\n,\nwhere\nJx =\nN\nX\ni=1\nmi(xi \u2212xP )2, Jy =\nN\nX\ni=1\nmi(yi \u2212yP )2,\nJxy =\nN\nX\ni=1\nmi(xi \u2212xP )(yi \u2212yP ).\nThe extremal planar moments of inertia are the eigenvalues of the matrix JP , while\nthe corresponding eigenvectors are orthogonal to the required extremal lines. Thus,\nJP1 and JP2 are the solutions of the quadratic equation\n\u00b52 \u2212(Jx + Jy)\u00b5 + JxJy \u2212J2\nxy = 0.\nOne gets that the maximal and the minimal planar moments of inertia are respc-\ntively\nJP1 = Jx + Jy +\n\u221a\nD\n2\n,\nJP2 = Jx + Jy \u2212\n\u221a\nD\n2\n,\nwhere D = (Jx \u2212Jy)2 + 4Jxy.\nAfter calculating the corresponding eigenvectors, one derives the equations of\nthe extremal lines. The equation of the line of the worst \ufb01t among the lines that\ncontain P is\n(4.6)\n\u22122Jxy(x \u2212xP ) + (Jx \u2212Jy \u2212\n\u221a\nD)(y \u2212yP ) = 0.\nThe equation of the line of the that best \ufb01t is\n(4.7)\n(Jy \u2212Jx +\n\u221a\nD)(x \u2212xP ) \u22122Jxy(y \u2212yP ) = 0.\nFrom\nJP1 = 2J1 \u2212m\u03bb2,\nJP2 = 2J1 \u2212m\u03bb1.\nwe get\n\u03bbi = 2J1 \u2212JPj\nm\n,\n(i, j) \u2208{(1, 2), (2, 1)}.\nThe line (4.6) is the tangent at P to the ellipse from the confocal pencil, which\npasses through P and has the pencil parameter \u03bb = \u03bb1.\nThe line (4.7) is the\ntangent at P to the hyperbola from the confocal pencil, which passes through P\nand has the pencil parameter \u03bb = \u03bb2.\n\u25a1\nExample 4.1. Let N points with masses m1, ..., mN be given in the plane. Let\nus also \ufb01x a point P in the plane. We will give the explicit formulas for the lines\nthat are the best and the worst \ufb01t for the given system of N points among the lines\nthat contain the given point P, using the confocal pencil of conics (3.6) associated\nto the given system of N points. Let C be the centroid of the system of N points.\nOn our way, we provide the solution to the Restricted PCA at the point P, see\n(4.10).\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n21\nDenote Cxy the principal coordinate system of the confocal pencil and J1\nand J2 as before, denote the principal planar moments of the inertia such that\nJ1 + ma2\n2 = J2. The confocal pencil of conics has the form\n(4.8)\nx2\n\u03b1 \u2212\u03bb +\ny2\n\u03b2 \u2212\u03bb = 1,\n\u03b1 = J1\nm ,\n\u03b2 = J1\nm \u2212a2\n2.\nFor the point P(xP , yP ), its Jacobi elliptic coordinates \u03bb1 < \u03bb2 are the solutions of\nthe quadratic equation\n(4.9)\n\u03bb2 \u2212(\u03b1 + \u03b2 \u2212x2\nP \u2212y2\nP )\u03bb + \u03b1\u03b2 \u2212\u03b2x2\nP \u2212\u03b1y2\nP = 0.\nThe formula (4.5) gives the connection of the extremal values JP1 and JP2\nof the principal planar inertia operator at the point P with the Jacobi elliptic\ncoordinates \u03bb1 and \u03bb2 of the point P, associated with the pencil of confocal conics:\nJP1 = 2J1 \u2212m\u03bb2,\nJP2 = 2J1 \u2212m\u03bb1. We have already mentioned that the line\nthat is the best \ufb01t among the lines that contain the point P is the tangent to the\n\u03bb2-coordinate line through P, which is a hyperbola. The line that is the worst \ufb01t\namong those that contain P is the tangent to the \u03bb1-coordinate line though P, and\nthis is an ellipse. In order to \ufb01nd the equations of these lines of the best and wort\n\ufb01t, let us write the coordinate transformation between the Cartesian coordinates\nand the Jacobi elliptic coordinates (see [5, 21])\nx2 = (\u03b1 \u2212\u03bb1)(\u03b1 \u2212\u03bb2)\n\u03b1 \u2212\u03b2\n,\ny2 = (\u03b2 \u2212\u03bb1)(\u03b2 \u2212\u03bb2)\n\u03b2 \u2212\u03b1\n.\nHaving in mind that the basal coordinate vectors are \u02dcn(i) =\n\u0010\n\u2202x\n\u2202\u03bbi , \u2202y\n\u2202\u03bbi\n\u0011\nfor i = 1, 2\none gets\n(4.10)\n\u02dcn(1) = \u22121\n2\n\u0010\n\u03b1 \u2212\u03bb2\nxP (\u03b1 \u2212\u03b2) ,\n\u03b2 \u2212\u03bb2\nyP (\u03b2 \u2212\u03b1)\n\u0011\n,\n\u02dcn(2) = \u22121\n2\n\u0010\n\u03b1 \u2212\u03bb1\nxP (\u03b1 \u2212\u03b2) ,\n\u03b2 \u2212\u03bb1\nyP (\u03b2 \u2212\u03b1)\n\u0011\n,\nwhere it is supposed that \u03bb1 and \u03bb2 are functions of xP and yP obtained from (4.9).\nThe pair (\u02dcn(1), \u02dcn(2)) from (4.10) is the solution of the Restricted PCA at the point\nP.\nOne \ufb01nally gets the equations of the line that is the best \ufb01t:\n(4.11)\n\u03b1 \u2212\u03bb2\nxP\n(x \u2212xP ) \u2212\u03b2 \u2212\u03bb2\nyP\n(y \u2212yP ) = 0.\nThe equations of the line that is the worst \ufb01t is obtained analogously:\n(4.12)\n\u03b1 \u2212\u03bb1\nxP\n(x \u2212xP ) \u2212\u03b2 \u2212\u03bb1\nyP\n(y \u2212yP ) = 0.\nExample 4.2. Two types of cells in a fraction of the spleens, see [27], [26],\n[12], [11]. The data in this example are the numbers of two types of cells in a\nspeci\ufb01ed fraction of the spleens of fetal mice. They are displayed in Table 1. On\nthe basis of sampling, it is reasonable to assume the original counts to be Poisson\nrandom variables, as explained in [12]. Therefore, the square roots of the counts are\ngiven in the last two columns of the table and they have, approximately, constant\nvariance equal to T \u22121 = 1/4.\nThe postulated model is yt = \u03b20 + \u03b21xt, where\n(Yt, Xt) = (yt, xt)+(et, ut). Here Yt is the square root of the number of cells forming\nrosettes for the t-th individual, and Xt is the square root of the number of nucleated\ncells for the t-th individual. On the basis of the sampling, the pair of errors (et, ut)\nhave a covariance matrix that is, approximately, \u03a3 = T \u22121E = diag(0.25, 0.25), with\nT = 4. The square roots of the counts cannot be exactly normally distributed, but\n22\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nwe assume the distribution is close enough to normal to permit the use of the\nformulas based on normality. Thus, here we assume \u03b7 = 1.\nTable 1. Numbers of two types of cells; following Cohen and\nD\u2019Eustachio (1978), see [27]\nj\nmj\nnj\nYj\nXj\n1\n52\n337\n7.211\n18.358\n2\n6\n141\n2.449\n11.874\n3\n14\n177\n3.742\n13.304\n4\n5\n116\n2.236\n10.770\n5\n5\n88\n2.236\n9.381\nThe coordinates of the centroid C are\n(\u00afx, \u00afy) = (12.7374, 3.5748).\nOne calculates that the components of the hyperplanar inertia operator JC at\nthe centroid C is: JXX = 47.7937, JY Y = 18.1021, JXY = 28.6318. The prin-\ncipal hyperplanar moments of inertia J1, J2 are the eigenvalues of the operator\nJC.\nThe corresponding eigenvectors are directions of the principal axes.\nThe\neigenvalues are J1 = 0.69605, J2 = 65.19978. The corresponding eigenvectors are\nn1 = (\u22120.51947, 0.85449)T, n2 = (\u22120.85449, \u22120.51947)T. The equation of the line\nthat best \ufb01ts uC contains the centroid C and is given by uC : y = 0.60793x\u22124.16865.\nThis coincides with the equations obtained in [27]. The equation of the line of the\nworst \ufb01t is y = \u22121.64493x + 24.52689.\nIn the original study a hypothesis of interest is that \u03b20 = 0. This corresponds\nto the condition the line of regression contains the origin of coordinate system.\nThus we will consider the origin of coordinate system, denoted by (X, Y ), as a\npoint P.\nWe want to apply Theorem 4.3 for a given point P.\nWe introduce\n( \u02dcX, \u02dcY ) as the principal coordinates having the centroid C as the origin. Using a\ncoordinate transformation, we recalculate the coordinates of the point P in the\nprincipal coordinates and get: ( \u02dcXP , \u02dcYP ) = (3.56202, 12.74098). Using (4.8), we\nget the pencil of conics associated with this data: it is de\ufb01ned with \u03b1 = 0.13921,\n\u03b2 = \u221212.76154:\n(4.13)\n\u02dcx2\n0.13921 \u2212\u03bb +\n\u02dcy2\n\u221212.76154 \u2212\u03bb = 1.\nThe Jacobi elliptic coordinates of the centroid C are: \u03bb1C = \u03b2 = \u221212.76154,\n\u03bb2C = \u03b1 = 0.13921.From J1 = 2J1 \u2212m\u03bb2C, we get J1 = m\u03bb2C = m\u03b1.Thus the\nmoment of the line uC is equal to m\u03bb2C = J1 = 0.69605.\nIn order to \ufb01nd the Jacobi elliptic coordinates of the point P, associated to\nthe pencil of conics (4.13), one needs to solve the quadratic equation (4.9). We\nget \u03bb1P = \u2212186.907 and \u03bb2P = \u22120.73589 and as the Jacobi elliptic coordinates of\nthe point P. Finally, the principal moments of inertia at the point P are J1P =\n2J1 \u2212m\u03bb2P = 5.071564,\nJ2P = 2J1 \u2212m\u03bb1P = 935.9271. Using formulae (4.12)\nand (4.11) one gets that the equation of the line uP that is the best \ufb01t among the\nlines that contain P is uP : \u02dcY = 3.841884 \u02dcX \u22120.94389. In the original coordinates\nthe equation of this line has the form y = 0.30014x. Thus, \u02c6\u03b21 = 0.30014. This\nquantity can be calculated also using [27], the equation at the bottom of p. 43,\nwith Mxx = 171.8001, Mxy = 51.26003, \u02c6\u03bb = 4.057252, \u03c3uu = 0.25, reads as\n\u02c6\u03b21 = (Mxx \u2212\u02c6\u03bb\u03c3uu)\u22121Mxy = 0.30014.\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n23\nThe line of best \ufb01t uP is tangent to the hyperbola from the confocal pencil of\nconics (4.13) which contains P:\n\u02dcX2\n0.8751 +\n\u02dcY 2\n\u221212.0256 = 1.\nThe moment of the line uP is m(2\u03bb2C \u2212\u03bb2P ) = J1P = 5.071564, with m = 5.\nThe line of the worst \ufb01t among the lines that contain P is \u02dcY = \u22120.26029 \u02dcX +\n13.66814. In the original coordinates the equation of this line has the form y =\n\u22123.331376x. It is tangent to the ellipse from the confocal family of conics (4.13)\nthat contains P:\n\u02dcX2\n187.0462 +\n\u02dcY 2\n174.1455 = 1.\n0 1 2 3 4 5 6 7 8 9 10111213141516171819\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nK\nK\nK\nK\nK\nb\nFigure 8. Example 4.2: The lines of the best and the worst \ufb01t\ntangent to the hyperbola and ellipse respectively, the confocal con-\nics through the point P, the origin.\nSee Fig. 8. The bold point is the center of masses C. The two conics from\nthe confocal pencil which pass through the point P, the origin, are presented. The\nline of the restricted orthogonal regression through the point P is tangent to the\nhyperbola, while the line of the worst \ufb01t through the point P is tangent to the\nellipse.\nFor testing the null hypothesis \u03b20 = 0, the test statistic is [27] N \u02c6\u03bb/(N \u22121),\nwhose null distribution can be approximated by Snedecor\u2019s F distribution with\ndegrees of freedom (N \u22121) and \u221e. Here, as in [27], \u02c6\u03bb is the smallest root of the\nequation det(JP \u2212\u03bb\u03a3) = 0, where, as above \u03a3 = T \u22121E = diag(0.25, 0.25), with\nT = 4.\nThis statistic can be expressed as 5\u02c6\u03bb/4 = J1P = 5(2\u03bb2C \u2212\u03bb2P ), See Theorem\n4.4 for a more general statement. For these data, the value of J1P is 5.071564.\nTherefore, the approximate p-value is P(F4,\u221e> 5.07) = 0.00043, which leads to\nrejection of the null hypothesis.\nRemark 4.1. The models with errors in variables for \u03b7 general were \ufb01rst stud-\nied by C.H. Kummell in 1879, see [27]. Now we want to treat the general cases of\n24\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nmodels with errors in variables in arbitrary dimension and with a nontrivial error\ncovariance matrix, here denoted as G. We assume G is known and positive de\ufb01nite.\nAs it is well known, see e.g. [27], in such cases, the orthogonal regression should\nbe performed not with respect to the ordinary Euclidean metric, but with respect\nto the metric generated by G. Thus, we consider a more general case when the\ninertia operator is de\ufb01ned with respect to the metric \u27e8\u00b7, \u00b7\u27e9G = \u27e8G\u00b7, \u00b7\u27e9, where \u27e8\u00b7, \u00b7\u27e9is\nthe Euclidean metric and G is a positive de\ufb01nite matrix. The inertia operator is\nde\ufb01ned by\n(4.14)\n(JG\nO nG, mG) =\nN\nX\ni=1\nmi\u27e8ri, nG\u27e9G\u27e8ri, mG\u27e9G\n=\nN\nX\ni=1\nmi\u27e8\n\u221a\nGri,\n\u221a\nGnG\u27e9\u27e8\n\u221a\nGri,\n\u221a\nGmG\u27e9.\nWe suppose that nG is a unit vector in the metric de\ufb01ned with G. Let us introduce\nn =\n\u221a\nGnG, m =\n\u221a\nGmG. Then, n is a unit vector in the standard Euclidean metric\n1 = \u27e8GnG, nG\u27e9= \u27e8\n\u221a\nGnG,\n\u221a\nGnG\u27e9= \u27e8n, n\u27e9. Thus, we introduce new coordinates:\nRi =\n\u221a\nGri. We get (JG\nO nG, mG) = (JOn, m), where JO is the inertia operator\nin the standard Euclidean metric in the new coordinates.The last relation can be\nrewritten as\n(JOn, m) = (JG\nO nG, mG) = \u27e8JG\nO\n\u221a\nG\u22121n,\n\u221a\nG\u22121m\u27e9\n= \u27e8\n\u221a\nG\u22121JG\nO\n\u221a\nG\u22121n, m\u27e9.\nThus, we conclude that JO =\n\u221a\nG\u22121JG\nO\n\u221a\nG\u22121. Also, if nG is an eigen-vector of the\noperator JG\nO , then n =\n\u221a\nGnG is an eigen-vector of the operator JO. The eigenvalue\nproblem (4.2) in the metric G case can be expressed as\n(4.15)\ndet(JG\nO \u2212\u00b5G) = det(\n\u221a\nG\u22121JG\nO\n\u221a\nG\u22121 \u2212\u00b5E) = det(JO \u2212\u00b5E) = 0.\nTheorem 4.4. Let the system of N points M1, ..., MN with unit masses be given\nin Rk, N \u2265k, with the centroid C and the associated pencil of confocal quadrics\n(3.6). For any point P(x01, ..., x0k) denote its Jacobi coordinates of the pencil of\nconfocal quadrics (3.6) by (\u03bb1P < \u00b7 \u00b7 \u00b7 < \u03bbkP ) and the Jacobi coordinates of the\ncentroid by (\u03bb1C < \u00b7 \u00b7 \u00b7 < \u03bbkC). Then:\n(a) The hyperplanar moment of the hyperplane of the best \ufb01t is equal to J1 =\nN\u03bbkC.\n(b) The hyperplanar moment of the hyperplane of the best \ufb01t that contains the\npoint P is equal to JP = N(2\u03bbkC \u2212\u03bbkP ).\n(c) The test statistic of the hypothesis that the hyperplane of the best \ufb01t con-\ntains the point P is:\n(4.16)\nN\nN \u2212k + 1(2\u03bbkC \u2212\u03bbkP ).\nwhose null distribution can be approximated by Snedecor\u2019s F distribution\nwith degrees of freedom (N \u2212k + 1) and \u221e.\nProof. The proof follows the lines of the consideration from Example 4.1,\nwith the obvious higher-dimensional adjustments. First, from the formula (4.5)\napplied to the centroid C we get:\nJ1 = 2J1 \u2212N\u03bbkC,\nand deduce\nJ1 = N\u03bbkC.\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n25\nFrom the last formula and (4.5) we get that the hyperplanar moment of the hyper-\nplane of the best \ufb01t is\nRSS2 = N\u03bbkC.\nBy using the same formulae, we get the hyperplanar moment of the hyperplane of\nthe best \ufb01t containing a given point P to be\nRSS1 = N(2\u03bbkC \u2212\u03bbkP ).\nFrom Theorem 2.3.2, formula (2.3.19), from [27] for the approximation of the test\nstatistic we get:\nN(N \u2212k \u22121)\u22121\u02dc\u03bb,\nwhere \u02dc\u03bb is the smallest root of det(M \u2212\u03bbT \u22121S) = 0, see formula (2.3.18) in [27].\nHere T is a positive scalar. To relate to our notation, one takes JG\nO = NM and\nG = T \u22121S. From there we get that the test statistic is approximated by\nN\nN \u2212k + 1(2\u03bbkC \u2212\u03bbkP ),\nwhere \u03bbkC and \u03bbkP are the largest Jacobi coordinates of the points C and P, where\nthe Euclidean coordinates of these points are calculated in the coordinates X = Gx,\nand the confocal pencil (4.4) (the same as (3.7)) is generated by\n\u221a\nG\u22121JG\nO\n\u221a\nG\u22121.\n\u25a1\nRemark 4.2. If we take G1 = S, then the test statistic is approximated by\nNT\nN \u2212k + 1(2\u03bbkC \u2212\u03bbkP ),\nwhere \u03bbkC and \u03bbkP are the largest Jacobi coordinates of the points C and P, where\nthe Euclidean coordinates of these points are now calculated in the coordinates\nX = G1x, and the confocal pencil is generated by\nq\nG\u22121\n1 JG\nO\nq\nG\u22121\n1 .\n5. The directional regressions and ellipsoids\nIn the previous sections we dealt primarily with geometric aspects of orthogonal\nregression. From the work of Galton [28] we know that conics play signi\ufb01cant role\nin simple linear regression. From Fig. 2 one observes that points of horizontal\nand vertical tangency to the Galton ellipse (aka the data ellipse) determine the\nlines of linear regression of x on y and of y on x respectively. This motivates our\nfurther study of linear regression in Rk in a geometric, invariant, i.e. coordinate free\nway. We consider linear regression in an arbitrary selected direction, not necessarily\nhorizontal or vertical. In Section 5.1 we deal with the planar case and generalize\nthis to an arbitrary k in Section 5.2. A higher-dimensional geometric generalization\nof the above observation about Galton ellipses is given in Theorem 5.3, see also\nCorollary 5.3. The family of confocal quadrics (3.6) which we associated with the\ngiven system of points comes into picture here in reaching our third goal: for a\ngiven direction, for a given system of N points, under the full rank assumption,\nand for a selected point, \ufb01nd the best \ufb01tting hyperplane in the given direction. This\nis done in Theorem 5.4. The e\ufb00ectiveness of the developed methods and results\nand their applications in statistics are illustrated in Examples 5.3 and 5.4.\n5.1. The directional axial moments in the plane. Given a line w in the\nplane. Given N points M1, ..., MN with masses m1, m2, ..., mN of the total mass m\nin the plane. The point C denotes the center of masses.\nFor a given line u \u2282R2, non-parallel to w, the axial moment of inertia in\ndirection w, denoted by Iw\nu , is de\ufb01ned as\nIw\nu =\nN\nX\nj=1\nmj \u02c6d2\nj,\n26\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nwhere \u02c6dj is the distance from the point Mj to the point of intersection of the line\nu with the line parallel to w through the point Mj, j = 1, . . . , N.\nSuppose the line u contains a point O. Denote by u0 the unit vector parallel to\nthe line u and by w0 the unit vector parallel to the line w. As before, see Section\n3.2, we denote by Iu the axial moment of inertia for the axis u and by IO the\noperator of inertia for the point O. Then the axial moment of inertia Iw\nu for the\naxis u in direction w can be rewritten in the form:\n(5.1)\nIw\nu =\nN\nX\ni=1\nmi \u02c6d2\ni = Iu \u00b7\n1\n1 \u2212\u27e8u0, w0\u27e92 =\n\u27e8IOu0, u0\u27e9\n1 \u2212\u27e8u0, w0\u27e92 .\nFigure\n9. With\nfor-\nmula\n(5.1)\nand\nwith\nProposition 5.1.\nFigure 10. With The-\norem 5.1.\nFrom the last formula and Fig. 9, one easily gets, the following, directional\nversion of the Huygens-Steiner Theorem:\nProposition 5.1. [The directional Huygens-Steiner Theorem] Let the axis u\ncontain the center of masses C and let u1 be a line parallel to u. Denote by Iw\nu1\nand Iw\nu the corresponding directional axial moments of inertia of a given system of\npoints with the total mass m in the direction w. Then\n(5.2)\nIw\nu1 = Iw\nu + m \u02c6d2,\nwhere \u02c6d is the distance between the points of intersection of a line parallel to w with\nthe parallel lines u and u1.\nThus, again we get a characterization of the center of masses, as a consequence\nof (5.2):\nCorollary 5.1. Given a direction w, the system of points and a direction u\nnot parallel to w. Among all the lines parallel to u, the least directional moment\nof inertia in direction w is attained by the line through the center of masses of the\nsystem of points.\nWe will investigate the lines which minimize the directional axial moment in\nthe direction w and call them the directional lines of regression in the direction\nw. From the above Corollary we see that they have to pass through the center of\nmasses C. De\ufb01ne the central axial ellipse of gyration as\n(5.3)\n\u27e8I\u22121\nC x, x\u27e9= 1.\nThis is the same as the data ellipse.\nDenote by \u02c6uw the direction of the directional line of regression in the direction\nw.\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n27\nTheorem 5.1. Let w and w1 form a pair of conjugate directions of the axial\nellipse of gyration (5.3). Then the direction of the directional line of regression in\nthe direction w is orthogonal to w1, i.e. \u27e8\u02c6uw, w1\u27e9= 0.\nProof. See Fig. 8. Let us use the formula (5.1) and calculate the gradient of\nIw\nu with respect to u. We get\ngradu0 Iw\nu = \u03b1ICu0 + \u03b2w0,\nwhere \u03b1, \u03b2 are some scalars and \u03b1 \u0338= 0. Since u0 is a unit vector, or in other words,\ng(u0) = \u27e8u0, u0\u27e9\u22121 = 0, in order to \ufb01nd the directional line of regression, we need\nto \ufb01nd the conditional extremum. Let us denote by \u00b5 the Lagrange multiplier.\nThus,\ngradu0 Iw\nu = \u00b5 gradu0 g(u0),\nonly if the vectors ICu and w0 are collinear. We get the condition \u02c6uw = I\u22121\nC w0.\nThe direction w1 is conjugate to w with respect to the the axial ellipse of gyration\n(5.3) if and only if \u27e8I\u22121\nC w0, w1\u27e9= 0. This proves the theorem.\n\u25a1\nNow we consider a similar problem, but searching for the best \ufb01t among the\nlines that contain a given point P, distinct from the center of masses: Find the line\nin the plane that contains a given point P and has the minimal directional axial\nmoment in the direction w.\nLet u and u1 be two parallel lines, that contains C and P respectively. We\nassume that u and u1 are not parallel to w. Applying the Huygens-Steiner theorem\n(5.2), we get Iw\nu1 = Iw\nu + m \u02c6d2. Since \u02c6d2 =\nd2\n1\u2212\u27e8u0,w0\u27e92 using (5.1), we get Iw\nu1 =\nIu+md2\n1\u2212\u27e8u0,w0\u27e92 =\nIu1\n1\u2212\u27e8u0,w0\u27e92 =\n\u27e8IP u0,u0\u27e9\n1\u2212\u27e8u0,w0\u27e92 . The last formula is an analog of (5.1). Thus,\nwe get:\nTheorem 5.2. Let w and w1 form a pair of conjugate directions of the axial\nellipse of gyration \u27e8I\u22121\nP x, x\u27e9= 1 at a given point P.\nThen the direction of the\ndirectional line of regression in the direction w at the point P is orthogonal to w1,\ni.e. \u27e8\u02c6uw, w1\u27e9= 0.\n5.2. The directional hyperplanar moments in Rk. Let us select a line w\nin Rk. Given N points M1, ..., MN with masses m1, m2, ..., mN of the total mass m\nin Rk. The point C denotes the center of masses.\nNow, we introduce the hyperplanar moment of inertia for the given system of\npoints and for a given hyperplane \u03c0 \u2282Rk, which is not parallel to w. We de\ufb01ne Jw\n\u03c0\nthe hyperplanar moment of inertia in direction w as follows: Jw\n\u03c0 =\nN\nP\nj=1\nmj \u02c6D2\nj, where\n\u02c6Dj is the distance between the point Mj and the intersection of the hyperplane \u03c0\nwith the line parallel to w through Mj, for j = 1, . . . , N. Let n be the unit vector\northogonal to the hyperplane \u03c0, and O a point contained in \u03c0. Then, using Fig. 11\nthe hyperplanar moment of inertia Jw\n\u03c0 can be rewritten in the form\n(5.4)\nJw\n\u03c0 =\nJ\u03c0\n\u27e8w0, n\u27e92 = \u27e8JOn, n\u27e9\n\u27e8w0, n\u27e92 .\nHere J\u03c0 is the hyperplanar moment of inertia and the operator JO is the hyperplanar\ninertia operator.\nProposition 5.2 (The hyperplanar directional Huygens-Steiner Theorem).\nLet the hyperplane \u03c0 contain the center of masses C and let \u03c01 be a hyperplane\nparallel to \u03c0. Denote by Jw\n\u03c01 and Jw\n\u03c0 the corresponding directional hyperplanar mo-\nments of inertia of a given system of points with the total mass m in the direction\nw. Then\n28\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\n(5.5)\nJw\n\u03c01 = Jw\n\u03c0 + m \u02c6D2,\nwhere \u02c6D is the distance between the points of intersection of a line parallel to w\nwith the parallel hyperplanes \u03c0 and \u03c01.\nThus, again we are getting a characterization of the center of masses, using\n(5.5):\nCorollary 5.2. Given a direction w, the system of points and one hyperplane\n\u03c0 not parallel to w. Among all the hyperplanes parallel to \u03c0, the least directional\nhyperplanar moment of inertia in direction w is attained for the hyperplane which\ncontains the center of masses of the system of points.\nFigure\n11. With\nequation (5.4).\nFigure 12. With The-\norem 5.3.\nWe will investigate the hyperplanes which minimize the directional hyperplanar\nmoment in the direction w and call them the directional hyperplanes of regression\nin the direction w. From the above Corollary it follows that they contain the center\nof masses C. De\ufb01ne the data ellipsoid as\n(5.6)\n\u27e8J\u22121\nC x, x\u27e9= 1.\nDenote by \u02c6\u03c0w the directional hyperplane of regression in the direction w.\nTheorem 5.3. Let the radius vector from the center of masses C in direction\nw intersect the data ellipsoid (5.6) at the point W and let the tangent hyperplane\nto the data ellipsoid at W be \u03c0W . The directional hyperplane of regression in the\ndirection w contains C and is parallel to the tangent hyperplane \u03c0W .\nProof. Let us use the formula (5.4) and calculate the gradient of Jw\n\u03c0 with\nrespect to the normal vector n. We get\ngradn Jw\n\u03c0 = \u03b1JCn + \u03b2w0,\nwhere \u03b1, \u03b2 are some scalars and \u03b1 \u0338= 0.\nSince n is a unit vector, i.e.\ng(n) =\n\u27e8n, n\u27e9\u22121 = 0, in order to \ufb01nd the directional hyperplane of regression, we need to\n\ufb01nd the conditional extremum. Let us denote by \u00b5 the Lagrange multiplier. Thus,\ngradn Jw\n\u03c0 = \u00b5 gradn g(n),\nonly if the vectors JCn and w0 are collinear. We get that \u02c6nw = J\u22121\nC w0 is the vector\northogonal to the directional hyperplane of regression in the direction w. On the\nother hand, J\u22121\nC w0 is orthogonal to the tangent hyperplane of the data ellipsoid\n(5.6) at its point W. See Fig. 10. This proves the theorem.\n\u25a1\nA statement similar to the last theorem can be extracted from [16], based on\ndi\ufb00erent considerations. For k = 2 we get an important specialization of Theorem\n5.3, see Fig. 13.\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n29\nCorollary 5.3. In the case k = 2 the line of regression in the direction w\ncontains the center of masses C and intersects the data ellipse at two points at\nwhich the tangents to the data ellipse are parallel to the line w. Equivalently, the\nline of regression is parallel to the tangent line of the data ellipse at the point of its\nintersection with the line parallel to w passing through the centroid C.\nFigure\n13. With\nCorollary 5.3.\nFigure 14. Simple lin-\near regression.\nExample 5.1. Let us consider the two-dimensional case of the last Corollary\nin more detail. Such a situation is typical in statistics in simple linear models.\nAssume that each observation from an experiment is a pair of numbers, and that\nthe goal is to predict one of the numbers from the other. Then the method of least\nsquares can be used for constructing a predictor of one of the variables from the\nother one, by using of a sample of observed pairs (see e.g. [15]).\nGiven N points (x(i), y(i)), N\ni=1 in the plane. The straight line\ny = f(x)\nthat minimizes the sum of the squares of the vertical deviations of all the points\nfrom the line is called the least-squares line in the direction of the y-axis. Similarly,\nthe straight line\nx = g(y)\nthat minimizes the sum of the squares of the horizontal deviations of all the points\nfrom the line is called the least-squares line in the direction of the x-axis.\nSee\nFig. 14, where the direction w1 is parallel to the y-axis, while w2 is parallel to\nthe x-axis. The least-squares line y = f(x) in the direction of the y-axis intersects\nthe data ellipse at the points of vertical tangency. The least-squares line x = g(y)\nin the direction of the x-axis intersects the data ellipse at the points of horizontal\ntangency. If\n(5.7)\nf(x) = \u02c6\u03b20 + \u02c6\u03b21x\nthen, it is well known (see e.g. [15]), that the pair of coe\ufb03cients (\u02c6\u03b20, \u02c6\u03b21) can be\ndetermined as follows\n(5.8)\n\u02c6\u03b21 =\nPN\ni=1(y(i) \u2212\u00afy)(x(i) \u2212\u00afx)\nPN\ni=1(x(i) \u2212\u00afx)2\n,\n\u02c6\u03b20 = \u00afy \u2212\u02c6\u03b21\u00afx,\nwhere (\u00afx, \u00afy) are the coordinates of the centroid of the given system of N points:\n\u00afx = 1\nN\nN\nX\ni=1\nx(i),\n\u00afy = 1\nN\nN\nX\ni=1\ny(i).\nBy exchanging the roles of x and y in the above formulas we get the coe\ufb03cients\n\u02c6\u03b10, \u02c6\u03b11 of the linear function g(y) = \u02c6\u03b10 + \u02c6\u03b11y.\nIf we assume that the y values are observed values of a collection of random\nvariables Y , then under the assumptions of the simple linear regression, the method\n30\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nof least squares provides the maximum likelihood estimates \u02c6\u03b20 and \u02c6\u03b21 for the pa-\nrameters of the model \u03b20 and \u03b21 respectively. Moreover\n\u02c6\u03c32 = 1\nN\nN\nX\ni=1\n(y(i) \u2212\u02c6\u03b20 \u2212\u02c6\u03b21x(i))2\nis the maximum likelihood estimate for the variance \u03c32 of the normal distribution\ndescribing the conditional distribution of Y given X = x. While \u02c6\u03b20 and \u02c6\u03b21 are un-\nbiased estimators, \u02c6\u03c32 is a biased estimator. The corresponding unbiased estimator\nis\n\u02c6\u03c3\u20322 =\nS2\nN \u22122,\nwhere\nS2 =\nN\nX\ni=1\n(Y (i) \u2212\u02c6\u03b20 \u2212\u02c6\u03b21x(i))2.\nNow, we will derive the formulas (5.7), (5.8) directly from Theorem 5.3 and\nCorollary 5.3. Let us present the formula for the line that minimize sum of the\nsquares of deviations in the direction w0 = (wx, wy)T . As mentioned before, this\nline contains the centroid (i.e. the center of masses) (\u00afx, \u00afy) of the system of points.\nLet its equation be of the form\ny \u2212\u00afy = \u03ba(x \u2212\u00afx).\nFrom Theorem 5.3 it follows that this line is parallel to \u02c6nw = J\u22121\nC w0. Since\nJC =\n\u0012J11\nJ12\nJ12\nJ22\n\u0013\none calculates\nJ\u22121\nC\n=\n1\ndet JC\n\u0012\nJ22\n\u2212J12\n\u2212J12\nJ11\n\u0013\n.\nThus,\n\u02c6nw =\n1\ndet JC\n\u0012 J22wx \u2212J12wy\n\u2212J12wx + J11wy\n\u0013\n,\nand one gets that the slope of the line is\n\u03ba = J22wx \u2212J12wy\nJ12wx \u2212J11wy\n.\nIn the special case of the vertical deviations, w0 = (0, 1), and the equation of the\nleast squares line becomes\ny \u2212\u00afy = J12\nJ11\n(x \u2212\u00afx).\nUsing\nJ12 =\nN\nX\ni=1\n(y(i) \u2212\u00afy)(x(i) \u2212\u00afx),\nJ11 =\nN\nX\ni=1\n(x(i) \u2212\u00afx)2,\nwe get both formulas (5.7) and (5.8).\nExample 5.2. In a similar manner as in Example 5.1, one can consider the\ngeneral linear model and multiple regression. One can derive the equations in Rk of\nthe hyperplane that minimizes the sum of the squares of deviations in the direction\nw0 = (w1, w2, ..., wk)T . The normal vector n = J\u22121\nC w0 and the equation of the\nplane is\nN\nX\ni=1\nni(xi \u2212\u00afxi) = 0.\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n31\nFor example, in the direction normal to hyperplane xi = 1, the vector of the normal\nto the plane n is the i-th column of the matrix of cofactors of JC.\nA standard derivation of formulas of the general linear model and multiple\nregression in direction xk one can \ufb01nd in e.g. [15], Ch. 11.5.\nLet the direction w and a system of N points in Rk with masses m1, ..., mN\nbe given under the full rank assumption, where k \u2a7e2. We are going to describe\nall hyperplanes that have the same directional hyperplanar moment of inertia in\ndirection w. We consider a coordinate system with the origin at the centroid C,\nand with the direction w coinciding with the axis Cxk.\nThe hyperplanar operator of inertia at the point C has the expression\nJC =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nJ11\nJ12\nJ13\n...\nJ1k\nJ12\nJ22\nJ23\n...\nJ2k\n...\nJ1k\u22121\nJ2k\u22121\nJ3k\u22121\n...\nJk\u22121k\nJ1k\nJ2k\nJ3k\n...\nJkk\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n.\nIt is convenient to introduce the submatrix K1 of the matrix JC and the vector b\nby:\nK1 =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\nJ11\nJ12\nJ13\n...\nJ1k\u22121\nJ12\nJ22\nJ23\n...\nJ2k\u22121\n...\nJ1k\u22121\nJ2k\u22121\nJ3k\u22121\n...\nJk\u22121k\u22121\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8,\nb = (J1k, ..., Jk\u22121k)T .\nIn the case when all masses are equal to mi =\n1\nN , the matrix K1 coincides with\nthe covariance matrix K. Assume that the hyperplane \u03c0 is not parallel to the axis\nCxk. The equation of \u03c0 can be given in the form\n(5.9)\nxk = \u03b20 \u2212\u03b21x1 \u2212\u03b22x2 \u2212... \u2212\u03b2k\u22121xk\u22121.\nLet us denote \u03b2 = (\u03b21, ..., \u03b2k\u22121).\nProposition 5.3. Given a real number \u00b5, let a hyperplane (5.9) have the\ndirectional hyperplanar moment of inertia in direction Cxk equal to \u00b5. Then the\ncoe\ufb03cients \u03b20, \u03b21, ..., \u03b2k\u22121 of the hyperplane (5.9) satisfy the equation\n(5.10)\n\u27e8K1\u03b2, \u03b2\u27e9\u22122\u27e8b, \u03b2\u27e9+ m\u03b22\n0 + Jkk = \u00b5.\nBy varying the moment of inertia \u00b5, the equations (5.10) represent the family of\nhomothetic ellipsoids in the space of parameters \u03b20, \u03b21, ..., \u03b2k\u22121.\nProof. We calculate the directional hyperplanar moment of inertia of the\nhyperplane (5.9):\n(5.11)\nJxk\n\u03c0\n=\nN\nX\ni=1\nmid2\ni =\nN\nX\ni=1\nmi\n\u0000x(i)\nk \u2212\u03b20 \u2212\u03b21x(i)\n1 \u2212... \u2212\u03b2k\u22121x(i)\nk\u22121\n\u00012\n= Jkk + m\u03b22\n0 + \u03b22\n1J2\n11 + ... + \u03b22\nk\u22121Jk\u22121 k\u22121 \u22122\nk\u22121\nX\ni=1\n\u03b2iJik + 2\nk\u22121\nX\ni,j=1\ni<j\n\u03b2i\u03b2jJij\n= \u27e8K1\u03b2, \u03b2\u27e9\u22122\u27e8b, \u03b2\u27e9+ m\u03b22\n0 + Jkk.\nThis gives a proof of the \ufb01rst part of the Proposition. We used here that C is the\ncentroid and consequently, PN\ni=1 mix(i)\nj\n= 0 for j = 1, ..., k.\nIn the space of parameters \u03b20, \u03b21, ..., \u03b2k\u22121, the equation (5.10) de\ufb01nes a family\nof quadrics that depend on \u00b5. Since K1 is a positive de\ufb01nite matrix, this is a family\nof ellipsoids.\n32\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\n\u25a1\nProposition 5.4. All ellipsoids from the family (5.10) have the same center\nat the point (0, \u02c6\u03b2), where \u02c6\u03b2 = K\u22121\n1 b are the coordinates of the best \ufb01t hyperplane.\nThe formula (5.10) can be rewritten in the form\n(5.12)\n\u27e8K1(\u03b2 \u2212\u02c6\u03b2), \u03b2 \u2212\u02c6\u03b2\u27e9= \u00b5 + \u27e8K\u22121\n1 b, b\u27e9\u2212m\u03b22\n0 \u2212Jkk.\nProof. We have\n\u00b5 = \u27e8K1\u03b2, \u03b2\u27e9\u22122\u27e8b, \u03b2\u27e9+ m\u03b22\n0 + Jkk\n= \u27e8K1(\u03b2 \u2212K\u22121\n1 b, \u03b2 \u2212K\u22121\n1 b\u27e9\u2212\u27e8K\u22121\n1 b, b\u27e9+ m\u03b22\n0 + Jkk.\nBy introducing \u02c6\u03b2 = K1\n1b, one gets the formula (5.12). The coordinates of the vector\n\u02c6\u03b2 give the best \ufb01t hyperplane obtained by least square method (see for example\nformula (3.6) from [31]).\n\u25a1\nProposition 5.4 shows that the intersection of the family (5.10) with the hy-\nperplane \u03b20 = 0 coincides with the family of homothetic ellipsoids of residuals\n(2.1).\nOne can always choose coordinates in which K1 is diagonal, and thus the for-\nmula (5.10) takes a simpler form. The direction w coincides with Cxk. In the\nchosen coordinates all the centrifugal moments of inertia Jij for i, j = 1, ..., k \u22121\nare equal to zero. In other words, the hyperplanar operator of inertia at the point\nC has the expression\nJC =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nJ1\n0\n0\n...\n0\nJ1k\n0\nJ2\n0\n...\n0\nJ2k\n...\n0\n0\n0\n...\nJk\u22121\nJk\u22121k\nJ1k\nJ2k\nJ3k\n...\nJk\u22121k\nJk\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n.\nLet us \ufb01x Jxk\n\u03c0\n= \u00b5. Then using that\ndet(JC) = J1J2...Jk\u22121\n\u0000Jk \u2212J1k\nJ1\n\u2212... \u2212Jk\u22121k\nJk\u22121\n\u0001\none gets that the equation of the family of ellipsoids (5.10) has the form\n(5.13)\nJ1\n\u0010\n\u03b21 \u2212J1k\nJ1\n\u00112\n+ ... + Jk\u22121\n\u0010\n\u03b2k\u22121 \u2212Jk\u22121k\nJk\u22121\n\u00112\n+ m\u03b22\n0 = \u00b5 \u2212\ndet(JC)\nJ1J2...Jk\u22121\n.\nIn such chosen coordinates, the center of the ellipsoids is (0, J1k\nJ1 , ..., Jk\u22121k\nJk\u22121 ).\nA geometric approach to the lasso with use of Proposition 5.3 and Proposition\n5.4 is presented in [?].\nNow, as in the two-dimensional case, we consider the following problem for\nhyperplanes that contain a given point P distinct from the center of masses: Find\nthe hyperplane that contains a given point P and has the minimal directional hy-\nperplanar moment in the direction w.\nLet \u03c0 and \u03c01 be two parallel hyperplanes that contain C and P respectively. We\nassume that \u03c0 and \u03c01 are not orthogonal to w. The Huygens-Steiner theorem (5.5)\ngives: Jw\n\u03c01 = Jw\n\u03c0 + m \u02c6D2. Since \u02c6D2 =\nD2\n\u27e8u0,w0\u27e92 using (5.4), we get Jw\n\u03c01 = J\u03c0+mD2\n\u27e8u0,w0\u27e92 =\nJ\u03c01\n\u27e8u0,w0\u27e92 =\n\u27e8JP u0,u0\u27e9\n\u27e8u0,w0\u27e92 . Here JP is given by (4.3).\nThe last formula is an analog\nof (5.4). In statistics, the data ellipsoid is typically considered at the centroid.\nIn mechanics ellipsoids of inertia are not exclusively considered at the center of\nmasses. Transporting this important \ufb02exibility from mechanics to statistics, we\nconsider the data ellipsoid at a given point. Let us recall that we described its\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n33\nprincipal hyperplanes in Theorem 4.2. Here the confocal pencil of quadrics (3.6) we\nconstructed associated to the given system of points appears again. The principal\nhyperplanes are tangent hyperplanes at the point P of those quadrics from the\nconfocal pencil (3.6), which contain P. We get\nTheorem 5.4. Given the data ellipsoid at a point P EP : \u27e8J\u22121\nP x, x\u27e9= 1. Let the\nradius vector from the point P in direction w intersect the ellipsoid EP at the point\nW. Let the tangent hyperplane to the ellipsoid EP at W be \u03c0W . The directional\nhyperplane of regression in the direction w, among the hyperplanes which contain\nthe point P, is parallel to the tangent hyperplane \u03c0W . In other words, the directional\nhyperplane of regression is the hyperplane conjugate to PW with respect to EP .\nExample 5.3. Let N points with masses m1, ..., mN be given in the plane. Let\nus also \ufb01x a point P in the plane. Among the lines that contain the given point\nP, we will derive formulas for the directional line of regression in the direction\nw0 = (wx, wy) for the given system of points in the plane, using the associated\nconfocal pencil of conics (3.6). We will provide the answer in terms of the Jacobi\nelliptic coordinates of the point P with respect to the associated confocal pencil of\nconics.\nWe use the notation from Example 4.1. Denote by C the centroid of the system\nof N points. Denote Cxy the principal coordinate system of the confocal pencil and\nJ1 and J2 as before, denote the principal planar moments of the inertia. Introduce\na2 such that J1 + ma2\n2 = J2. The confocal pencil of conics associated to the system\nof points (3.6) has the form\n(5.14)\nx2\n\u03b1 \u2212\u03bb +\ny2\n\u03b2 \u2212\u03bb = 1,\n\u03b1 = J1\nm ,\n\u03b2 = J1\nm \u2212a2\n2.\nWe calculated the principal vectors \u02dcn(1) and \u02dcn(2) for the planar operator of\ninertia at the point P, see (4.10). In the corresponding orthogonal basis [n(1), n(2)],\nthe planar operator of inertia has a diagonal form JP = diag(JP1, JP2). The matrix\nof the change of bases S is\nS =\n \n\u03b1\u2212\u03bb2\n\u22061xP\n\u03b1\u2212\u03bb1\n\u22062xP\n\u2212\u03b2\u2212\u03bb2\n\u22061yP\n\u2212\u03b2\u2212\u03bb1\n\u22062yP ,\n!\n, where\n\u22061 =\ns\n(\u03b1 \u2212\u03b2)2(\u03bb2 \u2212\u03bb1)\n(\u03b1 \u2212\u03bb1)(\u03b2 \u2212\u03bb1) , \u22062 =\ns\n(\u03b1 \u2212\u03b2)2(\u03bb2 \u2212\u03bb1)\n(\u03b1 \u2212\u03bb2)(\u03bb2 \u2212\u03b2) .\nIn the new basis, the coordinates of the vector w0 are\n\u02dcw1 = \u03b1 \u2212\u03bb2\n\u22061xP\nwx + \u03b1 \u2212\u03bb1\n\u22062xP\nwy,\n\u02dcw2 = \u2212\u03b2 \u2212\u03bb2\n\u22061yP\nwx \u2212\u03b2 \u2212\u03bb1\n\u22062yP\nwy.\nFrom Theorem 5.4 it follows that the regression line is parallel to J\u22121\nP w0. Thus,\nthe vector of the line of regression in the direction w0 is nw = ST ( \u02dc\nw1\nJP 1 ,\n\u02dc\nw2\nJP 2 )T . We\nget that nw := (n1, n2)T is given by\nn1 =\n1\n(\u03b1 \u2212\u03b2)(\u03bb2 \u2212\u03bb1)\nh\u0010(\u03b1 \u2212\u03bb2)(\u03b2 \u2212\u03bb1)\nJP1\n+ (\u03b1 \u2212\u03bb1)(\u03bb2 \u2212\u03b2)\nJP2\n\u0011\nwx\n+\np\n(\u03b1 \u2212\u03bb1)(\u03b1 \u2212\u03bb2)(\u03b2 \u2212\u03bb1)(\u03bb2 \u2212\u03b2)\n\u0010 1\nJP1\n\u2212\n1\nJP2\n\u0011\nwy\ni\nn2 =\n1\n(\u03b1 \u2212\u03b2)(\u03bb2 \u2212\u03bb1)\nhp\n(\u03b1 \u2212\u03bb1)(\u03b1 \u2212\u03bb2)(\u03b2 \u2212\u03bb1)(\u03bb2 \u2212\u03b2)\n\u0010 1\nJP1\n\u2212\n1\nJP2\n\u0011\nwx\n+\n\u0010(\u03b1 \u2212\u03bb1)(\u03bb2 \u2212\u03b2)\nJP1\n+ (\u03b2 \u2212\u03bb1)(\u03b1 \u2212\u03bb2)\nJP2\n\u0011\nwy\ni\n.\n34\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nFrom Example 4.1 we know that the formula (4.5) gives the connection of\nthe extremal values JP1 and JP2 of the principal planar inertia operator at the\npoint P with the Jacobi elliptic coordinates \u03bb1 and \u03bb2 of the point P, associated\nwith the pencil of confocal conics: JP1 = 2J1 \u2212m\u03bb1,\nJP2 = 2J1 \u2212m\u03bb2. The\ncoordinate transformation between the Cartesian coordinates and the Jacobi elliptic\ncoordinates also gives x2\nP and y2\nP in terms of the Jacobi elliptic coordinates \u03bb1 and\n\u03bb2 of the point P, associated with the confocal pencil (5.14):\nx2\nP = (\u03b1 \u2212\u03bb1)(\u03b1 \u2212\u03bb2)\n\u03b1 \u2212\u03b2\n,\ny2\nP = (\u03b2 \u2212\u03bb1)(\u03b2 \u2212\u03bb2)\n\u03b2 \u2212\u03b1\n.\nThus we get the formula for the line of regression in the direction w0 under the\nrestriction that it contains a given point P in terms of the Jacobi elliptic coordinates\nof the point P with respect to the confocal pencil of conics (5.14) associated with\nthe given system of points.\nThe line of the best \ufb01t passing through the point P is y\u2212yP = \u03ba(x\u2212xP ),\n\u03ba =\n\u2212n1/n2.\nExample 5.4. Predicting pressure from the boiling point of water. See [15],\n[24]. The results from experiments that were trying to obtain a method for esti-\nmating altitude were presented in [24]. A formula is available for altitude in terms\nof barometric pressure, but it was di\ufb03cult to carry a barometer to high altitudes\nin the XIX century. However, it might be easy for travelers to carry a thermometer\nand measure the boiling point of water. Table 2 contains the measured barometric\npressures and boiling points of water from 17 experiments.\nTable 2. 17 Forbes\u2019 observations\nj\n1\n2\n3\n4\n5\n6\nxj\n194.5\n194.3\n197.9\n198.4\n199.4\n199.9\nyj\n20.79\n20.79\n22.40\n22.67\n23.15\n23.35\nj\n7\n8\n9\n10\n11\n12\nxj\n200.9\n201.1\n201.4\n201.3\n203.6\n204.6\nyj\n23.89\n23.99\n24.02\n24.01\n25.14\n26.57\nj\n13\n14\n15\n16\n17\nxj\n209.5\n208.6\n210.7\n211.9\n212.2\nyj\n28.49\n27.76\n29.04\n29.88\n30.06\nOne can use the method of least squares to \ufb01t a linear relationship between\nboiling point and pressure. Let yi be the pressure for one of Forbes\u2019 observations,\nand let xi be the corresponding boiling point for i = 1, . . . , 17. Using the data in\nTable 2, we can compute the covariance matrix. The coordinates of the centroid\nare (\u00afx, \u00afy) = (202.9529, 25.05882). The components of the hyperplanar inertia op-\nerator at the centroid are Jxx = 530.78235, Jyy = 145.93778, Jxy = 277.54206. The\nprincipal moments of inertia are J1 = 0.63839, J2 = 676.08147. Then we compute\nthe line uC of the best \ufb01t as the least-squares line. The intercept and slope of the\nline uC are, respectively, \u02c6\u03b20 = \u221281.06373 and \u02c6\u03b21 = 0.5228. Using formula (5.4),\none calculates the directional moment of inertia Jw\nuC = 0.813143014.\nA traveler is interested in the barometric pressure when the boiling point of\nwater is 201.5 degrees.\nSuppose that this traveler would like to know whether\nthe pressure is 24.5.\nThus, the traveler might wish to test the null hypothesis\nH0 : \u03b20 + 201.5\u03b21 = 24.5, versus H1 : \u03b20 + 201.5\u03b21 \u0338= 24.5.\nHere we are now interested in the line of regression of y on x, i.e.\nin the\nvertical direction, under the restriction that it contains the point P(201.5, 24.5).\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n35\n195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nK\nK\nK\nK\nK\nK\nK K\nKK\nK\nK\nK\nK\nK\nK\nK\nb\nbP\nFigure 15. Example 5.3: The line of restricted regression and\nconfocal quadrics through the point P.\nNow, we introduce (\u02dcx, \u02dcy) as the principal coordinates at the centroid. Using the\ncoordinate transformation from the original to the principal coordinates, we cal-\nculate the coordinates of the point P in the principal system:\n\u02dcxP = 0.08698,\n\u02dcyP = \u22121.68554. Similarly, the vertical vector (0, 1) in the new coordinate system\nbecomes w0 = (\u22120.88594, 0.46381). Our pencil of conics associated to the data is\nde\ufb01ned with \u03b1 = 0.037552, \u03b2 = \u221239.69441:\n\u02dcx2\n0.037552 \u2212\u03bb +\n\u02dcy2\n\u221239.69441 \u2212\u03bb = 1.\nThe Jacobi elliptic coordinates of the point P are the solutions of the quadratic\nequation (4.9):\n\u03bb1P = \u221242.536, \u03bb2P = 0.03049,.\nThe principal hyperplanar\nmoments of inertia at the point P are J1P = 2J1 \u2212m\u03bb2P = 0.75841\nJ2P =\n2J1 \u2212m\u03bb1P = 724.3882 In the principal coordinates nw = \u02dcST ( \u02dcw1\nJP1 ,\n\u02dc\nw2\nJP2 )T .\nThe equation of the line of the best \ufb01t in the sense of least squares passing\nthrough the point P is uP : \u02dc\u02dcy = 1189.499993\u02dc\u02dcx \u2212211.1392. Using formula (5.4), we\ncalculate the corresponding directional moment of inertia Jw\nuP = 1.455877. In the\noriginal coordinates, the equation of the line of the best \ufb01t uP is y = 0.5141352x\u2212\n79.0982450.\nThe F-statistic F = RSS1\u2212RSS2\np2\u2212p1\n/ RSS2\nN\u2212p2 , for N = 17, p2 = 2, p1 = 1 is 11.85647,\nwhile p-value is P(F1,15 > 11.85647) = 0.003621119. The null hypothesis should be\nrejected, leading to the conclusion that pressure is not 24.5 (see Fig. 15).\nRemark 5.1. Our calculations from this example do not coincide with the\ncalculations from [15].\nFor example, for unconstrained problem they got \u02c6\u03b20 =\n\u221281.049 while the correct value is \u02c6\u03b20 = \u221281.06373, as above.\nIt is interesting to note that in [8], following [40], they discuss this experiment\nfrom the point of view of errors in measurement models.\nAcknowledgements. We are grateful to Maxim Arnold for indicating the ref-\nerence [29] and for interesting discussions. We are also grateful to Pankaj Choud-\nhary, Sam Efromovich, and Frank Konietschke for interesting discussions. We also\nexpress our sincere gratitude to the referees and editors for their very helpful and\nconstructive remarks and suggestions.\n36\nV. DRAGOVI\u00b4C, B. GAJI\u00b4C\nThis research has been partially supported by Mathematical Institute of the\nSerbian Academy of Sciences and Arts, the Science Fund of Serbia grant Integrabil-\nity and Extremal Problems in Mechanics, Geometry and Combinatorics, MEGIC,\nGrant No. 7744592 and the Ministry for Education, Science, and Technological\nDevelopment of Serbia and the Simons Foundation grant no. 854861.\nReferences\n[1] Adcock, R. J. (1877). Note on the method of least squares. Analyst 4, 183\u2013184.\n[2] Adcock. R. J. (1878). A problem in least squares. Analyst, 5, 53\u201354.\n[3] Akritas, M., Bershady, M. (1996). Linear Regression for Astronomical Data with Measure-\nment Errors and Intrinsic Scatter. The Astrophysical Journal, 470, 706\u2013714.\n[4] Andreson, T. W. (1958). An Introduction to Multivariate Statistical Analysis John Wiley\nand Sons.\n[5] Arnold, V. I. (1989). Mathematical methods of classical mechanics, Graduate Texts in Math-\nematics, 60, 2nd edition, Springer-Verlag, New York, pp. xvi+508\n[6] Arvidsson, D., Fridolfsson J., B\u00a8orjesson M. (2019). Measurement of physical activity in clinical\npractice using accelerometers Journal of Internal Medicine, 286,137-153.\n[7] Ben Gillen, B., Snowberg, E. and Yariv, L. (2019). Experimenting with Measurement Error:\nTechniques with Applications to the Caltech Cohort Study. Journal of Political Economy,\n127,\n[8] Carroll, R. J., Ruppert, D. (1996). The use and misuse of orthogonal regression estimation\nin linear errors-in-variables models. The American Statistician, 50, 1\u20136.\n[9] Carroll, R., Ruppert, D., Stefanski L., Crainiceanu, C. (2006). Measurement Error in Non-\nlinear Models, A Modern Perspective Second Edition, Monographs on Statistics and Applied\nProbability 105, Taylor and Francis.\n[10] Casella G., Berger, R. L. (2001). Statistical Inference, Second Edition, Duxbury Advanced\nSeries, Duxbury.\n[11] Cohen, J. E., D\u2019Eustachio, P. and Edelman, G. M. (1977). The speci\ufb01c antigenbinding cell\npopulations of individual fetal mouse spleens: Repertoire composition, size and genetic con-\ntrol. J. Exp. Med., 146, 394\u2013411.\n[12] Cohen, J. E. and D\u2019Eustachio, P. (1978). An a\ufb03ne linear model for the relation between two\nsets of frequency counts. Response to a query, Biometrics 34,514\u2013516.\n[13] Courel-Ibanez, J., Martinez-Cava, A., Moran-Navarro, R. et al. (2019). Reproducibility and\nRepeatability of Five Di\ufb00erent Technologies for Bar Velocity Measurement in Resistance\nTraining. Ann Biomed Eng 47, 1523?1538.\n[14] Cram\u00b4er, H. (1946). Mathematical Methods of Statistics, Princeton:\nPrinceton University\nPress.\n[15] DeGroot, M., Schervish, M. (2012). Probability and Statistics, Fourth Edition, Adison-Wesley.\n[16] Dempster, A. (1969). Elements of continuous multivariate analysis, Addison-Wesley.\n[17] Dragovi\u00b4c, V. Gaji\u00b4c, B. (2023) Points with rotational ellipsoids of inertia, envelopes of hy-\nperplanes which equally \ufb01t the system of points in Rk, and ellipsoidal billiards, Physica D:\nNonlinear Phenomena, Volume 451, 133776\n[18] Dragovi\u00b4c, V. Gaji\u00b4c, B. (2024) Geometric approach and closed formulae for the Lasso, in\npreparation.\n[19] Dragovi\u00b4c, V. Gaji\u00b4c, B. (2024) Analogues of Lasso in orthogonal regression, in preparation.\n[20] Dragovi\u00b4c, V., Gaji\u00b4c, B. (2023). Supplement to \u201cOrthogonal and Linear Regressions and Pen-\ncils of Confocal Quadrics\u201d, DOI:.\n[21] Dragovi\u00b4c, V. Radnovi\u00b4c, M. (2011). Poncelet Porisms and Beyond: Integrable Billiards, Hy-\nperelliptic Jacobians and Pencils of Quadrics, Frontiers in Mathematics, Basel: Springer.\n[22] Dragovi\u00b4c, V., Radnovi\u00b4c, M., (2008). Hyperelliptic Jacobians as billiard algebra of pencils of\nquadrics: Beyond Poncelet porisms, Advances in Mathematics, Vol. 219, p. 1577\u20131607.\n[23] T. B. Fomby, R. C. Hill, S. R. Johnson (1984). Advanced Econometric Methods, Springer\nScience + Business Media.\n[24] Forbes, J. D. (1857). Further experiments and remarks on the measurement of heights by the\nboiling point of water, Transactions of the Royal Society of Edinburgh, 21: 135-143.\n[25] Friendly, M., Monette, G., Fox, J. (2013). Elliptic insights: understanding statistical methods\nthrough elliptical geometry. Statist. Sci. 28, no. 1, 1\u201339.\n[26] Fuller, W. A. (1978). An a\ufb03ne linear model for the relation between two sets of frequency\ncounts: Response to query. Biometrics 34, 517\u2013521.\n[27] Fuller, W. A. (1987). Measurement Error Models, John Willey and Sons.\nREGRESSIONS AND PENCILS OF CONFOCAL QUADRICS\n37\n[28] Galton, F. (1886). Regression towards mediocrity in hereditary stature. Journal of the An-\nthropological Institute, 15, pp. 246\u2013263.\n[29] Gardiner, M. (1893). On \u201dConfocal Quadrics of Moments of Inertia\u201d pertaining to all Planes\nin Space, and Loci and Envelopes of Straight Lines whose \u201dMoments of Inertia\u201d are Constants,\nProceedings of the Royal Society of Victoria, 5, 200\u2013208.\n[30] Goodfellow, I., Bengio, Y., Courville, A. (2016). Deep learning The MIT Press.\n[31] Hastie, T., Tibshirani, R., Friedman, J. (2009) The elements of statistical learning Second\nEdition, Springer\n[32] Jacobi, C. (1865) Vorlesungen \u00a8uber dynamik, (in German) English Translation: (2009) Ja-\ncobi\u2019s Lectures on Dynamics, Springer, TRIM 51.\n[33] Jurdjevic, V. (2023) Integrable Systems: In the Footprints of the Greats, Mathematics 2023,\n11, 1063.\n[34] Kelly, B. (2007). Some Aspects of Measurement Error in Linear Regression of Astronomical\nData. The Astrophysical Journal, 665, 1489\u20131506\n[35] Meijer, E., Oczkowski, E. Wansbeek, T. (2021). How measurement error a\ufb00ects inference in\nlinear regression. Empir Econ 60, 131\u2013155\n[36] Pearson, K. (1901). On Lines and Planes of Closest Fit to Systems of Points in Space, Phil.\nMag. 2, 559\u2013572.\n[37] Rogers, A. J. (2013). Concentration Ellipsoids, Their Planes of Support, and the Linear\nRegression Model, Econometric Reviews, 32:2, 220-243, DOI:10.1080/07474938.2011.608055.\n[38] Seber, G., Lee, A. (2003) Linear Regression Analysis Second Edition, John Wiley and Sons.\n[39] Suslov, G.K. (1900) Fundamentals of Analytical Mechanics, Vol.1, Kiev, [in Russian].\n[40] Weisberg, S. (1985) Applied Linear Regression, Second Edition, John Willey and Sons.\nDepartment of Mathematical Sciences, The University of Texas at Dallas, Richard-\nson, TX, USA, Mathematical Institute, of the Serbian Academy of Sciences and Arts,\nBelgrade, Serbia\nEmail address: Vladimir.Dragovic@utdallas.edu\nMathematical Institute, of the Serbian Academy of Sciences and Arts, Belgrade,\nSerbia\nEmail address: gajab@mi.sanu.ac.rs\n"}