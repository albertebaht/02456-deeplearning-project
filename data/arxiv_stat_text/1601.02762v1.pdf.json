{"text": "Adaptive wavelet multivariate regression with errors in variables\nMicha\u00ebl Chichignoud \u2217, Van Ha Hoang \u2020, Thanh Mai Pham Ngoc \u2021and Vincent Rivoirard\u00a7\nJuly 30, 2021\nAbstract\nIn the multidimensional setting, we consider the errors-in-variables model. We aim at estimating\nthe unknown nonparametric multivariate regression function with errors in the covariates. We devise\nan adaptive estimator based on projection kernels on wavelets and a deconvolution operator. We\npropose an automatic and fully data driven procedure to select the wavelet level resolution.\nWe\nobtain an oracle inequality and optimal rates of convergence over anisotropic H\u00f6lder classes. Our\ntheoretical results are illustrated by some simulations.\nKeywords : Adaptive wavelet estimator. Anisotropic regression. Deconvolution. Measurement errors.\nPrimary subjects. 62G08\n1\nIntroduction\nWe consider the problem of multivariate nonparametric regression with errors in variables. We observe\nthe i.i.d dataset\n(W1, Y1), . . . , (Wn, Yn)\nwhere\nYl = m(Xl) + \u03b5l\nand\nWl = Xl + \u03b4l,\nwith Yl \u2208R. The covariates errors \u03b4l are i.i.d unobservable random variables having error density g. We\nassume that g is known. The \u03b4l\u2019s are independent of the Xl\u2019s and Yl\u2019s. The \u03b5l\u2019s are i.i.d standard normal\nrandom variables with variance s2. We wish to estimate the regression function m(x), x \u2208[0, 1]d, but\ndirect observations of the covariates Xl are not available. Instead due to the measuring mechanism or\nthe nature of the environment, the covariates Xl are measured with errors. Let us denote fX the density\nof the Xl\u2019s assumed to be positive and fW the density of the Wl\u2019s.\nUse of errors-in-variables models appears in many areas of science such as medicine, econometry or\nastrostatistics and is appropriate in a lot of practical experimental problems. For instance, in epidemio-\nlogic studies where risk factors are partially observed (see Whittemore and Keller (1988), Fan and Masry\n(1992)) or in environmental science where air quality is measured with errors (Delaigle et al. (2015)).\nIn the error-free case, that is \u03b4l = 0, one retrieves the classical multivariate nonparametric regression\nproblem. Estimating a function in a nonparametric way from data measured with error is not an easy\nproblem. Indeed, constructing a consistent estimator in this context is challenging as we have to face to a\ndeconvolution step in the estimation procedure. Deconvolution problems arise in many \ufb01elds where data\nare obtained with measurement errors and has attracted a lot of attention in the statistical literature, see\n\u2217ETH Z\u00fcrich\n\u2020Laboratoire Paul Painlev\u00e9 UMR CNRS 8524, Universit\u00e9 Lille 1- Sciences et Technologies.\n\u2021Laboratoire de Math\u00e9matiques, UMR 8628, Universit\u00e9 Paris Sud.\n\u00a7CEREMADE, UMR CNRS 7534, Universit\u00e9 Paris Dauphine.\n1\narXiv:1601.02762v1  [math.ST]  12 Jan 2016\nMeister (2009) for an excellent source of references. The nonparametric regression with errors-in-variables\nmodel has been the object of a lot of attention as well, we may cite the works of Fan and Masry (1992),\nFan and Truong (1993), Ioannides and Alevizos (1997), Koo and Lee (1998), Meister (2009), Comte and\nTaupin (2007), Chesneau (2010), Du et al. (2011), Carroll et al. (2009), Delaigle et al. (2015).\nThe\nliterature has mainly to do with kernel-based approaches, based on the Fourier transform. All the works\ncited have tackled the univariate case except for Fan and Masry (1992) where the authors explored the\nasymptotic normality for mixing processes. In the one dimensional setting, Chesneau (2010) used Meyer\nwavelets in order to devise his statistical procedure but his assumptions on the model are strong since the\ncorrupted observations Wl follow a uniform density on [0, 1]. Comte and Taupin (2007) investigated the\nmean integrated squared error with a penalized estimator based on projection methods upon Shannon\nbasis. But the authors do not give any clue about how to choose the resolution level of the Shannon\nbasis. Furthermore, the constants in the penalized term are calibrated via intense simulations.\nIn the present article, our aim is to study the multidimensional setting and the pointwise risk. We\nwould like to take into account the anisotropy for the function to estimate. Our approach relies on the\nuse of projection kernels on wavelets bases combined with a deconvolution operator taking into account\nthe noise in the covariates. When using wavelets, a crucial point lies in the choice of the resolution level.\nBut it is well-known that theoretical results in adaptive estimation do not provide the way to choose the\nnumerical constants in the resolution level and very often lead to conservative choices. We may cite the\nwork of Gach et al. (2013) which attempts to tackle this problem. For the density estimation problem and\nthe sup-norm loss, the authors based their statistical procedure on Haar projection kernels and provide\na way to choose locally the resolution level. Nonetheless, in practice, their procedure relies on heavy\nMonte Carlo simulations to calibrate the constants. In our paper the resolution level of our estimator is\noptimal and fully data-driven. It is automatically selected by a method inspired from Goldenshluger and\nLepski (2011) to tackle anisotropy problems. This method has been used recently in various contexts\n(see Doumic et al. (2012), Comte and Lacour (2013) and Bertin et al. (2013)). Furthermore, we do not\nresort to thresholding which is very popular when using wavelets and our selection rule is adaptive to the\nunknown regularity of the regression function. We obtain oracle inequalities and provide optimal rates\nof convergence for anisotropic H\u00f6lder classes. The performances of our adaptive estimator, the negative\nimpact of the errors in the covariates, the e\ufb00ects of the design density are assessed by examples based on\nsimulations.\nThe paper is organized as follows. In Section 2, we describe our estimation procedure. In Section 3,\nwe provide an oracle inequality and rates of convergences of our estimator for the pointwise risk. Section\n4 gives some numerical illustrations. Proofs of Theorems, propositions and technical lemmas are to be\nfound in section 5.\nNotation\nLet N = {0, 1, 2, . . . } and j = (j1, . . . , jd) \u2208Nd, we set Sj = Pd\ni=1 ji and for any y \u2208Rd, we\nset, with a slight abuse of notation,\n2jy := (2j1y1, . . . , 2jdyd)\nand for any k = (k1, \u00b7 \u00b7 \u00b7 , kd) \u2208Zd,\nhj,k(y) := 2\nSj\n2 h(2jy \u2212k) = 2\nSj\n2 h(2j1y1 \u2212k1, . . . , 2jdyd \u2212kd),\nfor any given function h. We denote by F the Fourier transform of any function f de\ufb01ned on Rd by\nF(f)(t) =\nZ\nRd e\u2212i<t,y>f(y)dy,\nt \u2208Rd,\nwhere < ., . > denotes the usual scalar product.\nFor two integers a, b, we denote a \u2227b := min(a, b) and a \u2228b := max(a, b). And \u230ay\u230bdenotes the largest\ninteger smaller than y : \u230ay\u230b\u2264y < \u230ay\u230b+ 1.\n2\n2\nThe estimation procedure\nFor estimating the regression function m, the idea consists in writing m as the ratio\nm(x) = m(x)fX(x)\nfX(x)\n,\nx \u2208[0, 1]d.\nIn the sequel, we denote\np(x) := m(x) \u00d7 fX(x).\nSo, we estimate p, then fX. Since estimating fX is a classical deconvolution problem, the main task\nconsists in estimating p. We propose a wavelet-based procedure with an automatic choice of the maximal\nresolution level. Section 2.1 describes the construction of the projection kernel on wavelet bases depending\non a maximal resolution level. Section 2.2 describes the Goldenshluger-Lepski procedure to select the\nresolution level adaptively.\n2.1\nApproximation kernels and family of estimators for p\nWe consider noise densities g = (g1, \u00b7 \u00b7 \u00b7 , gd) which satisfy the following relationship (see Fan and Koo\n(2002)) :\nF(g)(t) =\nd\nY\nl=1\nF(gl)(tl),\ntl \u2208R.\n(1)\nIn the sequel, we consider a father wavelet \u03d5 on the real line satisfying the following conditions:\n\u2022 (A1) The father wavelet \u03d5 is compactly supported on [\u2212A, A], where A is a positive integer.\n\u2022 (A2) There exists a positive integer N, such that for any x\nZ X\nk\u2208Z\n\u03d5(x \u2212k)\u03d5(y \u2212k)(y \u2212x)\u2113dy = \u03b40\u2113,\n\u2113= 0, . . . , N.\n\u2022 (A3) \u03d5 is of class Cr, where r \u22652.\nThese properties are satis\ufb01ed for instance by Daubechies and Coi\ufb02ets wavelets (see H\u00e4rdle et al. (1998),\nchapter 8). The associated projection kernel on the space\nVj := span{\u03d5jk, k \u2208Zd},\nj \u2208Nd,\nis given for any x and y by\nKj(x, y) =\nX\nk\n\u03d5jk(x)\u03d5jk(y),\nwhere for any x,\n\u03d5jk(x) =\nd\nY\nl=1\n2\njl\n2 \u03d5(2jlxl \u2212kl),\nj \u2208Nd, k \u2208Zd.\nTherefore, the projection of p on Vj can be written for any z,\npj(z) := Kj(p)(z) :=\nZ\nKj(z, y)p(y)dy =\nX\nk\npjk\u03d5jk(z)\nwith\npjk =\nZ\np(y)\u03d5jk(y)dy.\n3\nFirst we estimate unbiasedly any projection pj. Secondly to obtain the \ufb01nal estimate of p, it will remain\nto select a convenient value of j which will be done in section 2.2. The natural approach is based on\nunbiased estimation of the projection coe\ufb03cients pjk. To do so, we adapt the kernel approach proposed\nby Fan and Truong (1993) in our wavelets context. To this purpose, we set\n\u02c6pjk := 1\nn\nn\nX\nu=1\nYu \u00d7 (Dj\u03d5)j,k(Wu) = 2\nSj\n2 1\nn\nn\nX\nu=1\nYu\nZ\ne\u2212i<t,2jWu\u2212k>\nd\nY\nl=1\nF(\u03d5)(tl)\nF(gl)(2jltl)dt,\n\u02c6pj(x) = 1\nn\nX\nk\nn\nX\nu=1\nYu \u00d7 (Dj\u03d5)j,k(Wu)\u03d5jk(x),\nwhere the deconvolution operator Dj is de\ufb01ned as follows for a function f de\ufb01ned on R\n(Djf)(w) =\nZ\ne\u2212i<t,w>\nd\nY\nl=1\nF(f)(tl)\nF(gl)(2jltl)dt, w \u2208Rd.\n(2)\nLemma 3, proved in section 5.2.1 states that E[\u02c6pj(x)] = pj(x) which justi\ufb01es our approach. Furthermore,\nthe deconvolution operator (Djf)(w) in (2) is the multidimensional wavelet analogous of the operator\nKn(x) de\ufb01ned in (2.4) in Fan and Truong (1993): the Fourier transform of their kernel K has been\nreplaced in our procedure by the Fourier transform of the wavelet \u03d5jk and their bandwith h by 2\u2212j.\nNote that the de\ufb01nition of the estimator \u02c6pj(x) still makes sense when we do not have any noise on\nthe variables Xl i.e g(x) = \u03b40(x) because in this case F(g)(t) = 1.\n2.2\nSelection rule by using the Goldenshluger-Lepski methodology\nThe second and \ufb01nal step consists in selecting the multidimensional resolution level j depending on x\nand based on a data-driven selection rule inspired from a method exposed in Goldenshluger and Lepski\n(2011). To de\ufb01ne this latter we have to introduce some quantities. In the sequel we denote for any\nw \u2208Rd,\nTj(w) :=\nX\nk\n(Dj\u03d5)j,k(w)\u03d5jk(x)\nand\nUj(y, w) := y\nX\nk\n(Dj\u03d5)j,k(w)\u03d5jk(x) = y \u00d7 Tj(w),\nso we have\n\u02c6pj(x) = 1\nn\nn\nX\nu=1\nUj(Yu, Wu).\nProposition 1 in Section 5.2.1 shows that \u02c6pj(x) concentrates around pj(x). So the idea is to \ufb01nd a maximal\nresolution \u02c6j that mimics the oracle index. The oracle index minimizes a bias variance trade-o\ufb00. So we\nhave to \ufb01nd an estimation for the bias-variance decomposition of \u02c6pj(x). We denote \u03c32\nj := Var(Uj(Y1, W1))\nand the variance of \u02c6pj is thus equal to\n\u03c32\nj\nn . We set :\n\u02c6\u03c32\nj :=\n1\nn(n \u22121)\nn\nX\nl=2\nl\u22121\nX\nv=1\n(Uj(Yl, Wl) \u2212Uj(Yv, Wv))2,\n(3)\nand since E(\u02c6\u03c32\nj ) = \u03c32\nj , \u02c6\u03c32\nj is a natural estimator of \u03c32\nj . To devise our procedure, we introduce a slightly\noverestimate of \u03c32\nj given by:\n\u02dc\u03c32\nj,\u02dc\u03b3 := \u02c6\u03c32\nj + 2Cj\nr\n2\u02dc\u03b3\u02c6\u03c32\nj\nlog n\nn\n+ 8\u02dc\u03b3C2\nj\nlog n\nn\n,\n(4)\n4\nwhere \u02dc\u03b3 is a positive constant and\nCj :=\n\u0010\n\u2225m\u2225\u221e+ s\np\n2\u02dc\u03b3 log n\n\u0011\n\u2225Tj\u2225\u221e.\nFor any \u03b5 > 0, let \u03b3 > 0 and\n\u0393\u03b3(j) :=\ns\n2\u03b3(1 + \u03b5)\u02dc\u03c32\nj,\u02dc\u03b3 log n\nn\n+ cj\u03b3 log n\nn\n,\nwhere\ncj := 16 (2\u2225m\u2225\u221e+ s) \u2225Tj\u2225\u221e.\nLet\n\u0393\u03b3(j, j\u2032) := \u0393\u03b3(j) + \u0393\u03b3(j \u2227j\u2032),\nand\n\u0393\u2217\n\u03b3(j) := sup\nj\u2032 \u0393\u03b3(j, j\u2032).\n(5)\nWe now de\ufb01ne the selection rule for the resolution index. Let\n\u02c6Rj := sup\nj\u2032\nn\n|\u02c6pj\u2227j\u2032(x) \u2212\u02c6pj\u2032(x)| \u2212\u0393\u03b3(j\u2032, j)\no\n+ + \u0393\u2217\n\u03b3(j).\n(6)\nThen \u02c6p\u02c6j(x) is the \ufb01nal estimator of p(x) with \u02c6j such that\n\u02c6j := arg min\nj\u2208J\n\u02c6Rj,\n(7)\nwhere the set J is de\ufb01ned as\nJ :=\n\u001a\nj \u2208Nd :\n2Sj \u2264\n\u0016\nn\nlog2 n\n\u0017\u001b\n.\n(8)\nNow, we shall highlight how the above quantities interplay in the estimation of the risk decomposition of\n\u02c6pj. An inspection of the proof of Theorem 1 shows that a control of the bias of \u02c6pj is provided by :\nsup\nj\u2032\nn\n|\u02c6pj\u2227j\u2032(x) \u2212\u02c6pj\u2032(x)| \u2212\u0393\u03b3(j\u2032, j)\no\n+.\nThe term |\u02c6pj\u2227j\u2032(x)\u2212\u02c6pj\u2032| is classical when using the Goldenshluger Lepski method (see sections 2.1 and 5.2\nin Bertin et al. (2013)). Furthermore for technical reasons (see proof of Theorem 1), we do not estimate\nthe variance of \u02c6pj(x) by\n\u02c6\u03c32\nj\nn but we replace it by \u03932\n\u03b3(j). Note that we have the straightforward control\n\u0393\u03b3(j) \u2264C\n \n\u02c6\u03c3j\nr\nlog n\nn\n+ (Cj + cj)log n\nn\n!\n,\nwhere C is a constant depending on \u03b5, \u02dc\u03b3 and \u03b3. Actually we prove that \u03932\n\u03b3(j) is of order log n\nn \u03c32\nj (see Lemma\n6 and 10). The dependance of \u02dc\u03c32\nj,\u02dc\u03b3 (4) in \u2225m\u2225\u221eappears only in smaller order terms. In conclusion, up\nto the knowledge of \u2225m\u2225\u221ethe procedure is completely data-driven. Next section explains how to choose\nthe constants \u03b3 and \u02dc\u03b3. Our approach is non asymptotic and based on sharp concentration inequalities.\n5\n3\nRates of convergence\nThere exists C1 > 0 such that for any x \u2208[0, 1]d, fX(x) \u2265C1.\nAs we face a deconvolution problem, we need to de\ufb01ne the assumptions made on the smoothness of\nthe density of the errors covariates g. There exist positive constants cg and Cg such that\ncg(1 + |tl|)\u2212\u03bd \u2264|F(gl)(tl)| \u2264Cg(1 + |tl|)\u2212\u03bd,\n0 \u2264\u03bd \u2264r \u22122,\ntl \u2208R.\n(9)\nWe also require a condition for the derivative of the Fourier transform of g. There exists a positive\nconstant Cg such that\n|F\u2032(gl)(tl)| \u2264Cg(1 + |tl|)\u2212\u03bd\u22121,\ntl \u2208R.\n(10)\nLaplace and Gamma distributions satisfy the above assumptions (9) and (10). Assumptions (9) and (10)\ncontrol the decay of the Fourier transform of g at a polynomial rate. Hence we deal with a midly ill-posed\ninverse problem. The index \u03bd is usually known as the degree of ill-posedness of the deconvolution problem\nat hand.\n3.1\nOracle inequality and rates of convergence for p(\u00b7)\nFirst, we state an oracle inequality which highlights the bias-variance decomposition of the risk.\nTheorem 1. Let q \u22651 be \ufb01xed and let \u02c6j be the adaptive index de\ufb01ned as above. Then, it holds for any\n\u03b3 > q(\u03bd + 1) and \u02dc\u03b3 > 2q(\u03bd + 2),\nE\nh\f\f\f\u02c6p\u02c6j(x) \u2212p(x)\n\f\f\f\nqi\n\u2264R1\n\u0012\ninf\n\u03b7 E\n\u0002\b\nB(\u03b7) + \u0393\u2217\n\u03b3(\u03b7)\n\tq\u0003\u0013\n+ o(n\u2212q),\nwhere\nB(\u03b7) := max\n\u0012\nsup\nj\u2032 |E [\u02c6p\u03b7\u2227j\u2032(x)] \u2212E [\u02c6pj\u2032(x)]| , |E[\u02c6p\u03b7(x)] \u2212p(x)|\n\u0013\nand R1 a constant depending only on q.\nThe oracle inequality in Theorem 1 illustrates a bias-variance decomposition of the risk. The term\nB(\u03b7) is a bias term. Indeed, one recognizes on the right side the classical bias term\n|E[\u02c6p\u03b7(x)] \u2212p(x)| = |p\u03b7(x) \u2212p(x)|.\nConcerning |E [\u02c6p\u03b7\u2227j\u2032(x)] \u2212E [\u02c6pj\u2032(x)]|, for sake of clarity let us consider for instance the univariate case :\nif j\u2032 \u2264\u03b7 this term is equal to zero. If j\u2032 \u2265\u03b7, it turns to be\n|E [\u02c6p\u03b7(x)] \u2212E [\u02c6pj\u2032(x)] | = |p\u03b7(x) \u2212pj\u2032(x)| \u2264|p\u03b7(x) \u2212p(x)| + |pj\u2032(x) \u2212p(x)|.\nAs we have the following inclusion for the projection spaces V\u03b7 \u2282Vj\u2032, the term pj\u2032 is closer to p than p\u03b7\nfor the L2-distance. Hence we expect a good control of |pj\u2032(x) \u2212p(x)| with respect to |p\u03b7(x) \u2212p(x)|.\nWe study the rates of convergence of the estimators over anisotropic H\u00f6lder Classes. Let us de\ufb01ne\nthem.\nDe\ufb01nition 1 (Anisotropic H\u00f6lder Space). Let \u20d7\u03b2 = (\u03b21, \u03b22, . . . , \u03b2d) \u2208(R\u2217\n+)d and L > 0. We say that\nf : [0, 1]d \u2192R belongs to the anisotropic H\u00f6lder class Hd(\u20d7\u03b2, L) of functions if f is bounded and for any\nl = 1, ..., d and for all z \u2208R\nsup\nx\u2208[0,1]d\n\f\f\f\f\f\n\u2202\u230a\u03b2l\u230bf\n\u2202x\u230a\u03b2l\u230b\nl\n(x1, . . . , xl + z, . . . , xd) \u2212\u2202\u230a\u03b2l\u230bf\n\u2202x\u230a\u03b2l\u230b\nl\n(x1, . . . , xl, . . . , xd)\n\f\f\f\f\f \u2264L|z|\u03b2l\u2212\u230a\u03b2l\u230b.\n6\nThe following theorem gives the rate of convergence of the estimator \u02c6p\u02c6j(x) and justi\ufb01es the optimality\nof our oracle inequality.\nTheorem 2. Let q \u22651 be \ufb01xed and let \u02c6j be the adaptive index de\ufb01ned in (7). Then, for any \u20d7\u03b2 \u2208(0, 1]d\nand L > 0, it holds\nsup\np\u2208Hd(\u20d7\u03b2,L)\nE\n\f\f\f\u02c6p\u02c6j(x) \u2212p(x)\n\f\f\f\nq\n\u2264L\nq(2\u03bd+1)\n2 \u00af\n\u03b2+2\u03bd+1 R2\n\u0012log(n)\nn\n\u0013q \u00af\u03b2/(2 \u00af\u03b2+2\u03bd+1)\n,\nwith \u00af\u03b2 =\n1\n1\n\u03b21 +\u00b7\u00b7\u00b7+ 1\n\u03b2d\nand R2 a constant depending on \u03b3, q, \u03b5, \u02dc\u03b3, \u2225m\u2225\u221e, s, \u2225fX\u2225\u221e, \u03d5, cg, Cg, \u20d7\u03b2.\nRemark 1. The estimate \u02c6p achieves the optimal rate of convergence up to a logarithmic term (see section\n3.3 in Comte and Lacour (2013)). This logarithmic loss, due to adaptation, is known to be nevertheless\nunavoidable for d = 1 and one can conjecture that it is also the case for higher dimension (see Remark\n1 in Comte and Lacour (2013)) .\n3.2\nRates of convergence for m(\u00b7)\nAs mentioned above, the estimation of m requires an adaptive estimate of fX. This is due to kernel\nestimators, e.g. projection estimators do not need the additional estimate (see Bertin et al. (2013)). For\nthis purpose, we use an estimate introduced by Comte and Lacour (2013) (Section 3.4) denoted by \u02c6fX.\nThis estimate is constructed from a deconvolution kernel and the bandwidth is selected via a method\ndescribed in Goldenshluger and Lepski (2011). We will not give the explicit expression of \u02c6fX for ease of\nexposition. Then, we de\ufb01ne the estimate of m for all x in [0, 1]d :\n\u02c6m(x) =\n\u02c6p\u02c6j(x)\n\u02c6fX(x) \u2228n\u22121/2 .\n(11)\nThe term n\u22121/2 is added to avoid the drawback when \u02c6fX is closed to 0.\nTheorem 3. Let q \u22651 be \ufb01xed and let \u02c6m de\ufb01ned as above. Then, for any \u20d7\u03b2 \u2208(0, 1]d and L > 0, it holds\nsup\nm\u2208Hd(\u20d7\u03b2,L)\nE | \u02c6m(x) \u2212m(x)|q \u2264L\nq(2\u03bd+1)\n2 \u00af\n\u03b2+2\u03bd+1 R3\n\u0012log(n)\nn\n\u0013q \u00af\u03b2/(2 \u00af\u03b2+2\u03bd+1)\n,\nwith R3 a constant depending on \u03b3, q, \u03b5, \u02dc\u03b3, \u2225m\u2225\u221e, s, \u2225fX\u2225\u221e, \u03d5, cg, Cg, \u20d7\u03b2.\nThe estimate \u02c6m is again optimal up to a logarithmic term (see Remark 1).\n4\nNumerical results\nIn this section, we implement some simulations to illustrate the theoretical results. We aim at estimating\nthe Doppler regression function m at two points x0 = 0.25 and x0 = 0.90 (see Figure 1). We have\nn = 1024 observations and the regression errors \u03b5l\u2019s follow a standard normal density with variance\ns2 = 0.152. As for the design density of the Xl\u2019s, we consider the Beta density and the uniform density\non [0, 1]. The uniform distribution is quite classical in regression with random design. The Beta(2, 2) and\nBeta(0.5, 2) distributions re\ufb02ect two very di\ufb00erent behaviors on [0, 1]. Indeed, we recall that the Beta\ndensity with parameters (a, b) (denoted here by Beta(a, b)) is proportional to xa\u22121(1 \u2212x)b\u221211[0,1](x).\nIn Figure 2, we plot the noisy regression Doppler function according to the three design scenario. For\n7\nthe covariate errors \u03b4i\u2019s, we focus on the centered Laplace density with scale parameter \u03c3gL > 0 that we\ndenote gL. This latter has the following expression :\ngL(x) =\n1\n2\u03c3gL\ne\n\u2212|x|\n\u03c3gL .\nThe choice of the centered Laplace noise is motivated by the fact that the Fourier transform of gL is given\nby\nF(gL)(t) =\n1\n1 + \u03c32gLt2 ,\nand according to assumption (9), it gives an example of an ordinary smooth noise with degree of ill-\nposedness \u03bd = 2. Furthermore, when facing regression problems with errors in the design, it is common\nto compute the so-called reliability ratio (see Fan and Truong (1993)) which is given by\nRr :=\nVar(X)\nVar(X) + 2\u03c32gL\n.\nRr permits to assess the amount of noise in the covariates. The closer to 0 Rr is, the bigger the amount\nof noise in the covariates is and the more di\ufb03cult the deconvolution step will be. For instance, Fan\nand Truong (1993) chose Rr = 0.70. We computed the reliability ratio in Table 1 for the considered\nsimulations.\n\u03c3gL\ndesign of the Xi\nU[0, 1]\nBeta(2, 2)\nBeta(0.5, 2)\n0.075\n0.88\n0.81\n0.80\n0.10\n0.80\n0.71\n0.69\nTable 1: Reliability ratio.\nWe recall that our estimator of m(x) is given by the ratio of two estimators (see (11)) :\n\u02c6m(x) =\n\u02c6p\u02c6j(x)\n\u02c6fX(x) \u2228n\u22121/2 .\n(12)\nFirst, we compute \u02c6p\u02c6j(x) an estimator of p(x) = m(x) \u00d7 fX(x) which is denoted \"GL\" in the graphics\nbelow. We use coi\ufb02et wavelets of order 5. Then we divide \u02c6p\u02c6j(x) by the adaptive deconvolution density\nestimator \u02c6fX(x) of Comte and Lacour (2013). This latter is constructed with a deconvolution kernel and\nan adaptive bandwidth. For the selection of the coi\ufb02et level \u02c6j in \u02c6p\u02c6j(x), we advise to use \u02c6\u03c32\nj instead of \u02dc\u03c32\nj,\u02dc\u03b3\nand 2 maxi |Yi|\u2225Tj\u2225\u221e\n3\ninstead of cj. It remains to settle the value of the constant \u03b3. To do so, we compute\nthe pointwise risk of \u02c6p\u02c6j(x) in function of \u03b3: Figure 3 shows a clear \"dimension jump\" and accordingly\nthe value \u03b3 = 0.5 turns to be reasonable. Hence we \ufb01x \u03b3 = 0.5 for all simulations and our selection rule\nis completely data-driven.\n\u03c3gL\ndesign of the Xi\nU[0, 1]\nBeta(2, 2)\nBeta(0.5, 2)\n0.075\n0.0144\n0.0204\n0.0071\n0.10\n0.0156\n0.0206\n0.0072\n\u03c3gL\ndesign of the Xi\nU[0, 1]\nBeta(2, 2)\nBeta(0.5, 2)\n0.075\n0.0212\n0.0177\n0.1012\n0.10\n0.0192\n0.0195\n0.104\nTable 2: MAE of \u02c6m(x): on the left at x0 = 0.25 and on the right x0 = 0.90.\n8\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\n0\n0.25\n0.4\n0.6\n0.8\n0.9\n1\n0.15\n0.20\n0.25\n0.30\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\n0.80\n0.85\n0.90\n0.95\n1.00\n0.0\n0.1\n0.2\n0.3\n0.4\n(a)\n(b)\n(c)\nFigure 1: a/ Representation of Doppler function. b/ A zoom of Doppler function on [0.15, 0.30]. c/ A\nzoom of Doppler function on [0.80, 1].\n\u22120.5\n0.0\n0.5\n0\n0.25\n0.4\n0.6\n0.8\n0.9\n1\n\u22120.5\n0.0\n0.5\n0\n0.25\n0.4\n0.6\n0.8\n0.9\n1\n\u22120.5\n0.0\n0.5\n0\n0.25\n0.4\n0.6\n0.8\n0.9\n(a)\n(b)\n(c)\nFigure 2: a/ Noisy Doppler with Xi \u223cU[0, 1]. b/ Noisy Doppler with Xi \u223cBeta(2, 2). c/ Noisy Doppler\nfunction with Xi \u223cBeta(0.5, 2).\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.5\n1.0\n1.5\n2.0\ngamma\nPointwise risk\nOracle risk\nGL risk\nFigure 3: Pointwise risk of \u02c6p\u02c6j at x0 = 0.25 in function of parameter \u03b3 for the Beta(2, 2) design and\n\u03c3gL = 0.075.\nBoxplots in Figure 4 and 5 summarize our numerical experiments. Theorem 1 gives an oracle inequality\nfor the estimation of p(x). We compare the pointwise risk error of \u02c6p\u02c6j(x) (computed with 100 Monte Carlo\nrepetitions) with the oracle risk one. The oracle is \u02c6pjoracle with the index joracle de\ufb01ned as follows:\njoracle := arg min\nj\u2208J |\u02c6pj(x) \u2212p(x)|.\nIn Table 2, we have computed the MAE (Mean Absolute Error) of \u02c6m(x) over 100 Monte Carlo runs.\n9\nU[0, 1]\nBeta(2, 2)\nBeta(0.5, 2)\n\u03c3gL = 0.075\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nPointwise risk\nGL\nOracle\nG\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nPointwise risk\nGL\nOracle\nGG\nG\nG\nG\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nPointwise risk\nGL\nOracle\n\u03c3gL = 0.10\nG\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nPointwise risk\nGL\nOracle\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nPointwise risk\nGL\nOracle\nG\nGG\nG\nG\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nPointwise risk\nGL\nOracle\nFigure 4: Estimation of p(x) at x0 = 0.25\nU[0, 1]\nBeta(2, 2)\nBeta(0.5, 2)\n\u03c3gL = 0.075\nG\n0.00\n0.05\n0.10\n0.15\nPointwise risk\nGL\nOracle\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nPointwise risk\nGL\nOracle\nG\n0.000\n0.005\n0.010\n0.015\n0.020\nPointwise risk\nGL\nOracle\n\u03c3gL = 0.10\nG\nG\nG\nG\nG\n0.00\n0.05\n0.10\n0.15\nPointwise risk\nGL\nOracle\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nPointwise risk\nGL\nOracle\nG\nG\nG\n0.000\n0.005\n0.010\n0.015\n0.020\nPointwise risk\nGL\nOracle\nFigure 5: Estimation of p(x) at x0 = 0.90\nOur performances are close to those of the oracle (see Figure 4 and 5) and are quite satisfying both\nat x0 = 0.25 and x0 = 0.90. When going deeper into details, increasing the Laplace noise parameter\n\u03c3gL deteriorates sligthly the performances. Hence it seems that our procedure is robust to the noise\nin the covariates and accordingly to the deconvolution step. Concerning the role of the design density,\n10\nwhen considering the Beta(0.5, 2) distribution, we expect the performances to be better near 0 as the\nobservations tend to concentrate near 0 and to be bad close to 1. Indeed, this phenomenon is con\ufb01rmed\nby Table 2. And when comparing the Beta(2, 2) and Beta(0.5, 2) distributions, the performances are\nmuch better for the Beta(0.5, 2) at x0 = 0.25 whereas the Beta(2, 2) distribution yields better results at\nx0 = 0.90. This is what is expected as the two densities charge points near 0 and 1 di\ufb00erently.\n5\nProofs\n5.1\nProofs of theorems\nThis section is devoted to the proofs of theorems. These proofs use some propositions and technical\nlemmas which are respectively in section 5.2.1 and 5.2.2. In the sequel, C is a constant which may vary\nfrom one line to another one.\n5.1.1\nProof of Theorem 1\nProof. We \ufb01rstly recall the basic inequality (a1 +\u00b7 \u00b7 \u00b7+ap)q \u2264pq\u22121(aq\n1 +\u00b7 \u00b7 \u00b7+aq\np) for all a1, . . . , ap \u2208Rp\n+,\np \u2208N and q \u22651. For ease of exposition, we denote \u02c6p\u02c6j(x) = \u02c6p\u02c6j. So, we can show for any \u03b7 \u2208Nd:\n\f\f\f\u02c6p\u02c6j \u2212p(x)\n\f\f\f \u2264\n\f\f\f\u02c6p\u02c6j \u2212\u02c6p\u02c6j\u2227\u03b7\n\f\f\f +\n\f\f\f\u02c6p\u02c6j\u2227\u03b7 \u2212\u02c6p\u03b7\n\f\f\f + |\u02c6p\u03b7 \u2212p(x)|\n\u2264\n\f\f\f\u02c6p\u03b7\u2227\u02c6j \u2212\u02c6p\u02c6j\n\f\f\f \u2212\u0393\u03b3(\u02c6j, \u03b7) + \u0393\u03b3(\u02c6j, \u03b7) +\n\f\f\f\u02c6p\u02c6j\u2227\u03b7 \u2212\u02c6p\u03b7\n\f\f\f \u2212\u0393\u03b3(\u03b7, \u02c6j) + \u0393\u03b3(\u03b7, \u02c6j) + |\u02c6p\u03b7 \u2212p(x)|\n\u2264\n\f\f\f\u02c6p\u03b7\u2227\u02c6j \u2212\u02c6p\u02c6j\n\f\f\f \u2212\u0393\u03b3(\u02c6j, \u03b7) + \u0393\u03b3(\u03b7, \u02c6j) +\n\f\f\f\u02c6p\u02c6j\u2227\u03b7 \u2212\u02c6p\u03b7\n\f\f\f \u2212\u0393\u03b3(\u03b7, \u02c6j) + \u0393\u03b3(\u02c6j, \u03b7) + |\u02c6p\u03b7 \u2212p(x)|\n\u2264\n\f\f\f\u02c6p\u03b7\u2227\u02c6j \u2212\u02c6p\u02c6j\n\f\f\f \u2212\u0393\u03b3(\u02c6j, \u03b7) + \u0393\u2217\n\u03b3(\u03b7) +\n\f\f\f\u02c6p\u02c6j\u2227\u03b7 \u2212\u02c6p\u03b7\n\f\f\f \u2212\u0393\u03b3(\u03b7, \u02c6j) + \u0393\u2217\n\u03b3(\u02c6j) + |\u02c6p\u03b7 \u2212p(x)|\n\u2264\u02c6R\u03b7 + \u02c6R\u02c6j + |\u02c6p\u03b7 \u2212p(x)|\n\u2264\u02c6R\u03b7 + \u02c6R\u02c6j + |E[\u02c6p\u03b7] \u2212p(x)| + |\u02c6p\u03b7 \u2212E[\u02c6p\u03b7]|\n\u2264\u02c6R\u03b7 + \u02c6R\u02c6j + |E[\u02c6p\u03b7] \u2212p(x)| + |\u02c6p\u03b7 \u2212E[\u02c6p\u03b7]| \u2212\u0393\u03b3(\u03b7) + \u0393\u03b3(\u03b7)\n\u2264\u02c6R\u03b7 + \u02c6R\u02c6j + |E[\u02c6p\u03b7] \u2212p(x)| + sup\nj\u2032\nn\n|\u02c6pj\u2032 \u2212E[\u02c6pj\u2032]| \u2212\u0393\u03b3(j\u2032)\no\n+ + \u0393\u2217\n\u03b3(\u03b7)\nBy de\ufb01nition of \u02c6j, we recall that \u02c6R\u02c6j \u2264inf\u03b7 \u02c6R\u03b7 and\n\u02c6R\u03b7 \u2264sup\nj,j\u2032\nn\n|\u02c6pj\u2227j\u2032 \u2212E[\u02c6pj\u2227j\u2032]|\u2212\u0393\u03b3(j\u2227j\u2032)\no\n++sup\nj\u2032\nn\n|\u02c6pj\u2032 \u2212E[\u02c6pj\u2032]|\u2212\u0393\u03b3(j\u2032)\no\n++sup\nj\u2032 |E[\u02c6p\u03b7\u2227j\u2032] \u2212E[\u02c6pj\u2032]|+\u0393\u2217\n\u03b3(\u03b7).\nHence\n\f\f\f\u02c6p\u02c6j \u2212p(x)\n\f\f\f\n\u2264\n2\n\u0014\nsup\nj,j\u2032\nn\n|\u02c6pj\u2227j\u2032 \u2212E[\u02c6pj\u2227j\u2032]| \u2212\u0393\u03b3(j \u2227j\u2032)\no\n+ + sup\nj\u2032\nn\n|\u02c6pj\u2032 \u2212E[\u02c6pj\u2032]| \u2212\u0393\u03b3(j\u2032)\no\n+ + sup\nj\u2032 |E[\u02c6p\u03b7\u2227j\u2032] \u2212E[\u02c6pj\u2032]|\n\u0015\n+2\u0393\u2217\n\u03b3(\u03b7) + |E[\u02c6p\u03b7] \u2212p(x)| + sup\nj\u2032\nn\n|\u02c6pj\u2032 \u2212E[\u02c6pj\u2032]| \u2212\u0393\u03b3(j\u2032)\no\n+ + \u0393\u2217\n\u03b3(\u03b7)\nBy de\ufb01nition of B(\u03b7) = max\n\u0000supj\u2032 |E\u02c6p\u03b7\u2227j\u2032 \u2212E\u02c6pj\u2032| , |E\u02c6p\u03b7 \u2212p(x)|\n\u0001\n, we get\n\f\f\f\u02c6p\u02c6j \u2212p(x)\n\f\f\f \u22642 sup\nj,j\u2032\nn\n|\u02c6pj\u2227j\u2032 \u2212E[\u02c6pj\u2227j\u2032]| \u2212\u0393\u03b3(j \u2227j\u2032)\no\n+ + 3 sup\nj\u2032\nn\n|\u02c6pj\u2032 \u2212E[\u02c6pj\u2032]| \u2212\u0393\u03b3(j\u2032)\no\n+ + 3B(\u03b7) + 3\u0393\u2217\n\u03b3(\u03b7)\nConsequently\n\f\f\f\u02c6p\u02c6j \u2212p(x)\n\f\f\f\nq\n\u226432q\u22121\n\u0012\u0002\nB(\u03b7) + \u0393\u2217\n\u03b3(\u03b7)\n\u0003q + sup\nj\u2032\nn\n|\u02c6pj\u2032 \u2212E\u02c6pj\u2032| \u2212\u0393\u03b3(j\u2032)\noq\n+ + sup\nj,j\u2032\nn\n|\u02c6pj\u2227j\u2032 \u2212E\u02c6pj\u2227j\u2032| \u2212\u0393\u03b3(j \u2227j\u2032)\noq\n+\n\u0013\n.\n11\nUsing Proposition 2, we have\nE\n\f\f\f\u02c6p\u02c6j \u2212p(x)\n\f\f\f\nq\n\u2264C\n\u0000E\n\u0002\u0000B(\u03b7) + \u0393\u2217\n\u03b3(\u03b7)\n\u0001q\u0003\u0001\n+ o(n\u2212q).\nThen, we get\nE\n\f\f\f\u02c6p\u02c6j \u2212p(x)\n\f\f\f\nq\n\u2264R1\n\u0012\ninf\n\u03b7 E\n\u0002\u0000B(\u03b7) + \u0393\u2217\n\u03b3(\u03b7)\n\u0001q\u0003\u0013\n+ o(n\u2212q),\nwhere R1 is a constant only depending on q.\n\u25a1\n5.1.2\nProof of Theorem 2\nProof. The proof is a direct application of Theorem 1 together with a standard bias-variance trade-o\ufb00.\nWe \ufb01rst recall the assertion of this theorem:\nE\nh\f\f\f\u02c6p\u02c6j(x) \u2212p(x)\n\f\f\f\nqi\n\u2264C\n\u0012\ninf\n\u03b7 E\n\u0002\u0000B(\u03b7) + \u0393\u2217\n\u03b3(\u03b7)\n\u0001q\u0003\u0013\n+ o(n\u2212q).\nFor the bias term, we use Proposition 3 to get:\nB(\u03b7) \u2264CL\nd\nX\nl=1\n2\u2212\u03b7l\u03b2l, for all \u03b7 \u2208J.\nNow let us focus on E\n\u0002\n\u0393\u2217\n\u03b3(\u03b7)q\u0003\n. We have\nE [\u0393\u03b3(\u03b7)q]\n=\nE\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8ed\ns\n2\u03b3(1 + \u03b5)\u02dc\u03c32\n\u03b7,\u02dc\u03b3 log n\nn\n+ c\u03b7\u03b3 log n\nn\n\uf8f6\n\uf8f8\nq\uf8f9\n\uf8fb\n\u2264\n2q\u22121\n \u00122\u03b3(1 + \u03b5) log n\nn\n\u0013 q\n2\nE[\u02dc\u03c3q\n\u03b7,\u02dc\u03b3] +\n\u0012c\u03b7\u03b3 log n\nn\n\u0013q!\n\u2264\nC\n \u0012log n\nn\n\u0013 q\n2\n2S\u03b7(2\u03bd+1) q\n2 +\n\u0012c\u03b7 log n\nn\n\u0013q!\n,\nusing Lemma 6. But\nc\u03b7 = 16 (2\u2225m\u2225\u221e+ s) \u2225T\u03b7\u2225\u221e\u2264C2S\u03b7(\u03bd+1),\nusing Lemma 10. Hence\nE [\u0393\u03b3(\u03b7)q] \u2264C\n \u0012log n\nn\n\u0013 q\n2\n2S\u03b7(2\u03bd+1) q\n2 +\n\u0012log n\nn\n\u0013q\n2S\u03b7(\u03bd+1)q\n!\n.\nWe have\n\u0012log n\nn\n\u0013 q\n2\n2S\u03b7(2\u03bd+1) q\n2\n\u2265\n\u0012log n\nn\n\u0013q\n2S\u03b7(\u03bd+1)q\u21d0\u21d22S\u03b7 \u2264\nn\nlog n,\nwhich is true since by (8), 2S\u03b7 \u2264\nn\nlog2 n .\nThis yields\nE[\u0393\u2217\n\u03b3(\u03b7)q] \u2264C\n\u00122S\u03b7(2\u03bd+1) log n\nn\n\u0013 q\n2\n.\n12\nEventually, we obtain the bound for the pointwise risk:\nE\n\f\f\f\u02c6p\u02c6j(x) \u2212p(x)\n\f\f\f\nq\n\u2264C\n \ninf\n\u03b7\n(\nL\nd\nX\nl=1\n2\u2212\u03b7l\u03b2l +\nr\n2(2\u03bd+1)S\u03b7 log(n)\nn\n)q!\n+ o(n\u2212q).\nSetting the gradient of the right hand side of the inequality above with respect to \u03b7 it turns out that the\noptimal \u03b7l is proportional to\n2\nlog 2\n\u00af\u03b2\n\u03b2l(2 \u00af\u03b2+2\u03bd+1)(log L + 1\n2 log(\nn\nlog(n))), which leads for n large enough to\nE\n\f\f\f\u02c6p\u02c6j(x) \u2212p(x)\n\f\f\f\nq\n\u2264L\nq(2\u03bd+1)\n2 \u00af\n\u03b2+2\u03bd+1 R2\n\u0012log(n)\nn\n\u0013\n\u00af\n\u03b2q\n2 \u00af\n\u03b2+2\u03bd+1\n,\nwith R2 a constant depending on \u03b3, q, \u03b5, \u02dc\u03b3, \u2225m\u2225\u221e, s, \u2225fX\u2225\u221e, \u03d5, cg, Cg, \u20d7\u03b2. The proof of Theorem 2 is com-\npleted.\n\u25a1\n5.1.3\nProof of Theorem 3\nProof. We recall that m(x) =\np(x)\nfX(x) and \u02c6m(x) =\n\u02c6p\u02c6j(x)\n\u02c6\nfX(x)\u2228n\u22121/2 . We now state the main properties of the\nadaptive estimate \u02c6fX showed by Comte and Lacour (2013) (Theorem 2): for all q \u22651, all \u20d7\u03b2 \u2208(0, 1]d, all\nL > 0 and n large enough, it holds\nP (E1) := P\n\u0010\n| \u02c6fX(x) \u2212fX(x)| \u2265C\u03c6n(\u20d7\u03b2)\n\u0011\n\u2264n\u22122q,\n(13)\nand\nP\n\u0010\n| \u02c6fX(x) \u2212fX(x)| \u2264Cn\n\u0011\n= 1,\n(14)\nwhere \u03c6n(\u20d7\u03b2) := (log(n)/n)\n\u00af\u03b2/(2 \u00af\u03b2+2\u03bd+1). Although the construction of the estimate \u02c6fX(x) depends on q,\nwe remove the dependency for ease of exposition (see Comte and Lacour (2013) Section 3.4 for further\ndetails). From (13), we easily deduce, since fX(x) \u2265C1 > 0, for n large enough that\nP (E2) := P\n\u0012\n\u02c6fX(x) < C1\n2\n\u0013\n\u2264n\u22122q.\n(15)\nWe now start the proof of the theorem. We have together with (14)\n| \u02c6m(x) \u2212m(x)| =\n\f\f\f\f\f\n\u02c6p\u02c6j(x)\n\u02c6fX(x) \u2228n\u22121/2 \u2212p(x)\nfX(x)\n\f\f\f\f\f \u2264\n\f\f\f\f\f\n\u02c6p\u02c6j(x)\n\u02c6fX(x) \u2228n\u22121/2 \u2212\np(x)\n\u02c6fX(x) \u2228n\u22121/2\n\f\f\f\f\f +\n\f\f\f\f\f\np(x)\n\u02c6fX(x) \u2228n\u22121/2 \u2212p(x)\nfX(x)\n\f\f\f\f\f\n\u2264\n\f\f\f\f\f\n\u02c6p\u02c6j(x) \u2212p(x)\n\u02c6fX(x) \u2228n\u22121/2\n\f\f\f\f\f + \u2225m\u2225\u221e\u2225fX\u2225\u221e\n\f\f\f\f\f\n( \u02c6fX(x) \u2228n\u22121/2) \u2212fX(x)\nfX(x)( \u02c6fX(x) \u2228n\u22121/2)\n\f\f\f\f\f\n:= A1 + \u2225m\u2225\u221e\u2225fX\u2225\u221eA2.\nControl of E[Aq\n1].\nUsing Cauchy-Schwarz inequality and the inequality \u02c6fX(x) \u2228n\u22121/2 \u2265n\u22121/2, we\nobtain for n large enough\nE[Aq\n1] = E[Aq\n11Ec\n2] + E[Aq\n11E2] \u2264E[Aq\n11Ec\n2] +\nq\nE[A2q\n1 ]\np\nP(E2)\n\u2264CE\nh\f\f\f\u02c6p\u02c6j(x) \u2212p(x)\n\f\f\f\nqi\n+ nq/2\ns\nE\n\u0014\f\f\f\u02c6p\u02c6j(x) \u2212p(x)\n\f\f\f\n2q\u0015p\nP(E2).\nThen, using Theorem 2 and (15), we \ufb01nally have E[Aq\n1] \u2264C\u03c6q\nn(\u20d7\u03b2).\n13\nControl of E[Aq\n2].\nUsing (14) and the inequality \u02c6fX(x) \u2228n\u22121/2 \u2265n\u22121/2, it holds for n large enough\nE[Aq\n2] \u2264E[Aq\n21Ec\n1\u2229Ec\n2] + E[Aq\n2(1E1 + 1E2)] \u2264E[Aq\n21Ec\n1\u2229Ec\n2] + Cn3q/2(P(E1) + P(E2)).\nThen, using the de\ufb01nition of A2, (13) and (15), we obtain E[Aq\n2] \u2264C\u03c6q\nn(\u20d7\u03b2).\nEventually, by de\ufb01nitions of A1 and A2, the proof is completed and\nE[| \u02c6m(x) \u2212m(x)|q] \u2264C(E[Aq\n1] + E[Aq\n2]) \u2264L\nq(2\u03bd+1)\n2 \u00af\n\u03b2+2\u03bd+1 R3\n\u0012log(n)\nn\n\u0013q \u00af\u03b2/(2 \u00af\u03b2+2\u03bd+1)\n,\nwhere R3 is a constant depending on \u03b3, q, \u03b5, \u02dc\u03b3, \u2225m\u2225\u221e, s, \u2225fX\u2225\u221e, \u03d5, cg, Cg, \u20d7\u03b2. This completes the proof of\nTheorem 3.\n\u25a1\n5.2\nStatements and proofs of auxiliary results\nThis section is devoted to statements and proofs of auxiliary results used in section 5.1\n5.2.1\nStatements and proofs of propositions\nLet us start with Proposition 1 which states a concentration inequality of \u02c6pj around pj.\nProposition 1. Let j be \ufb01xed. For any u > 0,\nP\n\uf8eb\n\uf8ed|\u02c6pj(x) \u2212pj(x)| \u2265\ns\n2\u03c32\nj u\nn\n+ cju\nn\n\uf8f6\n\uf8f8\u22642e\u2212u,\n(16)\nwhere\n\u03c32\nj = Var(Y1Tj(W1)).\nFor any \u02dc\u03b3 > 1 we have for any \u02dc\u03b5 > 0 that there exists R4 only depending on \u02dc\u03b3 and \u02dc\u03b5 such that\nP(\u03c32\nj \u2265(1 + \u02dc\u03b5)\u02dc\u03c32\nj,\u02dc\u03b3) \u2264R4n\u2212\u02dc\u03b3,\n\u02dc\u03c32\nj,\u02dc\u03b3 being de\ufb01ned in (4).\nProof.\nFirst, note that\n\u02c6pj(x) =\nX\nk\n\u02c6pjk\u03d5jk(x) = 1\nn\nn\nX\nl=1\nYl\nX\nk\n(Dj\u03d5)j,k(Wl)\u03d5jk(x) = 1\nn\nn\nX\nl=1\nUj(Yl, Wl).\nTo prove Proposition 1, we apply the Bernstein inequality to the variables Uj(Yl, Wl) \u2212E[Uj(Yl, Wl)]\nthat are independent. Since,\nUj(Yl, Wl) = YlTj(Wl),\nand\nE [\u03b5lTj(Wl)] = 0,\nwe have for any q \u22652,\nAq :=\nn\nX\nl=1\nE[|Uj(Yl, Wl)\u2212E[Uj(Yl, Wl)]|q] =\nn\nX\nl=1\nE [|m(Xl)Tj(Wl) + \u03b5lTj(Wl) \u2212E[m(Xl)Tj(Wl)]|q] . (17)\n14\nWith q = 2,\nA2\n=\nn\nX\nl=1\nE[|Uj(Yl, Wl) \u2212E[Uj(Yl, Wl)]|2]\n=\nnVar(Y1Tj(W1))\n=\nnE[(m(X1)Tj(W1) + \u03b51Tj(W1) \u2212E[m(X1)Tj(W1)])2]\n=\nnE[\u03b52\n1T 2\nj (W1)] + nVar(m(X1)Tj(W1))\n=\nn\n\u0000\u03c32\n\u03b5E[T 2\nj (W1)] + Var(m(X1)Tj(W1))\n\u0001\n.\nNow, for any q \u22653, with Z \u223cN(0, 1),\nAq\n\u2264\nn2q\u22121 (E[|m(X1)Tj(W1) \u2212E[m(X1)Tj(W1)]|q] + E[|\u03b51Tj(W1)|q])\n\u2264\nn2q\u22121 (E[|m(X1)Tj(W1) \u2212E[m(X1)Tj(W1)]|q] + sqE[|Z|q]E[|Tj(W1)|q])\n\u2264\nn2q\u22121 \u0000E[|m(X1)TjW1) \u2212E[m(X1)Tj(W1)]|q] + sqE[|Z|q]E[T 2\nj (W1)]\u2225Tj\u2225q\u22122\n\u221e\n\u0001\n.\nFurthermore,\nE[|m(X1)Tj(W1) \u2212E[m(X1)Tj(W1)]|q]\n\u2264\nE[(m(X1)Tj(W1) \u2212E[m(X1)Tj(W1)])2] \u00d7 (2\u2225m\u2225\u221e\u2225Tj\u2225\u221e)q\u22122\n=\nVar(m(X1)Tj(W1)) \u00d7 (2\u2225m\u2225\u221e\u2225Tj\u2225\u221e)q\u22122.\nFinally,\nAq\n\u2264\nn2q\u22121\u2225Tj\u2225q\u22122\n\u221e\n\u0000Var(m(X1)Tj(W1)) \u00d7 (2\u2225m\u2225\u221e)q\u22122 + sqE[|Z|q]E[T 2\nj (W1)]\n\u0001\n\u2264\nn2q\u22121\u2225Tj\u2225q\u22122\n\u221eE[|Z|q]\n\u0000Var(m(X1)Tj(W1)) \u00d7 (2\u2225m\u2225\u221e)q\u22122 + sqE[T 2\nj (W1)]\n\u0001\n\u2264\nn2q\u22121\u2225Tj\u2225q\u22122\n\u221eE[|Z|q]\n\u0000Var(m(X1)Tj(W1)) + s2E[T 2\nj (W1)]\n\u0001\n\u00d7\n\u0000(2\u2225m\u2225\u221e)q\u22122 + sq\u22122\u0001\n\u2264\n2q\u22121\u2225Tj\u2225q\u22122\n\u221eE[|Z|q] \u00d7 A2 \u00d7 (2\u2225m\u2225\u221e+ s)q\u22122 .\nBesides we have (see page 23 in Patel and Read (1982)) denoting \u0393 the Gamma function\nE[|Z|q] = 2q/2\n\u221a\u03c0 \u0393\n\u0012q + 1\n2\n\u0013\n\u22642q/22\u22121/2q! \u22642(q\u22121)/2q!,\n(18)\nas\n1\n\u221a\u03c0 \u2264\n1\n\u221a\n2 and \u0393( q+1\n2 ) \u2264\u0393(q + 1) = q!. So, for q \u22653,\nAq\n\u2264\n2q\u22121\u2225Tj\u2225q\u22122\n\u221e2(q\u22121)/2q! \u00d7 A2 \u00d7 (2\u2225m\u2225\u221e+ s)q\u22122\n\u2264\nq!\n2 \u00d7 A2 \u00d7\n\u0010\n2\n3q\u22121\n2(q\u22122) \u2225Tj\u2225\u221e(2\u2225m\u2225\u221e+ s)\n\u0011q\u22122\n,\nThe function\n3q\u22121\n2(q\u22122) is decreasing in q. Hence for any q \u22653, 2\n3q\u22121\n2(q\u22122) \u226416.\nThus\nAq \u2264q!\n2 \u00d7 A2 \u00d7 cj\nq\u22122,\n(19)\nwith\ncj := 16\u2225Tj\u2225\u221e(2\u2225m\u2225\u221e+ s) .\nWe can now apply Proposition 2.9 of Massart (2007). We denote fW the density of the Wl\u2019s. We have\nE[T 2\nj (W1)]\n=\nZ\nT 2\nj (w)fW (w)dw\n\u2264\n\u2225fX\u2225\u221e\u2225Tj\u22252\n2,\n15\nsince the density fW is the convolution of fX and g, \u2225fW \u2225\u221e= \u2225fX \u22c6g\u2225\u221e\u2264\u2225fX\u2225\u221e. We have\nVar(m(X1)Tj(W1))\n\u2264\nE[m2(X1)T 2\nj (W1)]\n\u2264\n\u2225m\u22252\n\u221e\nZ\nT 2\nj (w)fW (w)dw\n\u2264\n\u2225m\u22252\n\u221e\u2225fX\u2225\u221e\u2225Tj\u22252\n2.\nTherefore, with\n\u03c32\nj = A2\nn = Var(Y1Tj(W1)),\n(20)\n\u03c32\nj\n=\n\u03c32\n\u03b5E[T 2\nj (W1)] + Var(m(X1)Tj(W1))\n(21)\n\u2264\n\u03c32\n\u03b5\u2225fX\u2225\u221e\u2225Tj\u22252\n2 + \u2225m\u22252\n\u221e\u2225fX\u2225\u221e\u2225Tj\u22252\n2\n\u2264\n\u2225fX\u2225\u221e\u2225Tj\u22252\n2(\u03c32\n\u03b5 + \u2225m\u22252\n\u221e).\n(22)\nWe conclude that for any u > 0,\nP\n\uf8eb\n\uf8ed|\u02c6pj(x) \u2212pj(x)| \u2265\ns\n2\u03c32\nj u\nn\n+ cju\nn\n\uf8f6\n\uf8f8\u22642e\u2212u.\n(23)\nNow, we can write\n\u02c6\u03c32\nj\n=\n1\nn(n \u22121)\nn\nX\nl=2\nl\u22121\nX\nv=1\n(Uj(Yl, Wl) \u2212Uj(Yv, Wv))2\n=\n1\nn(n \u22121)\nn\nX\nl=2\nl\u22121\nX\nv=1\n(Uj(Yl, Wl) \u2212E[Uj(Yl, Wl)] \u2212Uj(Yv, Wv) + E[Uj(Yv, Wv)])2\n=\ns2\nj \u2212\n2\nn(n \u22121)\u03bej,\nwith\ns2\nj\n:=\n1\nn(n \u22121)\nn\nX\nl=2\nl\u22121\nX\nv=1\n(Uj(Yl, Wl) \u2212E[Uj(Yl, Wl)])2 + (Uj(Yv, Wv) \u2212E[Uj(Yv, Wv)])2\n=\n1\nn\nn\nX\nl=1\n(Uj(Yl, Wl) \u2212E[Uj(Yl, Wl)])2\nand\n\u03bej :=\nn\nX\nl=2\nl\u22121\nX\nv=1\n(Uj(Yl, Wl) \u2212E[Uj(Yl, Wl)]) \u00d7 (Uj(Yv, Wv) \u2212E[Uj(Yv, Wv)]).\nIn the sequel, we denote for any \u02dc\u03b3 > 0,\n\u2126n(\u02dc\u03b3) =\n\u001a\nmax\n1\u2264l\u2264n |\u03b5l| \u2264s\np\n2\u02dc\u03b3 log n\n\u001b\n.\nWe have that\nP(\u2126n(\u02dc\u03b3)c) \u2264n1\u2212\u02dc\u03b3.\n(24)\nNote that on \u2126n(\u02dc\u03b3),\n\u2225Uj(\u00b7, \u00b7)\u2225\u221e\u2264Cj,\nwe recall that\nCj = (\u2225m\u2225\u221e+ s\np\n2\u02dc\u03b3 log n)\u2225Tj\u2225\u221e.\n16\nLemma 1. For any \u02dc\u03b3 > 1 and any u > 0, there exists a sequence en,j > 0 such that lim supj en,j = 0\nand\nP\n \n\u03c32\nj \u2265s2\nj + 2Cj\u03c3j\nr\n2u(1 + en,j)\nn\n+ \u03c32\nj u\n3n\n\f\f\f\f\f \u2126n(\u02dc\u03b3)\n!\n\u2264e\u2212u.\nProof.\nWe denote\nP\u2126n(\u02dc\u03b3)(\u00b7) = P (\u00b7|\u2126n(\u02dc\u03b3)) ,\nE\u2126n(\u02dc\u03b3)(\u00b7) = E (\u00b7|\u2126n(\u02dc\u03b3)) .\nNote that conditionally to \u2126n(\u02dc\u03b3) the variables Uj(Y1, W1), . . . , Uj(Yn, Wn) are independent. So, we\ncan apply the classical Bernstein inequality to the variables\nVl := \u03c32\nj \u2212(Uj(Yl, Wl) \u2212E[Uj(Yl, Wl)])2\nn\n\u2264\u03c32\nj\nn .\nFurthermore, as\nE\u2126n(\u02dc\u03b3)[Uj(Y1, W1)]\n=\nE[m(X1)Tj(W1)|\u2126n(\u02dc\u03b3)] + E[\u03b51Tj(W1)|\u2126n(\u02dc\u03b3)]\n=\nE[m(X1)Tj(W1)]\n=\nE[Uj(Y1, W1)]\n(25)\nwe get\nn\nX\nl=1\nE\u2126n(\u02dc\u03b3)[V 2\nl ]\n=\nE\u2126n(\u02dc\u03b3)\n\u0014\u0010\n\u03c32\nj \u2212(Uj(Y1, W1) \u2212E[Uj(Y1, W1)])2\u00112\u0015\nn\n=\n\u03c34\nj + E\u2126n(\u02dc\u03b3)\nh\n(Uj(Y1, W1) \u2212E[Uj(Y1, W1)])4i\n\u22122\u03c32\nj E\u2126n(\u02dc\u03b3)\nh\n(Uj(Y1, W1) \u2212E[Uj(Y1, W1)])2i\nn\n\u2264\n\u03c34\nj + (4C2\nj \u22122\u03c32\nj )E\u2126n(\u02dc\u03b3)\nh\n(Uj(Y1, W1) \u2212E[Uj(Y1, W1)])2i\nn\n.\nWe shall \ufb01nd an upperbound for E\u2126n(\u02dc\u03b3)\n\u0002\n(Uj(Y1, W1) \u2212E[Uj(Y1, W1)])2\u0003\n:\nE\u2126n(\u02dc\u03b3)\n\u0002\n(Uj(Y1, W1) \u2212E[Uj(Y1, W1)])2\u0003\n=\nVar(m(X1)Tj(W1)) + E[\u03b52\n1T 2\nj (W1)|\u2126n(\u02dc\u03b3)]\n=\nVar(m(X1)Tj(W1)) + E[T 2\nj (W1)]E[\u03b52\n11\u2126n(\u02dc\u03b3)]\nP(\u2126n(\u02dc\u03b3))\n\u2264\nVar(m(X1)Tj(W1)) + E[T 2\nj (W1)]\ns2\nP(\u2126n(\u02dc\u03b3))\n\u2264\nVar(m(X1)Tj(W1)) + E[T 2\nj (W1)]\ns2\n1 \u2212n1\u2212\u02dc\u03b3\n=\nVar(m(X1)Tj(W1)) + E[T 2\nj (W1)]s2(1 + \u02dcen),\nwhere \u02dcen = n1\u2212\u02dc\u03b3 + o(n1\u2212\u02dc\u03b3). Using (21) we have\nE\u2126n(\u02dc\u03b3)\n\u0002\n(Uj(Y1, W1) \u2212E[Uj(Y1, W1)])2\u0003\n\u2264(1 + en,j)\u03c32\nj ,\n(26)\nwhere (en,j) is a sequence such that lim supj en,j = 0.\n17\nNow let us \ufb01nd a lower bound for E\u2126n(\u02dc\u03b3)\n\u0002\n(Uj(Y1, W1) \u2212E[Uj(Y1, W1)])2\u0003\n:\nE\u2126n(\u02dc\u03b3)\n\u0002\n(Uj(Y1, W1) \u2212E[Uj(Y1, W1)])2\u0003\n=\nVar(m(X1)Tj(W1)) + E[T 2\nj (W1)]E[\u03b52\n11\u2126n(\u02dc\u03b3)]\nP(\u2126n(\u02dc\u03b3))\n\u2265\nVar(m(X1)Tj(W1)) + E[T 2\nj (W1)]E[\u03b52\n11\u2126n(\u02dc\u03b3)]\n=\nVar(m(X1)Tj(W1)) + E[T 2\nj (W1)]E[\u03b52\n1(1 \u22121\u2126c\nn(\u02dc\u03b3))]\n=\n\u03c32\nj \u2212E[T 2\nj (W1)]E[\u03b52\n11\u2126cn(\u02dc\u03b3)].\nNow using Cauchy Scharwz, (18) and (24) we have\nE\u2126n(\u02dc\u03b3)\n\u0002\n(Uj(Y1, W1) \u2212E[Uj(Y1, W1)])2\u0003\n\u2265\n\u03c32\nj \u2212E[T 2\nj (W1)](E[\u03b54\n1])\n1\n2 (P(\u2126c\nn(\u02dc\u03b3)))\n1\n2\n\u2265\n\u03c32\nj \u2212Cs2E[T 2\nj (W1)]n\n1\u2212\u02dc\u03b3\n2\n=\n\u03c32\nj (1 + \u02dcen,j),\n(27)\nwhere (\u02dcen,j) is a sequence such that lim supj \u02dcen,j = 0.\nFinally, using the bounds we just got for E\u2126n(\u02dc\u03b3)\n\u0002\n(Uj(Y1, W1) \u2212E[Uj(Y1, W1)])2\u0003\nyields\nn\nX\nl=1\nE\u2126n(\u02dc\u03b3)[V 2\nl ]\n\u2264\n\u03c34\nj + 4C2\nj \u03c32\nj (1 + en,j) \u22122\u03c34\nj (1 + \u02dcen,j)\nn\n\u2264\n4C2\nj \u03c32\nj (1 + en,j) \u2212\u03c34\nj (1 + 2\u02dcen,j)\nn\n\u2264\n4C2\nj \u03c32\nj (1 + en,j)\nn\n.\nWe obtain the claimed result.\n\u25a1\nNow, we deal with \u03bej.\nLemma 2. There exists an absolute constant c > 0 such that for any u > 1,\nP\n\u0000\u03bej \u2265c(n\u03c32\nj u + C2\nj u2)\n\f\f \u2126n(\u02dc\u03b3)\n\u0001\n\u22643e\u2212u.\nProof. Note that conditionally to \u2126n(\u02dc\u03b3), the vectors (Yl, Wl)1\u2264l\u2264n are independent. We remind that by\n(25), (26) and (27) we have\nE\u2126n(\u02dc\u03b3)[Uj(Y1, W1)] = E[Uj(Y1, W1)]\n(28)\nand\nE\u2126n(\u02dc\u03b3)\n\u0002\n(Uj(Y1, W1) \u2212E[Uj(Y1, W1)])2\u0003\n= (1 + en,j)\u03c32\nj .\nThe \u03bej can be written as\n\u03bej =\nn\nX\nl=2\nl\u22121\nX\nv=1\ngj(Yl, Wl, Yv, Wv),\nwith\ngj(y, w, y\u2032, w\u2032) = (Uj(y, w) \u2212E[Uj(Y1, W1)])) \u00d7 (Uj(y\u2032, w\u2032) \u2212E[Uj(Y1, W1)]).\nPrevious computations show that conditions (2.3) and (2.4) of Houdr\u00e9 and Reynaud-Bouret (2005) are\nsatis\ufb01ed. So that we are able to apply Theorem 3.1 of Houdr\u00e9 and Reynaud-Bouret (2005): there exist\nabsolute constants c1, c2, c3 and c4 such that for any u > 0,\nP\u2126n(\u02dc\u03b3)\n\u0010\n\u03bej \u2265c1C\u221au + c2Du + c3Bu3/2 + c4Au2\u0011\n\u22643e\u2212u,\nwhere A, B, C, and D are de\ufb01ned and controlled as follows. We have:\nA = \u2225gj\u2225\u221e\u22644C2\nj .\n18\nC2 =\nn\nX\nl=2\nl\u22121\nX\nv=1\nE\u2126n(\u02dc\u03b3)[g2\nj (Yl, Wl, Yv, Wv)] = n(n \u22121)\n2\n\u03c34\nj (1 + en,j)2.\nLet\nA =\n(\n(al)l, (bv)v :\nE\u2126n(\u02dc\u03b3)\n\" n\nX\nl=2\na2\nl (Yl, Wl)\n#\n\u22641, E\u2126n(\u02dc\u03b3)\n\"n\u22121\nX\nl=1\nb2\nl (Yl, Wl)\n#\n\u22641\n)\n.\nWe have:\nD\n=\nsup\n(al)l,(bv)v\u2208A\nE\u2126n(\u02dc\u03b3)\n\" n\nX\nl=2\nl\u22121\nX\nv=1\ngj(Yl, Wl, Yv, Wv)al(Yl, Wl)bv(Yv, Wv)\n#\n=\nsup\n(al)l,(bv)v\u2208A\n\" n\nX\nl=2\nl\u22121\nX\nv=1\nE\u2126n(\u02dc\u03b3) [(Uj(Yl, Wl) \u2212[Uj(Yl, Wl)]))al(Yl, Wl)]\n\u00d7 E\u2126n(\u02dc\u03b3) [(Uj(Yv, Wv) \u2212E[Uj(Yv, Wv)]))bv(Yv, Wv)]\n\u0003\n\u2264\nsup\n(al)l,(bv)v\u2208A\nn\nX\nl=2\nl\u22121\nX\nv=1\n\u03c32\nj (1 + en,j)\nq\nE\u2126n(\u02dc\u03b3)[a2\nl (Yl, Wl)]E\u2126n(\u02dc\u03b3)[b2v(Yv, Wv)]\n\u2264\n\u03c32\nj (1 + en,j)\nsup\n(al)l,(bv)v\u2208A\nn\nX\nl=2\n\u221a\nl \u22121\nv\nu\nu\ntE\u2126n(\u02dc\u03b3)[a2\nl (Yl, Wl)]\nl\u22121\nX\nv=1\nE\u2126n(\u02dc\u03b3)[b2v(Yv, Wv)]\n\u2264\n\u03c32\nj (1 + en,j)\nr\nn(n \u22121)\n2\n.\nFinally,\nB2\n=\nsup\ny,w\nn\u22121\nX\nv=1\nE\u2126n(\u02dc\u03b3)\n\u0002\n(Uj(y, w) \u2212E[Uj(Y1, W1)]))2 \u00d7 (Uj(Yv, Wv) \u2212E[Uj(Y1, W1)])2\u0003\n\u2264\n4(n \u22121)C2\nj \u03c32\nj (1 + en,j).\nTherefore, there exists an absolute constant c > 0 such that for any u > 1,\nc1C\u221au + c2Du + c3Bu3/2 + c4Au2 \u2264c(n\u03c32\nj u + C2\nj u2).\n\u25a1\nLet us go back to the proof of Proposition 1. We apply Lemmas 1 and 2 with u > 1 and we obtain, by\nsetting\nMj(u) = \u02c6\u03c32\nj + 2Cj\u03c3j\nr\n2u(1 + en,j)\nn\n+ \u03c32\nj u\n3n + 2c(n\u03c32\nj u + C2\nj u2)\nn(n \u22121)\n,\nP\n\u0000\u03c32\nj \u2265Mj(u)\n\u0001\n\u2264\nP\n \n\u03c32\nj \u2265s2\nj \u2212\n2\nn(n \u22121)\u03bej + 2Cj\u03c3j\nr\n2u(1 + en,j)\nn\n+ \u03c32\nj u\n3n + 2c(n\u03c32\nj u + C2\nj u2)\nn(n \u22121)\n!\n\u2264\nP\n \n\u03c32\nj \u2265s2\nj + 2Cj\u03c3j\nr\n2u(1 + en,j)\nn\n+ \u03c32\nj u\n3n\n\f\f\f\f\f \u2126n(\u02dc\u03b3)\n!\n+P\n\u0000\u03bej \u2265c(n\u03c32\nj u + C2\nj u2)\n\f\f \u2126n(\u02dc\u03b3)\n\u0001\n+ 1 \u2212P(\u2126n(\u02dc\u03b3)).\nTherefore, with u = \u02dc\u03b3 log n and \u02dc\u03b3 > 1, we obtain for n large enough:\nP\n\u0000\u03c32\nj \u2265Mj(\u02dc\u03b3 log n)\n\u0001\n\u22645n\u2212\u02dc\u03b3.\n19\nAnd there exist a and b two absolute constants such that\nP\n \n\u03c32\nj \u2265\u02c6\u03c32\nj + 2Cj\u03c3j\nr\n2\u02dc\u03b3 log n(1 + en,j)\nn\n+ \u03c32\nj a\u02dc\u03b3 log n\nn\n+ C2\nj b2\u02dc\u03b32 log2 n\nn2\n!\n\u22645n\u2212\u02dc\u03b3.\nNow, we set\n\u03b81 =\n\u0012\n1 \u2212a\u02dc\u03b3 log n\nn\n\u0013\n,\n\u03b82 = Cj\nr\n2\u02dc\u03b3 log n(1 + en,j)\nn\n,\n\u03b83 = \u02c6\u03c32\nj + C2\nj b2\u02dc\u03b32 log2 n\nn2\nso\nP\n\u0000\u03b81\u03c32\nj \u22122\u03b82\u03c3j \u2212\u03b83 \u22650\n\u0001\n\u22645n\u2212\u02dc\u03b3.\nWe study the polynomial\np(\u03c3) = \u03b81\u03c32 \u22122\u03b82\u03c3 \u2212\u03b83.\nSince \u03c3 \u22650, p(\u03c3) \u22650 means that\n\u03c3 \u22651\n\u03b81\n\u0012\n\u03b82 +\nq\n\u03b82\n2 + \u03b81\u03b83\n\u0013\n,\nwhich is equivalent to\n\u03c32 \u22651\n\u03b82\n1\n\u0012\n2\u03b82\n2 + \u03b81\u03b83 + 2\u03b82\nq\n\u03b82\n2 + \u03b81\u03b83\n\u0013\n.\nHence\nP\n\u0012\n\u03c32\nj \u22651\n\u03b82\n1\n\u0012\n2\u03b82\n2 + \u03b81\u03b83 + 2\u03b82\nq\n\u03b82\n2 + \u03b81\u03b83\n\u0013\u0013\n\u22645n\u2212\u02dc\u03b3.\nSo,\nP\n\u0012\n\u03c32\nj \u2265\u03b83\n\u03b81\n+ 2\u03b82\n\u221a\u03b83\n\u03b81\n\u221a\u03b81\n+ 4\u03b82\n2\n\u03b82\n1\n\u0013\n\u22645n\u2212\u02dc\u03b3.\nSo, there exist absolute constants \u03b4, \u03b7, and \u03c4 \u2032 depending only on \u02dc\u03b3 so that for n large enough,\nP\n \n\u03c32\nj \u2265\u02c6\u03c32\nj\n\u0012\n1 + \u03b4 log n\nn\n\u0013\n+\n\u0012\n1 + \u03b7 log n\nn\n\u0013\n2Cj\nr\n2\u02dc\u03b3\u02c6\u03c32\nj (1 + en,j)log n\nn\n+ 8\u02dc\u03b3C2\nj\nlog n\nn\n \n1 + \u03c4 \u2032\n\u0012log n\nn\n\u00131/2!!\n\u22645n\u2212\u02dc\u03b3.\nFinally, for all \u02dc\u03b5 > 0 there exists R4 depending on \u03b5\u2032 and \u02dc\u03b3 such that for n large enough\nP(\u03c32\nj \u2265(1 + \u03b5\u2032)\u02dc\u03c32\nj,\u02dc\u03b3) \u2264R4n\u2212\u02dc\u03b3.\nCombining this inequality with (23), we obtain the desired result of Proposition 1.\n\u25a1\nProposition 2 shows that the residual term in the oracle inequality is negligible.\nProposition 2. We have for any q \u22651,\nE\n\u0014\nsup\nj\u2208J\n(|\u02c6pj(x) \u2212pj(x)| \u2212\u0393\u03b3(j))q\n+\n\u0015\n= o(n\u2212q).\n(29)\nProof. We recall that J =\nn\nj \u2208Nd :\n2Sj \u2264\u230a\nn\nlog2 n\u230b\no\n.\nLet \u02dc\u03b3 > 0 and let us consider the event\n\u02dc\u2126\u02dc\u03b3 =\n\b\n\u03c32\nj \u2264(1 + \u03b5)\u02dc\u03c32\nj,\u02dc\u03b3, \u2200j \u2208J\n\t\n.\n20\nLet \u03b3 > 0. We set in the sequel\nE := E\n\uf8ee\n\uf8f0sup\nj\u2208J\n\uf8eb\n\uf8ed|\u02c6pj(x) \u2212pj(x)| \u2212\ns\n2\u03b3(1 + \u03b5)\u02dc\u03c32\nj,\u02dc\u03b3 log n\nn\n\u2212cj\u03b3 log n\nn\n\uf8f6\n\uf8f8\nq\n+\n1\u02dc\u2126\u02dc\u03b3\n\uf8f9\n\uf8fb,\nand Rj := |\u02c6pj(x) \u2212pj(x)|. We have:\nE\n=\nZ \u221e\n0\nP\n\uf8ee\n\uf8f0sup\nj\u2208J\n\uf8eb\n\uf8edRj \u2212\ns\n2\u03b3(1 + \u03b5)\u02dc\u03c32\nj,\u02dc\u03b3 log n\nn\n\u2212cj\u03b3 log n\nn\n\uf8f6\n\uf8f8\nq\n+\n1\u02dc\u2126\u02dc\u03b3 > y\n\uf8f9\n\uf8fbdy\n\u2264\nX\nj\u2208J\nZ \u221e\n0\nP\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8edRj \u2212\ns\n2\u03b3(1 + \u03b5)\u02dc\u03c32\nj,\u02dc\u03b3 log n\nn\n\u2212cj\u03b3 log n\nn\n\uf8f6\n\uf8f8\nq\n+\n1\u02dc\u2126\u02dc\u03b3 > y\n\uf8f9\n\uf8fbdy\n\u2264\nX\nj\u2208J\nZ \u221e\n0\nP\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8edRj \u2212\ns\n2\u03b3\u03c32\nj log n\nn\n\u2212cj\u03b3 log n\nn\n\uf8f6\n\uf8f8\nq\n> y\n\uf8f9\n\uf8fbdy.\nLet us take u such that\ny = h(u)q,\nwhere\nh(u) =\ns\n2\u03c32\nj u\nn\n+ cju\nn .\nNote that for any u > 0,\nh\u2032(u) \u2264h(u)\nu .\nHence\nE\n\u2264\nC\nX\nj\u2208J\nZ \u221e\n0\nP\n\uf8ee\n\uf8f0Rj >\ns\n2\u03b3\u03c32\nj log n\nn\n+ cj\u03b3 log n\nn\n+\ns\n2u\u03c32\nj\nn\n+ ucj\nn\n\uf8f9\n\uf8fbh(u)q\u22121h\u2032(u)du\n\u2264\nC\nX\nj\u2208J\nZ \u221e\n0\nP\n\uf8ee\n\uf8f0Rj >\ns\n2\u03c32\nj (\u03b3 log n + u)\nn\n+ cj(\u03b3 log n + u)\nn\n\uf8f9\n\uf8fbh(u)q\u22121h\u2032(u)du.\nNow using concentration inequality (16), we get\nE\n\u2264\nC\nX\nj\u2208J\nZ \u221e\n0\ne\u2212(\u03b3 log n+u)h(u)q\u22121h\u2032(u)du\n\u2264\nC\nX\nj\u2208J\nZ \u221e\n0\ne\u2212(\u03b3 log n+u)h(u)q 1\nudu\n\u2264\nCe\u2212\u03b3 log n X\nj\u2208J\nZ \u221e\n0\ne\u2212u\n\uf8eb\n\uf8ed\ns\n2\u03c32\nj u\nn\n+ cju\nn\n\uf8f6\n\uf8f8\nq\n1\nudu\n\u2264\nC\n\uf8eb\n\uf8ede\u2212\u03b3 log n X\nj\u2208J\n \n\u03c32\nj\nn\n!q/2 Z \u221e\n0\ne\u2212uu\nq\n2 \u22121du +\n\u0010cj\nn\n\u0011q Z \u221e\n0\ne\u2212uuq\u22121du\n\uf8f6\n\uf8f8.\n21\nNow using Lemma 10, we have that \u03c32\nj \u2264R102Sj(2\u03bd+1) and cj \u2264C2Sj(\u03bd+1). Hence,\nE\n\u2264\nC\n\uf8eb\n\uf8ede\u2212\u03b3 log n X\nj\u2208J\n\u00122Sj(2\u03bd+1)\nn\n\u0013q/2\n+\n\u00122Sj(\u03bd+1)\nn\n\u0013q\uf8f6\n\uf8f8\n\u2264\nCn\u2212\u03b3+q\u03bd(log n)\u2212(2\u03bd+1)q = o(n\u2212q),\nas soon as \u03b3 > q(\u03bd + 1).\nIt remains to \ufb01nd an upperbound for the following quantity:\nE\u2032 := E\n\uf8ee\n\uf8f0sup\nj\u2208J\n\uf8eb\n\uf8ed|\u02c6pj(x) \u2212pj(x)| \u2212\ns\n2\u03b3(1 + \u03b5)\u02dc\u03c32\nj,\u02dc\u03b3 log n\nn\n\u2212cj\u03b3 log n\nn\n\uf8f6\n\uf8f8\nq\n+\n1\u02dc\u2126c\n\u02dc\u03b3\n\uf8f9\n\uf8fb.\nWe have\nE\u2032\n\u2264\nE\n\u0014\nsup\nj\u2208J\n(|\u02c6pj(x) \u2212pj(x)|q 1\u02dc\u2126c\n\u02dc\u03b3\n\u0015\n\u2264\n2q\u22121\n\u0012\nE\n\u0014\nsup\nj\u2208J\n(|\u02c6pj(x)|)q1\u02dc\u2126c\n\u02dc\u03b3\n\u0015\n+ E\n\u0014\nsup\nj\u2208J\n(|pj(x)|)q1\u02dc\u2126c\n\u02dc\u03b3\n\u0015\u0013\n.\nFirst, let us deal with the term E\nh\nsupj\u2208J(|pj(x)|)q1\u02dc\u2126c\n\u02dc\u03b3\ni\n.\nFollowing the lines of the proof of Lemma 7 we easily get that P\nk \u03d52\njk(x) \u2264C2Sj, hence\n|pj(x)|\n=\n\f\f\f\f\f\nX\nk\npjk\u03d5jk(x)\n\f\f\f\f\f \u2264\n X\nk\np2\njk\n! 1\n2  X\nk\n\u03d52\njk(x)\n! 1\n2\n\u2264\nC\u2225p\u222522\nSj\n2 .\nNow using Proposition 1 which states that P(\u02dc\u2126c\n\u02dc\u03b3) \u2264Cn\u2212\u02dc\u03b3\nE\n\u0014\nsup\nj\u2208J\n(|pj(x)|)q1\u02dc\u2126c\n\u02dc\u03b3\n\u0015\n\u2264\nsup\nj\u2208J\n(\u2225p\u222522\nSj\n2 )qP(\u02dc\u2126c\n\u02dc\u03b3)\n(30)\n\u2264\nC\n\u0012\nn\nlog2 n\n\u0013 q\n2\nn\u2212\u02dc\u03b3.\n(31)\n22\nIt remains to \ufb01nd an upperbound for E\nh\nsupj\u2208J(|\u02c6pj(x)|)q1\u02dc\u2126c\n\u02dc\u03b3\ni\n. We have\nE\n\u0014\nsup\nj\u2208J\n(|\u02c6pj(x)|)q1\u02dc\u2126c\n\u02dc\u03b3\n\u0015\n=\nE\n\"\nsup\nj\u2208J\n\f\f\f\f\f\n1\nn\nn\nX\nl=1\nYlTj(Wl)\n\f\f\f\f\f\nq\n1\u02dc\u2126c\n\u02dc\u03b3\n#\n\u2264\n1\nnq E\n\"\nsup\nj\u2208J\n n\nX\nl=1\n|m(Xl) + \u03b5l| |Tj(Wl)|\n!q\n1\u02dc\u2126c\n\u02dc\u03b3\n#\n\u2264\nnq\u22121\nnq E\n\"\nsup\nj\u2208J\nn\nX\nl=1\n|m(Xl) + \u03b5l|q |Tj(Wl)|q1\u02dc\u2126c\n\u02dc\u03b3\n#\n\u2264\nC\nn E\n\"\nsup\nj\u2208J\nn\nX\nl=1\n(\u2225m\u2225q\n\u221e+ |\u03b5l|q)|Tj(Wl)|q1\u02dc\u2126c\n\u02dc\u03b3\n#\n\u2264\nC\n\u0012\nsup\nj\u2208J\n(\u2225Tj\u2225q\n\u221e)P(\u02dc\u2126c\n\u02dc\u03b3) + sup\nj\u2208J\n(\u2225Tj\u2225q\n\u221e)E\nh\n|\u03b51|q1\u02dc\u2126c\n\u02dc\u03b3\ni\u0013\n\u2264\nC\n\u0012\nsup\nj\u2208J\n(\u2225Tj\u2225q\n\u221e)P(\u02dc\u2126c\n\u02dc\u03b3) + \u03c3q\n\u03b5 sup\nj\u2208J\n(\u2225Tj\u2225q\n\u221e)\n\u0000E\n\u0002\n|Z|2q\u0003\u0001 1\n2 \u0010\nP(\u02dc\u2126c\n\u02dc\u03b3)\n\u0011 1\n2 \u0013\n,\nwhere Z \u223cN(0, 1). Using (18) and \u2225Tj\u2225\u221e\u2264T42Sj(\u03bd+1) , we get\nE\n\u0014\nsup\nj\u2208J\n(|\u02c6pj(x)|)q1\u02dc\u2126c\n\u02dc\u03b3\n\u0015\n\u2264\nC\n\u0012\nn\nlog2 n\n\u0013(\u03bd+1)q\nn\u2212\u02dc\u03b3\n2 ,\nWe have\nE\u2032\n\u2264\nCn\u2212\u02dc\u03b3\n2\n \u0012\nn\nlog2 n\n\u0013 q\n2\n+\n\u0012\nn\nlog2 n\n\u0013(\u03bd+1)q!\n=\no(n\u2212q),\nas soon as \u02dc\u03b3 > 2q(\u03bd + 2). This ends the proof of Proposition 2.\n\u25a1\nProposition 3 controls the bias term in the oracle inequality.\nProposition 3. For any j = (j1, . . . , jd) \u2208Zd and j\u2032 = (j\u2032\n1, . . . , j\u2032\nd) \u2208Zd and any x, if p \u2208Hd(\u20d7\u03b2, L)\n|pj\u2227j\u2032(x) \u2212pj\u2032(x)| \u2264R12L\nd\nX\nl=1\n2\u2212jl\u03b2l,\nwhere R12 is a constant only depending on \u03d5 and \u20d7\u03b2. We have denoted\nj \u2227j\u2032 = (j1 \u2227j\u2032\n1, . . . , jd \u2227j\u2032\nd).\nProof. We \ufb01rst state three lemmas.\nLemma 3. For any j and any k, we have:\nE[\u02c6pjk] = pjk.\nProof. Recall that\n\u02c6pjk := 1\nn\nn\nX\nu=1\nYu \u00d7 (Dj\u03d5)j,k(Wu) = 2\nSj\n2 1\nn\nn\nX\nu=1\nYu\nZ\ne\u2212i<t,2jWu\u2212k>\nd\nY\nl=1\nF(\u03d5)(tl)\nF(gl)(2jltl)dt.\n23\nLet us prove now that E(\u02c6pjk) = pjk.\nWe have\nE(\u02c6pjk) = 2\nSj\n2\n Z\nE(Y1e\u2212i<t,2jW1\u2212k>)\nd\nY\nl=1\nF(\u03d5)(tl)\nF(gl)(2jltl)dt\n!\n.\nWe shall develop the right member of the last equality. We have :\nE\nh\nY1e\u2212i<t,2jW1\u2212k>i\n=\nE\nh\n(m(X1) + \u03b51)e\u2212i<t,2jW1\u2212k>i\n=\nE\nh\nm(X1)e\u2212i<t,2jW1\u2212k>i\n=\nE\nh\nm(X1)e\u2212i<t,2jX1\u2212k>i\nE\nh\ne\u2212i<t,2j\u03b41>i\n=\nZ\nm(x)e\u2212i<t,2jx\u2212k>fX(x)dx \u00d7 F(g)(2jt)\n=\nei<t,k>F(p)(2jt)F(g)(2jt).\nConsequently\nE [\u02c6pjk]\n=\n2\nSj\n2\nZ\nei<t,k>F(p)(2jt)F(g)(2jt)\nd\nY\nl=1\nF(\u03d5)(tl)\nF(gl)(2jltl)dt\n=\n2\nSj\n2\nZ\nei<t,k>F(p)(2jt)\nd\nY\nl=1\nF(\u03d5)(tl)dt\n=\nZ\nF(p)(t)F(\u03d5jk)(t)dt.\nSince by Parseval equality, we have\npjk =\nZ\np(t)\u03d5jk(t)dt =\nZ\nF(p)(t)F(\u03d5jk)(t)dt,\nthe result follows.\nNote that in the case where we don\u2019t have any noise on the variable i.e g(x) = \u03b40(x), since F(g)(t) = 1,\nthe proof above remains valid and we get E[\u02c6pjk] = pjk.\n\u25a1\nLemma 4. If for any l, \u230a\u03b2l\u230b\u2264N, the following holds: for any j \u2208Zd and any p \u2208Hd(\u20d7\u03b2, L),\n|E[\u02c6pj(x)] \u2212p(x)| \u2264L(\u2225\u03d5\u2225\u221e\u2225\u03d5\u22251)d(2A + 1)d\nd\nX\nl=1\n(2A \u00d7 2\u2212jl)\u03b2l\n\u230a\u03b2l\u230b!\n.\nProof. Let x be \ufb01xed and j = (j1, . . . , jd) \u2208Zd. We have:\nZ\nKj(x, y)dy =\nZ X\nk1\n\u00b7 \u00b7 \u00b7\nX\nkd\nd\nY\nl=1\n[2jl\u03d5(2jlxl \u2212kl)\u03d5(2jlyl \u2212kl)dyl] = 1.\nTherefore, using lemma 3\nE[\u02c6pj(x)] \u2212p(x) = pj(x) \u2212p(x)\n=\nZ\nKj(x, y)(p(y) \u2212p(x))dy\n=\nX\nk\n\u03d5jk(x)\nZ\n\u03d5jk(y)(p(y) \u2212p(x))dy\n=\nX\nk1\u2208Zj,1(x)\n\u00b7 \u00b7 \u00b7\nX\nkd\u2208Zj,d(x)\n\u03d5jk(x)\nZ\nd\nY\nl=1\n2\njl\n2 \u03d5(2jlyl \u2212kl)(p(y) \u2212p(x))dy.\n24\nNow, we use that\np(y) \u2212p(x) =\nd\nX\nl=1\np(x1, . . . , xl\u22121, yl, yl+1, . . . , yd) \u2212p(x1, . . . xl\u22121, xl, yl+1, . . . , yd),\nwith p(x1, . . . , xl, yl+1, . . . , yd) = p(x1, . . . , xd) if l = d and p(x1, . . . , xl\u22121, yl, . . . , yd) = p(y1, . . . , yd) if\nl = 1. Furthermore, the Taylor expansion gives: for any l \u2208{1, . . . , d}, for some ul \u2208[0; 1],\np(x1, . . . , xl\u22121, yl, yl+1, . . . , yd) \u2212p(x1, . . . xl\u22121, xl, yl+1, . . . , yd) =\n\u230a\u03b2l\u230b\nX\nk=1\n\u2202kp\n\u2202xk\nl\n(x1, . . . xl\u22121, xl, yl+1, . . . , yd) \u00d7 (yl \u2212xl)k\nk!\n+\n\u2202\u230a\u03b2l\u230bp\n\u2202x\u230a\u03b2l\u230b\nl\n(x1, . . . xl\u22121, xl + (yl \u2212xl)ul, yl+1, . . . , yd) \u00d7 (yl \u2212xl)\u230a\u03b2l\u230b\n\u230a\u03b2l\u230b!\n\u2212\u2202\u230a\u03b2l\u230bp\n\u2202x\u230a\u03b2l\u230b\nl\n(x1, . . . xl\u22121, xl, yl+1, . . . , yd) \u00d7 (yl \u2212xl)\u230a\u03b2l\u230b\n\u230a\u03b2l\u230b!\n.\nUsing vanishing moments of K and p \u2208Hd(\u20d7\u03b2, L), we obtain:\n|pj(x) \u2212p(x)|\n\u2264\nX\nk1\u2208Zj,1(x)\n\u00b7 \u00b7 \u00b7\nX\nkd\u2208Zj,d(x)\n|\u03d5jk(x)|\nZ\nd\nY\nl=1\n2\njl\n2 |\u03d5(2jlyl \u2212kl)|\nd\nX\nl=1\nL|yl \u2212x\u2113|\u03b2l\n\u230a\u03b2l\u230b!\ndy\n\u2264\n\u2225\u03d5\u2225d\n\u221e\nX\nk1\u2208Zj,1(x)\n\u00b7 \u00b7 \u00b7\nX\nkd\u2208Zj,d(x)\nZ\n[\u2212A;A]d\nd\nY\nl=1\n|\u03d5(ul)|\nd\nX\nl=1\nL|2\u2212jl(ul + kl) \u2212xl|\u03b2l\n\u230a\u03b2l\u230b!\ndu.\nSince for any l, kl \u2208Zj,l(x), we \ufb01nally obtain\n|pj(x) \u2212p(x)|\n\u2264\n\u2225\u03d5\u2225d\n\u221e\nX\nk1\u2208Zj,1(x)\n\u00b7 \u00b7 \u00b7\nX\nkd\u2208Zj,d(x)\nZ\n[\u2212A;A]d\nd\nY\nl=1\n|\u03d5(ul)|\nd\nX\nl=1\nL(2A \u00d7 2\u2212jl)\u03b2l\n\u230a\u03b2l\u230b!\ndu\n\u2264\nL(\u2225\u03d5\u2225\u221e\u2225\u03d5\u22251)d(2A + 1)d\nd\nX\nl=1\n(2A \u00d7 2\u2212jl)\u03b2l\n\u230a\u03b2l\u230b!\n.\n\u25a1\nLemma 5. We have for any j = (j1, . . . , jd) \u2208Zd and j\u2032 = (j\u2032\n1, . . . , j\u2032\nd) \u2208Zd and any x,\nKj\u2032(pj)(x) = pj\u2227j\u2032(x).\nProof. We only deal with the case d = 2. The extension to the general case can be easily deduced. If\nfor i = 1, 2, ji \u2264j\u2032\ni the result is obvious. It is also the case if for l = 1, 2, j\u2032\nl \u2264jl. So, without loss of\ngenerality, we assume that j1 \u2264j\u2032\n1 and j\u2032\n2 \u2264j2. We have:\nKj\u2032(pj)(x)\n=\nZ\nKj\u2032(x, y)pj(y)dy\n=\nZ X\nk\n\u03d5j\u2032k(x)\u03d5j\u2032k(y)pj(y)dy\n=\nZ X\nk1\nX\nk2\n\u03d5j\u2032\n1k1(x1)\u03d5j\u2032\n2k2(x2)\u03d5j\u2032\n1k1(y1)\u03d5j\u2032\n2k2(y2)pj(y)dy1dy2\n=\nZ X\nk1\nX\nk2\n\u03d5j\u2032\n1k1(x1)\u03d5j\u2032\n2k2(x2)\u03d5j\u2032\n1k1(y1)\u03d5j\u2032\n2k2(y2)\n\u00d7\nX\n\u21131\nX\n\u21132\n\u03d5j1\u21131(y1)\u03d5j2\u21132(y2)\u03d5j1\u21131(u1)\u03d5j2\u21132(u2)p(u1, u2)du1du2dy1dy2.\n25\nSince j1 \u2264j\u2032\n1, we have in the one-dimensional case, by a slight abuse of notation, Vj1 \u2282Vj\u2032\n1 and\nZ X\nk1\n\u03d5j\u2032\n1k1(x1)\u03d5j\u2032\n1k1(y1)\u03d5j1\u21131(y1)dy1 =\nZ\nKj\u2032\n1(x1, y1)\u03d5j1\u21131(y1)dy1 = \u03d5j1\u21131(x1).\nSimilarly, since j\u2032\n2 \u2264j2, we have Vj\u2032\n2 \u2282Vj2 and\nZ X\n\u21132\n\u03d5j2\u21132(y2)\u03d5j2\u21132(u2)\u03d5j\u2032\n2k2(y2)dy2 =\nZ\nKj2(u2, y2)\u03d5j\u2032\n2k2(y2)dy2 = \u03d5j\u2032\n2k2(u2).\nTherefore, with \u02dcj = j \u2227j\u2032,\nKj\u2032(pj)(x)\n=\nZ X\nk2\nX\n\u21131\n\u03d5j\u2032\n2k2(x2)\u03d5j1\u21131(u1)\u03d5j1\u21131(x1)\u03d5j\u2032\n2k2(u2)p(u1, u2)du1du2\n=\nZ X\n\u21131\nX\n\u21132\n\u03d5\u02dcj2\u21132(x2)\u03d5\u02dcj1\u21131(u1)\u03d5\u02dcj1\u21131(x1)\u03d5\u02dcj2\u21132(u2)p(u1, u2)du1du2\n=\nZ X\n\u2113\n\u03d5\u02dcj\u2113(x)\u03d5\u02dcj\u2113(u)p(u)du\n=\np\u02dcj(x),\nwhich ends the proof of the lemma.\n\u25a1\nNow, we shall go back to the proof of Proposition 3. We easily deduce the result :\npj\u2227j\u2032(x) \u2212pj\u2032(x)\n=\nKj\u2032(pj)(x) \u2212Kj\u2032(p)(x)\n=\nZ\nKj\u2032(x, y)(pj(y) \u2212p(y))dy.\nTherefore,\n|pj\u2227j\u2032(x) \u2212pj\u2032(x)|\n\u2264\nZ\n|Kj\u2032(x, y)||pj(y) \u2212p(y)|dy\n\u2264\nR12L\nd\nX\nl=1\n2\u2212jl\u03b2l \u00d7\nZ\n|Kj\u2032(x, y)|dy,\nwhere R12 is a constant only depending on \u03d5 and \u20d7\u03b2. We conclude by observing that\nZ\n|Kj\u2032(x, y)|dy\n=\nZ X\nk1\n\u00b7 \u00b7 \u00b7\nX\nkd\nd\nY\nl=1\n[2j\u2032\nl|\u03d5(2j\u2032\nlxl \u2212kl)||\u03d5(2j\u2032\nlyl \u2212kl)|dyi]\n\u2264\n\u2225\u03d5\u2225d\n\u221e\nX\nk1\u2208Zj\u2032,1(x)\n\u00b7 \u00b7 \u00b7\nX\nkd\u2208Zj\u2032,d(x)\n\u0012Z\n|\u03d5(v)|dv\n\u0013d\n\u2264\n(\u2225\u03d5\u2225\u221e\u2225\u03d5\u22251(2A + 1))d .\nWe thus obtain the claimed result of Proposition 3.\n\u25a1\n5.2.2\nAppendix\nTechnical lemmas are stated and proved below.\n26\nLemma 6. We have\nE[(\u02dc\u03c3j,\u02dc\u03b3)q] \u2264R52Sj(2\u03bd+1) q\n2 ,\nwith R5 a constant depending on q, \u02dc\u03b3, \u2225m\u2225\u221e, s, \u2225fX\u2225\u221e, \u03d5, cg, Cg.\nProof. First, let us focus on the case q \u22652. We recall the expression of \u02dc\u03c32\nj,\u02dc\u03b3\n\u02dc\u03c32\nj,\u02dc\u03b3 = \u02c6\u03c32\nj + 2Cj\nr\n2\u02dc\u03b3\u02c6\u03c32\nj\nlog n\nn\n+ 8\u02dc\u03b3C2\nj\nlog n\nn\n.\nWe shall \ufb01rst prove that\nE[(\u02c6\u03c3j)q] \u2264C2Sj(2\u03bd+1) q\n2 .\nLet us remind that\n\u02c6\u03c32\nj =\n1\n2n(n \u22121)\nX\nl\u0338=v\n(Uj(Yl, Wl) \u2212Uj(Yv, Wv))2.\nWe easily get\n\u02c6\u03c32\nj \u2264C\nn\nX\nl\n(Uj(Yl, Wl) \u2212E[Uj(Y1, W1)])2.\nFirst let us remark that\n X\nl\n(Uj(Yl, Wl) \u2212E[Uj(Y1, W1)])2\n! q\n2\n\u2264C\n\uf8eb\n\uf8ed\n X\nl\n((Uj(Yl, Wl) \u2212E[Uj(Y1, W1)])2 \u2212\u03c32\nj )\n! q\n2\n+ n\nq\n2 \u03c3q\nj\n\uf8f6\n\uf8f8\nWe will use Rosenthal inequality (see H\u00e4rdle et al. (1998)) to \ufb01nd an upper bound for\nE\n\uf8ee\n\uf8f0\n X\nl\n((Uj(Yl, Wl) \u2212E[Uj(Y1, W1)])2 \u2212\u03c32\nj )\n! q\n2 \uf8f9\n\uf8fb.\nWe set\nBl := (Uj(Yl, Wl) \u2212E[Uj(Y1, W1)])2 \u2212\u03c32\nj .\nThe variables Bl are i.i.d and centered. We have to check that E[|Bl|\nq\n2 ] < \u221e. We have\nE[|Bl|\nq\n2 ] \u2264C(E[|(Uj(Yl, Wl) \u2212E[Uj(Y1, W1)]|q] + \u03c3q\nj),\nbut\nE[|(Uj(Yl, Wl) \u2212E[Uj(Y1, W1)]|q] = Aq\nn ,\nwith Aq de\ufb01ned in (17). Hence\nE[|Bl|\nq\n2 ] \u2264C\n\u0012Aq\nn + \u03c3q\nj\n\u0013\n.\n(32)\nUsing the control of Aq in (19), equation (20) and Lemma 10 we have\nAq\n\u2264\nCn\u03c32\nj \u2225Tj\u2225q\u22122\n\u221e\n\u2264\nCn2Sj(q\u03bd+q\u22121).\n(33)\nNow, we are able to apply the Rosenthal inequality to the variables Bl which yields\nE\n\uf8ee\n\uf8f0\n X\nl\nBl\n! q\n2 \uf8f9\n\uf8fb\n\u2264\nC\n\uf8eb\n\uf8edX\nl\nE[|Bl|\nq\n2 ] +\n X\nl\nE[B2\nl ]\n! q\n4 \uf8f6\n\uf8f8,\n27\nand using (32) and (33) we get\nE\n\uf8ee\n\uf8f0\n X\nl\nBl\n! q\n2 \uf8f9\n\uf8fb\n\u2264\nC\n\uf8eb\n\uf8edX\nl\n\u0012Aq\nn + \u03c3q\nj\n\u0013\n+\n X\nl\n\u0012A4\nn + \u03c34\nj\n\u0013! q\n4 \uf8f6\n\uf8f8\n\u2264\nC\n\u0010\nAq + n\u03c3q\nj + (A4)\nq\n4 + n\nq\n4 \u03c3q\nj\n\u0011\n\u2264\nC\n\u0010\nn2Sj(q\u03bd+q\u22121) + n2Sj(2\u03bd+1) q\n2 + (n2Sj(4\u03bd+3)\nq\n4\n\u0011\n.\nConsequently\nE[\u02c6\u03c3q\nj]\n\u2264\nCn\u2212q\n2\n\u0010\nn2Sj(q\u03bd+q\u22121) + n2Sj(2\u03bd+1) q\n2 + (n2Sj(4\u03bd+3)\nq\n4 + n\nq\n2 2Sj(2\u03bd+1) q\n2\n\u0011\n\u2264\nC(n1\u2212q\n2 2Sj(q\u03bd+q\u22121) + n1\u2212q\n2 2Sj(2\u03bd+1) q\n2 + n\u2212q\n4 2Sj(4\u03bd+3) q\n4 + 2Sj(2\u03bd+1) q\n2 ).\nLet us compare each term of the r.h.s of the last inequality. We have\nn1\u2212q\n2 2Sj(q\u03bd+q\u22121) \u22642Sj(2\u03bd+1) q\n2 \u21d0\u21d22Sj \u2264n,\nwhich is true by (8). Similarly we have\nn\u2212q\n4 2Sj(4\u03bd+3) q\n4 \u22642Sj(2\u03bd+1) q\n2 \u21d0\u21d22Sj \u2264n,\nand obviously\nn1\u2212q\n2 2Sj(2\u03bd+1) q\n2 \u22642Sj(2\u03bd+1) q\n2 .\nThus we get that the dominant term in r.h.s is 2Sj(2\u03bd+1) q\n2 . Hence\nE[\u02c6\u03c3q\nj] \u2264C2Sj(2\u03bd+1) q\n2 .\nNow using that\nE[\u02dc\u03c3q\nj,\u02dc\u03b3]\n\u2264\nC\n\uf8eb\n\uf8edE[\u02c6\u03c3q\nj] +\n \n2Cj\nr\n2\u02dc\u03b3 log n\nn\n! q\n2\nE[\u02c6\u03c3\nq\n2\nj ] +\n\u0012\n8\u02dc\u03b3C2\nj\nlog n\nn\n\u0013 q\n2\n\uf8f6\n\uf8f8,\nand since Cj \u2264C\u221alog n2Sj(\u03bd+1), we have\nE[\u02dc\u03c3q\nj,\u02dc\u03b3]\n\u2264\nC\n \n2Sj(2\u03bd+1) q\n2 + ((log n)n\u22121\n2 2Sj(\u03bd+1))\nq\n2 2Sj(2\u03bd+1) q\n4 +\n\u0012log2 n\nn\n22Sj(\u03bd+1)\n\u0013 q\n2 !\n.\nLet us compare the three terms of the right hand side. We have\n2Sj\nq(2\u03bd+1)\n2\n\u2265\n((log n)n\u22121\n2 2Sj(\u03bd+1))\nq\n2 2Sj(2\u03bd+1) q\n4 \u21d0\u21d22Sj(q\u03bd+ q\n2 ) \u2265(log n)\nq\n2 n\u2212q\n4 2Sj(q\u03bd+ 3q\n4 ) \u21d0\u21d22Sj \u2264\nn\nlog2 n,\nwhich is true by (8). Furthermore we have\n2Sj\nq(2\u03bd+1)\n2\n\u2265\n\u0012log2 n\nn\n22Sj(\u03bd+1)\n\u0013 q\n2\n\u21d0\u21d22Sj(q\u03bd+ q\n2 ) \u2265\n\u0012log2 n\nn\n\u0013 q\n2\n2Sj(q\u03bd+q) \u21d0\u21d22Sj \u2264\nn\nlog2 n,\n(34)\nwhich is true again by (8). Consequently\nE[\u02dc\u03c3q\nj,\u02dc\u03b3] \u2264R52Sj(2\u03bd+1) q\n2 ,\nwith R5 a constant depending on q, \u02dc\u03b3, \u2225m\u2225\u221e, s, \u2225fX\u2225\u221e, \u03d5, cg, Cg and the lemma is proved for q \u22652.\nFor the case q \u22642 the result follows from Jensen inequality.\n\u25a1\n28\nLemma 7. Under assumption (A1) on the father wavelet \u03d5, we have for any j = (j1, . . . , jd) and any\nx \u2208Rd,\nX\nk\n|\u03d5jk(x)| \u2264(2A + 1)d\u2225\u03d5\u2225d\n\u221e2\nSj\n2 .\nProof. Let x \u2208Rd be \ufb01xed. We set for any j and any l \u2208{1, . . . , d},\nZj,l(x) =\n\b\nkl :\n|2jlxl \u2212kl| \u2264A\n\t\n,\nwhose cardinal is smaller or equal to (2A + 1). Since\n\u03d5jk(x) =\nd\nY\nl=1\n2\njl\n2 \u03d5(2jlxl \u2212kl),\nthen\n\u03d5jk(x) \u0338= 0 \u21d2\u2200l \u2208{1, . . . , d}, kl \u2208Zj,l(x).\nNow,\nX\nk\n|\u03d5jk(x)|\n=\nX\nk1\u2208Zj,1(x)\n\u00b7 \u00b7 \u00b7\nX\nkd\u2208Zj,d(x)\nd\nY\nl=1\n2\njl\n2 |\u03d5(2jlxl \u2212kl)|\n\u2264\nX\nk1\u2208Zj,1(x)\n\u00b7 \u00b7 \u00b7\nX\nkd\u2208Zj,d(x)\n\u2225\u03d5\u2225d\n\u221e2\nSj\n2\n\u2264\n(2A + 1)d\u2225\u03d5\u2225d\n\u221e2\nSj\n2 .\n\u25a1\nLemma 8. Under condition (A1) and \u03d5 is Cr, there exist constants R6 and R7 depending on \u03d5 such that\n|F(\u03d5)(t)| \u2264R6(1 + |t|)\u2212r,\nfor any t.\n(35)\nand\n\f\f\fF(\u03d5)(t)\n\u2032\f\f\f \u2264R7(1 + |t|)\u2212r,\nfor any t.\n(36)\nProof. First, let us focus on the case |t| \u22651.\nWe have by integration by parts that\nF(\u03d5)(t) =\nZ\ne\u2212itx\u03d5(x)dx =\n\u0014\n\u22121\nite\u2212itx\u03d5(x)\n\u0015\u221e\n\u2212\u221e\n+ 1\nit\nZ\ne\u2212itx\u03d5\u2032(x)dx.\nUsing that the father wavelet \u03d5 is compactly supported on [\u2212A, A], we get\nF(\u03d5)(t) = 1\nit\nZ\ne\u2212itx\u03d5\u2032(x)dx.\nBy successive integration by parts and using that |t| \u22651 one gets\n|F(\u03d5)(t)| =\n\f\f\f\f\n1\n(it)r\nZ\ne\u2212itx\u03d5(r)(x)dx\n\f\f\f\f \u2264\n2r\n(1 + |t|)r\nZ\n|\u03d5(r)(x)|dx,\nthe integral\nR A\n\u2212A |\u03d5(r)(x)|dx being \ufb01nite.\nFor the derivative we have\nF(\u03d5)(t)\n\u2032 = i\nZ\neitxx\u03d5(x)dx.\n29\nFollowing the same scheme as for F(\u03d5)(t), one gets by integration by parts and using the Leibniz formula\nthat\n\f\f\fF(\u03d5)(t)\n\u2032\f\f\f\n=\n\f\f\f\f\n1\n(it)r\nZ\neitx dr\ndxr (x\u03d5(x))dx\n\f\f\f\f =\n\f\f\f\f\f\n1\n(it)r\nZ\neitx\nr\nX\nk=0\n\u0012r\nk\n\u0013\nx(k)\u03d5(x)(r\u2212k)dx\n\f\f\f\f\f\n\u2264\n2r\n(1 + |t|)r\nr\nX\nk=0\n\u0012r\nk\n\u0013 Z\n|x(k)\u03d5(x)(r\u2212k)|dx,\nthe quantity Pr\nk=0\n\u0000r\nk\n\u0001 R A\n\u2212A |x(k)\u03d5(x)(r\u2212k)|dx being \ufb01nite.\nHence the lemma is proved for |t| \u22651.\nThe result for |t| \u22641 is obvious since\n|F(\u03d5)(t)| =\n\f\f\f\f\nZ\ne\u2212itx\u03d5(x)dx\n\f\f\f\f \u2264\nZ\n|\u03d5(x)|dx < \u221e,\nand\n\f\f\fF(\u03d5)(t)\n\u2032\f\f\f =\n\f\f\f\fi\nZ\neitxx\u03d5(x)dx\n\f\f\f\f \u2264\nZ\n|x\u03d5(x)|dx < \u221e.\nThen the lemma is proved for any t.\n\u25a1\nLemma 9. Under conditions (A1) and (A3), for \u03bd \u22650, we have\n|(Dj\u03d5)(w)| \u2264R82Sj\u03bd\nd\nY\nl=1\n(1 + |wl|)\u22121, w \u2208Rd\nwhere R8 is a constant depending on \u03d5, Cg and cg.\nProof. If all the |wl| < 1 then using (9), Lemma 8 and r \u2265\u03bd + 2 with \u03bd \u22650 we have\n|(Dj\u03d5)(w)|\n\u2264\nd\nY\nl=1\nZ\n|F(\u03d5)(tl)|\n|F(gl)(2jltl)|dtl\n(37)\n\u2264\nC\nd\nY\nl=1\nZ \f\fF(\u03d5)(tl)(1 + 2jl|tl|)\u03bd\f\f dtl\n(38)\n\u2264\nC2Sj\u03bd\nd\nY\nl=1\nZ\n(1 + |tl|)\u03bd\u2212rdtl\n(39)\n\u2264\nC2Sj\u03bd \u2264C2Sj\u03bd\nd\nY\nl=1\n(1 + |wl|)\u22121.\n(40)\nNow we consider the case where there exists at least one wl such that |wl| \u22651. We have\n(Dj\u03d5)(w) =\nd\nY\nl=1,|wl|\u22641\nZ\ne\u2212itlwl\nF(\u03d5)(tl)\nF(gl)(2jltl)dtl \u00d7\nd\nY\nl=1,|wl|\u22651\nZ\ne\u2212itlwl\nF(\u03d5)(tl)\nF(gl)(2jltl)dtl.\nFor the left-hand product on |wl| \u22641 we use the result (40). Now let us consider the right-hand product\nwith |wl| \u22651. We set in the sequel\n\u03b7l(tl) :=\nF(\u03d5)(tl)\nF(gl)(2jltl).\n30\nWe have\nd\nY\nl=1,|wl|\u22651\nZ\ne\u2212itlwl\nF(\u03d5)(tl)\nF(gl)(2jltl)dtl =\nd\nY\nl=1,|wl|\u22651\nZ\ne\u2212itlwl\u03b7l(tl)dtl.\nSince |\u03b7l(tl)| \u21920 when tl \u2192\u00b1\u221e, an integration by part yields\nZ\ne\u2212itlwl\u03b7l(tl)dtl = iw\u22121\nl\nZ\ne\u2212itlwl\u03b7\u2032\nl(tl)dtl.\nLet us compute the derivative of \u03b7l(tl)\n\u03b7\u2032\nl(tl) = F(\u03d5)(tl)\n\u2032F(g)(2jltl) \u22122jlF\u2032(g)(2jltl)F(\u03d5)(tl)\n(F(g)(2jltl))2\n.\nUsing Lemma 8, (9) and (10)\n|\u03b7\u2032\nl(tl)|\n\u2264\n\f\f\f\f\f\nF(\u03d5)(tl)\n\u2032\nF(g)(2jltl)\n\f\f\f\f\f + 2jl\n\f\f\f\f\nF\u2032(g)(2jltl)F(\u03d5)(tl)\n(F(g)(2jltl))2\n\f\f\f\f\n\u2264\nC\n\u0000(1 + |tl|)\u2212r(1 + 2jl|tl|)\u03bd + 2jl(1 + 2jl|tl|)\u2212\u03bd\u22121(1 + |tl|)\u2212r(1 + 2jl|tl|)2\u03bd\u0001\n\u2264\nC\n\u00002jl\u03bd(1 + |tl|)\u2212r(2\u2212jl + |tl|)\u03bd + 2jl(1 + 2jl|tl|)\u03bd\u22121(1 + |tl|)\u2212r\u0001\n\u2264\nC\n\u00002jl\u03bd(1 + |tl|)\u2212r(2\u2212jl + |tl|)\u03bd + 2jl\u03bd(2\u2212jl + |tl|)\u03bd\u22121(1 + |tl|)\u2212r\u0001\n\u2264\nC2jl\u03bd \u0000(1 + |tl|)\u2212r(2\u2212jl + |tl|)\u03bd + (2\u2212jl + |tl|)\u03bd\u22121(1 + |tl|)\u2212r\u0001\n.\nTherefore,\n\f\f\f\f\nZ\ne\u2212itlwl\u03b7l(tl)dtl\n\f\f\f\f\n\u2264\n|wl|\u22121\nZ\n|\u03b7\u2032\nl(tl)|dtl\n\u2264\nC|wl|\u221212jl\u03bd\nZ \u0000(1 + |tl|)\u2212r(2\u2212jl + |tl|)\u03bd + (2\u2212jl + |tl|)\u03bd\u22121(1 + |tl|)\u2212r\u0001\ndtl\n\u2264\nC|wl|\u221212jl\u03bd(D1 + D2 + D3),\nwith D1, D2 and D3 de\ufb01ned below.\nD1\n:=\nZ\n|tl|\u22642\u2212jl\n\u0000(1 + |tl|)\u2212r(2\u2212jl + |tl|)\u03bd + (2\u2212jl + |tl|)\u03bd\u22121(1 + |tl|)\u2212r\u0001\ndtl\n\u2264\nC\nZ\n|tl|\u22642\u2212jl\n\u0000(2\u2212jl + |tl|)\u03bd + (2\u2212jl + |tl|)\u03bd\u22121\u0001\ndtl\n\u2264\nC2\u2212jl(2\u2212jl\u03bd + 2\u2212jl(\u03bd\u22121))\n\u2264\nC.\nD2\n:=\nZ\n2\u2212jl\u2264|tl|\u22641\n\u0000(1 + |tl|)\u2212r(2\u2212jl + |tl|)\u03bd + (2\u2212jl + |tl|)\u03bd\u22121(1 + |tl|)\u2212r\u0001\ndtl\n\u2264\nC\nZ\n2\u2212jl\u2264|tl|\u22641\n\u0000(2\u2212jl + |tl|)\u03bd + (2\u2212jl + |tl|)\u03bd\u22121\u0001\ndtl\n\u2264\nC\nZ 2jl\n1\n((2\u2212jl + 2\u2212jls)\u03bd + (2\u2212jl + 2\u2212jls)\u03bd\u22121)2\u2212jlds\n\u2264\nC2\u2212jl(\u03bd+1)\nZ 2jl\n1\ns\u03bdds + C2\u2212jl\u03bd\nZ 2jl\n1\ns\u03bd\u22121ds\n\u2264\nC,\n31\nas soon as \u03bd > 0.\nD3\n:=\nZ\n|tl|\u22651\n\u0000(1 + |tl|)\u2212r(2\u2212jl + |tl|)\u03bd + (2\u2212jl + |tl|)\u03bd\u22121(1 + |tl|)\u2212r\u0001\ndti\n\u2264\nC\nZ\n|tl|\u22651\n\u0000|tl|\u03bd\u2212r + |tl|\u03bd\u22121\u2212r\u0001\ndtl\n\u2264\nC,\nsince \u03bd \u2212r \u2264\u22122.\nWhen \u03bd = 0 we still have\n\f\f\f\f\nZ\ne\u2212itlwl\u03b7l(tl)dtl\n\f\f\f\f \u2264C|wl|\u221212jl\u03bd = C|wl|\u22121.\nIndeed when \u03bd = 0\n\u03b7l(tl) = F(\u03d5)(tl),\nand\n\f\f\f\fiw\u22121\nl\nZ\ne\u2212itlwl\u03b7\u2032\nl(tl)dtl\n\f\f\f\f\n=\n\f\f\f\fiw\u22121\nl\nZ\ne\u2212itlwlF(\u03d5)(tl)\n\u2032dtl\n\f\f\f\f\n\u2264\n|wl|\u22121\nZ \f\f\fF(\u03d5)(tl)\n\u2032\f\f\f dtl\n\u2264\nC|wl|\u22121\nZ\n(1 + |t|)\u2212rdt < C|wl|\u22121,\nusing Lemma 8 and r \u22652.\n\u25a1\nLemma 10. There exist constants T3 depending on \u2225m\u2225\u221e, \u03c3\u03b5, \u2225fX\u2225\u221e, \u03d5 ,cg, Cg and T4 depending on\n\u03d5, cg, Cg such that\n\u03c32\nj \u2264R102Sj(2\u03bd+1),\n\u2225Tj\u2225\u221e\u2264R112Sj(\u03bd+1).\nProof. We have\n\u03c32\nj = Var(Uj(Y1, W1))\n\u2264\nE\nh\n|Uj(Y1, W1)|2i\n=\nE\n\uf8ee\n\uf8f0\n\f\f\f\f\fY1\nX\nk\n(Dj\u03d5)j,k (W1)\u03d5jk(x)\n\f\f\f\f\f\n2\uf8f9\n\uf8fb\n=\nE\n\uf8ee\n\uf8f0\n\f\f\f\f\f(m(X1) + \u03b51)\nX\nk\n(Dj\u03d5)j,k (W1)\u03d5jk(x)\n\f\f\f\f\f\n2\uf8f9\n\uf8fb\n\u2264\n2(\u2225m\u22252\n\u221e+ \u03c32\n\u03b5)E\n\uf8ee\n\uf8f0\n\f\f\f\f\f\nX\nk\n(Dj\u03d5)j,k (W1)\u03d5jk(x)\n\f\f\f\f\f\n2\uf8f9\n\uf8fb\n\u2264\n2(\u2225m\u22252\n\u221e+ \u03c32\n\u03b5)\nZ \f\f\f\f\f\nX\nk\n(Dj\u03d5)j,k (w)\u03d5jk(x)\n\f\f\f\f\f\n2\nfW (w)dw\n\u2264\n2(\u2225m\u22252\n\u221e+ \u03c32\n\u03b5)\u2225fX\u2225\u221e\nZ\n2Sj\n\f\f\f\f\f\nX\nk\n(Dj\u03d5) (2jw \u2212k)\u03d5jk(x)\n\f\f\f\f\f\n2\ndw.\n32\nNow making the change of variable z = 2jw \u2212k, we get using Lemma 7 and Lemma 9 to bound (Dj\u03d5)(z)\n\u03c32\nj\n\u2264\n2(\u2225m\u22252\n\u221e+ \u03c32\n\u03b5)\u2225fX\u2225\u221e\nZ \f\f\f\f\f\nX\nk\n(Dj\u03d5) (z)\u03d5jk(x)\n\f\f\f\f\f\n2\ndz\n\u2264\nC\nZ\n22Sj\u03bd\nd\nY\ni=l\n1\n(1 + |zl|)2\n X\nk\n|\u03d5jk(x)|\n!2\ndz\n\u2264\nR102Sj(2\u03bd+1),\nwhere R10 is a constant depending on \u2225m\u2225\u221e, s, \u2225fX\u2225\u221e, \u03d5, cg, Cg. This gives the bound for \u03c32\nj .\nFor \u2225Tj\u2225\u221e, using again Lemma 7 and Lemma 9, we have\n\u2225Tj\u2225\u221e\n\u2264\nmax\nk\n\u2225(Dj\u03d5)j,k\u2225\u221e\nX\nk\n|\u03d5jk(x)| \u22642\nSj\n2 \u2225(Dj\u03d5)\u2225\u221e\nX\nk\n|\u03d5jk(x)|\n\u2264\nR112Sj(\u03bd+1),\nwhere R11 is a constant depending on \u03d5, cg, Cg.\n\u25a1\nAcknowledgements: The research of Thanh Mai Pham Ngoc and Vincent Rivoirard is partly sup-\nported by the french Agence Nationale de la Recherche (ANR 2011 BS01 010 01 projet Calibration).\nMicha\u00ebl Chichignoud now works at Winton Capital Management, supported in part as member of the\nGerman-Swiss Research Group FOR916 (Statistical Regularization and Qualitative Constraints) with\ngrant number 20PA20E-134495/1.\nReferences\nBertin, K., Lacour, C., and Rivoirard, V. (2013). Adaptive pointwise estimation of conditional density\nfunction. Submitted.\nCarroll, R. J., Delaigle, A., and Hall, P. (2009). Nonparametric prediction in measurement error models.\nJ. Amer. Statist. Assoc., 104(487):993\u20131003.\nChesneau, C. (2010). On adaptive wavelet estimation of the regression function and its derivatives in an\nerrors-in-variables model. Curr. Dev. Theory Appl. Wavelets, 4(2):185\u2013208.\nComte, F. and Lacour, C. (2013). Anisotropic adaptive kernel deconvolution. Ann. Inst. Henri Poincar\u00e9\nProbab. Stat., 49(2):569\u2013609.\nComte, F. and Taupin, M.-L. (2007). Adaptive estimation in a nonparametric regression model with\nerrors-in-variables. Statist. Sinica, 17(3):1065\u20131090.\nDelaigle, A., Hall, P., and Jamshidi, F. (2015). Con\ufb01dence bands in non-parametric errors-in-variables\nregression. J. R. Stat. Soc. Ser. B. Stat. Methodol., 77(1):149\u2013169.\nDoumic, M., Ho\ufb00mann, M., Reynaud-Bouret, P., and Rivoirard, V. (2012). Nonparametric estimation of\nthe division rate of a size-structured population. SIAM J. Numer. Anal., 50(2):925\u2013950.\nDu, L., Zou, C., and Wang, Z. (2011). Nonparametric regression function estimation for errors-in-variables\nmodels with validation data. Statist. Sinica, 21(3):1093\u20131113.\nFan, J. and Koo, J.-Y. (2002). Wavelet deconvolution. IEEE Trans. Inform. Theory, 48(3):734\u2013747.\n33\nFan, J. and Masry, E. (1992). Multivariate regression estimation with errors-in-variables: asymptotic\nnormality for mixing processes. J. Multivariate Anal., 43(2):237\u2013271.\nFan, J. and Truong, Y. K. (1993). Nonparametric regression with errors in variables. Ann. Statist.,\n21(4):1900\u20131925.\nGach, F., Nickl, R., and Spokoiny, V. (2013). Spatially adaptive density estimation by localised Haar\nprojections. Ann. Inst. Henri Poincar\u00e9 Probab. Stat., 49(3):900\u2013914.\nGoldenshluger, A. and Lepski, O. (2011).\nBandwidth selection in kernel density estimation: oracle\ninequalities and adaptive minimax optimality. Ann. Statist., 39(3):1608\u20131632.\nH\u00e4rdle, W., Kerkyacharian, G., Picard, D., and Tsybakov, A. (1998). Wavelets, approximation, and\nstatistical applications, volume 129 of Lecture Notes in Statistics. Springer-Verlag, New York.\nIoannides, D. A. and Alevizos, P. D. (1997).\nNonparametric regression with errors in variables and\napplications. Statist. Probab. Lett., 32(1):35\u201343.\nKoo, J.-Y. and Lee, K.-W. (1998). B-spline estimation of regression functions with errors in variable.\nStatist. Probab. Lett., 40(1):57\u201366.\nMeister, A. (2009). Deconvolution problems in nonparametric statistics, volume 193 of Lecture Notes in\nStatistics. Springer-Verlag, Berlin.\nPatel, J. K. and Read, C. B. (1982).\nHandbook of the normal distribution, volume 40 of Statistics:\nTextbooks and Monographs. Marcel Dekker, Inc., New York.\nWhittemore, A. S. and Keller, J. B. (1988). Approximations for regression with covariate measurement\nerror. J. Amer. Statist. Assoc., 83(404):1057\u20131066.\n34\n"}