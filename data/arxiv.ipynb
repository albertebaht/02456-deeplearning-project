{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import kagglehub # to access the data\n",
    "import fitz  # PyMuPDF to extract text from PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Kaggle arXiv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download arXiv dataset\n",
    "path = kagglehub.dataset_download(\"Cornell-University/arxiv\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi',\n",
      "       'report-no', 'categories', 'license', 'abstract', 'versions',\n",
      "       'update_date', 'authors_parsed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the metadata - need to be chunks cause it is huge\n",
    "chunk_size = 10000\n",
    "json_chunks = pd.read_json('test/arxiv-metadata-oai-snapshot.json', lines=True, chunksize=chunk_size)\n",
    "\n",
    "# Process each chunk\n",
    "for chunk in json_chunks:\n",
    "    # Perform operations on each chunk here\n",
    "    print(chunk.columns)\n",
    "    break\n",
    "    print(chunk[\"reference\"].head())\n",
    "    # print(chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a subset of ML papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated a subset of papers which were tagged with one of the categories [\"cs.LG\", \"stat.ML\", \"cs.AI\", \"stat.ML\", \"cs.CV\", \"cs.NE\"] - can choose different field or include more / less depending on how big we want the dataset \n",
    "\n",
    "The format is a jason similar to that from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 359531 Machine Learning papers.\n"
     ]
    }
   ],
   "source": [
    "# Define a list of Machine Learning categories (from arXiv)\n",
    "ml_categories = [\"cs.LG\", \"stat.ML\", \"cs.AI\", \"stat.ML\", \"cs.CV\", \"cs.NE\"]\n",
    "\n",
    "# Create an empty list to store filtered papers\n",
    "ml_papers = []\n",
    "\n",
    "# Open and process the JSON file line by line (assuming one paper per line)\n",
    "with open('test/arxiv-metadata-oai-snapshot.json', 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Parse each line as a JSON object\n",
    "            paper = json.loads(line)\n",
    "            categories = paper.get('categories', []).split()\n",
    "            # Check if any of the categories match the Machine Learning categories\n",
    "            if any(category in ml_categories for category in categories):\n",
    "                ml_papers.append(paper)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e} - skipping this line.\")\n",
    "\n",
    "# Save the filtered papers to a new JSON file\n",
    "with open('arxiv_ml_papers.json', 'w') as outfile:\n",
    "    json.dump(ml_papers, outfile, indent=2)\n",
    "\n",
    "print(f\"Found {len(ml_papers)} Machine Learning papers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape pdf for references "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the reference section of a paper - might be usefull for generating relations, but need a LM to produce the relations - maybe it is possible for the model to generate the relations with just the raw text from the papers - would be nicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] Chan, Y. T. Ho, K. C. 1994 , A simple and efﬁcient estimator for hy- perbolic location, IEEE Transactions on Signal Processing 42(8), 1905– 1915. [2] Cherkassky, V. Mulier, F. 1998 , Leraning from Data: Concepts, Theory, and Methods, John Wiley & Sons inc., New York. [3] Friedlander, B. 1987 , A passive localization algorithm and its accuracy analysis, IEEE Journal of Oceanic Engineering OE-12(1), 234–245. [4] Grabec, I. Antoloviˇc, B. 1994 , Intelligent locator of AE sources, in T. Kishi, Y. Mori M. Enoki, eds, The 12th International Acoustic Emission Symposium, Vol. 7 of Progress in Acoustic Emission, The Japanese Society for Non-Destructive Inspection, Tokyo, Japan, pp. 565–570. [5] Grabec, I. Sachse, W. 1991 , ‘Automatic modeling of physical phenomena: Application to ultrasonic data’, J. Appl. Phys. 69(9), 6233–6244. [6] Grabec, I. Sachse, W. 1997 , Synergetics of Measurement, Prediction and Control, Springer-Verlag, Berlin. [7] Kosel, T. Grabec, I. 1998 , Intelligent locator of discrete and continuous acoustic emission sources, in J. Grum, ed., Application of Contemporary Non-destructive Testing in Engineering, The 5th International Conference of Slovenian Society for Nondestructive Testing, Slovenian Society for Nondestructive Testing, Ljubljana, Slovenia, pp. 39–54. [8] McIntire, P. Miller, R. K., eds 1987 , Acoustic Emission Testing, Vol. 5 of Nondestructive Testing Handbook, 2 edn, American Society for Non- destructive Testing, Philadelphia, USA. [9] Specht, D. F. 1991 , A general regression neural network, IEEE Trans. on Neural Networks 2(6), 568–576. [10] Tobias, A. 1976 , Acoustic emission source location in two dimensions by an array of three sensors, Non-Destructive Testing 9(2), 9–12. [11] Ziola, S. M. Gorman, M. R. 1991 , Source location in thin plates using cross-correlation, J. Acoust. Soc. Am. 90(5), 2551–2556. \n"
     ]
    }
   ],
   "source": [
    "### example of extracting references from a pdf\n",
    "def extract_references_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    references = []\n",
    "\n",
    "    # Loop through all pages of the PDF\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text(\"text\")\n",
    "        \n",
    "        # Look for references by searching for patterns (e.g., \"References\", \"Bibliography\")\n",
    "        # The exact pattern depends on the format of the paper. \n",
    "        # A simple example is to look for the string \"References\":\n",
    "        if \"REFERENCES\" in text:\n",
    "            references_section = text.split(\"REFERENCES\")[1]  # Get text after \"References\"\n",
    "            # Combine the text after \"REFERENCES\" into one block of text\n",
    "            references_block = \" \".join(references_section.split(\"\\n\"))\n",
    "            \n",
    "            # Append the block as a single reference section\n",
    "            references.append(references_block)\n",
    "            \n",
    "    return references\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"/Users/emilienilsson/Documents/DTU/9semester/02456-dl/project/arxiv_ml_papers/0704.0047.pdf\"\n",
    "references = extract_references_from_pdf(pdf_path)\n",
    "for ref in references:\n",
    "    print(ref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This funciton should be used if we want to seperate the references - should make it easier to make the relations manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1: \n",
      "Reference 2: [1] Chan, Y. T. Ho, K. C. 1994 , A simple and efﬁcient estimator for hy- perbolic location, IEEE Transactions on Signal Processing 42(8), 1905– 1915.\n",
      "Reference 3: [2] Cherkassky, V. Mulier, F. 1998 , Leraning from Data: Concepts, Theory, and Methods, John Wiley & Sons inc., New York.\n",
      "Reference 4: [3] Friedlander, B. 1987 , A passive localization algorithm and its accuracy analysis, IEEE Journal of Oceanic Engineering OE-12(1), 234–245.\n",
      "Reference 5: [4] Grabec, I. Antoloviˇc, B. 1994 , Intelligent locator of AE sources, in T. Kishi, Y. Mori M. Enoki, eds, The 12th International Acoustic Emission Symposium, Vol. 7 of Progress in Acoustic Emission, The Japanese Society for Non-Destructive Inspection, Tokyo, Japan, pp. 565–570.\n",
      "Reference 6: [5] Grabec, I. Sachse, W. 1991 , ‘Automatic modeling of physical phenomena: Application to ultrasonic data’, J. Appl. Phys. 69(9), 6233–6244.\n",
      "Reference 7: [6] Grabec, I. Sachse, W. 1997 , Synergetics of Measurement, Prediction and Control, Springer-Verlag, Berlin.\n",
      "Reference 8: [7] Kosel, T. Grabec, I. 1998 , Intelligent locator of discrete and continuous acoustic emission sources, in J. Grum, ed., Application of Contemporary Non-destructive Testing in Engineering, The 5th International Conference of Slovenian Society for Nondestructive Testing, Slovenian Society for Nondestructive Testing, Ljubljana, Slovenia, pp. 39–54.\n",
      "Reference 9: [8] McIntire, P. Miller, R. K., eds 1987 , Acoustic Emission Testing, Vol. 5 of Nondestructive Testing Handbook, 2 edn, American Society for Non- destructive Testing, Philadelphia, USA.\n",
      "Reference 10: [9] Specht, D. F. 1991 , A general regression neural network, IEEE Trans. on Neural Networks 2(6), 568–576.\n",
      "Reference 11: [10] Tobias, A. 1976 , Acoustic emission source location in two dimensions by an array of three sensors, Non-Destructive Testing 9(2), 9–12.\n",
      "Reference 12: [11] Ziola, S. M. Gorman, M. R. 1991 , Source location in thin plates using cross-correlation, J. Acoust. Soc. Am. 90(5), 2551–2556. \n"
     ]
    }
   ],
   "source": [
    "def extract_references_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    references = []\n",
    "    \n",
    "    # Regex pattern to match references by index (e.g., [1], [2], [3], etc.)\n",
    "    ref_pattern = re.compile(r'\\[(\\d+)\\]')  # Matches [1], [2], [3], etc.\n",
    "    \n",
    "    # Variable to temporarily store reference content\n",
    "    current_reference = []\n",
    "    inside_references = False\n",
    "    \n",
    "    # Loop through all pages of the PDF\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text(\"text\")\n",
    "        \n",
    "        # Look for references section\n",
    "        if \"REFERENCES\" in text or \"BIBLIOGRAPHY\" in text:\n",
    "            inside_references = True\n",
    "            # Get the part of the text after the \"References\" section\n",
    "            references_section = text.split(\"REFERENCES\")[1] if \"REFERENCES\" in text else text.split(\"BIBLIOGRAPHY\")[1]\n",
    "        \n",
    "        if inside_references:\n",
    "            # Split the section into lines and process each line\n",
    "            for line in references_section.split(\"\\n\"):\n",
    "                # Check if the line contains a reference index (e.g., [1], [2], etc.)\n",
    "                if ref_pattern.search(line):\n",
    "                    # If there's a current reference being built, save it\n",
    "                    if current_reference:\n",
    "                        references.append(\" \".join(current_reference))  # Join the lines of the reference\n",
    "                    # Start a new reference\n",
    "                    current_reference = [line.strip()]  # Start new reference with the current line\n",
    "                else:\n",
    "                    # If it's part of the current reference, add it to the current reference\n",
    "                    current_reference.append(line.strip())\n",
    "\n",
    "            # After finishing all lines in this section, add the last reference\n",
    "            if current_reference:\n",
    "                references.append(\" \".join(current_reference))\n",
    "            break  # We can stop processing after the references section\n",
    "\n",
    "    return references\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"/Users/emilienilsson/Documents/DTU/9semester/02456-dl/project/arxiv_ml_papers/0704.0047.pdf\"\n",
    "references = extract_references_from_pdf(pdf_path)\n",
    "for i, ref in enumerate(references):\n",
    "    print(f\"Reference {i+1}: {ref}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Full-Text PDFs from arXiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download all papers to pdf format from the machine learning papers - this is a lot do it on hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Downloading the PDF for a given arXiv ID\n",
    "def download_pdf(arxiv_id, save_path):\n",
    "    url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        pdf_path = os.path.join(save_path, f\"{arxiv_id}.pdf\")\n",
    "        with open(pdf_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        #print(f\"Downloaded {arxiv_id}.pdf\")\n",
    "    else:\n",
    "        print(f\"Failed to download {arxiv_id}.pdf\")\n",
    "\n",
    "# Download PDFs for all Machine Learning papers\n",
    "save_path = \"arxiv_ml_papers\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "for paper in ml_papers:\n",
    "    arxiv_id = paper.get(\"id\")\n",
    "    download_pdf(arxiv_id, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text from PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract text from the pdfs - should this be saved as a json or txt?\n",
    "also run on hpc this is a lot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    # Extract text from each page\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "# pdf_path = \"arxiv_pdf/2101.00001.pdf\"  # Replace with your downloaded PDF\n",
    "# paper_text = extract_text_from_pdf(pdf_path)\n",
    "# print(paper_text[:1000])  # Print the first 1000 characters of the extracted text\n",
    "\n",
    "# extract text from all pdfs and save each in a json file\n",
    "pdf_dir = \"arxiv_ml_papers\"\n",
    "text_dir = \"arxiv_ml_text\"\n",
    "os.makedirs(text_dir, exist_ok=True)\n",
    "\n",
    "for paper in ml_papers:\n",
    "    try:\n",
    "        arxiv_id = paper.get(\"id\")\n",
    "        pdf_path = os.path.join(pdf_dir, f\"{arxiv_id}.pdf\")\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        text_path = os.path.join(text_dir, f\"{arxiv_id}.json\")\n",
    "        with open(text_path, \"w\") as f:\n",
    "            json.dump({\"text\": text}, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {arxiv_id}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
