{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilienilsson/anaconda3/envs/deep/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/emilienilsson/anaconda3/envs/deep/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "# from openai import OpenAI\n",
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from retry import retry\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from graphdatascience import GraphDataScience\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEO4j setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"NEO4J_URI\"] = os.getenv('NEO4J_URI')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('NEO4J_USERNAME')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('NEO4J_PASSWORD')\n",
    "os.environ[\"OPENAI_API_KEY\"] =  os.getenv('OPENAI_API_KEY')\n",
    "NEO4j_URI = os.getenv('NEO4J_URI')\n",
    "NEO4j_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4j_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "\n",
    "graph = Neo4jGraph(url = NEO4j_URI, username=NEO4j_USERNAME, password = NEO4j_PASSWORD, refresh_schema=False)\n",
    "\n",
    "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:AtomicFact) REQUIRE c.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:KeyElement) REQUIRE c.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.graphs.neo4j_graph.Neo4jGraph at 0x177c950d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = os.getenv('NEO4J_URI')\n",
    "user = os.getenv('NEO4J_USERNAME')\n",
    "password = os.getenv('NEO4J_PASSWORD')\n",
    "driver = GraphDatabase.driver(host, auth=(user, password))\n",
    "\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "username = os.getenv('NEO4J_USERNAME')\n",
    "password = os.getenv('NEO4J_PASSWORD')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate embeddings for key elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),\n",
    "    url=NEO4J_URI,\n",
    "    username=username,\n",
    "    password= password,\n",
    "    node_label=\"KeyElement\", #[\"Document\",\"Chunk\", \"AtomicFact\", \"KeyElement\"],\n",
    "    text_node_properties=[\"id\"],#[\"id\", \"text\", \"index\"], #['name', 'description', 'status'],\n",
    "    embedding_node_property=\"embedding\", #'embedding'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph data science set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|██████████| 100.0/100 [00:01<00:00, 85.29%/s] \n"
     ]
    }
   ],
   "source": [
    "# project graph\n",
    "gds = GraphDataScience(\n",
    "    NEO4J_URI,\n",
    "    auth=(username,password)\n",
    ")\n",
    "\n",
    "G, result = gds.graph.project(\n",
    "    \"entities\",                   #  Graph name\n",
    "    \"KeyElement\",                 #  Node projection\n",
    "    \"*\",                          #  Relationship projection\n",
    "    nodeProperties=[\"embedding\"]  #  Configuration parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest neighbors similarity search using the embedding property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbours: 100%|██████████| 100.0/100 [00:04<00:00, 20.75%/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ranIterations                                                            35\n",
       "nodePairsConsidered                                                11996717\n",
       "didConverge                                                            True\n",
       "preProcessingMillis                                                       2\n",
       "computeMillis                                                          5137\n",
       "mutateMillis                                                            100\n",
       "postProcessingMillis                                                      0\n",
       "nodesCompared                                                         13710\n",
       "relationshipsWritten                                                 102038\n",
       "similarityDistribution    {'min': 0.9499969482421875, 'p5': 0.9520568847...\n",
       "configuration             {'mutateProperty': 'score', 'jobId': 'f24cbc9d...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_threshold = 0.95\n",
    "\n",
    "gds.knn.mutate(\n",
    "  G,\n",
    "  nodeProperties=['embedding'],\n",
    "  mutateRelationshipType= 'SIMILAR',\n",
    "  mutateProperty= 'score',\n",
    "  similarityCutoff=similarity_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "writeMillis                                                            125\n",
       "nodePropertiesWritten                                                13710\n",
       "componentCount                                                        1354\n",
       "componentDistribution    {'min': 1, 'p5': 1, 'max': 12067, 'p999': 19, ...\n",
       "postProcessingMillis                                                     7\n",
       "preProcessingMillis                                                      0\n",
       "computeMillis                                                           25\n",
       "configuration            {'writeProperty': 'wcc', 'jobId': 'bced7c1c-09...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.wcc.write(\n",
    "    G,\n",
    "    writeProperty=\"wcc\",\n",
    "    relationshipTypes=[\"SIMILAR\"]\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Duplicate Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'combinedResult': ['Dose effect', 'Noise Effect']},\n",
       " {'combinedResult': ['Larger σ', 'large K', 'large k0', 'large τ']},\n",
       " {'combinedResult': ['Threshold', 'Threshold c', 'Threshold t', 'Thresholds']},\n",
       " {'combinedResult': ['Label', 'PATEL', 'Patel']},\n",
       " {'combinedResult': ['Fan et al. (1993)', 'Fang et al. (1990)']}]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_edit_distance = 3\n",
    "potential_duplicate_candidates = graph.query(\n",
    "    \"\"\"MATCH (e:`KeyElement`)\n",
    "    WHERE size(e.id) > 4 // longer than 4 characters\n",
    "    WITH e.wcc AS community, collect(e) AS nodes, count(*) AS count\n",
    "    WHERE count > 1\n",
    "    UNWIND nodes AS node\n",
    "    // Add text distance\n",
    "    WITH distinct\n",
    "      [n IN nodes WHERE apoc.text.distance(toLower(node.id), toLower(n.id)) < $distance | n.id] AS intermediate_results\n",
    "    WHERE size(intermediate_results) > 1\n",
    "    WITH collect(intermediate_results) AS results\n",
    "    // combine groups together if they share elements\n",
    "    UNWIND range(0, size(results)-1, 1) as index\n",
    "    WITH results, index, results[index] as result\n",
    "    WITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n",
    "            CASE WHEN index <> index2 AND\n",
    "                size(apoc.coll.intersection(acc, results[index2])) > 0\n",
    "                THEN apoc.coll.union(acc, results[index2])\n",
    "                ELSE acc\n",
    "            END\n",
    "    )) as combinedResult\n",
    "    WITH distinct(combinedResult) as combinedResult\n",
    "    // extra filtering\n",
    "    WITH collect(combinedResult) as allCombinedResults\n",
    "    UNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\n",
    "    WITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\n",
    "    WHERE NOT any(x IN range(0,size(allCombinedResults)-1,1)\n",
    "        WHERE x <> combinedResultIndex\n",
    "        AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n",
    "    )\n",
    "    RETURN combinedResult\n",
    "    \"\"\", params={'distance': word_edit_distance})\n",
    "potential_duplicate_candidates[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Duplicate Entities with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a data processing assistant. Your task is to identify duplicate entities in a list and decide which of them should be merged.\n",
    "The entities might be slightly different in format or content, but essentially refer to the same thing. Use your analytical skills to determine duplicates.\n",
    "\n",
    "Here are the rules for identifying duplicates:\n",
    "1. Entities with minor typographical differences should be considered duplicates.\n",
    "2. Refrain from merging entities based on equations, as they may have different meanings.\n",
    "3. Entities that refer to the same real-world object or concept, even if described differently, should be considered duplicates.\n",
    "4. If it refers to different numbers, dates, or products, do not merge results\n",
    "5. Mathematical equations must be identical to be considered duplicates.\n",
    "\"\"\"\n",
    "user_template = \"\"\"\n",
    "Here is the list of entities to process:\n",
    "{entities}\n",
    "\n",
    "Please identify duplicates, merge them, and provide the merged list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilienilsson/anaconda3/envs/deep/lib/python3.11/site-packages/pydantic/json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=PydanticUndefined description='Entities that represent the same object or real-world entity and should be merged' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
      "/Users/emilienilsson/anaconda3/envs/deep/lib/python3.11/site-packages/pydantic/json_schema.py:2191: PydanticJsonSchemaWarning: Default value default=PydanticUndefined description='Lists of entities that represent the same object or real-world entity and should be merged' extra={} is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    }
   ],
   "source": [
    "class DuplicateEntities(BaseModel):\n",
    "    entities: List[str] = Field(\n",
    "        description=\"Entities that represent the same object or real-world entity and should be merged\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Disambiguate(BaseModel):\n",
    "    merge_entities: Optional[List[DuplicateEntities]] = Field(\n",
    "        description=\"Lists of entities that represent the same object or real-world entity and should be merged\"\n",
    "    )\n",
    "\n",
    "extraction_llm = ChatOpenAI(model_name=\"gpt-4o-mini\").with_structured_output(\n",
    "    Disambiguate\n",
    ")\n",
    "\n",
    "extraction_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            user_template,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extraction_chain = extraction_prompt | extraction_llm\n",
    "\n",
    "@retry(tries=3, delay=2)\n",
    "def entity_resolution(entities: List[str]) -> Optional[List[str]]:\n",
    "    return [\n",
    "        el.entities\n",
    "        for el in extraction_chain.invoke({\"entities\": entities}).merge_entities\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   4%|▎         | 58/1656 [00:24<13:41,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing future: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   4%|▍         | 63/1656 [00:26<11:53,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing future: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  16%|█▌        | 263/1656 [01:17<06:39,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing future: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  22%|██▏       | 371/1656 [01:43<05:22,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing future: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  29%|██▉       | 488/1656 [02:10<04:34,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing future: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  40%|████      | 670/1656 [02:59<11:24,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing future: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  44%|████▍     | 735/1656 [03:19<04:55,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing future: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  52%|█████▏    | 854/1656 [03:53<10:11,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing future: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  52%|█████▏    | 859/1656 [03:55<04:31,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing future: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  63%|██████▎   | 1049/1656 [04:40<03:38,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing future: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  88%|████████▊ | 1458/1656 [06:17<00:41,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing future: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  93%|█████████▎| 1540/1656 [06:37<00:36,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing future: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 1656/1656 [07:15<00:00,  3.80it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_WORKERS = 4\n",
    "\n",
    "merged_entities = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submitting all tasks and creating a list of future objects\n",
    "    futures = [\n",
    "        executor.submit(entity_resolution, el['combinedResult'])\n",
    "        for el in potential_duplicate_candidates\n",
    "    ]\n",
    "\n",
    "    for future in tqdm(\n",
    "        as_completed(futures), total=len(futures), desc=\"Processing documents\"\n",
    "    ):\n",
    "        try:\n",
    "            to_merge = future.result()\n",
    "            # Ensure to_merge is not None and is iterable\n",
    "            if to_merge:  # this checks if it's not None or an empty value\n",
    "                merged_entities.extend(to_merge)\n",
    "        except Exception as e:\n",
    "            # Handle any exception raised during the processing of a future\n",
    "            print(f\"Error processing future: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge entities in the graph using query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break down merged entities into chunks of 100\n",
    "merged_entities_chunks = [merged_entities[i:i + 100] for i in range(0, len(merged_entities), 100)]\n",
    "\n",
    "\n",
    "for i, chunk in enumerate(merged_entities_chunks):\n",
    "  try:\n",
    "    graph.query(\"\"\"\n",
    "    UNWIND $data AS candidates\n",
    "    CALL {\n",
    "      WITH candidates\n",
    "      MATCH (e:KeyElement) WHERE e.id IN candidates\n",
    "      RETURN collect(e) AS nodes\n",
    "    }\n",
    "    CALL apoc.refactor.mergeNodes(nodes, {properties: {\n",
    "        description:'combine',\n",
    "        `.*`: 'discard'\n",
    "    }})\n",
    "    YIELD node\n",
    "    RETURN count(*)\n",
    "    \"\"\", params={\"data\": chunk})\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    print(\"failed to merge chunk\", i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count(e)': 18401}]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\"\"\"MATCH (e) RETURN count(e)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count(e)': 10795}]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\"\"\"MATCH (e:KeyElement) RETURN count(e)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph({'graphName': 'entities', 'nodeCount': 11484, 'relationshipCount': 85313, 'database': 'neo4j', 'configuration': {'relationshipProjection': {'__ALL__': {'aggregation': 'DEFAULT', 'orientation': 'NATURAL', 'indexInverse': False, 'properties': {}, 'type': '*'}}, 'readConcurrency': 4, 'relationshipProperties': {}, 'nodeProperties': {}, 'jobId': '359d3bf1-53f2-4d92-982a-6697ff240bcd', 'nodeProjection': {'KeyElement': {'label': 'KeyElement', 'properties': {'embedding': {'property': 'embedding', 'defaultValue': None}}}}, 'logProgress': True, 'validateRelationships': False, 'sudo': False}, 'schema': {'graphProperties': {}, 'nodes': {'KeyElement': {'embedding': 'List of Float (DefaultValue(null), PERSISTENT)'}}, 'relationships': {'__ALL__': {}, 'SIMILAR': {'score': 'Float (DefaultValue(NaN), PERSISTENT, Aggregation.NONE)'}}}, 'memoryUsage': '95 MiB'})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.graph.export(G, dbName = \"thisisauniquename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.graph.get('entities')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
